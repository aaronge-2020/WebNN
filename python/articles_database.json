[
    {
        "title": "Deep Learning for Computational Chemistry",
        "authors": [
            "Garrett B. Goh",
            "Nathan O. Hodas",
            "Abhinav Vishnu"
        ],
        "abstract": "The rise and fall of artificial neural networks is well documented in the\nscientific literature of both computer science and computational chemistry. Yet\nalmost two decades later, we are now seeing a resurgence of interest in deep\nlearning, a machine learning algorithm based on multilayer neural networks.\nWithin the last few years, we have seen the transformative impact of deep\nlearning in many domains, particularly in speech recognition and computer\nvision, to the extent that the majority of expert practitioners in those field\nare now regularly eschewing prior established models in favor of deep learning\nmodels. In this review, we provide an introductory overview into the theory of\ndeep neural networks and their unique properties that distinguish them from\ntraditional machine learning algorithms used in cheminformatics. By providing\nan overview of the variety of emerging applications of deep neural networks, we\nhighlight its ubiquity and broad applicability to a wide range of challenges in\nthe field, including QSAR, virtual screening, protein structure prediction,\nquantum chemistry, materials design and property prediction. In reviewing the\nperformance of deep neural networks, we observed a consistent outperformance\nagainst non-neural networks state-of-the-art models across disparate research\ntopics, and deep neural network based models often exceeded the \"glass ceiling\"\nexpectations of their respective tasks. Coupled with the maturity of\nGPU-accelerated computing for training deep neural networks and the exponential\ngrowth of chemical data on which to train these networks on, we anticipate that\ndeep learning algorithms will be a valuable tool for computational chemistry.",
        "categories": [
            "stat.ML",
            "cs.AI",
            "cs.CE",
            "cs.LG",
            "physics.chem-ph"
        ],
        "link": "http://arxiv.org/pdf/1701.04503v1",
        "date": "2017-01-17 01:15:14+00:00"
    },
    {
        "title": "Introduction to Neural Network Verification",
        "authors": [
            "Aws Albarghouthi"
        ],
        "abstract": "Deep learning has transformed the way we think of software and what it can\ndo. But deep neural networks are fragile and their behaviors are often\nsurprising. In many settings, we need to provide formal guarantees on the\nsafety, security, correctness, or robustness of neural networks. This book\ncovers foundational ideas from formal verification and their adaptation to\nreasoning about neural networks and deep learning.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.PL"
        ],
        "link": "http://arxiv.org/pdf/2109.10317v2",
        "date": "2021-09-21 16:57:01+00:00"
    },
    {
        "title": "Deep Learning in Bioinformatics",
        "authors": [
            "Seonwoo Min",
            "Byunghan Lee",
            "Sungroh Yoon"
        ],
        "abstract": "In the era of big data, transformation of biomedical big data into valuable\nknowledge has been one of the most important challenges in bioinformatics. Deep\nlearning has advanced rapidly since the early 2000s and now demonstrates\nstate-of-the-art performance in various fields. Accordingly, application of\ndeep learning in bioinformatics to gain insight from data has been emphasized\nin both academia and industry. Here, we review deep learning in bioinformatics,\npresenting examples of current research. To provide a useful and comprehensive\nperspective, we categorize research both by the bioinformatics domain (i.e.,\nomics, biomedical imaging, biomedical signal processing) and deep learning\narchitecture (i.e., deep neural networks, convolutional neural networks,\nrecurrent neural networks, emergent architectures) and present brief\ndescriptions of each study. Additionally, we discuss theoretical and practical\nissues of deep learning in bioinformatics and suggest future research\ndirections. We believe that this review will provide valuable insights and\nserve as a starting point for researchers to apply deep learning approaches in\ntheir bioinformatics studies.",
        "categories": [
            "cs.LG",
            "q-bio.GN"
        ],
        "link": "http://arxiv.org/pdf/1603.06430v5",
        "date": "2016-03-21 13:55:02+00:00"
    },
    {
        "title": "Inference Compilation and Universal Probabilistic Programming",
        "authors": [
            "Tuan Anh Le",
            "Atilim Gunes Baydin",
            "Frank Wood"
        ],
        "abstract": "We introduce a method for using deep neural networks to amortize the cost of\ninference in models from the family induced by universal probabilistic\nprogramming languages, establishing a framework that combines the strengths of\nprobabilistic programming and deep learning methods. We call what we do\n\"compilation of inference\" because our method transforms a denotational\nspecification of an inference problem in the form of a probabilistic program\nwritten in a universal programming language into a trained neural network\ndenoted in a neural network specification language. When at test time this\nneural network is fed observational data and executed, it performs approximate\ninference in the original model specified by the probabilistic program. Our\ntraining objective and learning procedure are designed to allow the trained\nneural network to be used as a proposal distribution in a sequential importance\nsampling inference engine. We illustrate our method on mixture models and\nCaptcha solving and show significant speedups in the efficiency of inference.",
        "categories": [
            "cs.AI",
            "cs.LG",
            "stat.ML",
            "68T37, 68T05",
            "G.3; I.2.6"
        ],
        "link": "http://arxiv.org/pdf/1610.09900v2",
        "date": "2016-10-31 12:53:20+00:00"
    },
    {
        "title": "Chemception: A Deep Neural Network with Minimal Chemistry Knowledge Matches the Performance of Expert-developed QSAR/QSPR Models",
        "authors": [
            "Garrett B. Goh",
            "Charles Siegel",
            "Abhinav Vishnu",
            "Nathan O. Hodas",
            "Nathan Baker"
        ],
        "abstract": "In the last few years, we have seen the transformative impact of deep\nlearning in many applications, particularly in speech recognition and computer\nvision. Inspired by Google's Inception-ResNet deep convolutional neural network\n(CNN) for image classification, we have developed \"Chemception\", a deep CNN for\nthe prediction of chemical properties, using just the images of 2D drawings of\nmolecules. We develop Chemception without providing any additional explicit\nchemistry knowledge, such as basic concepts like periodicity, or advanced\nfeatures like molecular descriptors and fingerprints. We then show how\nChemception can serve as a general-purpose neural network architecture for\npredicting toxicity, activity, and solvation properties when trained on a\nmodest database of 600 to 40,000 compounds. When compared to multi-layer\nperceptron (MLP) deep neural networks trained with ECFP fingerprints,\nChemception slightly outperforms in activity and solvation prediction and\nslightly underperforms in toxicity prediction. Having matched the performance\nof expert-developed QSAR/QSPR deep learning models, our work demonstrates the\nplausibility of using deep neural networks to assist in computational chemistry\nresearch, where the feature engineering process is performed primarily by a\ndeep learning algorithm.",
        "categories": [
            "stat.ML",
            "cs.AI",
            "cs.CE",
            "cs.CV",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1706.06689v1",
        "date": "2017-06-20 22:25:57+00:00"
    },
    {
        "title": "Deep Canonically Correlated LSTMs",
        "authors": [
            "Neil Mallinar",
            "Corbin Rosset"
        ],
        "abstract": "We examine Deep Canonically Correlated LSTMs as a way to learn nonlinear\ntransformations of variable length sequences and embed them into a correlated,\nfixed dimensional space. We use LSTMs to transform multi-view time-series data\nnon-linearly while learning temporal relationships within the data. We then\nperform correlation analysis on the outputs of these neural networks to find a\ncorrelated subspace through which we get our final representation via\nprojection. This work follows from previous work done on Deep Canonical\nCorrelation (DCCA), in which deep feed-forward neural networks were used to\nlearn nonlinear transformations of data while maximizing correlation.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1801.05407v1",
        "date": "2018-01-16 18:44:31+00:00"
    },
    {
        "title": "OmicsMapNet: Transforming omics data to take advantage of Deep Convolutional Neural Network for discovery",
        "authors": [
            "Shiyong Ma",
            "Zhen Zhang"
        ],
        "abstract": "We developed OmicsMapNet approach to take advantage of existing deep leaning\nframeworks to analyze high-dimensional omics data as 2-dimensional images. The\nomics data of individual samples were first rearranged into 2D images in which\nmolecular features related in functions, ontologies, or other relationships\nwere organized in spatially adjacent and patterned locations. Deep learning\nneural networks were trained to classify the images. Molecular features\ninformative of classes of different phenotypes were subsequently identified. As\nan example, we used the KEGG BRITE database to rearrange RNA-Seq expression\ndata of TCGA diffuse glioma samples as treemaps to capture the functional\nhierarchical structure of genes in 2D images. Deep Convolutional Neural\nNetworks (CNN) were derived using tools from TensorFlow to learn the grade of\nTCGA LGG and GBM samples with relatively high accuracy. The most contributory\nfeatures in the trained CNN were confirmed in pathway analysis for their\nplausible functional involvement.",
        "categories": [
            "stat.ML",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1804.05283v2",
        "date": "2018-04-14 22:22:21+00:00"
    },
    {
        "title": "Neural Algorithmic Reasoning",
        "authors": [
            "Petar Veli\u010dkovi\u0107",
            "Charles Blundell"
        ],
        "abstract": "Algorithms have been fundamental to recent global technological advances and,\nin particular, they have been the cornerstone of technical advances in one\nfield rapidly being applied to another. We argue that algorithms possess\nfundamentally different qualities to deep learning methods, and this strongly\nsuggests that, were deep learning methods better able to mimic algorithms,\ngeneralisation of the sort seen with algorithms would become possible with deep\nlearning -- something far out of the reach of current machine learning methods.\nFurthermore, by representing elements in a continuous space of learnt\nalgorithms, neural networks are able to adapt known algorithms more closely to\nreal-world problems, potentially finding more efficient and pragmatic solutions\nthan those proposed by human computer scientists.\n  Here we present neural algorithmic reasoning -- the art of building neural\nnetworks that are able to execute algorithmic computation -- and provide our\nopinion on its transformative potential for running classical algorithms on\ninputs previously considered inaccessible to them.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DS",
            "math.OC",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2105.02761v1",
        "date": "2021-05-06 15:33:57+00:00"
    },
    {
        "title": "Automatic Gradient Descent: Deep Learning without Hyperparameters",
        "authors": [
            "Jeremy Bernstein",
            "Chris Mingard",
            "Kevin Huang",
            "Navid Azizan",
            "Yisong Yue"
        ],
        "abstract": "The architecture of a deep neural network is defined explicitly in terms of\nthe number of layers, the width of each layer and the general network topology.\nExisting optimisation frameworks neglect this information in favour of implicit\narchitectural information (e.g. second-order methods) or architecture-agnostic\ndistance functions (e.g. mirror descent). Meanwhile, the most popular optimiser\nin practice, Adam, is based on heuristics. This paper builds a new framework\nfor deriving optimisation algorithms that explicitly leverage neural\narchitecture. The theory extends mirror descent to non-convex composite\nobjective functions: the idea is to transform a Bregman divergence to account\nfor the non-linear structure of neural architecture. Working through the\ndetails for deep fully-connected networks yields automatic gradient descent: a\nfirst-order optimiser without any hyperparameters. Automatic gradient descent\ntrains both fully-connected and convolutional networks out-of-the-box and at\nImageNet scale. A PyTorch implementation is available at\nhttps://github.com/jxbz/agd and also in Appendix B. Overall, the paper supplies\na rigorous theoretical foundation for a next-generation of\narchitecture-dependent optimisers that work automatically and without\nhyperparameters.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.NA",
            "cs.NE",
            "math.NA",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2304.05187v1",
        "date": "2023-04-11 12:45:52+00:00"
    },
    {
        "title": "Learning Neural Networks on SVD Boosted Latent Spaces for Semantic Classification",
        "authors": [
            "Sahil Sidheekh"
        ],
        "abstract": "The availability of large amounts of data and compelling computation power\nhave made deep learning models much popular for text classification and\nsentiment analysis. Deep neural networks have achieved competitive performance\non the above tasks when trained on naive text representations such as word\ncount, term frequency, and binary matrix embeddings. However, many of the above\nrepresentations result in the input space having a dimension of the order of\nthe vocabulary size, which is enormous. This leads to a blow-up in the number\nof parameters to be learned, and the computational cost becomes infeasible when\nscaling to domains that require retaining a colossal vocabulary. This work\nproposes using singular value decomposition to transform the high dimensional\ninput space to a lower-dimensional latent space. We show that neural networks\ntrained on this lower-dimensional space are not only able to retain performance\nwhile savoring significant reduction in the computational complexity but, in\nmany situations, also outperforms the classical neural networks trained on the\nnative input space.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2101.00563v1",
        "date": "2021-01-03 05:30:37+00:00"
    },
    {
        "title": "Are Efficient Deep Representations Learnable?",
        "authors": [
            "Maxwell Nye",
            "Andrew Saxe"
        ],
        "abstract": "Many theories of deep learning have shown that a deep network can require\ndramatically fewer resources to represent a given function compared to a\nshallow network. But a question remains: can these efficient representations be\nlearned using current deep learning techniques? In this work, we test whether\nstandard deep learning methods can in fact find the efficient representations\nposited by several theories of deep representation. Specifically, we train deep\nneural networks to learn two simple functions with known efficient solutions:\nthe parity function and the fast Fourier transform. We find that using\ngradient-based optimization, a deep network does not learn the parity function,\nunless initialized very close to a hand-coded exact solution. We also find that\na deep linear neural network does not learn the fast Fourier transform, even in\nthe best-case scenario of infinite training data, unless the weights are\ninitialized very close to the exact hand-coded solution. Our results suggest\nthat not every element of the class of compositional functions can be learned\nefficiently by a deep network, and further restrictions are necessary to\nunderstand what functions are both efficiently representable and learnable.",
        "categories": [
            "cs.LG",
            "cs.NE",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1807.06399v1",
        "date": "2018-07-17 13:08:21+00:00"
    },
    {
        "title": "Expressive Power and Loss Surfaces of Deep Learning Models",
        "authors": [
            "Simant Dube"
        ],
        "abstract": "The goals of this paper are two-fold. The first goal is to serve as an\nexpository tutorial on the working of deep learning models which emphasizes\ngeometrical intuition about the reasons for success of deep learning. The\nsecond goal is to complement the current results on the expressive power of\ndeep learning models and their loss surfaces with novel insights and results.\nIn particular, we describe how deep neural networks carve out manifolds\nespecially when the multiplication neurons are introduced. Multiplication is\nused in dot products and the attention mechanism and it is employed in capsule\nnetworks and self-attention based transformers. We also describe how random\npolynomial, random matrix, spin glass and computational complexity perspectives\non the loss surfaces are interconnected.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2108.03579v3",
        "date": "2021-08-08 06:28:09+00:00"
    },
    {
        "title": "Bucketed PCA Neural Networks with Neurons Mirroring Signals",
        "authors": [
            "Jackie Shen"
        ],
        "abstract": "The bucketed PCA neural network (PCA-NN) with transforms is developed here in\nan effort to benchmark deep neural networks (DNN's), for problems on supervised\nclassification. Most classical PCA models apply PCA to the entire training data\nset to establish a reductive representation and then employ non-network tools\nsuch as high-order polynomial classifiers. In contrast, the bucketed PCA-NN\napplies PCA to individual buckets which are constructed in two consecutive\nphases, as well as retains a genuine architecture of a neural network. This\nfacilitates a fair apple-to-apple comparison to DNN's, esp. to reveal that a\nmajor chunk of accuracy achieved by many impressive DNN's could possibly be\nexplained by the bucketed PCA-NN (e.g., 96% out of 98% for the MNIST data set\nas an example). Compared with most DNN's, the three building blocks of the\nbucketed PCA-NN are easier to comprehend conceptually - PCA, transforms, and\nbucketing for error correction. Furthermore, unlike the somewhat quasi-random\nneurons ubiquitously observed in DNN's, the PCA neurons resemble or mirror the\ninput signals and are more straightforward to decipher as a result.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "math.OC",
            "stat.ML",
            "I.2.10; I.2.6"
        ],
        "link": "http://arxiv.org/pdf/2108.00605v1",
        "date": "2021-08-02 02:43:59+00:00"
    },
    {
        "title": "Gaining Insight into SARS-CoV-2 Infection and COVID-19 Severity Using Self-supervised Edge Features and Graph Neural Networks",
        "authors": [
            "Arijit Sehanobish",
            "Neal G. Ravindra",
            "David van Dijk"
        ],
        "abstract": "A molecular and cellular understanding of how SARS-CoV-2 variably infects and\ncauses severe COVID-19 remains a bottleneck in developing interventions to end\nthe pandemic. We sought to use deep learning to study the biology of SARS-CoV-2\ninfection and COVID-19 severity by identifying transcriptomic patterns and cell\ntypes associated with SARS-CoV-2 infection and COVID-19 severity. To do this,\nwe developed a new approach to generating self-supervised edge features. We\npropose a model that builds on Graph Attention Networks (GAT), creates edge\nfeatures using self-supervised learning, and ingests these edge features via a\nSet Transformer. This model achieves significant improvements in predicting the\ndisease state of individual cells, given their transcriptome. We apply our\nmodel to single-cell RNA sequencing datasets of SARS-CoV-2 infected lung\norganoids and bronchoalveolar lavage fluid samples of patients with COVID-19,\nachieving state-of-the-art performance on both datasets with our model. We then\nborrow from the field of explainable AI (XAI) to identify the features (genes)\nand cell types that discriminate bystander vs. infected cells across time and\nmoderate vs. severe COVID-19 disease. To the best of our knowledge, this\nrepresents the first application of deep learning to identifying the molecular\nand cellular determinants of SARS-CoV-2 infection and COVID-19 severity using\nsingle-cell omics data.",
        "categories": [
            "cs.LG",
            "q-bio.GN",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2006.12971v2",
        "date": "2020-06-23 13:22:16+00:00"
    },
    {
        "title": "Learned Hardware/Software Co-Design of Neural Accelerators",
        "authors": [
            "Zhan Shi",
            "Chirag Sakhuja",
            "Milad Hashemi",
            "Kevin Swersky",
            "Calvin Lin"
        ],
        "abstract": "The use of deep learning has grown at an exponential rate, giving rise to\nnumerous specialized hardware and software systems for deep learning. Because\nthe design space of deep learning software stacks and hardware accelerators is\ndiverse and vast, prior work considers software optimizations separately from\nhardware architectures, effectively reducing the search space. Unfortunately,\nthis bifurcated approach means that many profitable design points are never\nexplored. This paper instead casts the problem as hardware/software co-design,\nwith the goal of automatically identifying desirable points in the joint design\nspace. The key to our solution is a new constrained Bayesian optimization\nframework that avoids invalid solutions by exploiting the highly constrained\nfeatures of this design space, which are semi-continuous/semi-discrete. We\nevaluate our optimization framework by applying it to a variety of neural\nmodels, improving the energy-delay product by 18% (ResNet) and 40% (DQN) over\nhand-tuned state-of-the-art systems, as well as demonstrating strong results on\nother neural network architectures, such as MLPs and Transformers.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.AR",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2010.02075v1",
        "date": "2020-10-05 15:12:52+00:00"
    },
    {
        "title": "Bayesian Hypernetworks",
        "authors": [
            "David Krueger",
            "Chin-Wei Huang",
            "Riashat Islam",
            "Ryan Turner",
            "Alexandre Lacoste",
            "Aaron Courville"
        ],
        "abstract": "We study Bayesian hypernetworks: a framework for approximate Bayesian\ninference in neural networks. A Bayesian hypernetwork $\\h$ is a neural network\nwhich learns to transform a simple noise distribution, $p(\\vec\\epsilon) =\n\\N(\\vec 0,\\mat I)$, to a distribution $q(\\pp) := q(h(\\vec\\epsilon))$ over the\nparameters $\\pp$ of another neural network (the \"primary network\")\\@. We train\n$q$ with variational inference, using an invertible $\\h$ to enable efficient\nestimation of the variational lower bound on the posterior $p(\\pp | \\D)$ via\nsampling. In contrast to most methods for Bayesian deep learning, Bayesian\nhypernets can represent a complex multimodal approximate posterior with\ncorrelations between parameters, while enabling cheap iid sampling of~$q(\\pp)$.\nIn practice, Bayesian hypernets can provide a better defense against\nadversarial examples than dropout, and also exhibit competitive performance on\na suite of tasks which evaluate model uncertainty, including regularization,\nactive learning, and anomaly detection.",
        "categories": [
            "stat.ML",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1710.04759v2",
        "date": "2017-10-13 00:27:57+00:00"
    },
    {
        "title": "Transformer-based World Models Are Happy With 100k Interactions",
        "authors": [
            "Jan Robine",
            "Marc H\u00f6ftmann",
            "Tobias Uelwer",
            "Stefan Harmeling"
        ],
        "abstract": "Deep neural networks have been successful in many reinforcement learning\nsettings. However, compared to human learners they are overly data hungry. To\nbuild a sample-efficient world model, we apply a transformer to real-world\nepisodes in an autoregressive manner: not only the compact latent states and\nthe taken actions but also the experienced or predicted rewards are fed into\nthe transformer, so that it can attend flexibly to all three modalities at\ndifferent time steps. The transformer allows our world model to access previous\nstates directly, instead of viewing them through a compressed recurrent state.\nBy utilizing the Transformer-XL architecture, it is able to learn long-term\ndependencies while staying computationally efficient. Our transformer-based\nworld model (TWM) generates meaningful, new experience, which is used to train\na policy that outperforms previous model-free and model-based reinforcement\nlearning algorithms on the Atari 100k benchmark.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2303.07109v1",
        "date": "2023-03-13 13:43:59+00:00"
    },
    {
        "title": "A Law of Data Separation in Deep Learning",
        "authors": [
            "Hangfeng He",
            "Weijie J. Su"
        ],
        "abstract": "Multilayer neural networks have achieved superhuman performance in many\nartificial intelligence applications. However, their black-box nature obscures\nthe underlying mechanism for transforming input data into labels throughout all\nlayers, thus hindering architecture design for new tasks and interpretation for\nhigh-stakes decision makings. We addressed this problem by introducing a\nprecise law that governs how real-world deep neural networks separate data\naccording to their class membership from the bottom layers to the top layers in\nclassification problems. This law shows that each layer roughly improves a\ncertain measure of data separation by an \\textit{equal} multiplicative factor.\nThis law manifests in modern architectures such as AlexNet, VGGNet, and ResNet\nin the late phase of training. This law together with the perspective of data\nseparation offers practical guidelines for designing network architectures,\nimproving model robustness and out-of-sample performance during training, as\nwell as interpreting deep learning predictions.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "cs.IT",
            "math.IT",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2210.17020v1",
        "date": "2022-10-31 02:25:38+00:00"
    },
    {
        "title": "Analyzing biological and artificial neural networks: challenges with opportunities for synergy?",
        "authors": [
            "David G. T. Barrett",
            "Ari S. Morcos",
            "Jakob H. Macke"
        ],
        "abstract": "Deep neural networks (DNNs) transform stimuli across multiple processing\nstages to produce representations that can be used to solve complex tasks, such\nas object recognition in images. However, a full understanding of how they\nachieve this remains elusive. The complexity of biological neural networks\nsubstantially exceeds the complexity of DNNs, making it even more challenging\nto understand the representations that they learn. Thus, both machine learning\nand computational neuroscience are faced with a shared challenge: how can we\nanalyze their representations in order to understand how they solve complex\ntasks?\n  We review how data-analysis concepts and techniques developed by\ncomputational neuroscientists can be useful for analyzing representations in\nDNNs, and in turn, how recently developed techniques for analysis of DNNs can\nbe useful for understanding representations in biological neural networks. We\nexplore opportunities for synergy between the two fields, such as the use of\nDNNs as in-silico model systems for neuroscience, and how this synergy can lead\nto new hypotheses about the operating principles of biological neural networks.",
        "categories": [
            "q-bio.NC",
            "cs.AI",
            "cs.CV",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1810.13373v1",
        "date": "2018-10-31 16:09:44+00:00"
    },
    {
        "title": "Deep Transformers without Shortcuts: Modifying Self-attention for Faithful Signal Propagation",
        "authors": [
            "Bobby He",
            "James Martens",
            "Guodong Zhang",
            "Aleksandar Botev",
            "Andrew Brock",
            "Samuel L Smith",
            "Yee Whye Teh"
        ],
        "abstract": "Skip connections and normalisation layers form two standard architectural\ncomponents that are ubiquitous for the training of Deep Neural Networks (DNNs),\nbut whose precise roles are poorly understood. Recent approaches such as Deep\nKernel Shaping have made progress towards reducing our reliance on them, using\ninsights from wide NN kernel theory to improve signal propagation in vanilla\nDNNs (which we define as networks without skips or normalisation). However,\nthese approaches are incompatible with the self-attention layers present in\ntransformers, whose kernels are intrinsically more complicated to analyse and\ncontrol. And so the question remains: is it possible to train deep vanilla\ntransformers? We answer this question in the affirmative by designing several\napproaches that use combinations of parameter initialisations, bias matrices\nand location-dependent rescaling to achieve faithful signal propagation in\nvanilla transformers. Our methods address various intricacies specific to\nsignal propagation in transformers, including the interaction with positional\nencoding and causal masking. In experiments on WikiText-103 and C4, our\napproaches enable deep transformers without normalisation to train at speeds\nmatching their standard counterparts, and deep vanilla transformers to reach\nthe same performance as standard ones after about 5 times more iterations.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2302.10322v1",
        "date": "2023-02-20 21:26:25+00:00"
    },
    {
        "title": "Deep Learning Meets Sparse Regularization: A Signal Processing Perspective",
        "authors": [
            "Rahul Parhi",
            "Robert D. Nowak"
        ],
        "abstract": "Deep learning has been wildly successful in practice and most\nstate-of-the-art machine learning methods are based on neural networks.\nLacking, however, is a rigorous mathematical theory that adequately explains\nthe amazing performance of deep neural networks. In this article, we present a\nrelatively new mathematical framework that provides the beginning of a deeper\nunderstanding of deep learning. This framework precisely characterizes the\nfunctional properties of neural networks that are trained to fit to data. The\nkey mathematical tools which support this framework include transform-domain\nsparse regularization, the Radon transform of computed tomography, and\napproximation theory, which are all techniques deeply rooted in signal\nprocessing. This framework explains the effect of weight decay regularization\nin neural network training, the use of skip connections and low-rank weight\nmatrices in network architectures, the role of sparsity in neural networks, and\nexplains why neural networks can perform well in high-dimensional problems.",
        "categories": [
            "stat.ML",
            "cs.LG",
            "eess.SP"
        ],
        "link": "http://arxiv.org/pdf/2301.09554v2",
        "date": "2023-01-23 17:16:21+00:00"
    },
    {
        "title": "Training BatchNorm and Only BatchNorm: On the Expressive Power of Random Features in CNNs",
        "authors": [
            "Jonathan Frankle",
            "David J. Schwab",
            "Ari S. Morcos"
        ],
        "abstract": "A wide variety of deep learning techniques from style transfer to multitask\nlearning rely on training affine transformations of features. Most prominent\namong these is the popular feature normalization technique BatchNorm, which\nnormalizes activations and then subsequently applies a learned affine\ntransform. In this paper, we aim to understand the role and expressive power of\naffine parameters used to transform features in this way. To isolate the\ncontribution of these parameters from that of the learned features they\ntransform, we investigate the performance achieved when training only these\nparameters in BatchNorm and freezing all weights at their random\ninitializations. Doing so leads to surprisingly high performance considering\nthe significant limitations that this style of training imposes. For example,\nsufficiently deep ResNets reach 82% (CIFAR-10) and 32% (ImageNet, top-5)\naccuracy in this configuration, far higher than when training an equivalent\nnumber of randomly chosen parameters elsewhere in the network. BatchNorm\nachieves this performance in part by naturally learning to disable around a\nthird of the random features. Not only do these results highlight the\nexpressive power of affine parameters in deep learning, but - in a broader\nsense - they characterize the expressive power of neural networks constructed\nsimply by shifting and rescaling random features.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.NE",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2003.00152v3",
        "date": "2020-02-29 01:57:37+00:00"
    },
    {
        "title": "Sequential Coordination of Deep Models for Learning Visual Arithmetic",
        "authors": [
            "Eric Crawford",
            "Guillaume Rabusseau",
            "Joelle Pineau"
        ],
        "abstract": "Achieving machine intelligence requires a smooth integration of perception\nand reasoning, yet models developed to date tend to specialize in one or the\nother; sophisticated manipulation of symbols acquired from rich perceptual\nspaces has so far proved elusive. Consider a visual arithmetic task, where the\ngoal is to carry out simple arithmetical algorithms on digits presented under\nnatural conditions (e.g. hand-written, placed randomly). We propose a\ntwo-tiered architecture for tackling this problem. The lower tier consists of a\nheterogeneous collection of information processing modules, which can include\npre-trained deep neural networks for locating and extracting characters from\nthe image, as well as modules performing symbolic transformations on the\nrepresentations extracted by perception. The higher tier consists of a\ncontroller, trained using reinforcement learning, which coordinates the modules\nin order to solve the high-level task. For instance, the controller may learn\nin what contexts to execute the perceptual networks and what symbolic\ntransformations to apply to their outputs. The resulting model is able to solve\na variety of tasks in the visual arithmetic domain, and has several advantages\nover standard, architecturally homogeneous feedforward networks including\nimproved sample efficiency.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1809.04988v1",
        "date": "2018-09-13 14:27:25+00:00"
    },
    {
        "title": "Group Equivariant Deep Reinforcement Learning",
        "authors": [
            "Arnab Kumar Mondal",
            "Pratheeksha Nair",
            "Kaleem Siddiqi"
        ],
        "abstract": "In Reinforcement Learning (RL), Convolutional Neural Networks(CNNs) have been\nsuccessfully applied as function approximators in Deep Q-Learning algorithms,\nwhich seek to learn action-value functions and policies in various\nenvironments. However, to date, there has been little work on the learning of\nsymmetry-transformation equivariant representations of the input environment\nstate. In this paper, we propose the use of Equivariant CNNs to train RL agents\nand study their inductive bias for transformation equivariant Q-value\napproximation. We demonstrate that equivariant architectures can dramatically\nenhance the performance and sample efficiency of RL agents in a highly\nsymmetric environment while requiring fewer parameters. Additionally, we show\nthat they are robust to changes in the environment caused by affine\ntransformations.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2007.03437v1",
        "date": "2020-07-01 02:38:48+00:00"
    },
    {
        "title": "Controlling Level of Unconsciousness by Titrating Propofol with Deep Reinforcement Learning",
        "authors": [
            "Gabe Schamberg",
            "Marcus Badgeley",
            "Emery N. Brown"
        ],
        "abstract": "Reinforcement Learning (RL) can be used to fit a mapping from patient state\nto a medication regimen. Prior studies have used deterministic and value-based\ntabular learning to learn a propofol dose from an observed anesthetic state.\nDeep RL replaces the table with a deep neural network and has been used to\nlearn medication regimens from registry databases. Here we perform the first\napplication of deep RL to closed-loop control of anesthetic dosing in a\nsimulated environment. We use the cross-entropy method to train a deep neural\nnetwork to map an observed anesthetic state to a probability of infusing a\nfixed propofol dosage. During testing, we implement a deterministic policy that\ntransforms the probability of infusion to a continuous infusion rate. The model\nis trained and tested on simulated pharmacokinetic/pharmacodynamic models with\nrandomized parameters to ensure robustness to patient variability. The deep RL\nagent significantly outperformed a proportional-integral-derivative controller\n(median absolute performance error 1.7% +/- 0.6 and 3.4% +/- 1.2). Modeling\ncontinuous input variables instead of a table affords more robust pattern\nrecognition and utilizes our prior domain knowledge. Deep RL learned a smooth\npolicy with a natural interpretation to data scientists and anesthesia care\nproviders alike.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2008.12333v2",
        "date": "2020-08-27 18:47:08+00:00"
    },
    {
        "title": "Invariant-equivariant representation learning for multi-class data",
        "authors": [
            "Ilya Feige"
        ],
        "abstract": "Representations learnt through deep neural networks tend to be highly\ninformative, but opaque in terms of what information they learn to encode. We\nintroduce an approach to probabilistic modelling that learns to represent data\nwith two separate deep representations: an invariant representation that\nencodes the information of the class from which the data belongs, and an\nequivariant representation that encodes the symmetry transformation defining\nthe particular data point within the class manifold (equivariant in the sense\nthat the representation varies naturally with symmetry transformations). This\napproach is based primarily on the strategic routing of data through the two\nlatent variables, and thus is conceptually transparent, easy to implement, and\nin-principle generally applicable to any data comprised of discrete classes of\ncontinuous distributions (e.g. objects in images, topics in language,\nindividuals in behavioural data). We demonstrate qualitatively compelling\nrepresentation learning and competitive quantitative performance, in both\nsupervised and semi-supervised settings, versus comparable modelling approaches\nin the literature with little fine tuning.",
        "categories": [
            "stat.ML",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1902.03251v2",
        "date": "2019-02-08 19:01:13+00:00"
    },
    {
        "title": "SWNet: Small-World Neural Networks and Rapid Convergence",
        "authors": [
            "Mojan Javaheripi",
            "Bita Darvish Rouhani",
            "Farinaz Koushanfar"
        ],
        "abstract": "Training large and highly accurate deep learning (DL) models is\ncomputationally costly. This cost is in great part due to the excessive number\nof trained parameters, which are well-known to be redundant and compressible\nfor the execution phase. This paper proposes a novel transformation which\nchanges the topology of the DL architecture such that it reaches an optimal\ncross-layer connectivity. This transformation leverages our important\nobservation that for a set level of accuracy, convergence is fastest when\nnetwork topology reaches the boundary of a Small-World Network. Small-world\ngraphs are known to possess a specific connectivity structure that enables\nenhanced signal propagation among nodes. Our small-world models, called SWNets,\nprovide several intriguing benefits: they facilitate data (gradient) flow\nwithin the network, enable feature-map reuse by adding long-range connections\nand accommodate various network architectures/datasets. Compared to densely\nconnected networks (e.g., DenseNets), SWNets require a substantially fewer\nnumber of training parameters while maintaining a similar level of\nclassification accuracy. We evaluate our networks on various DL model\narchitectures and image classification datasets, namely, CIFAR10, CIFAR100, and\nILSVRC (ImageNet). Our experiments demonstrate an average of ~2.1x improvement\nin convergence speed to the desired accuracy",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1904.04862v1",
        "date": "2019-04-09 18:41:26+00:00"
    },
    {
        "title": "AutoAssist: A Framework to Accelerate Training of Deep Neural Networks",
        "authors": [
            "Jiong Zhang",
            "Hsiang-fu Yu",
            "Inderjit S. Dhillon"
        ],
        "abstract": "Deep neural networks have yielded superior performance in many applications;\nhowever, the gradient computation in a deep model with millions of instances\nlead to a lengthy training process even with modern GPU/TPU hardware\nacceleration. In this paper, we propose AutoAssist, a simple framework to\naccelerate training of a deep neural network. Typically, as the training\nprocedure evolves, the amount of improvement in the current model by a\nstochastic gradient update on each instance varies dynamically. In AutoAssist,\nwe utilize this fact and design a simple instance shrinking operation, which is\nused to filter out instances with relatively low marginal improvement to the\ncurrent model; thus the computationally intensive gradient computations are\nperformed on informative instances as much as possible. We prove that the\nproposed technique outperforms vanilla SGD with existing importance sampling\napproaches for linear SVM problems, and establish an O(1/k) convergence for\nstrongly convex problems. In order to apply the proposed techniques to\naccelerate training of deep models, we propose to jointly train a very\nlightweight Assistant network in addition to the original deep network referred\nto as Boss. The Assistant network is designed to gauge the importance of a\ngiven instance with respect to the current Boss such that a shrinking operation\ncan be applied in the batch generator. With careful design, we train the Boss\nand Assistant in a nonblocking and asynchronous fashion such that overhead is\nminimal. We demonstrate that AutoAssist reduces the number of epochs by 40% for\ntraining a ResNet to reach the same test accuracy on an image classification\ndata set and saves 30% training time needed for a transformer model to yield\nthe same BLEU scores on a translation dataset.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1905.03381v1",
        "date": "2019-05-08 22:36:37+00:00"
    },
    {
        "title": "A Mathematical Theory of Deep Convolutional Neural Networks for Feature Extraction",
        "authors": [
            "Thomas Wiatowski",
            "Helmut B\u00f6lcskei"
        ],
        "abstract": "Deep convolutional neural networks have led to breakthrough results in\nnumerous practical machine learning tasks such as classification of images in\nthe ImageNet data set, control-policy-learning to play Atari games or the board\ngame Go, and image captioning. Many of these applications first perform feature\nextraction and then feed the results thereof into a trainable classifier. The\nmathematical analysis of deep convolutional neural networks for feature\nextraction was initiated by Mallat, 2012. Specifically, Mallat considered\nso-called scattering networks based on a wavelet transform followed by the\nmodulus non-linearity in each network layer, and proved translation invariance\n(asymptotically in the wavelet scale parameter) and deformation stability of\nthe corresponding feature extractor. This paper complements Mallat's results by\ndeveloping a theory that encompasses general convolutional transforms, or in\nmore technical parlance, general semi-discrete frames (including\nWeyl-Heisenberg filters, curvelets, shearlets, ridgelets, wavelets, and learned\nfilters), general Lipschitz-continuous non-linearities (e.g., rectified linear\nunits, shifted logistic sigmoids, hyperbolic tangents, and modulus functions),\nand general Lipschitz-continuous pooling operators emulating, e.g.,\nsub-sampling and averaging. In addition, all of these elements can be different\nin different network layers. For the resulting feature extractor we prove a\ntranslation invariance result of vertical nature in the sense of the features\nbecoming progressively more translation-invariant with increasing network\ndepth, and we establish deformation sensitivity bounds that apply to signal\nclasses such as, e.g., band-limited functions, cartoon functions, and Lipschitz\nfunctions.",
        "categories": [
            "cs.IT",
            "cs.AI",
            "cs.LG",
            "math.FA",
            "math.IT",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1512.06293v3",
        "date": "2015-12-19 22:31:24+00:00"
    },
    {
        "title": "SDE-Net: Equipping Deep Neural Networks with Uncertainty Estimates",
        "authors": [
            "Lingkai Kong",
            "Jimeng Sun",
            "Chao Zhang"
        ],
        "abstract": "Uncertainty quantification is a fundamental yet unsolved problem for deep\nlearning. The Bayesian framework provides a principled way of uncertainty\nestimation but is often not scalable to modern deep neural nets (DNNs) that\nhave a large number of parameters. Non-Bayesian methods are simple to implement\nbut often conflate different sources of uncertainties and require huge\ncomputing resources. We propose a new method for quantifying uncertainties of\nDNNs from a dynamical system perspective. The core of our method is to view DNN\ntransformations as state evolution of a stochastic dynamical system and\nintroduce a Brownian motion term for capturing epistemic uncertainty. Based on\nthis perspective, we propose a neural stochastic differential equation model\n(SDE-Net) which consists of (1) a drift net that controls the system to fit the\npredictive function; and (2) a diffusion net that captures epistemic\nuncertainty. We theoretically analyze the existence and uniqueness of the\nsolution to SDE-Net. Our experiments demonstrate that the SDE-Net model can\noutperform existing uncertainty estimation methods across a series of tasks\nwhere uncertainty plays a fundamental role.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2008.10546v1",
        "date": "2020-08-24 16:33:54+00:00"
    },
    {
        "title": "Deep transformation models: Tackling complex regression problems with neural network based transformation models",
        "authors": [
            "Beate Sick",
            "Torsten Hothorn",
            "Oliver D\u00fcrr"
        ],
        "abstract": "We present a deep transformation model for probabilistic regression. Deep\nlearning is known for outstandingly accurate predictions on complex data but in\nregression tasks, it is predominantly used to just predict a single number.\nThis ignores the non-deterministic character of most tasks. Especially if\ncrucial decisions are based on the predictions, like in medical applications,\nit is essential to quantify the prediction uncertainty. The presented deep\nlearning transformation model estimates the whole conditional probability\ndistribution, which is the most thorough way to capture uncertainty about the\noutcome. We combine ideas from a statistical transformation model (most likely\ntransformation) with recent transformation models from deep learning\n(normalizing flows) to predict complex outcome distributions. The core of the\nmethod is a parameterized transformation function which can be trained with the\nusual maximum likelihood framework using gradient descent. The method can be\ncombined with existing deep learning architectures. For small machine learning\nbenchmark datasets, we report state of the art performance for most dataset and\npartly even outperform it. Our method works for complex input data, which we\ndemonstrate by employing a CNN architecture on image data.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2004.00464v1",
        "date": "2020-04-01 14:23:12+00:00"
    },
    {
        "title": "Sequential Attacks on Agents for Long-Term Adversarial Goals",
        "authors": [
            "Edgar Tretschk",
            "Seong Joon Oh",
            "Mario Fritz"
        ],
        "abstract": "Reinforcement learning (RL) has advanced greatly in the past few years with\nthe employment of effective deep neural networks (DNNs) on the policy networks.\nWith the great effectiveness came serious vulnerability issues with DNNs that\nsmall adversarial perturbations on the input can change the output of the\nnetwork. Several works have pointed out that learned agents with a DNN policy\nnetwork can be manipulated against achieving the original task through a\nsequence of small perturbations on the input states. In this paper, we\ndemonstrate furthermore that it is also possible to impose an arbitrary\nadversarial reward on the victim policy network through a sequence of attacks.\nOur method involves the latest adversarial attack technique, Adversarial\nTransformer Network (ATN), that learns to generate the attack and is easy to\nintegrate into the policy network. As a result of our attack, the victim agent\nis misguided to optimise for the adversarial reward over time. Our results\nexpose serious security threats for RL applications in safety-critical systems\nincluding drones, medical analysis, and self-driving cars.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CR",
            "cs.CV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1805.12487v2",
        "date": "2018-05-31 14:22:09+00:00"
    },
    {
        "title": "Batch Normalization Orthogonalizes Representations in Deep Random Networks",
        "authors": [
            "Hadi Daneshmand",
            "Amir Joudaki",
            "Francis Bach"
        ],
        "abstract": "This paper underlines a subtle property of batch-normalization (BN):\nSuccessive batch normalizations with random linear transformations make hidden\nrepresentations increasingly orthogonal across layers of a deep neural network.\nWe establish a non-asymptotic characterization of the interplay between depth,\nwidth, and the orthogonality of deep representations. More precisely, under a\nmild assumption, we prove that the deviation of the representations from\northogonality rapidly decays with depth up to a term inversely proportional to\nthe network width. This result has two main implications: 1) Theoretically, as\nthe depth grows, the distribution of the representation -- after the linear\nlayers -- contracts to a Wasserstein-2 ball around an isotropic Gaussian\ndistribution. Furthermore, the radius of this Wasserstein ball shrinks with the\nwidth of the network. 2) In practice, the orthogonality of the representations\ndirectly influences the performance of stochastic gradient descent (SGD). When\nrepresentations are initially aligned, we observe SGD wastes many iterations to\northogonalize representations before the classification. Nevertheless, we\nexperimentally show that starting optimization from orthogonal representations\nis sufficient to accelerate SGD, with no need for BN.",
        "categories": [
            "stat.ML",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2106.03970v1",
        "date": "2021-06-07 21:14:59+00:00"
    },
    {
        "title": "HyperNCA: Growing Developmental Networks with Neural Cellular Automata",
        "authors": [
            "Elias Najarro",
            "Shyam Sudhakaran",
            "Claire Glanois",
            "Sebastian Risi"
        ],
        "abstract": "In contrast to deep reinforcement learning agents, biological neural networks\nare grown through a self-organized developmental process. Here we propose a new\nhypernetwork approach to grow artificial neural networks based on neural\ncellular automata (NCA). Inspired by self-organising systems and\ninformation-theoretic approaches to developmental biology, we show that our\nHyperNCA method can grow neural networks capable of solving common\nreinforcement learning tasks. Finally, we explore how the same approach can be\nused to build developmental metamorphosis networks capable of transforming\ntheir weights to solve variations of the initial RL task.",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2204.11674v1",
        "date": "2022-04-25 14:08:50+00:00"
    },
    {
        "title": "The Mori-Zwanzig formulation of deep learning",
        "authors": [
            "Daniele Venturi",
            "Xiantao Li"
        ],
        "abstract": "We develop a new formulation of deep learning based on the Mori-Zwanzig (MZ)\nformalism of irreversible statistical mechanics. The new formulation is built\nupon the well-known duality between deep neural networks and discrete dynamical\nsystems, and it allows us to directly propagate quantities of interest\n(conditional expectations and probability density functions) forward and\nbackward through the network by means of exact linear operator equations. Such\nnew equations can be used as a starting point to develop new effective\nparameterizations of deep neural networks, and provide a new framework to study\ndeep-learning via operator theoretic methods. The proposed MZ formulation of\ndeep learning naturally introduces a new concept, i.e., the memory of the\nneural network, which plays a fundamental role in low-dimensional modeling and\nparameterization. By using the theory of contraction mappings, we develop\nsufficient conditions for the memory of the neural network to decay with the\nnumber of layers. This allows us to rigorously transform deep networks into\nshallow ones, e.g., by reducing the number of neurons per layer (using\nprojection operators), or by reducing the total number of layers (using the\ndecay property of the memory operator).",
        "categories": [
            "cs.LG",
            "cond-mat.stat-mech",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2209.05544v3",
        "date": "2022-09-12 18:44:50+00:00"
    },
    {
        "title": "Universal approximations of permutation invariant/equivariant functions by deep neural networks",
        "authors": [
            "Akiyoshi Sannai",
            "Yuuki Takai",
            "Matthieu Cordonnier"
        ],
        "abstract": "In this paper, we develop a theory about the relationship between\n$G$-invariant/equivariant functions and deep neural networks for finite group\n$G$. Especially, for a given $G$-invariant/equivariant function, we construct\nits universal approximator by deep neural network whose layers equip\n$G$-actions and each affine transformations are $G$-equivariant/invariant. Due\nto representation theory, we can show that this approximator has exponentially\nfewer free parameters than usual models.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1903.01939v3",
        "date": "2019-03-05 17:17:02+00:00"
    },
    {
        "title": "Teaching Temporal Logics to Neural Networks",
        "authors": [
            "Christopher Hahn",
            "Frederik Schmitt",
            "Jens U. Kreber",
            "Markus N. Rabe",
            "Bernd Finkbeiner"
        ],
        "abstract": "We study two fundamental questions in neuro-symbolic computing: can deep\nlearning tackle challenging problems in logics end-to-end, and can neural\nnetworks learn the semantics of logics. In this work we focus on linear-time\ntemporal logic (LTL), as it is widely used in verification. We train a\nTransformer on the problem to directly predict a solution, i.e. a trace, to a\ngiven LTL formula. The training data is generated with classical solvers,\nwhich, however, only provide one of many possible solutions to each formula. We\ndemonstrate that it is sufficient to train on those particular solutions to\nformulas, and that Transformers can predict solutions even to formulas from\nbenchmarks from the literature on which the classical solver timed out.\nTransformers also generalize to the semantics of the logics: while they often\ndeviate from the solutions found by the classical solvers, they still predict\ncorrect solutions to most formulas.",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2003.04218v3",
        "date": "2020-03-06 14:46:49+00:00"
    },
    {
        "title": "A Shallow High-Order Parametric Approach to Data Visualization and Compression",
        "authors": [
            "Martin Renqiang Min",
            "Hongyu Guo",
            "Dongjin Song"
        ],
        "abstract": "Explicit high-order feature interactions efficiently capture essential\nstructural knowledge about the data of interest and have been used for\nconstructing generative models. We present a supervised discriminative\nHigh-Order Parametric Embedding (HOPE) approach to data visualization and\ncompression. Compared to deep embedding models with complicated deep\narchitectures, HOPE generates more effective high-order feature mapping through\nan embarrassingly simple shallow model. Furthermore, two approaches to\ngenerating a small number of exemplars conveying high-order interactions to\nrepresent large-scale data sets are proposed. These exemplars in combination\nwith the feature mapping learned by HOPE effectively capture essential data\nvariations. Moreover, through HOPE, these exemplars are employed to increase\nthe computational efficiency of kNN classification for fast information\nretrieval by thousands of times. For classification in two-dimensional\nembedding space on MNIST and USPS datasets, our shallow method HOPE with simple\nSigmoid transformations significantly outperforms state-of-the-art supervised\ndeep embedding models based on deep neural networks, and even achieved\nhistorically low test error rate of 0.65% in two-dimensional space on MNIST,\nwhich demonstrates the representational efficiency and power of supervised\nshallow models with high-order feature interactions.",
        "categories": [
            "cs.AI",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1608.04689v1",
        "date": "2016-08-16 17:54:40+00:00"
    },
    {
        "title": "Forward Thinking: Building Deep Random Forests",
        "authors": [
            "Kevin Miller",
            "Chris Hettinger",
            "Jeffrey Humpherys",
            "Tyler Jarvis",
            "David Kartchner"
        ],
        "abstract": "The success of deep neural networks has inspired many to wonder whether other\nlearners could benefit from deep, layered architectures. We present a general\nframework called forward thinking for deep learning that generalizes the\narchitectural flexibility and sophistication of deep neural networks while also\nallowing for (i) different types of learning functions in the network, other\nthan neurons, and (ii) the ability to adaptively deepen the network as needed\nto improve results. This is done by training one layer at a time, and once a\nlayer is trained, the input data are mapped forward through the layer to create\na new learning problem. The process is then repeated, transforming the data\nthrough multiple layers, one at a time, rendering a new dataset, which is\nexpected to be better behaved, and on which a final output layer can achieve\ngood performance. In the case where the neurons of deep neural nets are\nreplaced with decision trees, we call the result a Forward Thinking Deep Random\nForest (FTDRF). We demonstrate a proof of concept by applying FTDRF on the\nMNIST dataset. We also provide a general mathematical formulation that allows\nfor other types of deep learning problems to be considered.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1705.07366v1",
        "date": "2017-05-20 22:39:51+00:00"
    },
    {
        "title": "An Overview of Neural Network Compression",
        "authors": [
            "James O' Neill"
        ],
        "abstract": "Overparameterized networks trained to convergence have shown impressive\nperformance in domains such as computer vision and natural language processing.\nPushing state of the art on salient tasks within these domains corresponds to\nthese models becoming larger and more difficult for machine learning\npractitioners to use given the increasing memory and storage requirements, not\nto mention the larger carbon footprint. Thus, in recent years there has been a\nresurgence in model compression techniques, particularly for deep convolutional\nneural networks and self-attention based networks such as the Transformer.\n  Hence, this paper provides a timely overview of both old and current\ncompression techniques for deep neural networks, including pruning,\nquantization, tensor decomposition, knowledge distillation and combinations\nthereof.\n  We assume a basic familiarity with deep learning architectures\\footnote{For\nan introduction to deep learning, see ~\\citet{goodfellow2016deep}}, namely,\nRecurrent Neural\nNetworks~\\citep[(RNNs)][]{rumelhart1985learning,hochreiter1997long},\nConvolutional Neural Networks~\\citep{fukushima1980neocognitron}~\\footnote{For\nan up to date overview see~\\citet{khan2019survey}} and Self-Attention based\nnetworks~\\citep{vaswani2017attention}\\footnote{For a general overview of\nself-attention networks, see ~\\citet{chaudhari2019attentive}.},\\footnote{For\nmore detail and their use in natural language processing,\nsee~\\citet{hu2019introductory}}. Most of the papers discussed are proposed in\nthe context of at least one of these DNN architectures.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2006.03669v2",
        "date": "2020-06-05 20:28:56+00:00"
    },
    {
        "title": "From Deep to Shallow: Transformations of Deep Rectifier Networks",
        "authors": [
            "Senjian An",
            "Farid Boussaid",
            "Mohammed Bennamoun",
            "Jiankun Hu"
        ],
        "abstract": "In this paper, we introduce transformations of deep rectifier networks,\nenabling the conversion of deep rectifier networks into shallow rectifier\nnetworks. We subsequently prove that any rectifier net of any depth can be\nrepresented by a maximum of a number of functions that can be realized by a\nshallow network with a single hidden layer. The transformations of both deep\nrectifier nets and deep residual nets are conducted to demonstrate the\nadvantages of the residual nets over the conventional neural nets and the\nadvantages of the deep neural nets over the shallow neural nets. In summary,\nfor two rectifier nets with different depths but with same total number of\nhidden units, the corresponding single hidden layer representation of the\ndeeper net is much more complex than the corresponding single hidden\nrepresentation of the shallower net. Similarly, for a residual net and a\nconventional rectifier net with the same structure except for the skip\nconnections in the residual net, the corresponding single hidden layer\nrepresentation of the residual net is much more complex than the corresponding\nsingle hidden layer representation of the conventional net.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1703.10355v1",
        "date": "2017-03-30 08:37:14+00:00"
    },
    {
        "title": "Bayesian Conditional Generative Adverserial Networks",
        "authors": [
            "M. Ehsan Abbasnejad",
            "Qinfeng Shi",
            "Iman Abbasnejad",
            "Anton van den Hengel",
            "Anthony Dick"
        ],
        "abstract": "Traditional GANs use a deterministic generator function (typically a neural\nnetwork) to transform a random noise input $z$ to a sample $\\mathbf{x}$ that\nthe discriminator seeks to distinguish. We propose a new GAN called Bayesian\nConditional Generative Adversarial Networks (BC-GANs) that use a random\ngenerator function to transform a deterministic input $y'$ to a sample\n$\\mathbf{x}$. Our BC-GANs extend traditional GANs to a Bayesian framework, and\nnaturally handle unsupervised learning, supervised learning, and\nsemi-supervised learning problems. Experiments show that the proposed BC-GANs\noutperforms the state-of-the-arts.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1706.05477v1",
        "date": "2017-06-17 05:29:13+00:00"
    },
    {
        "title": "The Expressive Power of Tuning Only the Norm Layers",
        "authors": [
            "Angeliki Giannou",
            "Shashank Rajput",
            "Dimitris Papailiopoulos"
        ],
        "abstract": "Feature normalization transforms such as Batch and Layer-Normalization have\nbecome indispensable ingredients of state-of-the-art deep neural networks.\nRecent studies on fine-tuning large pretrained models indicate that just tuning\nthe parameters of these affine transforms can achieve high accuracy for\ndownstream tasks. These findings open the questions about the expressive power\nof tuning the normalization layers of frozen networks. In this work, we take\nthe first step towards this question and show that for random ReLU networks,\nfine-tuning only its normalization layers can reconstruct any target network\nthat is $O(\\sqrt{\\text{width}})$ times smaller. We show that this holds even\nfor randomly sparsified networks, under sufficient overparameterization, in\nagreement with prior empirical work.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2302.07937v1",
        "date": "2023-02-15 20:44:31+00:00"
    },
    {
        "title": "On variation of gradients of deep neural networks",
        "authors": [
            "Yongdai Kim",
            "Dongha Kim"
        ],
        "abstract": "We provide a theoretical explanation of the role of the number of nodes at\neach layer in deep neural networks. We prove that the largest variation of a\ndeep neural network with ReLU activation function arises when the layer with\nthe fewest nodes changes its activation pattern. An important implication is\nthat deep neural network is a useful tool to generate functions most of whose\nvariations are concentrated on a smaller area of the input space near the\nboundaries corresponding to the layer with the fewest nodes. In turn, this\nproperty makes the function more invariant to input transformation. That is,\nour theoretical result gives a clue about how to design the architecture of a\ndeep neural network to increase complexity and transformation invariancy\nsimultaneously.",
        "categories": [
            "stat.ML",
            "cs.LG",
            "math.ST",
            "stat.TH"
        ],
        "link": "http://arxiv.org/pdf/1812.00308v1",
        "date": "2018-12-02 02:37:56+00:00"
    },
    {
        "title": "Can We Scale Transformers to Predict Parameters of Diverse ImageNet Models?",
        "authors": [
            "Boris Knyazev",
            "Doha Hwang",
            "Simon Lacoste-Julien"
        ],
        "abstract": "Pretraining a neural network on a large dataset is becoming a cornerstone in\nmachine learning that is within the reach of only a few communities with\nlarge-resources. We aim at an ambitious goal of democratizing pretraining.\nTowards that goal, we train and release a single neural network that can\npredict high quality ImageNet parameters of other neural networks. By using\npredicted parameters for initialization we are able to boost training of\ndiverse ImageNet models available in PyTorch. When transferred to other\ndatasets, models initialized with predicted parameters also converge faster and\nreach competitive final performance.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2303.04143v1",
        "date": "2023-03-07 18:56:59+00:00"
    },
    {
        "title": "Deep interpretable ensembles",
        "authors": [
            "Lucas Kook",
            "Andrea G\u00f6tschi",
            "Philipp FM Baumann",
            "Torsten Hothorn",
            "Beate Sick"
        ],
        "abstract": "Ensembles improve prediction performance and allow uncertainty quantification\nby aggregating predictions from multiple models. In deep ensembling, the\nindividual models are usually black box neural networks, or recently, partially\ninterpretable semi-structured deep transformation models. However,\ninterpretability of the ensemble members is generally lost upon aggregation.\nThis is a crucial drawback of deep ensembles in high-stake decision fields, in\nwhich interpretable models are desired. We propose a novel transformation\nensemble which aggregates probabilistic predictions with the guarantee to\npreserve interpretability and yield uniformly better predictions than the\nensemble members on average. Transformation ensembles are tailored towards\ninterpretable deep transformation models but are applicable to a wider range of\nprobabilistic neural networks. In experiments on several publicly available\ndata sets, we demonstrate that transformation ensembles perform on par with\nclassical deep ensembles in terms of prediction performance, discrimination,\nand calibration. In addition, we demonstrate how transformation ensembles\nquantify both aleatoric and epistemic uncertainty, and produce minimax optimal\npredictions under certain conditions.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2205.12729v1",
        "date": "2022-05-25 12:39:39+00:00"
    },
    {
        "title": "Neural networks trained with SGD learn distributions of increasing complexity",
        "authors": [
            "Maria Refinetti",
            "Alessandro Ingrosso",
            "Sebastian Goldt"
        ],
        "abstract": "The ability of deep neural networks to generalise well even when they\ninterpolate their training data has been explained using various \"simplicity\nbiases\". These theories postulate that neural networks avoid overfitting by\nfirst learning simple functions, say a linear classifier, before learning more\ncomplex, non-linear functions. Meanwhile, data structure is also recognised as\na key ingredient for good generalisation, yet its role in simplicity biases is\nnot yet understood. Here, we show that neural networks trained using stochastic\ngradient descent initially classify their inputs using lower-order input\nstatistics, like mean and covariance, and exploit higher-order statistics only\nlater during training. We first demonstrate this distributional simplicity bias\n(DSB) in a solvable model of a neural network trained on synthetic data. We\nempirically demonstrate DSB in a range of deep convolutional networks and\nvisual transformers trained on CIFAR10, and show that it even holds in networks\npre-trained on ImageNet. We discuss the relation of DSB to other simplicity\nbiases and consider its implications for the principle of Gaussian universality\nin learning.",
        "categories": [
            "stat.ML",
            "cond-mat.dis-nn",
            "cond-mat.stat-mech",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2211.11567v1",
        "date": "2022-11-21 15:27:22+00:00"
    },
    {
        "title": "Rapid Feature Learning with Stacked Linear Denoisers",
        "authors": [
            "Zhixiang Eddie Xu",
            "Kilian Q. Weinberger",
            "Fei Sha"
        ],
        "abstract": "We investigate unsupervised pre-training of deep architectures as feature\ngenerators for \"shallow\" classifiers. Stacked Denoising Autoencoders (SdA),\nwhen used as feature pre-processing tools for SVM classification, can lead to\nsignificant improvements in accuracy - however, at the price of a substantial\nincrease in computational cost. In this paper we create a simple algorithm\nwhich mimics the layer by layer training of SdAs. However, in contrast to SdAs,\nour algorithm requires no training through gradient descent as the parameters\ncan be computed in closed-form. It can be implemented in less than 20 lines of\nMATLABTMand reduces the computation time from several hours to mere seconds. We\nshow that our feature transformation reliably improves the results of SVM\nclassification significantly on all our data sets - often outperforming SdAs\nand even deep neural networks in three out of four deep learning benchmarks.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1105.0972v1",
        "date": "2011-05-05 04:02:35+00:00"
    },
    {
        "title": "Understanding Graph Neural Networks with Asymmetric Geometric Scattering Transforms",
        "authors": [
            "Michael Perlmutter",
            "Feng Gao",
            "Guy Wolf",
            "Matthew Hirn"
        ],
        "abstract": "The scattering transform is a multilayered wavelet-based deep learning\narchitecture that acts as a model of convolutional neural networks. Recently,\nseveral works have introduced generalizations of the scattering transform for\nnon-Euclidean settings such as graphs. Our work builds upon these constructions\nby introducing windowed and non-windowed geometric scattering transforms for\ngraphs based upon a very general class of asymmetric wavelets. We show that\nthese asymmetric graph scattering transforms have many of the same theoretical\nguarantees as their symmetric counterparts. As a result, the proposed\nconstruction unifies and extends known theoretical results for many of the\nexisting graph scattering architectures. In doing so, this work helps bridge\nthe gap between geometric scattering and other graph neural networks by\nintroducing a large family of networks with provable stability and invariance\nguarantees. These results lay the groundwork for future deep learning\narchitectures for graph-structured data that have learned filters and also\nprovably have desirable theoretical properties.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1911.06253v3",
        "date": "2019-11-14 17:23:06+00:00"
    },
    {
        "title": "Deep learning with differential Gaussian process flows",
        "authors": [
            "Pashupati Hegde",
            "Markus Heinonen",
            "Harri L\u00e4hdesm\u00e4ki",
            "Samuel Kaski"
        ],
        "abstract": "We propose a novel deep learning paradigm of differential flows that learn a\nstochastic differential equation transformations of inputs prior to a standard\nclassification or regression function. The key property of differential\nGaussian processes is the warping of inputs through infinitely deep, but\ninfinitesimal, differential fields, that generalise discrete layers into a\ndynamical system. We demonstrate state-of-the-art results that exceed the\nperformance of deep Gaussian processes and neural networks",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1810.04066v2",
        "date": "2018-10-09 15:15:23+00:00"
    },
    {
        "title": "Ghosts in Neural Networks: Existence, Structure and Role of Infinite-Dimensional Null Space",
        "authors": [
            "Sho Sonoda",
            "Isao Ishikawa",
            "Masahiro Ikeda"
        ],
        "abstract": "Overparametrization has been remarkably successful for deep learning studies.\nThis study investigates an overlooked but important aspect of overparametrized\nneural networks, that is, the null components in the parameters of neural\nnetworks, or the ghosts. Since deep learning is not explicitly regularized,\ntypical deep learning solutions contain null components. In this paper, we\npresent a structure theorem of the null space for a general class of neural\nnetworks. Specifically, we show that any null element can be uniquely written\nby the linear combination of ridgelet transforms. In general, it is quite\ndifficult to fully characterize the null space of an arbitrarily given\noperator. Therefore, the structure theorem is a great advantage for\nunderstanding a complicated landscape of neural network parameters. As\napplications, we discuss the roles of ghosts on the generalization performance\nof deep learning.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2106.04770v1",
        "date": "2021-06-09 02:05:38+00:00"
    },
    {
        "title": "Deep Random Splines for Point Process Intensity Estimation of Neural Population Data",
        "authors": [
            "Gabriel Loaiza-Ganem",
            "Sean M. Perkins",
            "Karen E. Schroeder",
            "Mark M. Churchland",
            "John P. Cunningham"
        ],
        "abstract": "Gaussian processes are the leading class of distributions on random\nfunctions, but they suffer from well known issues including difficulty scaling\nand inflexibility with respect to certain shape constraints (such as\nnonnegativity). Here we propose Deep Random Splines, a flexible class of random\nfunctions obtained by transforming Gaussian noise through a deep neural network\nwhose output are the parameters of a spline. Unlike Gaussian processes, Deep\nRandom Splines allow us to readily enforce shape constraints while inheriting\nthe richness and tractability of deep generative models. We also present an\nobservational model for point process data which uses Deep Random Splines to\nmodel the intensity function of each point process and apply it to neural\npopulation data to obtain a low-dimensional representation of spiking activity.\nInference is performed via a variational autoencoder that uses a novel\nrecurrent encoder architecture that can handle multiple point processes as\ninput. We use a newly collected dataset where a primate completes a pedaling\ntask, and observe better dimensionality reduction with our model than with\ncompeting alternatives.",
        "categories": [
            "stat.ML",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1903.02610v6",
        "date": "2019-03-06 21:01:03+00:00"
    },
    {
        "title": "PRoA: A Probabilistic Robustness Assessment against Functional Perturbations",
        "authors": [
            "Tianle Zhang",
            "Wenjie Ruan",
            "Jonathan E. Fieldsend"
        ],
        "abstract": "In safety-critical deep learning applications robustness measurement is a\nvital pre-deployment phase. However, existing robustness verification methods\nare not sufficiently practical for deploying machine learning systems in the\nreal world. On the one hand, these methods attempt to claim that no\nperturbations can ``fool'' deep neural networks (DNNs), which may be too\nstringent in practice. On the other hand, existing works rigorously consider\n$L_p$ bounded additive perturbations on the pixel space, although\nperturbations, such as colour shifting and geometric transformations, are more\npractically and frequently occurring in the real world. Thus, from the\npractical standpoint, we present a novel and general {\\it probabilistic\nrobustness assessment method} (PRoA) based on the adaptive concentration, and\nit can measure the robustness of deep learning models against functional\nperturbations. PRoA can provide statistical guarantees on the probabilistic\nrobustness of a model, \\textit{i.e.}, the probability of failure encountered by\nthe trained model after deployment. Our experiments demonstrate the\neffectiveness and flexibility of PRoA in terms of evaluating the probabilistic\nrobustness against a broad range of functional perturbations, and PRoA can\nscale well to various large-scale deep neural networks compared to existing\nstate-of-the-art baselines. For the purpose of reproducibility, we release our\ntool on GitHub: \\url{ https://github.com/TrustAI/PRoA}.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "stat.ML",
            "68T07",
            "I.2; I.2.6"
        ],
        "link": "http://arxiv.org/pdf/2207.02036v1",
        "date": "2022-07-05 13:27:38+00:00"
    },
    {
        "title": "A Neural Lambda Calculus: Neurosymbolic AI meets the foundations of computing and functional programming",
        "authors": [
            "Jo\u00e3o Flach",
            "Luis C. Lamb"
        ],
        "abstract": "Over the last decades, deep neural networks based-models became the dominant\nparadigm in machine learning. Further, the use of artificial neural networks in\nsymbolic learning has been seen as increasingly relevant recently. To study the\ncapabilities of neural networks in the symbolic AI domain, researchers have\nexplored the ability of deep neural networks to learn mathematical\nconstructions, such as addition and multiplication, logic inference, such as\ntheorem provers, and even the execution of computer programs. The latter is\nknown to be too complex a task for neural networks. Therefore, the results were\nnot always successful, and often required the introduction of biased elements\nin the learning process, in addition to restricting the scope of possible\nprograms to be executed. In this work, we will analyze the ability of neural\nnetworks to learn how to execute programs as a whole. To do so, we propose a\ndifferent approach. Instead of using an imperative programming language, with\ncomplex structures, we use the Lambda Calculus ({\\lambda}-Calculus), a simple,\nbut Turing-Complete mathematical formalism, which serves as the basis for\nmodern functional programming languages and is at the heart of computability\ntheory. We will introduce the use of integrated neural learning and lambda\ncalculi formalization. Finally, we explore execution of a program in\n{\\lambda}-Calculus is based on reductions, we will show that it is enough to\nlearn how to perform these reductions so that we can execute any program.\nKeywords: Machine Learning, Lambda Calculus, Neurosymbolic AI, Neural Networks,\nTransformer Model, Sequence-to-Sequence Models, Computational Models",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.LO"
        ],
        "link": "http://arxiv.org/pdf/2304.09276v1",
        "date": "2023-04-18 20:30:16+00:00"
    },
    {
        "title": "Deep Transform and Metric Learning Network: Wedding Deep Dictionary Learning and Neural Networks",
        "authors": [
            "Wen Tang",
            "Emilie Chouzenoux",
            "Jean-Christophe Pesquet",
            "Hamid Krim"
        ],
        "abstract": "On account of its many successes in inference tasks and denoising\napplications, Dictionary Learning (DL) and its related sparse optimization\nproblems have garnered a lot of research interest. While most solutions have\nfocused on single layer dictionaries, the improved recently proposed Deep DL\n(DDL) methods have also fallen short on a number of issues. We propose herein,\na novel DDL approach where each DL layer can be formulated as a combination of\none linear layer and a Recurrent Neural Network (RNN). The RNN is shown to\nflexibly account for the layer-associated and learned metric. Our proposed work\nunveils new insights into Neural Networks and DDL and provides a new, efficient\nand competitive approach to jointly learn a deep transform and a metric for\ninference applications. Extensive experiments are carried out to demonstrate\nthat the proposed method can not only outperform existing DDL but also\nstate-of-the-art generic CNNs.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2002.07898v2",
        "date": "2020-02-18 22:04:11+00:00"
    },
    {
        "title": "Deep Learning by Scattering",
        "authors": [
            "St\u00e9phane Mallat",
            "Ir\u00e8ne Waldspurger"
        ],
        "abstract": "We introduce general scattering transforms as mathematical models of deep\nneural networks with l2 pooling. Scattering networks iteratively apply complex\nvalued unitary operators, and the pooling is performed by a complex modulus. An\nexpected scattering defines a contractive representation of a high-dimensional\nprobability distribution, which preserves its mean-square norm. We show that\nunsupervised learning can be casted as an optimization of the space contraction\nto preserve the volume occupied by unlabeled examples, at each layer of the\nnetwork. Supervised learning and classification are performed with an averaged\nscattering, which provides scattering estimations for multiple classes.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1306.5532v2",
        "date": "2013-06-24 07:52:45+00:00"
    },
    {
        "title": "Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges",
        "authors": [
            "Michael M. Bronstein",
            "Joan Bruna",
            "Taco Cohen",
            "Petar Veli\u010dkovi\u0107"
        ],
        "abstract": "The last decade has witnessed an experimental revolution in data science and\nmachine learning, epitomised by deep learning methods. Indeed, many\nhigh-dimensional learning tasks previously thought to be beyond reach -- such\nas computer vision, playing Go, or protein folding -- are in fact feasible with\nappropriate computational scale. Remarkably, the essence of deep learning is\nbuilt from two simple algorithmic principles: first, the notion of\nrepresentation or feature learning, whereby adapted, often hierarchical,\nfeatures capture the appropriate notion of regularity for each task, and\nsecond, learning by local gradient-descent type methods, typically implemented\nas backpropagation.\n  While learning generic functions in high dimensions is a cursed estimation\nproblem, most tasks of interest are not generic, and come with essential\npre-defined regularities arising from the underlying low-dimensionality and\nstructure of the physical world. This text is concerned with exposing these\nregularities through unified geometric principles that can be applied\nthroughout a wide spectrum of applications.\n  Such a 'geometric unification' endeavour, in the spirit of Felix Klein's\nErlangen Program, serves a dual purpose: on one hand, it provides a common\nmathematical framework to study the most successful neural network\narchitectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand,\nit gives a constructive procedure to incorporate prior physical knowledge into\nneural architectures and provide principled way to build future architectures\nyet to be invented.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CG",
            "cs.CV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2104.13478v2",
        "date": "2021-04-27 21:09:51+00:00"
    },
    {
        "title": "Joint Binary Neural Network for Multi-label Learning with Applications to Emotion Classification",
        "authors": [
            "Huihui He",
            "Rui Xia"
        ],
        "abstract": "Recently the deep learning techniques have achieved success in multi-label\nclassification due to its automatic representation learning ability and the\nend-to-end learning framework. Existing deep neural networks in multi-label\nclassification can be divided into two kinds: binary relevance neural network\n(BRNN) and threshold dependent neural network (TDNN). However, the former needs\nto train a set of isolate binary networks which ignore dependencies between\nlabels and have heavy computational load, while the latter needs an additional\nthreshold function mechanism to transform the multi-class probabilities to\nmulti-label outputs. In this paper, we propose a joint binary neural network\n(JBNN), to address these shortcomings. In JBNN, the representation of the text\nis fed to a set of logistic functions instead of a softmax function, and the\nmultiple binary classifications are carried out synchronously in one neural\nnetwork framework. Moreover, the relations between labels are captured via\ntraining on a joint binary cross entropy (JBCE) loss. To better meet\nmulti-label emotion classification, we further proposed to incorporate the\nprior label relations into the JBCE loss. The experimental results on the\nbenchmark dataset show that our model performs significantly better than the\nstate-of-the-art multi-label emotion classification methods, in both\nclassification performance and computational efficiency.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1802.00891v1",
        "date": "2018-02-03 01:42:32+00:00"
    },
    {
        "title": "Nested Hyperbolic Spaces for Dimensionality Reduction and Hyperbolic NN Design",
        "authors": [
            "Xiran Fan",
            "Chun-Hao Yang",
            "Baba C. Vemuri"
        ],
        "abstract": "Hyperbolic neural networks have been popular in the recent past due to their\nability to represent hierarchical data sets effectively and efficiently. The\nchallenge in developing these networks lies in the nonlinearity of the\nembedding space namely, the Hyperbolic space. Hyperbolic space is a homogeneous\nRiemannian manifold of the Lorentz group. Most existing methods (with some\nexceptions) use local linearization to define a variety of operations\nparalleling those used in traditional deep neural networks in Euclidean spaces.\nIn this paper, we present a novel fully hyperbolic neural network which uses\nthe concept of projections (embeddings) followed by an intrinsic aggregation\nand a nonlinearity all within the hyperbolic space. The novelty here lies in\nthe projection which is designed to project data on to a lower-dimensional\nembedded hyperbolic space and hence leads to a nested hyperbolic space\nrepresentation independently useful for dimensionality reduction. The main\ntheoretical contribution is that the proposed embedding is proved to be\nisometric and equivariant under the Lorentz transformations. This projection is\ncomputationally efficient since it can be expressed by simple linear\noperations, and, due to the aforementioned equivariance property, it allows for\nweight sharing. The nested hyperbolic space representation is the core\ncomponent of our network and therefore, we first compare this ensuing nested\nhyperbolic space representation with other dimensionality reduction methods\nsuch as tangent PCA, principal geodesic analysis (PGA) and HoroPCA. Based on\nthis equivariant embedding, we develop a novel fully hyperbolic graph\nconvolutional neural network architecture to learn the parameters of the\nprojection. Finally, we present experiments demonstrating comparative\nperformance of our network on several publicly available data sets.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2112.03402v1",
        "date": "2021-12-03 03:20:27+00:00"
    },
    {
        "title": "Network Parameter Learning Using Nonlinear Transforms, Local Representation Goals and Local Propagation Constraints",
        "authors": [
            "Dimche Kostadinov",
            "Behrooz Razdehi",
            "Slava Voloshynovskiy"
        ],
        "abstract": "In this paper, we introduce a novel concept for learning of the parameters in\na neural network. Our idea is grounded on modeling a learning problem that\naddresses a trade-off between (i) satisfying local objectives at each node and\n(ii) achieving desired data propagation through the network under (iii) local\npropagation constraints. We consider two types of nonlinear transforms which\ndescribe the network representations. One of the nonlinear transforms serves as\nactivation function. The other one enables a locally adjusted, deviation\ncorrective components to be included in the update of the network weights in\norder to enable attaining target specific representations at the last network\nnode. Our learning principle not only provides insight into the understanding\nand the interpretation of the learning dynamics, but it offers theoretical\nguarantees over decoupled and parallel parameter estimation strategy that\nenables learning in synchronous and asynchronous mode. Numerical experiments\nvalidate the potential of our approach on image recognition task. The\npreliminary results show advantages in comparison to the state-of-the-art\nmethods, w.r.t. the learning time and the network size while having competitive\nrecognition accuracy.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC",
            "cs.NE",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1902.00016v1",
        "date": "2019-01-31 14:43:55+00:00"
    },
    {
        "title": "Shared Representational Geometry Across Neural Networks",
        "authors": [
            "Qihong Lu",
            "Po-Hsuan Chen",
            "Jonathan W. Pillow",
            "Peter J. Ramadge",
            "Kenneth A. Norman",
            "Uri Hasson"
        ],
        "abstract": "Different neural networks trained on the same dataset often learn similar\ninput-output mappings with very different weights. Is there some correspondence\nbetween these neural network solutions? For linear networks, it has been shown\nthat different instances of the same network architecture encode the same\nrepresentational similarity matrix, and their neural activity patterns are\nconnected by orthogonal transformations. However, it is unclear if this holds\nfor non-linear networks. Using a shared response model, we show that different\nneural networks encode the same input examples as different orthogonal\ntransformations of an underlying shared representation. We test this claim\nusing both standard convolutional neural networks and residual networks on\nCIFAR10 and CIFAR100.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1811.11684v2",
        "date": "2018-11-28 17:07:30+00:00"
    },
    {
        "title": "A New Neural Network Architecture Invariant to the Action of Symmetry Subgroups",
        "authors": [
            "Piotr Kicki",
            "Mete Ozay",
            "Piotr Skrzypczy\u0144ski"
        ],
        "abstract": "We propose a computationally efficient $G$-invariant neural network that\napproximates functions invariant to the action of a given permutation subgroup\n$G \\leq S_n$ of the symmetric group on input data. The key element of the\nproposed network architecture is a new $G$-invariant transformation module,\nwhich produces a $G$-invariant latent representation of the input data.\nTheoretical considerations are supported by numerical experiments, which\ndemonstrate the effectiveness and strong generalization properties of the\nproposed method in comparison to other $G$-invariant neural networks.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "I.2.6"
        ],
        "link": "http://arxiv.org/pdf/2012.06452v1",
        "date": "2020-12-11 16:19:46+00:00"
    },
    {
        "title": "Stochastic Gradient Push for Distributed Deep Learning",
        "authors": [
            "Mahmoud Assran",
            "Nicolas Loizou",
            "Nicolas Ballas",
            "Michael Rabbat"
        ],
        "abstract": "Distributed data-parallel algorithms aim to accelerate the training of deep\nneural networks by parallelizing the computation of large mini-batch gradient\nupdates across multiple nodes. Approaches that synchronize nodes using exact\ndistributed averaging (e.g., via AllReduce) are sensitive to stragglers and\ncommunication delays. The PushSum gossip algorithm is robust to these issues,\nbut only performs approximate distributed averaging. This paper studies\nStochastic Gradient Push (SGP), which combines PushSum with stochastic gradient\nupdates. We prove that SGP converges to a stationary point of smooth,\nnon-convex objectives at the same sub-linear rate as SGD, and that all nodes\nachieve consensus. We empirically validate the performance of SGP on image\nclassification (ResNet-50, ImageNet) and machine translation (Transformer,\nWMT'16 En-De) workloads. Our code will be made publicly available.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC",
            "cs.MA",
            "math.OC",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1811.10792v3",
        "date": "2018-11-27 03:47:26+00:00"
    },
    {
        "title": "A Frobenius norm regularization method for convolutional kernels to avoid unstable gradient problem",
        "authors": [
            "Pei-Chang Guo"
        ],
        "abstract": "Convolutional neural network is a very important model of deep learning. It\ncan help avoid the exploding/vanishing gradient problem and improve the\ngeneralizability of a neural network if the singular values of the Jacobian of\na layer are bounded around $1$ in the training process. We propose a new\npenalty function for a convolutional kernel to let the singular values of the\ncorresponding transformation matrix are bounded around $1$. We show how to\ncarry out the gradient type methods. The penalty is about the structured\ntransformation matrix corresponding to a convolutional kernel. This provides a\nnew regularization method about the weights of convolutional layers.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1907.11235v1",
        "date": "2019-07-25 23:43:05+00:00"
    },
    {
        "title": "Learning Unbiased Representations via R\u00e9nyi Minimization",
        "authors": [
            "Vincent Grari",
            "Oualid El Hajouji",
            "Sylvain Lamprier",
            "Marcin Detyniecki"
        ],
        "abstract": "In recent years, significant work has been done to include fairness\nconstraints in the training objective of machine learning algorithms. Many\nstate-of the-art algorithms tackle this challenge by learning a fair\nrepresentation which captures all the relevant information to predict the\noutput Y while not containing any information about a sensitive attribute S. In\nthis paper, we propose an adversarial algorithm to learn unbiased\nrepresentations via the Hirschfeld-Gebelein-Renyi (HGR) maximal correlation\ncoefficient. We leverage recent work which has been done to estimate this\ncoefficient by learning deep neural network transformations and use it as a\nminmax game to penalize the intrinsic bias in a multi dimensional latent\nrepresentation. Compared to other dependence measures, the HGR coefficient\ncaptures more information about the non-linear dependencies with the sensitive\nvariable, making the algorithm more efficient in mitigating bias in the\nrepresentation. We empirically evaluate and compare our approach and\ndemonstrate significant improvements over existing works in the field.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CY",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2009.03183v1",
        "date": "2020-09-07 15:48:24+00:00"
    },
    {
        "title": "Causal inference using deep neural networks",
        "authors": [
            "Ye Yuan",
            "Xueying Ding",
            "Ziv Bar-Joseph"
        ],
        "abstract": "Causal inference from observation data is a core problem in many scientific\nfields. Here we present a general supervised deep learning framework that\ninfers causal interactions by transforming the input vectors to an image-like\nrepresentation for every pair of inputs. Given a training dataset we first\nconstruct a normalized empirical probability density distribution (NEPDF)\nmatrix. We then train a convolutional neural network (CNN) on NEPDFs for\ncausality predictions. We tested the method on several different simulated and\nreal world data and compared it to prior methods for causal inference. As we\nshow, the method is general, can efficiently handle very large datasets and\nimproves upon prior methods.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2011.12508v1",
        "date": "2020-11-25 04:22:14+00:00"
    },
    {
        "title": "Invariant Integration in Deep Convolutional Feature Space",
        "authors": [
            "Matthias Rath",
            "Alexandru Paul Condurache"
        ],
        "abstract": "In this contribution, we show how to incorporate prior knowledge to a deep\nneural network architecture in a principled manner. We enforce feature space\ninvariances using a novel layer based on invariant integration. This allows us\nto construct a complete feature space invariant to finite transformation\ngroups.\n  We apply our proposed layer to explicitly insert invariance properties for\nvision-related classification tasks, demonstrate our approach for the case of\nrotation invariance and report state-of-the-art performance on the\nRotated-MNIST dataset. Our method is especially beneficial when training with\nlimited data.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2004.09166v1",
        "date": "2020-04-20 09:45:43+00:00"
    },
    {
        "title": "Neural Piecewise-Constant Delay Differential Equations",
        "authors": [
            "Qunxi Zhu",
            "Yifei Shen",
            "Dongsheng Li",
            "Wei Lin"
        ],
        "abstract": "Continuous-depth neural networks, such as the Neural Ordinary Differential\nEquations (ODEs), have aroused a great deal of interest from the communities of\nmachine learning and data science in recent years, which bridge the connection\nbetween deep neural networks and dynamical systems. In this article, we\nintroduce a new sort of continuous-depth neural network, called the Neural\nPiecewise-Constant Delay Differential Equations (PCDDEs). Here, unlike the\nrecently proposed framework of the Neural Delay Differential Equations (DDEs),\nwe transform the single delay into the piecewise-constant delay(s). The Neural\nPCDDEs with such a transformation, on one hand, inherit the strength of\nuniversal approximating capability in Neural DDEs. On the other hand, the\nNeural PCDDEs, leveraging the contributions of the information from the\nmultiple previous time steps, further promote the modeling capability without\naugmenting the network dimension. With such a promotion, we show that the\nNeural PCDDEs do outperform the several existing continuous-depth neural\nframeworks on the one-dimensional piecewise-constant delay population dynamics\nand real-world datasets, including MNIST, CIFAR10, and SVHN.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "math.DS",
            "nlin.CD",
            "34Kxx, 93Cxx, 92B2",
            "I.2.6"
        ],
        "link": "http://arxiv.org/pdf/2201.00960v1",
        "date": "2022-01-04 03:44:15+00:00"
    },
    {
        "title": "Finding trainable sparse networks through Neural Tangent Transfer",
        "authors": [
            "Tianlin Liu",
            "Friedemann Zenke"
        ],
        "abstract": "Deep neural networks have dramatically transformed machine learning, but\ntheir memory and energy demands are substantial. The requirements of real\nbiological neural networks are rather modest in comparison, and one feature\nthat might underlie this austerity is their sparse connectivity. In deep\nlearning, trainable sparse networks that perform well on a specific task are\nusually constructed using label-dependent pruning criteria. In this article, we\nintroduce Neural Tangent Transfer, a method that instead finds trainable sparse\nnetworks in a label-free manner. Specifically, we find sparse networks whose\ntraining dynamics, as characterized by the neural tangent kernel, mimic those\nof dense networks in function space. Finally, we evaluate our label-agnostic\napproach on several standard classification tasks and show that the resulting\nsparse networks achieve higher classification performance while converging\nfaster.",
        "categories": [
            "cs.LG",
            "cs.NE",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2006.08228v2",
        "date": "2020-06-15 08:58:01+00:00"
    },
    {
        "title": "You say Normalizing Flows I see Bayesian Networks",
        "authors": [
            "Antoine Wehenkel",
            "Gilles Louppe"
        ],
        "abstract": "Normalizing flows have emerged as an important family of deep neural networks\nfor modelling complex probability distributions. In this note, we revisit their\ncoupling and autoregressive transformation layers as probabilistic graphical\nmodels and show that they reduce to Bayesian networks with a pre-defined\ntopology and a learnable density at each node. From this new perspective, we\nprovide three results. First, we show that stacking multiple transformations in\na normalizing flow relaxes independence assumptions and entangles the model\ndistribution. Second, we show that a fundamental leap of capacity emerges when\nthe depth of affine flows exceeds 3 transformation layers. Third, we prove the\nnon-universality of the affine normalizing flow, regardless of its depth.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2006.00866v2",
        "date": "2020-06-01 11:54:50+00:00"
    },
    {
        "title": "Synthetic Datasets for Neural Program Synthesis",
        "authors": [
            "Richard Shin",
            "Neel Kant",
            "Kavi Gupta",
            "Christopher Bender",
            "Brandon Trabucco",
            "Rishabh Singh",
            "Dawn Song"
        ],
        "abstract": "The goal of program synthesis is to automatically generate programs in a\nparticular language from corresponding specifications, e.g. input-output\nbehavior. Many current approaches achieve impressive results after training on\nrandomly generated I/O examples in limited domain-specific languages (DSLs), as\nwith string transformations in RobustFill. However, we empirically discover\nthat applying test input generation techniques for languages with control flow\nand rich input space causes deep networks to generalize poorly to certain data\ndistributions; to correct this, we propose a new methodology for controlling\nand evaluating the bias of synthetic data distributions over both programs and\nspecifications. We demonstrate, using the Karel DSL and a small Calculator DSL,\nthat training deep networks on these distributions leads to improved\ncross-distribution generalization performance.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.PL",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1912.12345v1",
        "date": "2019-12-27 21:28:10+00:00"
    },
    {
        "title": "Deep Forest",
        "authors": [
            "Zhi-Hua Zhou",
            "Ji Feng"
        ],
        "abstract": "Current deep learning models are mostly build upon neural networks, i.e.,\nmultiple layers of parameterized differentiable nonlinear modules that can be\ntrained by backpropagation. In this paper, we explore the possibility of\nbuilding deep models based on non-differentiable modules. We conjecture that\nthe mystery behind the success of deep neural networks owes much to three\ncharacteristics, i.e., layer-by-layer processing, in-model feature\ntransformation and sufficient model complexity. We propose the gcForest\napproach, which generates \\textit{deep forest} holding these characteristics.\nThis is a decision tree ensemble approach, with much less hyper-parameters than\ndeep neural networks, and its model complexity can be automatically determined\nin a data-dependent way. Experiments show that its performance is quite robust\nto hyper-parameter settings, such that in most cases, even across different\ndata from different domains, it is able to get excellent performance by using\nthe same default setting. This study opens the door of deep learning based on\nnon-differentiable modules, and exhibits the possibility of constructing deep\nmodels without using backpropagation.",
        "categories": [
            "cs.LG",
            "stat.ML",
            "68Q32, 68T01"
        ],
        "link": "http://arxiv.org/pdf/1702.08835v4",
        "date": "2017-02-28 16:10:31+00:00"
    },
    {
        "title": "Significant Wave Height Prediction based on Wavelet Graph Neural Network",
        "authors": [
            "Delong Chen",
            "Fan Liu",
            "Zheqi Zhang",
            "Xiaomin Lu",
            "Zewen Li"
        ],
        "abstract": "Computational intelligence-based ocean characteristics forecasting\napplications, such as Significant Wave Height (SWH) prediction, are crucial for\navoiding social and economic loss in coastal cities. Compared to the\ntraditional empirical-based or numerical-based forecasting models, \"soft\ncomputing\" approaches, including machine learning and deep learning models,\nhave shown numerous success in recent years. In this paper, we focus on\nenabling the deep learning model to learn both short-term and long-term\nspatial-temporal dependencies for SWH prediction. A Wavelet Graph Neural\nNetwork (WGNN) approach is proposed to integrate the advantages of wavelet\ntransform and graph neural network. Several parallel graph neural networks are\nseparately trained on wavelet decomposed data, and the reconstruction of each\nmodel's prediction forms the final SWH prediction. Experimental results show\nthat the proposed WGNN approach outperforms other models, including the\nnumerical models, the machine learning models, and several deep learning\nmodels.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2107.09483v1",
        "date": "2021-07-20 13:34:48+00:00"
    },
    {
        "title": "Playing the lottery with rewards and multiple languages: lottery tickets in RL and NLP",
        "authors": [
            "Haonan Yu",
            "Sergey Edunov",
            "Yuandong Tian",
            "Ari S. Morcos"
        ],
        "abstract": "The lottery ticket hypothesis proposes that over-parameterization of deep\nneural networks (DNNs) aids training by increasing the probability of a \"lucky\"\nsub-network initialization being present rather than by helping the\noptimization process (Frankle & Carbin, 2019). Intriguingly, this phenomenon\nsuggests that initialization strategies for DNNs can be improved substantially,\nbut the lottery ticket hypothesis has only previously been tested in the\ncontext of supervised learning for natural image tasks. Here, we evaluate\nwhether \"winning ticket\" initializations exist in two different domains:\nnatural language processing (NLP) and reinforcement learning (RL).For NLP, we\nexamined both recurrent LSTM models and large-scale Transformer models (Vaswani\net al., 2017). For RL, we analyzed a number of discrete-action space tasks,\nincluding both classic control and pixel control. Consistent with workin\nsupervised image classification, we confirm that winning ticket initializations\ngenerally outperform parameter-matched random initializations, even at extreme\npruning rates for both NLP and RL. Notably, we are able to find winning ticket\ninitializations for Transformers which enable models one-third the size to\nachieve nearly equivalent performance. Together, these results suggest that the\nlottery ticket hypothesis is not restricted to supervised learning of natural\nimages, but rather represents a broader phenomenon in DNNs.",
        "categories": [
            "stat.ML",
            "cs.AI",
            "cs.LG",
            "cs.NE"
        ],
        "link": "http://arxiv.org/pdf/1906.02768v3",
        "date": "2019-06-06 18:38:38+00:00"
    },
    {
        "title": "Deep Learning without Shortcuts: Shaping the Kernel with Tailored Rectifiers",
        "authors": [
            "Guodong Zhang",
            "Aleksandar Botev",
            "James Martens"
        ],
        "abstract": "Training very deep neural networks is still an extremely challenging task.\nThe common solution is to use shortcut connections and normalization layers,\nwhich are both crucial ingredients in the popular ResNet architecture. However,\nthere is strong evidence to suggest that ResNets behave more like ensembles of\nshallower networks than truly deep ones. Recently, it was shown that deep\nvanilla networks (i.e. networks without normalization layers or shortcut\nconnections) can be trained as fast as ResNets by applying certain\ntransformations to their activation functions. However, this method (called\nDeep Kernel Shaping) isn't fully compatible with ReLUs, and produces networks\nthat overfit significantly more than ResNets on ImageNet. In this work, we\nrectify this situation by developing a new type of transformation that is fully\ncompatible with a variant of ReLUs -- Leaky ReLUs. We show in experiments that\nour method, which introduces negligible extra computational cost, achieves\nvalidation accuracies with deep vanilla networks that are competitive with\nResNets (of the same width/depth), and significantly higher than those obtained\nwith the Edge of Chaos (EOC) method. And unlike with EOC, the validation\naccuracies we obtain do not get worse with depth.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2203.08120v1",
        "date": "2022-03-15 17:49:08+00:00"
    },
    {
        "title": "A Wide and Deep Neural Network for Survival Analysis from Anatomical Shape and Tabular Clinical Data",
        "authors": [
            "Sebastian P\u00f6lsterl",
            "Ignacio Sarasua",
            "Benjam\u00edn Guti\u00e9rrez-Becker",
            "Christian Wachinger"
        ],
        "abstract": "We introduce a wide and deep neural network for prediction of progression\nfrom patients with mild cognitive impairment to Alzheimer's disease.\nInformation from anatomical shape and tabular clinical data (demographics,\nbiomarkers) are fused in a single neural network. The network is invariant to\nshape transformations and avoids the need to identify point correspondences\nbetween shapes. To account for right censored time-to-event data, i.e., when it\nis only known that a patient did not develop Alzheimer's disease up to a\nparticular time point, we employ a loss commonly used in survival analysis. Our\nnetwork is trained end-to-end to combine information from a patient's\nhippocampus shape and clinical biomarkers. Our experiments on data from the\nAlzheimer's Disease Neuroimaging Initiative demonstrate that our proposed model\nis able to learn a shape descriptor that augments clinical biomarkers and\noutperforms a deep neural network on shape alone and a linear model on common\nclinical biomarkers.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1909.03890v1",
        "date": "2019-09-09 14:37:47+00:00"
    },
    {
        "title": "Probabilistic symmetries and invariant neural networks",
        "authors": [
            "Benjamin Bloem-Reddy",
            "Yee Whye Teh"
        ],
        "abstract": "Treating neural network inputs and outputs as random variables, we\ncharacterize the structure of neural networks that can be used to model data\nthat are invariant or equivariant under the action of a compact group. Much\nrecent research has been devoted to encoding invariance under symmetry\ntransformations into neural network architectures, in an effort to improve the\nperformance of deep neural networks in data-scarce, non-i.i.d., or unsupervised\nsettings. By considering group invariance from the perspective of probabilistic\nsymmetry, we establish a link between functional and probabilistic symmetry,\nand obtain generative functional representations of probability distributions\nthat are invariant or equivariant under the action of a compact group. Our\nrepresentations completely characterize the structure of neural networks that\ncan be used to model such distributions and yield a general program for\nconstructing invariant stochastic or deterministic neural networks. We\ndemonstrate that examples from the recent literature are special cases, and\ndevelop the details of the general program for exchangeable sequences and\narrays.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1901.06082v2",
        "date": "2019-01-18 04:32:14+00:00"
    },
    {
        "title": "What Happened to My Dog in That Network: Unraveling Top-down Generators in Convolutional Neural Networks",
        "authors": [
            "Patrick W. Gallagher",
            "Shuai Tang",
            "Zhuowen Tu"
        ],
        "abstract": "Top-down information plays a central role in human perception, but plays\nrelatively little role in many current state-of-the-art deep networks, such as\nConvolutional Neural Networks (CNNs). This work seeks to explore a path by\nwhich top-down information can have a direct impact within current deep\nnetworks. We explore this path by learning and using \"generators\" corresponding\nto the network internal effects of three types of transformation (each a\nrestriction of a general affine transformation): rotation, scaling, and\ntranslation. We demonstrate how these learned generators can be used to\ntransfer top-down information to novel settings, as mediated by the \"feature\nflows\" that the transformations (and the associated generators) correspond to\ninside the network. Specifically, we explore three aspects: 1) using generators\nas part of a method for synthesizing transformed images --- given a previously\nunseen image, produce versions of that image corresponding to one or more\nspecified transformations, 2) \"zero-shot learning\" --- when provided with a\nfeature flow corresponding to the effect of a transformation of unknown amount,\nleverage learned generators as part of a method by which to perform an accurate\ncategorization of the amount of transformation, even for amounts never observed\nduring training, and 3) (inside-CNN) \"data augmentation\" --- improve the\nclassification performance of an existing network by using the learned\ngenerators to directly provide additional training \"inside the CNN\".",
        "categories": [
            "cs.NE",
            "cs.CV",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1511.07125v1",
        "date": "2015-11-23 07:48:01+00:00"
    },
    {
        "title": "On the Global Convergence of Training Deep Linear ResNets",
        "authors": [
            "Difan Zou",
            "Philip M. Long",
            "Quanquan Gu"
        ],
        "abstract": "We study the convergence of gradient descent (GD) and stochastic gradient\ndescent (SGD) for training $L$-hidden-layer linear residual networks (ResNets).\nWe prove that for training deep residual networks with certain linear\ntransformations at input and output layers, which are fixed throughout\ntraining, both GD and SGD with zero initialization on all hidden weights can\nconverge to the global minimum of the training loss. Moreover, when\nspecializing to appropriate Gaussian random linear transformations, GD and SGD\nprovably optimize wide enough deep linear ResNets. Compared with the global\nconvergence result of GD for training standard deep linear networks (Du & Hu\n2019), our condition on the neural network width is sharper by a factor of\n$O(\\kappa L)$, where $\\kappa$ denotes the condition number of the covariance\nmatrix of the training data. We further propose a modified identity input and\noutput transformations, and show that a $(d+k)$-wide neural network is\nsufficient to guarantee the global convergence of GD/SGD, where $d,k$ are the\ninput and output dimensions respectively.",
        "categories": [
            "cs.LG",
            "math.OC",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2003.01094v1",
        "date": "2020-03-02 18:34:49+00:00"
    },
    {
        "title": "Neural Transformation Learning for Deep Anomaly Detection Beyond Images",
        "authors": [
            "Chen Qiu",
            "Timo Pfrommer",
            "Marius Kloft",
            "Stephan Mandt",
            "Maja Rudolph"
        ],
        "abstract": "Data transformations (e.g. rotations, reflections, and cropping) play an\nimportant role in self-supervised learning. Typically, images are transformed\ninto different views, and neural networks trained on tasks involving these\nviews produce useful feature representations for downstream tasks, including\nanomaly detection. However, for anomaly detection beyond image data, it is\noften unclear which transformations to use. Here we present a simple end-to-end\nprocedure for anomaly detection with learnable transformations. The key idea is\nto embed the transformed data into a semantic space such that the transformed\ndata still resemble their untransformed form, while different transformations\nare easily distinguishable. Extensive experiments on time series demonstrate\nthat our proposed method outperforms existing approaches in the one-vs.-rest\nsetting and is competitive in the more challenging n-vs.-rest anomaly detection\ntask. On tabular datasets from the medical and cyber-security domains, our\nmethod learns domain-specific transformations and detects anomalies more\naccurately than previous work.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2103.16440v4",
        "date": "2021-03-30 15:38:18+00:00"
    },
    {
        "title": "Topographic VAEs learn Equivariant Capsules",
        "authors": [
            "T. Anderson Keller",
            "Max Welling"
        ],
        "abstract": "In this work we seek to bridge the concepts of topographic organization and\nequivariance in neural networks. To accomplish this, we introduce the\nTopographic VAE: a novel method for efficiently training deep generative models\nwith topographically organized latent variables. We show that such a model\nindeed learns to organize its activations according to salient characteristics\nsuch as digit class, width, and style on MNIST. Furthermore, through\ntopographic organization over time (i.e. temporal coherence), we demonstrate\nhow predefined latent space transformation operators can be encouraged for\nobserved transformed input sequences -- a primitive form of unsupervised\nlearned equivariance. We demonstrate that this model successfully learns sets\nof approximately equivariant features (i.e. \"capsules\") directly from sequences\nand achieves higher likelihood on correspondingly transforming test sequences.\nEquivariance is verified quantitatively by measuring the approximate\ncommutativity of the inference network and the sequence transformations.\nFinally, we demonstrate approximate equivariance to complex transformations,\nexpanding upon the capabilities of existing group equivariant neural networks.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.NE"
        ],
        "link": "http://arxiv.org/pdf/2109.01394v2",
        "date": "2021-09-03 09:25:57+00:00"
    },
    {
        "title": "Dual-label Deep LSTM Dereverberation For Speaker Verification",
        "authors": [
            "Hao Zhang",
            "Stephen Zahorian",
            "Xiao Chen",
            "Peter Guzewich",
            "Xiaoyu Liu"
        ],
        "abstract": "In this paper, we present a reverberation removal approach for speaker\nverification, utilizing dual-label deep neural networks (DNNs). The networks\nperform feature mapping between the spectral features of reverberant and clean\nspeech. Long short term memory recurrent neural networks (LSTMs) are trained to\nmap corrupted Mel filterbank (MFB) features to two sets of labels: i) the clean\nMFB features, and ii) either estimated pitch tracks or the fast Fourier\ntransform (FFT) spectrogram of clean speech. The performance of reverberation\nremoval is evaluated by equal error rates (EERs) of speaker verification\nexperiments.",
        "categories": [
            "eess.AS",
            "cs.LG",
            "cs.SD",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1809.03868v1",
        "date": "2018-09-08 04:55:24+00:00"
    },
    {
        "title": "AcceRL: Policy Acceleration Framework for Deep Reinforcement Learning",
        "authors": [
            "Hongjie Zhang"
        ],
        "abstract": "Deep reinforcement learning has achieved great success in various fields with\nits super decision-making ability. However, the policy learning process\nrequires a large amount of training time, causing energy consumption. Inspired\nby the redundancy of neural networks, we propose a lightweight parallel\ntraining framework based on neural network compression, AcceRL, to accelerate\nthe policy learning while ensuring policy quality. Specifically, AcceRL speeds\nup the experience collection by flexibly combining various neural network\ncompression methods. Overall, the AcceRL consists of five components, namely\nActor, Learner, Compressor, Corrector, and Monitor. The Actor uses the\nCompressor to compress the Learner's policy network to interact with the\nenvironment. And the generated experiences are transformed by the Corrector\nwith Off-Policy methods, such as V-trace, Retrace and so on. Then the corrected\nexperiences are feed to the Learner for policy learning. We believe this is the\nfirst general reinforcement learning framework that incorporates multiple\nneural network compression techniques. Extensive experiments conducted in gym\nshow that the AcceRL reduces the time cost of the actor by about 2.0 X to 4.13\nX compared to the traditional methods. Furthermore, the AcceRL reduces the\nwhole training time by about 29.8% to 40.3% compared to the traditional methods\nwhile keeps the same policy quality.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2211.15023v1",
        "date": "2022-11-28 03:12:11+00:00"
    },
    {
        "title": "Coordination Among Neural Modules Through a Shared Global Workspace",
        "authors": [
            "Anirudh Goyal",
            "Aniket Didolkar",
            "Alex Lamb",
            "Kartikeya Badola",
            "Nan Rosemary Ke",
            "Nasim Rahaman",
            "Jonathan Binas",
            "Charles Blundell",
            "Michael Mozer",
            "Yoshua Bengio"
        ],
        "abstract": "Deep learning has seen a movement away from representing examples with a\nmonolithic hidden state towards a richly structured state. For example,\nTransformers segment by position, and object-centric architectures decompose\nimages into entities. In all these architectures, interactions between\ndifferent elements are modeled via pairwise interactions: Transformers make use\nof self-attention to incorporate information from other positions;\nobject-centric architectures make use of graph neural networks to model\ninteractions among entities. However, pairwise interactions may not achieve\nglobal coordination or a coherent, integrated representation that can be used\nfor downstream tasks. In cognitive science, a global workspace architecture has\nbeen proposed in which functionally specialized components share information\nthrough a common, bandwidth-limited communication channel. We explore the use\nof such a communication channel in the context of deep learning for modeling\nthe structure of complex environments. The proposed method includes a shared\nworkspace through which communication among different specialist modules takes\nplace but due to limits on the communication bandwidth, specialist modules must\ncompete for access. We show that capacity limitations have a rational basis in\nthat (1) they encourage specialization and compositionality and (2) they\nfacilitate the synchronization of otherwise independent specialists.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2103.01197v2",
        "date": "2021-03-01 18:43:48+00:00"
    },
    {
        "title": "Stein Neural Sampler",
        "authors": [
            "Tianyang Hu",
            "Zixiang Chen",
            "Hanxi Sun",
            "Jincheng Bai",
            "Mao Ye",
            "Guang Cheng"
        ],
        "abstract": "We propose two novel samplers to generate high-quality samples from a given\n(un-normalized) probability density. Motivated by the success of generative\nadversarial networks, we construct our samplers using deep neural networks that\ntransform a reference distribution to the target distribution. Training schemes\nare developed to minimize two variations of the Stein discrepancy, which is\ndesigned to work with un-normalized densities. Once trained, our samplers are\nable to generate samples instantaneously. We show that the proposed methods are\ntheoretically sound and experience fewer convergence issues compared with\ntraditional sampling approaches according to our empirical studies.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1810.03545v2",
        "date": "2018-10-08 16:06:40+00:00"
    },
    {
        "title": "Deep Manifold Transformation for Nonlinear Dimensionality Reduction",
        "authors": [
            "Stan Z. Li",
            "Zelin Zang",
            "Lirong Wu"
        ],
        "abstract": "Manifold learning-based encoders have been playing important roles in\nnonlinear dimensionality reduction (NLDR) for data exploration. However,\nexisting methods can often fail to preserve geometric, topological and/or\ndistributional structures of data. In this paper, we propose a deep manifold\nlearning framework, called deep manifold transformation (DMT) for unsupervised\nNLDR and embedding learning. DMT enhances deep neural networks by using\ncross-layer local geometry-preserving (LGP) constraints. The LGP constraints\nconstitute the loss for deep manifold learning and serve as geometric\nregularizers for NLDR network training. Extensive experiments on synthetic and\nreal-world data demonstrate that DMT networks outperform existing leading\nmanifold-based NLDR methods in terms of preserving the structures of data.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "cs.HC",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2010.14831v3",
        "date": "2020-10-28 09:09:41+00:00"
    },
    {
        "title": "Deep Signature Transforms",
        "authors": [
            "Patric Bonnier",
            "Patrick Kidger",
            "Imanol Perez Arribas",
            "Cristopher Salvi",
            "Terry Lyons"
        ],
        "abstract": "The signature is an infinite graded sequence of statistics known to\ncharacterise a stream of data up to a negligible equivalence class. It is a\ntransform which has previously been treated as a fixed feature transformation,\non top of which a model may be built. We propose a novel approach which\ncombines the advantages of the signature transform with modern deep learning\nframeworks. By learning an augmentation of the stream prior to the signature\ntransform, the terms of the signature may be selected in a data-dependent way.\nMore generally, we describe how the signature transform may be used as a layer\nanywhere within a neural network. In this context it may be interpreted as a\npooling operation. We present the results of empirical experiments to back up\nthe theoretical justification. Code available at\nhttps://github.com/patrick-kidger/Deep-Signature-Transforms.",
        "categories": [
            "cs.LG",
            "stat.ML",
            "68T01"
        ],
        "link": "http://arxiv.org/pdf/1905.08494v2",
        "date": "2019-05-21 08:39:55+00:00"
    },
    {
        "title": "ProjectionNet: Learning Efficient On-Device Deep Networks Using Neural Projections",
        "authors": [
            "Sujith Ravi"
        ],
        "abstract": "Deep neural networks have become ubiquitous for applications related to\nvisual recognition and language understanding tasks. However, it is often\nprohibitive to use typical neural networks on devices like mobile phones or\nsmart watches since the model sizes are huge and cannot fit in the limited\nmemory available on such devices. While these devices could make use of machine\nlearning models running on high-performance data centers with CPUs or GPUs,\nthis is not feasible for many applications because data can be privacy\nsensitive and inference needs to be performed directly \"on\" device.\n  We introduce a new architecture for training compact neural networks using a\njoint optimization framework. At its core lies a novel objective that jointly\ntrains using two different types of networks--a full trainer neural network\n(using existing architectures like Feed-forward NNs or LSTM RNNs) combined with\na simpler \"projection\" network that leverages random projections to transform\ninputs or intermediate representations into bits. The simpler network encodes\nlightweight and efficient-to-compute operations in bit space with a low memory\nfootprint. The two networks are trained jointly using backpropagation, where\nthe projection network learns from the full network similar to apprenticeship\nlearning. Once trained, the smaller network can be used directly for inference\nat low memory and computation cost. We demonstrate the effectiveness of the new\napproach at significantly shrinking the memory requirements of different types\nof neural networks while preserving good accuracy on visual recognition and\ntext classification tasks. We also study the question \"how many neural bits are\nrequired to solve a given task?\" using the new framework and show empirical\nresults contrasting model predictive capacity (in bits) versus accuracy on\nseveral datasets.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.NE"
        ],
        "link": "http://arxiv.org/pdf/1708.00630v2",
        "date": "2017-08-02 07:58:45+00:00"
    },
    {
        "title": "Intrinsic dimension of data representations in deep neural networks",
        "authors": [
            "Alessio Ansuini",
            "Alessandro Laio",
            "Jakob H. Macke",
            "Davide Zoccolan"
        ],
        "abstract": "Deep neural networks progressively transform their inputs across multiple\nprocessing layers. What are the geometrical properties of the representations\nlearned by these networks? Here we study the intrinsic dimensionality (ID) of\ndata-representations, i.e. the minimal number of parameters needed to describe\na representation. We find that, in a trained network, the ID is orders of\nmagnitude smaller than the number of units in each layer. Across layers, the ID\nfirst increases and then progressively decreases in the final layers.\nRemarkably, the ID of the last hidden layer predicts classification accuracy on\nthe test set. These results can neither be found by linear dimensionality\nestimates (e.g., with principal component analysis), nor in representations\nthat had been artificially linearized. They are neither found in untrained\nnetworks, nor in networks that are trained on randomized labels. This suggests\nthat neural networks that can generalize are those that transform the data into\nlow-dimensional, but not necessarily flat manifolds.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1905.12784v2",
        "date": "2019-05-29 23:36:34+00:00"
    },
    {
        "title": "Quantised Transforming Auto-Encoders: Achieving Equivariance to Arbitrary Transformations in Deep Networks",
        "authors": [
            "Jianbo Jiao",
            "Jo\u00e3o F. Henriques"
        ],
        "abstract": "In this work we investigate how to achieve equivariance to input\ntransformations in deep networks, purely from data, without being given a model\nof those transformations. Convolutional Neural Networks (CNNs), for example,\nare equivariant to image translation, a transformation that can be easily\nmodelled (by shifting the pixels vertically or horizontally). Other\ntransformations, such as out-of-plane rotations, do not admit a simple analytic\nmodel. We propose an auto-encoder architecture whose embedding obeys an\narbitrary set of equivariance relations simultaneously, such as translation,\nrotation, colour changes, and many others. This means that it can take an input\nimage, and produce versions transformed by a given amount that were not\nobserved before (e.g. a different point of view of the same object, or a colour\nvariation). Despite extending to many (even non-geometric) transformations, our\nmodel reduces exactly to a CNN in the special case of translation-equivariance.\nEquivariances are important for the interpretability and robustness of deep\nnetworks, and we demonstrate results of successful re-rendering of transformed\nversions of input images on several synthetic and real datasets, as well as\nresults on object pose estimation.",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2111.12873v1",
        "date": "2021-11-25 02:26:38+00:00"
    },
    {
        "title": "Bandlimiting Neural Networks Against Adversarial Attacks",
        "authors": [
            "Yuping Lin",
            "Kasra Ahmadi K. A.",
            "Hui Jiang"
        ],
        "abstract": "In this paper, we study the adversarial attack and defence problem in deep\nlearning from the perspective of Fourier analysis. We first explicitly compute\nthe Fourier transform of deep ReLU neural networks and show that there exist\ndecaying but non-zero high frequency components in the Fourier spectrum of\nneural networks. We demonstrate that the vulnerability of neural networks\ntowards adversarial samples can be attributed to these insignificant but\nnon-zero high frequency components. Based on this analysis, we propose to use a\nsimple post-averaging technique to smooth out these high frequency components\nto improve the robustness of neural networks against adversarial attacks.\nExperimental results on the ImageNet dataset have shown that our proposed\nmethod is universally effective to defend many existing adversarial attacking\nmethods proposed in the literature, including FGSM, PGD, DeepFool and C&W\nattacks. Our post-averaging method is simple since it does not require any\nre-training, and meanwhile it can successfully defend over 95% of the\nadversarial samples generated by these methods without introducing any\nsignificant performance degradation (less than 1%) on the original clean\nimages.",
        "categories": [
            "cs.LG",
            "cs.CR",
            "cs.CV",
            "cs.NE",
            "stat.ML",
            "I.1.5"
        ],
        "link": "http://arxiv.org/pdf/1905.12797v1",
        "date": "2019-05-30 00:34:50+00:00"
    },
    {
        "title": "Forward Thinking: Building and Training Neural Networks One Layer at a Time",
        "authors": [
            "Chris Hettinger",
            "Tanner Christensen",
            "Ben Ehlert",
            "Jeffrey Humpherys",
            "Tyler Jarvis",
            "Sean Wade"
        ],
        "abstract": "We present a general framework for training deep neural networks without\nbackpropagation. This substantially decreases training time and also allows for\nconstruction of deep networks with many sorts of learners, including networks\nwhose layers are defined by functions that are not easily differentiated, like\ndecision trees. The main idea is that layers can be trained one at a time, and\nonce they are trained, the input data are mapped forward through the layer to\ncreate a new learning problem. The process is repeated, transforming the data\nthrough multiple layers, one at a time, rendering a new data set, which is\nexpected to be better behaved, and on which a final output layer can achieve\ngood performance. We call this forward thinking and demonstrate a proof of\nconcept by achieving state-of-the-art accuracy on the MNIST dataset for\nconvolutional neural networks. We also provide a general mathematical\nformulation of forward thinking that allows for other types of deep learning\nproblems to be considered.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1706.02480v1",
        "date": "2017-06-08 08:53:00+00:00"
    },
    {
        "title": "Two geometric input transformation methods for fast online reinforcement learning with neural nets",
        "authors": [
            "Sina Ghiassian",
            "Huizhen Yu",
            "Banafsheh Rafiee",
            "Richard S. Sutton"
        ],
        "abstract": "We apply neural nets with ReLU gates in online reinforcement learning. Our\ngoal is to train these networks in an incremental manner, without the\ncomputationally expensive experience replay. By studying how individual neural\nnodes behave in online training, we recognize that the global nature of ReLU\ngates can cause undesirable learning interference in each node's learning\nbehavior. We propose reducing such interferences with two efficient input\ntransformation methods that are geometric in nature and match well the\ngeometric property of ReLU gates. The first one is tile coding, a classic\nbinary encoding scheme originally designed for local generalization based on\nthe topological structure of the input space. The second one (EmECS) is a new\nmethod we introduce; it is based on geometric properties of convex sets and\ntopological embedding of the input space into the boundary of a convex set. We\ndiscuss the behavior of the network when it operates on the transformed inputs.\nWe also compare it experimentally with some neural nets that do not use the\nsame input transformations, and with the classic algorithm of tile coding plus\na linear function approximator, and on several online reinforcement learning\ntasks, we show that the neural net with tile coding or EmECS can achieve not\nonly faster learning but also more accurate approximations. Our results\nstrongly suggest that geometric input transformation of this type can be\neffective for interference reduction and takes us a step closer to fully\nincremental reinforcement learning with neural nets.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1805.07476v2",
        "date": "2018-05-18 23:35:14+00:00"
    },
    {
        "title": "An Adaptive and Momental Bound Method for Stochastic Learning",
        "authors": [
            "Jianbang Ding",
            "Xuancheng Ren",
            "Ruixuan Luo",
            "Xu Sun"
        ],
        "abstract": "Training deep neural networks requires intricate initialization and careful\nselection of learning rates. The emergence of stochastic gradient optimization\nmethods that use adaptive learning rates based on squared past gradients, e.g.,\nAdaGrad, AdaDelta, and Adam, eases the job slightly. However, such methods have\nalso been proven problematic in recent studies with their own pitfalls\nincluding non-convergence issues and so on. Alternative variants have been\nproposed for enhancement, such as AMSGrad, AdaShift and AdaBound. In this work,\nwe identify a new problem of adaptive learning rate methods that exhibits at\nthe beginning of learning where Adam produces extremely large learning rates\nthat inhibit the start of learning. We propose the Adaptive and Momental Bound\n(AdaMod) method to restrict the adaptive learning rates with adaptive and\nmomental upper bounds. The dynamic learning rate bounds are based on the\nexponential moving averages of the adaptive learning rates themselves, which\nsmooth out unexpected large learning rates and stabilize the training of deep\nneural networks. Our experiments verify that AdaMod eliminates the extremely\nlarge learning rates throughout the training and brings significant\nimprovements especially on complex networks such as DenseNet and Transformer,\ncompared to Adam. Our implementation is available at:\nhttps://github.com/lancopku/AdaMod",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1910.12249v1",
        "date": "2019-10-27 12:22:08+00:00"
    },
    {
        "title": "Elimination of All Bad Local Minima in Deep Learning",
        "authors": [
            "Kenji Kawaguchi",
            "Leslie Pack Kaelbling"
        ],
        "abstract": "In this paper, we theoretically prove that adding one special neuron per\noutput unit eliminates all suboptimal local minima of any deep neural network,\nfor multi-class classification, binary classification, and regression with an\narbitrary loss function, under practical assumptions. At every local minimum of\nany deep neural network with these added neurons, the set of parameters of the\noriginal neural network (without added neurons) is guaranteed to be a global\nminimum of the original neural network. The effects of the added neurons are\nproven to automatically vanish at every local minimum. Moreover, we provide a\nnovel theoretical characterization of a failure mode of eliminating suboptimal\nlocal minima via an additional theorem and several examples. This paper also\nintroduces a novel proof technique based on the perturbable gradient basis\n(PGB) necessary condition of local minima, which provides new insight into the\nelimination of local minima and is applicable to analyze various models and\ntransformations of objective functions beyond the elimination of local minima.",
        "categories": [
            "cs.LG",
            "cs.NE",
            "math.OC",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1901.00279v2",
        "date": "2019-01-02 06:40:36+00:00"
    },
    {
        "title": "Neural Contextual Bandits with Deep Representation and Shallow Exploration",
        "authors": [
            "Pan Xu",
            "Zheng Wen",
            "Handong Zhao",
            "Quanquan Gu"
        ],
        "abstract": "We study a general class of contextual bandits, where each context-action\npair is associated with a raw feature vector, but the reward generating\nfunction is unknown. We propose a novel learning algorithm that transforms the\nraw feature vector using the last hidden layer of a deep ReLU neural network\n(deep representation learning), and uses an upper confidence bound (UCB)\napproach to explore in the last linear layer (shallow exploration). We prove\nthat under standard assumptions, our proposed algorithm achieves\n$\\tilde{O}(\\sqrt{T})$ finite-time regret, where $T$ is the learning time\nhorizon. Compared with existing neural contextual bandit algorithms, our\napproach is computationally much more efficient since it only needs to explore\nin the last layer of the deep neural network.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2012.01780v1",
        "date": "2020-12-03 09:17:55+00:00"
    },
    {
        "title": "Neural Networks, Hypersurfaces, and Radon Transforms",
        "authors": [
            "Soheil Kolouri",
            "Xuwang Yin",
            "Gustavo K. Rohde"
        ],
        "abstract": "Connections between integration along hypersufaces, Radon transforms, and\nneural networks are exploited to highlight an integral geometric mathematical\ninterpretation of neural networks. By analyzing the properties of neural\nnetworks as operators on probability distributions for observed data, we show\nthat the distribution of outputs for any node in a neural network can be\ninterpreted as a nonlinear projection along hypersurfaces defined by level\nsurfaces over the input data space. We utilize these descriptions to provide\nnew interpretation for phenomena such as nonlinearity, pooling, activation\nfunctions, and adversarial examples in neural network-based learning problems.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1907.02220v1",
        "date": "2019-07-04 05:01:14+00:00"
    },
    {
        "title": "Effective Theory of Transformers at Initialization",
        "authors": [
            "Emily Dinan",
            "Sho Yaida",
            "Susan Zhang"
        ],
        "abstract": "We perform an effective-theory analysis of forward-backward signal\npropagation in wide and deep Transformers, i.e., residual neural networks with\nmulti-head self-attention blocks and multilayer perceptron blocks. This\nanalysis suggests particular width scalings of initialization and training\nhyperparameters for these models. We then take up such suggestions, training\nVision and Language Transformers in practical setups.",
        "categories": [
            "cs.LG",
            "cs.CL",
            "hep-th",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2304.02034v1",
        "date": "2023-04-04 18:00:01+00:00"
    },
    {
        "title": "Identity Matters in Deep Learning",
        "authors": [
            "Moritz Hardt",
            "Tengyu Ma"
        ],
        "abstract": "An emerging design principle in deep learning is that each layer of a deep\nartificial neural network should be able to easily express the identity\ntransformation. This idea not only motivated various normalization techniques,\nsuch as \\emph{batch normalization}, but was also key to the immense success of\n\\emph{residual networks}.\n  In this work, we put the principle of \\emph{identity parameterization} on a\nmore solid theoretical footing alongside further empirical progress. We first\ngive a strikingly simple proof that arbitrarily deep linear residual networks\nhave no spurious local optima. The same result for linear feed-forward networks\nin their standard parameterization is substantially more delicate. Second, we\nshow that residual networks with ReLu activations have universal finite-sample\nexpressivity in the sense that the network can represent any function of its\nsample provided that the model has more parameters than the sample size.\n  Directly inspired by our theory, we experiment with a radically simple\nresidual architecture consisting of only residual convolutional layers and ReLu\nactivations, but no batch normalization, dropout, or max pool. Our model\nimproves significantly on previous all-convolutional networks on the CIFAR10,\nCIFAR100, and ImageNet classification benchmarks.",
        "categories": [
            "cs.LG",
            "cs.NE",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1611.04231v3",
        "date": "2016-11-14 02:44:18+00:00"
    },
    {
        "title": "Fourier Sensitivity and Regularization of Computer Vision Models",
        "authors": [
            "Kiran Krishnamachari",
            "See-Kiong Ng",
            "Chuan-Sheng Foo"
        ],
        "abstract": "Recent work has empirically shown that deep neural networks latch on to the\nFourier statistics of training data and show increased sensitivity to\nFourier-basis directions in the input. Understanding and modifying this\nFourier-sensitivity of computer vision models may help improve their\nrobustness. Hence, in this paper we study the frequency sensitivity\ncharacteristics of deep neural networks using a principled approach. We first\npropose a basis trick, proving that unitary transformations of the\ninput-gradient of a function can be used to compute its gradient in the basis\ninduced by the transformation. Using this result, we propose a general measure\nof any differentiable model's Fourier-sensitivity using the unitary\nFourier-transform of its input-gradient. When applied to deep neural networks,\nwe find that computer vision models are consistently sensitive to particular\nfrequencies dependent on the dataset, training method and architecture. Based\non this measure, we further propose a Fourier-regularization framework to\nmodify the Fourier-sensitivities and frequency bias of models. Using our\nproposed regularizer-family, we demonstrate that deep neural networks obtain\nimproved classification accuracy on robustness evaluations.",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2301.13514v1",
        "date": "2023-01-31 10:05:35+00:00"
    },
    {
        "title": "Neural Time Series Analysis with Fourier Transform: A Survey",
        "authors": [
            "Kun Yi",
            "Qi Zhang",
            "Shoujin Wang",
            "Hui He",
            "Guodong Long",
            "Zhendong Niu"
        ],
        "abstract": "Recently, Fourier transform has been widely introduced into deep neural\nnetworks to further advance the state-of-the-art regarding both accuracy and\nefficiency of time series analysis. The advantages of the Fourier transform for\ntime series analysis, such as efficiency and global view, have been rapidly\nexplored and exploited, exhibiting a promising deep learning paradigm for time\nseries analysis. However, although increasing attention has been attracted and\nresearch is flourishing in this emerging area, there lacks a systematic review\nof the variety of existing studies in the area. To this end, in this paper, we\nprovide a comprehensive review of studies on neural time series analysis with\nFourier transform. We aim to systematically investigate and summarize the\nlatest research progress. Accordingly, we propose a novel taxonomy to\ncategorize existing neural time series analysis methods from four perspectives,\nincluding characteristics, usage paradigms, network design, and applications.\nWe also share some new research directions in this vibrant area.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2302.02173v1",
        "date": "2023-02-04 14:33:07+00:00"
    },
    {
        "title": "Applying Cyclical Learning Rate to Neural Machine Translation",
        "authors": [
            "Choon Meng Lee",
            "Jianfeng Liu",
            "Wei Peng"
        ],
        "abstract": "In training deep learning networks, the optimizer and related learning rate\nare often used without much thought or with minimal tuning, even though it is\ncrucial in ensuring a fast convergence to a good quality minimum of the loss\nfunction that can also generalize well on the test dataset. Drawing inspiration\nfrom the successful application of cyclical learning rate policy for computer\nvision related convolutional networks and datasets, we explore how cyclical\nlearning rate can be applied to train transformer-based neural networks for\nneural machine translation. From our carefully designed experiments, we show\nthat the choice of optimizers and the associated cyclical learning rate policy\ncan have a significant impact on the performance. In addition, we establish\nguidelines when applying cyclical learning rates to neural machine translation\ntasks. Thus with our work, we hope to raise awareness of the importance of\nselecting the right optimizers and the accompanying learning rate policy, at\nthe same time, encourage further research into easy-to-use learning rate\npolicies.",
        "categories": [
            "cs.LG",
            "cs.CL",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2004.02401v1",
        "date": "2020-04-06 04:45:49+00:00"
    },
    {
        "title": "Transformations between deep neural networks",
        "authors": [
            "Tom Bertalan",
            "Felix Dietrich",
            "Ioannis G. Kevrekidis"
        ],
        "abstract": "We propose to test, and when possible establish, an equivalence between two\ndifferent artificial neural networks by attempting to construct a data-driven\ntransformation between them, using manifold-learning techniques. In particular,\nwe employ diffusion maps with a Mahalanobis-like metric. If the construction\nsucceeds, the two networks can be thought of as belonging to the same\nequivalence class.\n  We first discuss transformation functions between only the outputs of the two\nnetworks; we then also consider transformations that take into account outputs\n(activations) of a number of internal neurons from each network. In general,\nWhitney's theorem dictates the number of measurements from one of the networks\nrequired to reconstruct each and every feature of the second network. The\nconstruction of the transformation function relies on a consistent, intrinsic\nrepresentation of the network input space.\n  We illustrate our algorithm by matching neural network pairs trained to learn\n(a) observations of scalar functions; (b) observations of two-dimensional\nvector fields; and (c) representations of images of a moving three-dimensional\nobject (a rotating horse). The construction of such equivalence classes across\ndifferent network instantiations clearly relates to transfer learning. We also\nexpect that it will be valuable in establishing equivalence between different\nMachine Learning-based models of the same phenomenon observed through different\ninstruments and by different research groups.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2007.05646v3",
        "date": "2020-07-10 23:32:12+00:00"
    },
    {
        "title": "Investigating the Compositional Structure Of Deep Neural Networks",
        "authors": [
            "Francesco Craighero",
            "Fabrizio Angaroni",
            "Alex Graudenzi",
            "Fabio Stella",
            "Marco Antoniotti"
        ],
        "abstract": "The current understanding of deep neural networks can only partially explain\nhow input structure, network parameters and optimization algorithms jointly\ncontribute to achieve the strong generalization power that is typically\nobserved in many real-world applications. In order to improve the comprehension\nand interpretability of deep neural networks, we here introduce a novel\ntheoretical framework based on the compositional structure of piecewise linear\nactivation functions. By defining a direct acyclic graph representing the\ncomposition of activation patterns through the network layers, it is possible\nto characterize the instances of the input data with respect to both the\npredicted label and the specific (linear) transformation used to perform\npredictions. Preliminary tests on the MNIST dataset show that our method can\ngroup input instances with regard to their similarity in the internal\nrepresentation of the neural network, providing an intuitive measure of input\ncomplexity.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2002.06967v1",
        "date": "2020-02-17 14:16:17+00:00"
    },
    {
        "title": "Deep Network Classification by Scattering and Homotopy Dictionary Learning",
        "authors": [
            "John Zarka",
            "Louis Thiry",
            "Tom\u00e1s Angles",
            "St\u00e9phane Mallat"
        ],
        "abstract": "We introduce a sparse scattering deep convolutional neural network, which\nprovides a simple model to analyze properties of deep representation learning\nfor classification. Learning a single dictionary matrix with a classifier\nyields a higher classification accuracy than AlexNet over the ImageNet 2012\ndataset. The network first applies a scattering transform that linearizes\nvariabilities due to geometric transformations such as translations and small\ndeformations. A sparse $\\ell^1$ dictionary coding reduces intra-class\nvariability while preserving class separation through projections over unions\nof linear spaces. It is implemented in a deep convolutional network with a\nhomotopy algorithm having an exponential convergence. A convergence proof is\ngiven in a general framework that includes ALISTA. Classification results are\nanalyzed on ImageNet.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1910.03561v3",
        "date": "2019-10-08 17:47:44+00:00"
    },
    {
        "title": "Geometric Wavelet Scattering Networks on Compact Riemannian Manifolds",
        "authors": [
            "Michael Perlmutter",
            "Feng Gao",
            "Guy Wolf",
            "Matthew Hirn"
        ],
        "abstract": "The Euclidean scattering transform was introduced nearly a decade ago to\nimprove the mathematical understanding of convolutional neural networks.\nInspired by recent interest in geometric deep learning, which aims to\ngeneralize convolutional neural networks to manifold and graph-structured\ndomains, we define a geometric scattering transform on manifolds. Similar to\nthe Euclidean scattering transform, the geometric scattering transform is based\non a cascade of wavelet filters and pointwise nonlinearities. It is invariant\nto local isometries and stable to certain types of diffeomorphisms. Empirical\nresults demonstrate its utility on several geometric learning tasks. Our\nresults generalize the deformation stability and local translation invariance\nof Euclidean scattering, and demonstrate the importance of linking the used\nfilter structures to the underlying geometry of the data.",
        "categories": [
            "stat.ML",
            "cs.CV",
            "cs.LG",
            "math.FA"
        ],
        "link": "http://arxiv.org/pdf/1905.10448v3",
        "date": "2019-05-24 21:19:04+00:00"
    },
    {
        "title": "Transformer and Snowball Graph Convolution Learning for Biomedical Graph Classification",
        "authors": [
            "Jinlong Hu",
            "Yangmin Huang",
            "Shoubin Dong"
        ],
        "abstract": "Graph or network has been widely used for describing and modeling complex\nsystems in biomedicine. Deep learning methods, especially graph neural networks\n(GNNs), have been developed to learn and predict with such structured data. In\nthis paper, we proposed a novel transformer and snowball encoding networks\n(TSEN) for biomedical graph classification, which introduced transformer\narchitecture with graph snowball connection into GNNs for learning whole-graph\nrepresentation. TSEN combined graph snowball connection with graph transformer\nby snowball encoding layers, which enhanced the power to capture multi-scale\ninformation and global patterns to learn the whole-graph features. On the other\nhand, TSEN also used snowball graph convolution as position embedding in\ntransformer structure, which was a simple yet effective method for capturing\nlocal patterns naturally. Results of experiments using four graph\nclassification datasets demonstrated that TSEN outperformed the\nstate-of-the-art typical GNN models and the graph-transformer based GNN models.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2303.16132v1",
        "date": "2023-03-28 16:56:47+00:00"
    },
    {
        "title": "Neural Group Actions",
        "authors": [
            "Span Spanbauer",
            "Luke Sciarappa"
        ],
        "abstract": "We introduce an algorithm for designing Neural Group Actions, collections of\ndeep neural network architectures which model symmetric transformations\nsatisfying the laws of a given finite group. This generalizes involutive neural\nnetworks $\\mathcal{N}$, which satisfy $\\mathcal{N}(\\mathcal{N}(x))=x$ for any\ndata $x$, the group law of $\\mathbb{Z}_2$. We show how to optionally enforce an\nadditional constraint that the group action be volume-preserving. We\nconjecture, by analogy to a universality result for involutive neural networks,\nthat generative models built from Neural Group Actions are universal\napproximators for collections of probabilistic transitions adhering to the\ngroup laws. We demonstrate experimentally that a Neural Group Action for the\nquaternion group $Q_8$ can learn how a set of nonuniversal quantum gates\nsatisfying the $Q_8$ group laws act on single qubit quantum states.",
        "categories": [
            "stat.ML",
            "cs.LG",
            "cs.NE"
        ],
        "link": "http://arxiv.org/pdf/2010.03733v1",
        "date": "2020-10-08 02:27:05+00:00"
    },
    {
        "title": "Improving Deep Image Clustering With Spatial Transformer Layers",
        "authors": [
            "Thiago V. M. Souza",
            "Cleber Zanchettin"
        ],
        "abstract": "Image clustering is an important but challenging task in machine learning. As\nin most image processing areas, the latest improvements came from models based\non the deep learning approach. However, classical deep learning methods have\nproblems to deal with spatial image transformations like scale and rotation. In\nthis paper, we propose the use of visual attention techniques to reduce this\nproblem in image clustering methods. We evaluate the combination of a deep\nimage clustering model called Deep Adaptive Clustering (DAC) with the Spatial\nTransformer Networks (STN). The proposed model is evaluated in the datasets\nMNIST and FashionMNIST and outperformed the baseline model.",
        "categories": [
            "cs.CV",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1902.05401v2",
        "date": "2019-02-09 01:56:24+00:00"
    },
    {
        "title": "Wavelet Networks: Scale Equivariant Learning From Raw Waveforms",
        "authors": [
            "David W. Romero",
            "Erik J. Bekkers",
            "Jakub M. Tomczak",
            "Mark Hoogendoorn"
        ],
        "abstract": "Inducing symmetry equivariance in deep neural architectures has resolved into\nimproved data efficiency and generalization. In this work, we utilize the\nconcept of scale and translation equivariance to tackle the problem of learning\non time-series from raw waveforms. As a result, we obtain representations that\nlargely resemble those of the wavelet transform at the first layer, but that\nevolve into much more descriptive ones as a function of depth. Our empirical\nresults support the suitability of our Wavelet Networks which with a simple\narchitecture design perform consistently better than CNNs on raw waveforms and\non par with spectrogram-based methods.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2006.05259v1",
        "date": "2020-06-09 13:50:34+00:00"
    },
    {
        "title": "Bayesian Neural Network Versus Ex-Post Calibration For Prediction Uncertainty",
        "authors": [
            "Satya Borgohain",
            "Klaus Ackermann",
            "Ruben Loaiza-Maya"
        ],
        "abstract": "Probabilistic predictions from neural networks which account for predictive\nuncertainty during classification is crucial in many real-world and high-impact\ndecision making settings. However, in practice most datasets are trained on\nnon-probabilistic neural networks which by default do not capture this inherent\nuncertainty. This well-known problem has led to the development of post-hoc\ncalibration procedures, such as Platt scaling (logistic), isotonic and beta\ncalibration, which transforms the scores into well calibrated empirical\nprobabilities. A plausible alternative to the calibration approach is to use\nBayesian neural networks, which directly models a predictive distribution.\nAlthough they have been applied to images and text datasets, they have seen\nlimited adoption in the tabular and small data regime. In this paper, we\ndemonstrate that Bayesian neural networks yields competitive performance when\ncompared to calibrated neural networks and conduct experiments across a wide\narray of datasets.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "stat.ML",
            "I.2"
        ],
        "link": "http://arxiv.org/pdf/2209.14594v1",
        "date": "2022-09-29 07:22:19+00:00"
    },
    {
        "title": "Oracle-Preserving Latent Flows",
        "authors": [
            "Alexander Roman",
            "Roy T. Forestano",
            "Konstantin T. Matchev",
            "Katia Matcheva",
            "Eyup B. Unlu"
        ],
        "abstract": "We develop a deep learning methodology for the simultaneous discovery of\nmultiple nontrivial continuous symmetries across an entire labelled dataset.\nThe symmetry transformations and the corresponding generators are modeled with\nfully connected neural networks trained with a specially constructed loss\nfunction ensuring the desired symmetry properties. The two new elements in this\nwork are the use of a reduced-dimensionality latent space and the\ngeneralization to transformations invariant with respect to high-dimensional\noracles. The method is demonstrated with several examples on the MNIST digit\ndataset.",
        "categories": [
            "cs.LG",
            "hep-ph",
            "math.GR",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2302.00806v1",
        "date": "2023-02-02 00:13:32+00:00"
    },
    {
        "title": "Markov-Lipschitz Deep Learning",
        "authors": [
            "Stan Z. Li",
            "Zelin Zang",
            "Lirong Wu"
        ],
        "abstract": "We propose a novel framework, called Markov-Lipschitz deep learning (MLDL),\nto tackle geometric deterioration caused by collapse, twisting, or crossing in\nvector-based neural network transformations for manifold-based representation\nlearning and manifold data generation. A prior constraint, called locally\nisometric smoothness (LIS), is imposed across-layers and encoded into a Markov\nrandom field (MRF)-Gibbs distribution. This leads to the best possible\nsolutions for local geometry preservation and robustness as measured by locally\ngeometric distortion and locally bi-Lipschitz continuity. Consequently, the\nlayer-wise vector transformations are enhanced into well-behaved,\nLIS-constrained metric homeomorphisms. Extensive experiments, comparisons, and\nablation study demonstrate significant advantages of MLDL for manifold learning\nand manifold data generation. MLDL is general enough to enhance any vector\ntransformation-based networks. The code is available at\nhttps://github.com/westlake-cairi/Markov-Lipschitz-Deep-Learning.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2006.08256v5",
        "date": "2020-06-15 09:46:42+00:00"
    },
    {
        "title": "Deciphering the Language of Nature: A transformer-based language model for deleterious mutations in proteins",
        "authors": [
            "Theodore Jiang",
            "Li Fang",
            "Kai Wang"
        ],
        "abstract": "Various machine-learning models, including deep neural network models, have\nalready been developed to predict deleteriousness of missense (non-synonymous)\nmutations. Potential improvements to the current state of the art, however, may\nstill benefit from a fresh look at the biological problem using more\nsophisticated self-adaptive machine-learning approaches. Recent advances in the\nnatural language processing field show transformer models-a type of deep neural\nnetwork-to be particularly powerful at modeling sequence information with\ncontext dependence. In this study, we introduce MutFormer, a transformer-based\nmodel for the prediction of deleterious missense mutations, which uses\nreference and mutated protein sequences from the human genome as the primary\nfeatures. MutFormer takes advantage of a combination of self-attention layers\nand convolutional layers to learn both long-range and short-range dependencies\nbetween amino acid mutations in a protein sequence. In this study, we first\npre-trained MutFormer on reference protein sequences and mutated protein\nsequences resulting from common genetic variants observed in human populations.\nWe next examined different fine-tuning methods to successfully apply the model\nto deleteriousness prediction of missense mutations. Finally, we evaluated\nMutFormer's performance on multiple testing data sets. We found that MutFormer\nshowed similar or improved performance over a variety of existing tools,\nincluding those that used conventional machine-learning approaches. We conclude\nthat MutFormer successfully considers sequence features that are not explored\nin previous studies and could potentially complement existing computational\npredictions or empirically generated functional scores to improve our\nunderstanding of disease variants.",
        "categories": [
            "q-bio.GN",
            "cs.LG",
            "q-bio.QM"
        ],
        "link": "http://arxiv.org/pdf/2110.14746v4",
        "date": "2021-10-27 20:17:35+00:00"
    },
    {
        "title": "How deep is deep enough? -- Quantifying class separability in the hidden layers of deep neural networks",
        "authors": [
            "Achim Schilling",
            "Claus Metzner",
            "Jonas Rietsch",
            "Richard Gerum",
            "Holger Schulze",
            "Patrick Krauss"
        ],
        "abstract": "Deep neural networks typically outperform more traditional machine learning\nmodels in their ability to classify complex data, and yet is not clear how the\nindividual hidden layers of a deep network contribute to the overall\nclassification performance. We thus introduce a Generalized Discrimination\nValue (GDV) that measures, in a non-invasive manner, how well different data\nclasses separate in each given network layer. The GDV can be used for the\nautomatic tuning of hyper-parameters, such as the width profile and the total\ndepth of a network. Moreover, the layer-dependent GDV(L) provides new insights\ninto the data transformations that self-organize during training: In the case\nof multi-layer perceptrons trained with error backpropagation, we find that\nclassification of highly complex data sets requires a temporal {\\em reduction}\nof class separability, marked by a characteristic 'energy barrier' in the\ninitial part of the GDV(L) curve. Even more surprisingly, for a given data set,\nthe GDV(L) is running through a fixed 'master curve', independently from the\ntotal number of network layers. Furthermore, applying the GDV to Deep Belief\nNetworks reveals that also unsupervised training with the Contrastive\nDivergence method can systematically increase class separability over tens of\nlayers, even though the system does not 'know' the desired class labels. These\nresults indicate that the GDV may become a useful tool to open the black box of\ndeep learning.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1811.01753v2",
        "date": "2018-11-05 14:46:12+00:00"
    },
    {
        "title": "Path-Level Network Transformation for Efficient Architecture Search",
        "authors": [
            "Han Cai",
            "Jiacheng Yang",
            "Weinan Zhang",
            "Song Han",
            "Yong Yu"
        ],
        "abstract": "We introduce a new function-preserving transformation for efficient neural\narchitecture search. This network transformation allows reusing previously\ntrained networks and existing successful architectures that improves sample\nefficiency. We aim to address the limitation of current network transformation\noperations that can only perform layer-level architecture modifications, such\nas adding (pruning) filters or inserting (removing) a layer, which fails to\nchange the topology of connection paths. Our proposed path-level transformation\noperations enable the meta-controller to modify the path topology of the given\nnetwork while keeping the merits of reusing weights, and thus allow efficiently\ndesigning effective structures with complex path topologies like Inception\nmodels. We further propose a bidirectional tree-structured reinforcement\nlearning meta-controller to explore a simple yet highly expressive\ntree-structured architecture space that can be viewed as a generalization of\nmulti-branch architectures. We experimented on the image classification\ndatasets with limited computational resources (about 200 GPU-hours), where we\nobserved improved parameter efficiency and better test results (97.70% test\naccuracy on CIFAR-10 with 14.3M parameters and 74.6% top-1 accuracy on ImageNet\nin the mobile setting), demonstrating the effectiveness and transferability of\nour designed architectures.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1806.02639v1",
        "date": "2018-06-07 12:25:05+00:00"
    },
    {
        "title": "Deep Reconstruction-Classification Networks for Unsupervised Domain Adaptation",
        "authors": [
            "Muhammad Ghifary",
            "W. Bastiaan Kleijn",
            "Mengjie Zhang",
            "David Balduzzi",
            "Wen Li"
        ],
        "abstract": "In this paper, we propose a novel unsupervised domain adaptation algorithm\nbased on deep learning for visual object recognition. Specifically, we design a\nnew model called Deep Reconstruction-Classification Network (DRCN), which\njointly learns a shared encoding representation for two tasks: i) supervised\nclassification of labeled source data, and ii) unsupervised reconstruction of\nunlabeled target data.In this way, the learnt representation not only preserves\ndiscriminability, but also encodes useful information from the target domain.\nOur new DRCN model can be optimized by using backpropagation similarly as the\nstandard neural networks.\n  We evaluate the performance of DRCN on a series of cross-domain object\nrecognition tasks, where DRCN provides a considerable improvement (up to ~8% in\naccuracy) over the prior state-of-the-art algorithms. Interestingly, we also\nobserve that the reconstruction pipeline of DRCN transforms images from the\nsource domain into images whose appearance resembles the target dataset. This\nsuggests that DRCN's performance is due to constructing a single composite\nrepresentation that encodes information about both the structure of target\nimages and the classification of source images. Finally, we provide a formal\nanalysis to justify the algorithm's objective in domain adaptation context.",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1607.03516v2",
        "date": "2016-07-12 20:48:58+00:00"
    },
    {
        "title": "Action-Conditional Video Prediction using Deep Networks in Atari Games",
        "authors": [
            "Junhyuk Oh",
            "Xiaoxiao Guo",
            "Honglak Lee",
            "Richard Lewis",
            "Satinder Singh"
        ],
        "abstract": "Motivated by vision-based reinforcement learning (RL) problems, in particular\nAtari games from the recent benchmark Aracade Learning Environment (ALE), we\nconsider spatio-temporal prediction problems where future (image-)frames are\ndependent on control variables or actions as well as previous frames. While not\ncomposed of natural scenes, frames in Atari games are high-dimensional in size,\ncan involve tens of objects with one or more objects being controlled by the\nactions directly and many other objects being influenced indirectly, can\ninvolve entry and departure of objects, and can involve deep partial\nobservability. We propose and evaluate two deep neural network architectures\nthat consist of encoding, action-conditional transformation, and decoding\nlayers based on convolutional neural networks and recurrent neural networks.\nExperimental results show that the proposed architectures are able to generate\nvisually-realistic frames that are also useful for control over approximately\n100-step action-conditional futures in some games. To the best of our\nknowledge, this paper is the first to make and evaluate long-term predictions\non high-dimensional video conditioned by control inputs.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "link": "http://arxiv.org/pdf/1507.08750v2",
        "date": "2015-07-31 04:43:30+00:00"
    },
    {
        "title": "Dense Transformer Networks",
        "authors": [
            "Jun Li",
            "Yongjun Chen",
            "Lei Cai",
            "Ian Davidson",
            "Shuiwang Ji"
        ],
        "abstract": "The key idea of current deep learning methods for dense prediction is to\napply a model on a regular patch centered on each pixel to make pixel-wise\npredictions. These methods are limited in the sense that the patches are\ndetermined by network architecture instead of learned from data. In this work,\nwe propose the dense transformer networks, which can learn the shapes and sizes\nof patches from data. The dense transformer networks employ an encoder-decoder\narchitecture, and a pair of dense transformer modules are inserted into each of\nthe encoder and decoder paths. The novelty of this work is that we provide\ntechnical solutions for learning the shapes and sizes of patches from data and\nefficiently restoring the spatial correspondence required for dense prediction.\nThe proposed dense transformer modules are differentiable, thus the entire\nnetwork can be trained. We apply the proposed networks on natural and\nbiological image segmentation tasks and show superior performance is achieved\nin comparison to baseline methods.",
        "categories": [
            "cs.CV",
            "cs.LG",
            "cs.NE",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1705.08881v2",
        "date": "2017-05-24 17:50:32+00:00"
    },
    {
        "title": "Stability of Graph Scattering Transforms",
        "authors": [
            "Fernando Gama",
            "Joan Bruna",
            "Alejandro Ribeiro"
        ],
        "abstract": "Scattering transforms are non-trainable deep convolutional architectures that\nexploit the multi-scale resolution of a wavelet filter bank to obtain an\nappropriate representation of data. More importantly, they are proven invariant\nto translations, and stable to perturbations that are close to translations.\nThis stability property dons the scattering transform with a robustness to\nsmall changes in the metric domain of the data. When considering network data,\nregular convolutions do not hold since the data domain presents an irregular\nstructure given by the network topology.\n  In this work, we extend scattering transforms to network data by using\nmultiresolution graph wavelets, whose computation can be obtained by means of\ngraph convolutions. Furthermore, we prove that the resulting graph scattering\ntransforms are stable to metric perturbations of the underlying network. This\nrenders graph scattering transforms robust to changes on the network topology,\nmaking it particularly useful for cases of transfer learning, topology\nestimation or time-varying graphs.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1906.04784v1",
        "date": "2019-06-11 19:42:36+00:00"
    },
    {
        "title": "Approximation analysis of CNNs from feature extraction view",
        "authors": [
            "Han Feng",
            "Jianfei Li",
            "Ding-Xuan Zhou"
        ],
        "abstract": "Deep learning based on deep neural networks has been very successful in many\npractical applications, but it lacks enough theoretical understanding due to\nthe network architectures and structures. In this paper, we establish the\nanalysis for linear feature extraction by deep multi-channel convolutional\nneural networks(CNNs), which demonstrates the power of deep learning over\ntraditional linear transformations, like Fourier, Wavelets, and Redundant\ndictionary coding methods. Moreover, we give an exact construction presenting\nhow linear features extraction can be conducted efficiently with multi-channel\nCNNs. It can be applied to lower the essential dimension for approximating a\nhigh-dimensional function. Rates of function approximation by such deep\nnetworks implemented with channels and followed by fully-connected layers are\ninvestigated as well. Harmonic analysis for factorizing linear features into\nmulti-resolution convolutions plays an essential role in our work.\nNevertheless, a dedicate vectorization of matrices is constructed, which\nbridges 1D CNN and 2D CNN and allows us have corresponding 2D analysis.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "math.FA"
        ],
        "link": "http://arxiv.org/pdf/2210.09041v1",
        "date": "2022-10-14 04:09:01+00:00"
    },
    {
        "title": "Revealing Fundamental Physics from the Daya Bay Neutrino Experiment using Deep Neural Networks",
        "authors": [
            "Evan Racah",
            "Seyoon Ko",
            "Peter Sadowski",
            "Wahid Bhimji",
            "Craig Tull",
            "Sang-Yun Oh",
            "Pierre Baldi",
            "Prabhat"
        ],
        "abstract": "Experiments in particle physics produce enormous quantities of data that must\nbe analyzed and interpreted by teams of physicists. This analysis is often\nexploratory, where scientists are unable to enumerate the possible types of\nsignal prior to performing the experiment. Thus, tools for summarizing,\nclustering, visualizing and classifying high-dimensional data are essential. In\nthis work, we show that meaningful physical content can be revealed by\ntransforming the raw data into a learned high-level representation using deep\nneural networks, with measurements taken at the Daya Bay Neutrino Experiment as\na case study. We further show how convolutional deep neural networks can\nprovide an effective classification filter with greater than 97% accuracy\nacross different classes of physics events, significantly better than other\nmachine learning approaches.",
        "categories": [
            "stat.ML",
            "cs.LG",
            "physics.data-an"
        ],
        "link": "http://arxiv.org/pdf/1601.07621v3",
        "date": "2016-01-28 01:53:13+00:00"
    },
    {
        "title": "Deep CORAL: Correlation Alignment for Deep Domain Adaptation",
        "authors": [
            "Baochen Sun",
            "Kate Saenko"
        ],
        "abstract": "Deep neural networks are able to learn powerful representations from large\nquantities of labeled input data, however they cannot always generalize well\nacross changes in input distributions. Domain adaptation algorithms have been\nproposed to compensate for the degradation in performance due to domain shift.\nIn this paper, we address the case when the target domain is unlabeled,\nrequiring unsupervised adaptation. CORAL is a \"frustratingly easy\" unsupervised\ndomain adaptation method that aligns the second-order statistics of the source\nand target distributions with a linear transformation. Here, we extend CORAL to\nlearn a nonlinear transformation that aligns correlations of layer activations\nin deep neural networks (Deep CORAL). Experiments on standard benchmark\ndatasets show state-of-the-art performance.",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.NE"
        ],
        "link": "http://arxiv.org/pdf/1607.01719v1",
        "date": "2016-07-06 17:35:55+00:00"
    },
    {
        "title": "Genetic Programming-Based Evolutionary Deep Learning for Data-Efficient Image Classification",
        "authors": [
            "Ying Bi",
            "Bing Xue",
            "Mengjie Zhang"
        ],
        "abstract": "Data-efficient image classification is a challenging task that aims to solve\nimage classification using small training data. Neural network-based deep\nlearning methods are effective for image classification, but they typically\nrequire large-scale training data and have major limitations such as requiring\nexpertise to design network architectures and having poor interpretability.\nEvolutionary deep learning is a recent hot topic that combines evolutionary\ncomputation with deep learning. However, most evolutionary deep learning\nmethods focus on evolving architectures of neural networks, which still suffer\nfrom limitations such as poor interpretability. To address this, this paper\nproposes a new genetic programming-based evolutionary deep learning approach to\ndata-efficient image classification. The new approach can automatically evolve\nvariable-length models using many important operators from both image and\nclassification domains. It can learn different types of image features from\ncolour or gray-scale images, and construct effective and diverse ensembles for\nimage classification. A flexible multi-layer representation enables the new\napproach to automatically construct shallow or deep models/trees for different\ntasks and perform effective transformations on the input data via multiple\ninternal nodes. The new approach is applied to solve five image classification\ntasks with different training set sizes. The results show that it achieves\nbetter performance in most cases than deep learning methods for data-efficient\nimage classification. A deep analysis shows that the new approach has good\nconvergence and evolves models with high interpretability, different\nlengths/sizes/shapes, and good transferability.",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2209.13233v1",
        "date": "2022-09-27 08:10:16+00:00"
    },
    {
        "title": "Deep Transformer Q-Networks for Partially Observable Reinforcement Learning",
        "authors": [
            "Kevin Esslinger",
            "Robert Platt",
            "Christopher Amato"
        ],
        "abstract": "Real-world reinforcement learning tasks often involve some form of partial\nobservability where the observations only give a partial or noisy view of the\ntrue state of the world. Such tasks typically require some form of memory,\nwhere the agent has access to multiple past observations, in order to perform\nwell. One popular way to incorporate memory is by using a recurrent neural\nnetwork to access the agent's history. However, recurrent neural networks in\nreinforcement learning are often fragile and difficult to train, susceptible to\ncatastrophic forgetting and sometimes fail completely as a result. In this\nwork, we propose Deep Transformer Q-Networks (DTQN), a novel architecture\nutilizing transformers and self-attention to encode an agent's history. DTQN is\ndesigned modularly, and we compare results against several modifications to our\nbase model. Our experiments demonstrate the transformer can solve partially\nobservable tasks faster and more stably than previous recurrent approaches.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2206.01078v2",
        "date": "2022-06-02 15:04:18+00:00"
    },
    {
        "title": "Disentangled Deep Autoencoding Regularization for Robust Image Classification",
        "authors": [
            "Zhenyu Duan",
            "Martin Renqiang Min",
            "Li Erran Li",
            "Mingbo Cai",
            "Yi Xu",
            "Bingbing Ni"
        ],
        "abstract": "In spite of achieving revolutionary successes in machine learning, deep\nconvolutional neural networks have been recently found to be vulnerable to\nadversarial attacks and difficult to generalize to novel test images with\nreasonably large geometric transformations. Inspired by a recent neuroscience\ndiscovery revealing that primate brain employs disentangled shape and\nappearance representations for object recognition, we propose a general\ndisentangled deep autoencoding regularization framework that can be easily\napplied to any deep embedding based classification model for improving the\nrobustness of deep neural networks. Our framework effectively learns\ndisentangled appearance code and geometric code for robust image\nclassification, which is the first disentangling based method defending against\nadversarial attacks and complementary to standard defense methods. Extensive\nexperiments on several benchmark datasets show that, our proposed\nregularization framework leveraging disentangled embedding significantly\noutperforms traditional unregularized convolutional neural networks for image\nclassification on robustness against adversarial attacks and generalization to\nnovel test data.",
        "categories": [
            "cs.CV",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1902.11134v1",
        "date": "2019-02-27 04:49:57+00:00"
    },
    {
        "title": "Wide & Deep Learning for Recommender Systems",
        "authors": [
            "Heng-Tze Cheng",
            "Levent Koc",
            "Jeremiah Harmsen",
            "Tal Shaked",
            "Tushar Chandra",
            "Hrishi Aradhye",
            "Glen Anderson",
            "Greg Corrado",
            "Wei Chai",
            "Mustafa Ispir",
            "Rohan Anil",
            "Zakaria Haque",
            "Lichan Hong",
            "Vihan Jain",
            "Xiaobing Liu",
            "Hemal Shah"
        ],
        "abstract": "Generalized linear models with nonlinear feature transformations are widely\nused for large-scale regression and classification problems with sparse inputs.\nMemorization of feature interactions through a wide set of cross-product\nfeature transformations are effective and interpretable, while generalization\nrequires more feature engineering effort. With less feature engineering, deep\nneural networks can generalize better to unseen feature combinations through\nlow-dimensional dense embeddings learned for the sparse features. However, deep\nneural networks with embeddings can over-generalize and recommend less relevant\nitems when the user-item interactions are sparse and high-rank. In this paper,\nwe present Wide & Deep learning---jointly trained wide linear models and deep\nneural networks---to combine the benefits of memorization and generalization\nfor recommender systems. We productionized and evaluated the system on Google\nPlay, a commercial mobile app store with over one billion active users and over\none million apps. Online experiment results show that Wide & Deep significantly\nincreased app acquisitions compared with wide-only and deep-only models. We\nhave also open-sourced our implementation in TensorFlow.",
        "categories": [
            "cs.LG",
            "cs.IR",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1606.07792v1",
        "date": "2016-06-24 19:07:02+00:00"
    },
    {
        "title": "Deep-RBF Networks Revisited: Robust Classification with Rejection",
        "authors": [
            "Pourya Habib Zadeh",
            "Reshad Hosseini",
            "Suvrit Sra"
        ],
        "abstract": "One of the main drawbacks of deep neural networks, like many other\nclassifiers, is their vulnerability to adversarial attacks. An important reason\nfor their vulnerability is assigning high confidence to regions with few or\neven no feature points. By feature points, we mean a nonlinear transformation\nof the input space extracting a meaningful representation of the input data. On\nthe other hand, deep-RBF networks assign high confidence only to the regions\ncontaining enough feature points, but they have been discounted due to the\nwidely-held belief that they have the vanishing gradient problem. In this\npaper, we revisit the deep-RBF networks by first giving a general formulation\nfor them, and then proposing a family of cost functions thereof inspired by\nmetric learning. In the proposed deep-RBF learning algorithm, the vanishing\ngradient problem does not occur. We make these networks robust to adversarial\nattack by adding the reject option to their output layer. Through several\nexperiments on the MNIST dataset, we demonstrate that our proposed method not\nonly achieves significant classification accuracy but is also very resistant to\nvarious adversarial attacks.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1812.03190v1",
        "date": "2018-12-07 19:25:52+00:00"
    },
    {
        "title": "Semantic Robustness of Models of Source Code",
        "authors": [
            "Goutham Ramakrishnan",
            "Jordan Henkel",
            "Zi Wang",
            "Aws Albarghouthi",
            "Somesh Jha",
            "Thomas Reps"
        ],
        "abstract": "Deep neural networks are vulnerable to adversarial examples - small input\nperturbations that result in incorrect predictions. We study this problem for\nmodels of source code, where we want the network to be robust to source-code\nmodifications that preserve code functionality. (1) We define a powerful\nadversary that can employ sequences of parametric, semantics-preserving program\ntransformations; (2) we show how to perform adversarial training to learn\nmodels robust to such adversaries; (3) we conduct an evaluation on different\nlanguages and architectures, demonstrating significant quantitative gains in\nrobustness.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2002.03043v2",
        "date": "2020-02-07 23:26:17+00:00"
    },
    {
        "title": "DeepCodec: Adaptive Sensing and Recovery via Deep Convolutional Neural Networks",
        "authors": [
            "Ali Mousavi",
            "Gautam Dasarathy",
            "Richard G. Baraniuk"
        ],
        "abstract": "In this paper we develop a novel computational sensing framework for sensing\nand recovering structured signals. When trained on a set of representative\nsignals, our framework learns to take undersampled measurements and recover\nsignals from them using a deep convolutional neural network. In other words, it\nlearns a transformation from the original signals to a near-optimal number of\nundersampled measurements and the inverse transformation from measurements to\nsignals. This is in contrast to traditional compressive sensing (CS) systems\nthat use random linear measurements and convex optimization or iterative\nalgorithms for signal recovery. We compare our new framework with\n$\\ell_1$-minimization from the phase transition point of view and demonstrate\nthat it outperforms $\\ell_1$-minimization in the regions of phase transition\nplot where $\\ell_1$-minimization cannot recover the exact solution. In\naddition, we experimentally demonstrate how learning measurements enhances the\noverall recovery performance, speeds up training of recovery framework, and\nleads to having fewer parameters to learn.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1707.03386v1",
        "date": "2017-07-11 17:49:20+00:00"
    },
    {
        "title": "Deep Clustering With Intra-class Distance Constraint for Hyperspectral Images",
        "authors": [
            "Jinguang Sun",
            "Wanli Wang",
            "Xian Wei",
            "Li Fang",
            "Xiaoliang Tang",
            "Yusheng Xu",
            "Hui Yu",
            "Wei Yao"
        ],
        "abstract": "The high dimensionality of hyperspectral images often results in the\ndegradation of clustering performance. Due to the powerful ability of deep\nfeature extraction and non-linear feature representation, the clustering\nalgorithm based on deep learning has become a hot research topic in the field\nof hyperspectral remote sensing. However, most deep clustering algorithms for\nhyperspectral images utilize deep neural networks as feature extractor without\nconsidering prior knowledge constraints that are suitable for clustering. To\nsolve this problem, we propose an intra-class distance constrained deep\nclustering algorithm for high-dimensional hyperspectral images. The proposed\nalgorithm constrains the feature mapping procedure of the auto-encoder network\nby intra-class distance so that raw images are transformed from the original\nhigh-dimensional space to the low-dimensional feature space that is more\nconducive to clustering. Furthermore, the related learning process is treated\nas a joint optimization problem of deep feature extraction and clustering.\nExperimental results demonstrate the intense competitiveness of the proposed\nalgorithm in comparison with state-of-the-art clustering methods of\nhyperspectral images.",
        "categories": [
            "cs.LG",
            "eess.IV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1904.00562v1",
        "date": "2019-04-01 04:42:18+00:00"
    },
    {
        "title": "Separability is not the best goal for machine learning",
        "authors": [
            "Wlodzislaw Duch"
        ],
        "abstract": "Neural networks use their hidden layers to transform input data into linearly\nseparable data clusters, with a linear or a perceptron type output layer making\nthe final projection on the line perpendicular to the discriminating\nhyperplane. For complex data with multimodal distributions this transformation\nis difficult to learn. Projection on $k\\geq 2$ line segments is the simplest\nextension of linear separability, defining much easier goal for the learning\nprocess. Simple problems are 2-separable, but problems with inherent complex\nlogic may be solved in a simple way by $k$-separable projections. The\ndifficulty of learning non-linear data distributions is shifted to separation\nof line intervals, simplifying the transformation of data by hidden network\nlayers. For classification of difficult Boolean problems, such as the parity\nproblem, linear projection combined with \\ksep is sufficient and provides a\npowerful new target for learning. More complex targets may also be defined,\nchanging the goal of learning from linear discrimination to creation of data\ndistributions that can easily be handled by specialized models selected to\nanalyze output distributions. This approach can replace many layers of\ntransformation required by deep learning models.",
        "categories": [
            "cs.LG",
            "stat.ML",
            "68T05, 68Q32"
        ],
        "link": "http://arxiv.org/pdf/1807.02873v1",
        "date": "2018-07-08 19:53:58+00:00"
    },
    {
        "title": "DNNSurv: Deep Neural Networks for Survival Analysis Using Pseudo Values",
        "authors": [
            "Lili Zhao",
            "Dai Feng"
        ],
        "abstract": "There has been increasing interest in modelling survival data using deep\nlearning methods in medical research. Current approaches have focused on\ndesigning special cost functions to handle censored survival data. We propose a\nvery different method with two steps. In the first step, we transform each\nsubject's survival time into a series of jackknife pseudo conditional survival\nprobabilities and then use these pseudo probabilities as a quantitative\nresponse variable in the deep neural network model. By using the pseudo values,\nwe reduce a complex survival analysis to a standard regression problem, which\ngreatly simplifies the neural network construction. Our two-step approach is\nsimple, yet very flexible in making risk predictions for survival data, which\nis very appealing from the practice point of view. The source code is freely\navailable at http://github.com/lilizhaoUM/DNNSurv.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1908.02337v2",
        "date": "2019-08-06 19:16:58+00:00"
    },
    {
        "title": "Ridge Regression with Over-Parametrized Two-Layer Networks Converge to Ridgelet Spectrum",
        "authors": [
            "Sho Sonoda",
            "Isao Ishikawa",
            "Masahiro Ikeda"
        ],
        "abstract": "Characterization of local minima draws much attention in theoretical studies\nof deep learning. In this study, we investigate the distribution of parameters\nin an over-parametrized finite neural network trained by ridge regularized\nempirical square risk minimization (RERM). We develop a new theory of ridgelet\ntransform, a wavelet-like integral transform that provides a powerful and\ngeneral framework for the theoretical study of neural networks involving not\nonly the ReLU but general activation functions. We show that the distribution\nof the parameters converges to a spectrum of the ridgelet transform. This\nresult provides a new insight into the characterization of the local minima of\nneural networks, and the theoretical background of an inductive bias theory\nbased on lazy regimes. We confirm the visual resemblance between the parameter\ndistribution trained by SGD, and the ridgelet spectrum calculated by numerical\nintegration through numerical experiments with finite models.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2007.03441v2",
        "date": "2020-07-07 13:47:57+00:00"
    },
    {
        "title": "Dual-constrained Deep Semi-Supervised Coupled Factorization Network with Enriched Prior",
        "authors": [
            "Yan Zhang",
            "Zhao Zhang",
            "Yang Wang",
            "Zheng Zhang",
            "Li Zhang",
            "Shuicheng Yan",
            "Meng Wang"
        ],
        "abstract": "Nonnegative matrix factorization is usually powerful for learning the\n\"shallow\" parts-based representation, but it clearly fails to discover deep\nhierarchical information within both the basis and representation spaces. In\nthis paper, we technically propose a new enriched prior based Dual-constrained\nDeep Semi-Supervised Coupled Factorization Network, called DS2CF-Net, for\nlearning the hierarchical coupled representations. To ex-tract hidden deep\nfeatures, DS2CF-Net is modeled as a deep-structure and geometrical\nstructure-constrained neural network. Specifically, DS2CF-Net designs a deep\ncoupled factorization architecture using multi-layers of linear\ntransformations, which coupled updates the bases and new representations in\neach layer. To improve the discriminating ability of learned deep\nrepresentations and deep coefficients, our network clearly considers enriching\nthe supervised prior by the joint deep coefficients-regularized label\nprediction, and incorporates enriched prior information as additional label and\nstructure constraints. The label constraint can enable the samples of the same\nlabel to have the same coordinate in the new feature space, while the structure\nconstraint forces the coefficient matrices in each layer to be block-diagonal\nso that the enhanced prior using the self-expressive label propagation are more\naccurate. Our network also integrates the adaptive dual-graph learning to\nretain the local manifold structures of both the data manifold and feature\nmanifold by minimizing the reconstruction errors in each layer. Extensive\nexperiments on several real databases demonstrate that our DS2CF-Net can obtain\nstate-of-the-art performance for representation learning and clustering.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2009.03714v2",
        "date": "2020-09-08 13:10:21+00:00"
    },
    {
        "title": "Comment on Transferability and Input Transformation with Additive Noise",
        "authors": [
            "Hoki Kim",
            "Jinseong Park",
            "Jaewook Lee"
        ],
        "abstract": "Adversarial attacks have verified the existence of the vulnerability of\nneural networks. By adding small perturbations to a benign example, adversarial\nattacks successfully generate adversarial examples that lead misclassification\nof deep learning models. More importantly, an adversarial example generated\nfrom a specific model can also deceive other models without modification. We\ncall this phenomenon ``transferability\". Here, we analyze the relationship\nbetween transferability and input transformation with additive noise by\nmathematically proving that the modified optimization can produce more\ntransferable adversarial examples.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CR"
        ],
        "link": "http://arxiv.org/pdf/2206.09075v1",
        "date": "2022-06-18 00:52:27+00:00"
    },
    {
        "title": "Deep Optimisation: Solving Combinatorial Optimisation Problems using Deep Neural Networks",
        "authors": [
            "J. R. Caldwell",
            "R. A. Watson",
            "C. Thies",
            "J. D. Knowles"
        ],
        "abstract": "Deep Optimisation (DO) combines evolutionary search with Deep Neural Networks\n(DNNs) in a novel way - not for optimising a learning algorithm, but for\nfinding a solution to an optimisation problem. Deep learning has been\nsuccessfully applied to classification, regression, decision and generative\ntasks and in this paper we extend its application to solving optimisation\nproblems. Model Building Optimisation Algorithms (MBOAs), a branch of\nevolutionary algorithms, have been successful in combining machine learning\nmethods and evolutionary search but, until now, they have not utilised DNNs. DO\nis the first algorithm to use a DNN to learn and exploit the problem structure\nto adapt the variation operator (changing the neighbourhood structure of the\nsearch process). We demonstrate the performance of DO using two theoretical\noptimisation problems within the MAXSAT class. The Hierarchical Transformation\nOptimisation Problem (HTOP) has controllable deep structure that provides a\nclear evaluation of how DO works and why using a layerwise technique is\nessential for learning and exploiting problem structure. The Parity Modular\nConstraint Problem (MCparity) is a simplistic example of a problem containing\nhigher-order dependencies (greater than pairwise) which DO can solve and state\nof the art MBOAs cannot. Further, we show that DO can exploit deep structure in\nTSP instances. Together these results show that there exists problems that DO\ncan find and exploit deep problem structure that other algorithms cannot.\nMaking this connection between DNNs and optimisation allows for the utilisation\nof advanced tools applicable to DNNs that current MBOAs are unable to use.",
        "categories": [
            "cs.LG",
            "cs.NE",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1811.00784v1",
        "date": "2018-11-02 09:04:29+00:00"
    },
    {
        "title": "On the Expressive Power of Deep Learning: A Tensor Analysis",
        "authors": [
            "Nadav Cohen",
            "Or Sharir",
            "Amnon Shashua"
        ],
        "abstract": "It has long been conjectured that hypotheses spaces suitable for data that is\ncompositional in nature, such as text or images, may be more efficiently\nrepresented with deep hierarchical networks than with shallow ones. Despite the\nvast empirical evidence supporting this belief, theoretical justifications to\ndate are limited. In particular, they do not account for the locality, sharing\nand pooling constructs of convolutional networks, the most successful deep\nlearning architecture to date. In this work we derive a deep network\narchitecture based on arithmetic circuits that inherently employs locality,\nsharing and pooling. An equivalence between the networks and hierarchical\ntensor factorizations is established. We show that a shallow network\ncorresponds to CP (rank-1) decomposition, whereas a deep network corresponds to\nHierarchical Tucker decomposition. Using tools from measure theory and matrix\nalgebra, we prove that besides a negligible set, all functions that can be\nimplemented by a deep network of polynomial size, require exponential size in\norder to be realized (or even approximated) by a shallow network. Since\nlog-space computation transforms our networks into SimNets, the result applies\ndirectly to a deep learning architecture demonstrating promising empirical\nperformance. The construction and theory developed in this paper shed new light\non various practices and ideas employed by the deep learning community.",
        "categories": [
            "cs.NE",
            "cs.LG",
            "cs.NA",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1509.05009v3",
        "date": "2015-09-16 19:32:54+00:00"
    },
    {
        "title": "Avoiding pathologies in very deep networks",
        "authors": [
            "David Duvenaud",
            "Oren Rippel",
            "Ryan P. Adams",
            "Zoubin Ghahramani"
        ],
        "abstract": "Choosing appropriate architectures and regularization strategies for deep\nnetworks is crucial to good predictive performance. To shed light on this\nproblem, we analyze the analogous problem of constructing useful priors on\ncompositions of functions. Specifically, we study the deep Gaussian process, a\ntype of infinitely-wide, deep neural network. We show that in standard\narchitectures, the representational capacity of the network tends to capture\nfewer degrees of freedom as the number of layers increases, retaining only a\nsingle degree of freedom in the limit. We propose an alternate network\narchitecture which does not suffer from this pathology. We also examine deep\ncovariance functions, obtained by composing infinitely many feature transforms.\nLastly, we characterize the class of models obtained by performing dropout on\nGaussian processes.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1402.5836v3",
        "date": "2014-02-24 14:27:40+00:00"
    },
    {
        "title": "Transformers learn in-context by gradient descent",
        "authors": [
            "Johannes von Oswald",
            "Eyvind Niklasson",
            "Ettore Randazzo",
            "Jo\u00e3o Sacramento",
            "Alexander Mordvintsev",
            "Andrey Zhmoginov",
            "Max Vladymyrov"
        ],
        "abstract": "Transformers have become the state-of-the-art neural network architecture\nacross numerous domains of machine learning. This is partly due to their\ncelebrated ability to transfer and to learn in-context based on few examples.\nNevertheless, the mechanisms by which Transformers become in-context learners\nare not well understood and remain mostly an intuition. Here, we argue that\ntraining Transformers on auto-regressive tasks can be closely related to\nwell-known gradient-based meta-learning formulations. We start by providing a\nsimple weight construction that shows the equivalence of data transformations\ninduced by 1) a single linear self-attention layer and by 2) gradient-descent\n(GD) on a regression loss. Motivated by that construction, we show empirically\nthat when training self-attention-only Transformers on simple regression tasks\neither the models learned by GD and Transformers show great similarity or,\nremarkably, the weights found by optimization match the construction. Thus we\nshow how trained Transformers implement gradient descent in their forward pass.\nThis allows us, at least in the domain of regression problems, to\nmechanistically understand the inner workings of optimized Transformers that\nlearn in-context. Furthermore, we identify how Transformers surpass plain\ngradient descent by an iterative curvature correction and learn linear models\non deep data representations to solve non-linear regression tasks. Finally, we\ndiscuss intriguing parallels to a mechanism identified to be crucial for\nin-context learning termed induction-head (Olsson et al., 2022) and show how it\ncould be understood as a specific case of in-context learning by gradient\ndescent learning within Transformers.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "link": "http://arxiv.org/pdf/2212.07677v1",
        "date": "2022-12-15 09:21:21+00:00"
    },
    {
        "title": "Group-Equivariant Neural Networks with Fusion Diagrams",
        "authors": [
            "Zimu Li",
            "Han Zheng",
            "Erik Thiede",
            "Junyu Liu",
            "Risi Kondor"
        ],
        "abstract": "Many learning tasks in physics and chemistry involve global spatial\nsymmetries as well as permutational symmetry between particles. The standard\napproach to such problems is equivariant neural networks, which employ tensor\nproducts between various tensors that transform under the spatial group.\nHowever, as the number of different tensors and the complexity of relationships\nbetween them increases, the bookkeeping associated with ensuring parsimony as\nwell as equivariance quickly becomes nontrivial. In this paper, we propose to\nuse fusion diagrams, a technique widely used in simulating SU($2$)-symmetric\nquantum many-body problems, to design new equivariant components for use in\nequivariant neural networks. This yields a diagrammatic approach to\nconstructing new neural network architectures. We show that when applied to\nparticles in a given local neighborhood, the resulting components, which we\ncall fusion blocks, are universal approximators of any continuous equivariant\nfunction defined on the neighborhood. As a practical demonstration, we\nincorporate a fusion block into a pre-existing equivariant architecture\n(Cormorant) and show that it improves performance on benchmark molecular\nlearning tasks.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "quant-ph",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2211.07482v1",
        "date": "2022-11-14 16:06:59+00:00"
    },
    {
        "title": "On regularization for a convolutional kernel in neural networks",
        "authors": [
            "Peichang Guo",
            "Qiang Ye"
        ],
        "abstract": "Convolutional neural network is an important model in deep learning. To avoid\nexploding/vanishing gradient problems and to improve the generalizability of a\nneural network, it is desirable to have a convolution operation that nearly\npreserves the norm, or to have the singular values of the transformation matrix\ncorresponding to a convolutional kernel bounded around $1$. We propose a\npenalty function that can be used in the optimization of a convolutional neural\nnetwork to constrain the singular values of the transformation matrix around\n$1$. We derive an algorithm to carry out the gradient descent minimization of\nthis penalty function in terms of convolution kernels. Numerical examples are\npresented to demonstrate the effectiveness of the method.",
        "categories": [
            "cs.LG",
            "cs.NA",
            "math.NA",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1906.04866v2",
        "date": "2019-06-12 00:07:32+00:00"
    },
    {
        "title": "Quasi-Equivalence of Width and Depth of Neural Networks",
        "authors": [
            "Feng-Lei Fan",
            "Rongjie Lai",
            "Ge Wang"
        ],
        "abstract": "While classic studies proved that wide networks allow universal\napproximation, recent research and successes of deep learning demonstrate the\npower of deep networks. Based on a symmetric consideration, we investigate if\nthe design of artificial neural networks should have a directional preference,\nand what the mechanism of interaction is between the width and depth of a\nnetwork. Inspired by the De Morgan law, we address this fundamental question by\nestablishing a quasi-equivalence between the width and depth of ReLU networks\nin two aspects. First, we formulate two transforms for mapping an arbitrary\nReLU network to a wide network and a deep network respectively for either\nregression or classification so that the essentially same capability of the\noriginal network can be implemented. Then, we replace the mainstream artificial\nneuron type with a quadratic counterpart, and utilize the factorization and\ncontinued fraction representations of the same polynomial function to construct\na wide network and a deep network, respectively. Based on our findings, a deep\nnetwork has a wide equivalent, and vice versa, subject to an arbitrarily small\nerror.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2002.02515v7",
        "date": "2020-02-06 21:17:32+00:00"
    },
    {
        "title": "Insights on representational similarity in neural networks with canonical correlation",
        "authors": [
            "Ari S. Morcos",
            "Maithra Raghu",
            "Samy Bengio"
        ],
        "abstract": "Comparing different neural network representations and determining how\nrepresentations evolve over time remain challenging open questions in our\nunderstanding of the function of neural networks. Comparing representations in\nneural networks is fundamentally difficult as the structure of representations\nvaries greatly, even across groups of networks trained on identical tasks, and\nover the course of training. Here, we develop projection weighted CCA\n(Canonical Correlation Analysis) as a tool for understanding neural networks,\nbuilding off of SVCCA, a recently proposed method (Raghu et al., 2017). We\nfirst improve the core method, showing how to differentiate between signal and\nnoise, and then apply this technique to compare across a group of CNNs,\ndemonstrating that networks which generalize converge to more similar\nrepresentations than networks which memorize, that wider networks converge to\nmore similar solutions than narrow networks, and that trained networks with\nidentical topology but different learning rates converge to distinct clusters\nwith diverse representations. We also investigate the representational dynamics\nof RNNs, across both training and sequential timesteps, finding that RNNs\nconverge in a bottom-up pattern over the course of training and that the hidden\nstate is highly variable over the course of a sequence, even when accounting\nfor linear transforms. Together, these results provide new insights into the\nfunction of CNNs and RNNs, and demonstrate the utility of using CCA to\nunderstand representations.",
        "categories": [
            "stat.ML",
            "cs.AI",
            "cs.CV",
            "cs.LG",
            "cs.NE"
        ],
        "link": "http://arxiv.org/pdf/1806.05759v3",
        "date": "2018-06-14 22:34:11+00:00"
    },
    {
        "title": "Network Learning with Local Propagation",
        "authors": [
            "Dimche Kostadinov",
            "Behrooz Razeghi",
            "Sohrab Ferdowsi",
            "Slava Voloshynovskiy"
        ],
        "abstract": "This paper presents a locally decoupled network parameter learning with local\npropagation. Three elements are taken into account: (i) sets of nonlinear\ntransforms that describe the representations at all nodes, (ii) a local\nobjective at each node related to the corresponding local representation goal,\nand (iii) a local propagation model that relates the nonlinear error vectors at\neach node with the goal error vectors from the directly connected nodes. The\nmodeling concepts (i), (ii) and (iii) offer several advantages, including (a) a\nunified learning principle for any network that is represented as a graph, (b)\nunderstanding and interpretation of the local and the global learning dynamics,\n(c) decoupled and parallel parameter learning, (d) a possibility for learning\nin infinitely long, multi-path and multi-goal networks. Numerical experiments\nvalidate the potential of the learning principle. The preliminary results show\nadvantages in comparison to the state-of-the-art methods, w.r.t. the learning\ntime and the network size while having comparable recognition accuracy.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.NE",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1805.07802v1",
        "date": "2018-05-20 17:21:05+00:00"
    },
    {
        "title": "A Convergence Theory Towards Practical Over-parameterized Deep Neural Networks",
        "authors": [
            "Asaf Noy",
            "Yi Xu",
            "Yonathan Aflalo",
            "Lihi Zelnik-Manor",
            "Rong Jin"
        ],
        "abstract": "Deep neural networks' remarkable ability to correctly fit training data when\noptimized by gradient-based algorithms is yet to be fully understood. Recent\ntheoretical results explain the convergence for ReLU networks that are wider\nthan those used in practice by orders of magnitude. In this work, we take a\nstep towards closing the gap between theory and practice by significantly\nimproving the known theoretical bounds on both the network width and the\nconvergence time. We show that convergence to a global minimum is guaranteed\nfor networks with widths quadratic in the sample size and linear in their depth\nat a time logarithmic in both. Our analysis and convergence bounds are derived\nvia the construction of a surrogate network with fixed activation patterns that\ncan be transformed at any time to an equivalent ReLU network of a reasonable\nsize. This construction can be viewed as a novel technique to accelerate\ntraining, while its tight finite-width equivalence to Neural Tangent Kernel\n(NTK) suggests it can be utilized to study generalization as well.",
        "categories": [
            "cs.LG",
            "math.OC",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2101.04243v2",
        "date": "2021-01-12 00:40:45+00:00"
    },
    {
        "title": "Deep Conditional Transformation Models",
        "authors": [
            "Philipp F. M. Baumann",
            "Torsten Hothorn",
            "David R\u00fcgamer"
        ],
        "abstract": "Learning the cumulative distribution function (CDF) of an outcome variable\nconditional on a set of features remains challenging, especially in\nhigh-dimensional settings. Conditional transformation models provide a\nsemi-parametric approach that allows to model a large class of conditional CDFs\nwithout an explicit parametric distribution assumption and with only a few\nparameters. Existing estimation approaches within this class are, however,\neither limited in their complexity and applicability to unstructured data\nsources such as images or text, lack interpretability, or are restricted to\ncertain types of outcomes. We close this gap by introducing the class of deep\nconditional transformation models which unifies existing approaches and allows\nto learn both interpretable (non-)linear model terms and more complex neural\nnetwork predictors in one holistic framework. To this end we propose a novel\nnetwork architecture, provide details on different model definitions and derive\nsuitable constraints as well as network regularization terms. We demonstrate\nthe efficacy of our approach through numerical experiments and applications.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2010.07860v4",
        "date": "2020-10-15 16:25:45+00:00"
    },
    {
        "title": "Recognition of Acoustic Events Using Masked Conditional Neural Networks",
        "authors": [
            "Fady Medhat",
            "David Chesmore",
            "John Robinson"
        ],
        "abstract": "Automatic feature extraction using neural networks has accomplished\nremarkable success for images, but for sound recognition, these models are\nusually modified to fit the nature of the multi-dimensional temporal\nrepresentation of the audio signal in spectrograms. This may not efficiently\nharness the time-frequency representation of the signal. The ConditionaL Neural\nNetwork (CLNN) takes into consideration the interrelation between the temporal\nframes, and the Masked ConditionaL Neural Network (MCLNN) extends upon the CLNN\nby forcing a systematic sparseness over the network's weights using a binary\nmask. The masking allows the network to learn about frequency bands rather than\nbins, mimicking a filterbank used in signal transformations such as MFCC.\nAdditionally, the Mask is designed to consider various combinations of\nfeatures, which automates the feature hand-crafting process. We applied the\nMCLNN for the Environmental Sound Recognition problem using the Urbansound8k,\nYorNoise, ESC-10 and ESC-50 datasets. The MCLNN have achieved competitive\nperformance compared to state-of-the-art Convolutional Neural Networks and\nhand-crafted attempts.",
        "categories": [
            "cs.LG",
            "cs.SD",
            "eess.AS",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1802.02617v2",
        "date": "2018-02-07 19:58:50+00:00"
    },
    {
        "title": "Deep equilibrium models as estimators for continuous latent variables",
        "authors": [
            "Russell Tsuchida",
            "Cheng Soon Ong"
        ],
        "abstract": "Principal Component Analysis (PCA) and its exponential family extensions have\nthree components: observations, latents and parameters of a linear\ntransformation. We consider a generalised setting where the canonical\nparameters of the exponential family are a nonlinear transformation of the\nlatents. We show explicit relationships between particular neural network\narchitectures and the corresponding statistical models. We find that deep\nequilibrium models -- a recently introduced class of implicit neural networks\n-- solve maximum a-posteriori (MAP) estimates for the latents and parameters of\nthe transformation. Our analysis provides a systematic way to relate activation\nfunctions, dropout, and layer structure, to statistical assumptions about the\nobservations, thus providing foundational principles for unsupervised DEQs. For\nhierarchical latents, individual neurons can be interpreted as nodes in a deep\ngraphical model. Our DEQ feature maps are end-to-end differentiable, enabling\nfine-tuning for downstream tasks.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2211.05943v1",
        "date": "2022-11-11 01:21:34+00:00"
    },
    {
        "title": "Learning Spatially Structured Image Transformations Using Planar Neural Networks",
        "authors": [
            "Joel Michelson",
            "Joshua H. Palmer",
            "Aneesha Dasari",
            "Maithilee Kunda"
        ],
        "abstract": "Learning image transformations is essential to the idea of mental simulation\nas a method of cognitive inference. We take a connectionist modeling approach,\nusing planar neural networks to learn fundamental imagery transformations, like\ntranslation, rotation, and scaling, from perceptual experiences in the form of\nimage sequences. We investigate how variations in network topology, training\ndata, and image shape, among other factors, affect the efficiency and\neffectiveness of learning visual imagery transformations, including\neffectiveness of transfer to operating on new types of data.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1912.01553v2",
        "date": "2019-12-03 17:54:35+00:00"
    },
    {
        "title": "Rank Diminishing in Deep Neural Networks",
        "authors": [
            "Ruili Feng",
            "Kecheng Zheng",
            "Yukun Huang",
            "Deli Zhao",
            "Michael Jordan",
            "Zheng-Jun Zha"
        ],
        "abstract": "The rank of neural networks measures information flowing across layers. It is\nan instance of a key structural condition that applies across broad domains of\nmachine learning. In particular, the assumption of low-rank feature\nrepresentations leads to algorithmic developments in many architectures. For\nneural networks, however, the intrinsic mechanism that yields low-rank\nstructures remains vague and unclear. To fill this gap, we perform a rigorous\nstudy on the behavior of network rank, focusing particularly on the notion of\nrank deficiency. We theoretically establish a universal monotonic decreasing\nproperty of network rank from the basic rules of differential and algebraic\ncomposition, and uncover rank deficiency of network blocks and deep function\ncoupling. By virtue of our numerical tools, we provide the first empirical\nanalysis of the per-layer behavior of network rank in practical settings, i.e.,\nResNets, deep MLPs, and Transformers on ImageNet. These empirical results are\nin direct accord with our theory. Furthermore, we reveal a novel phenomenon of\nindependence deficit caused by the rank deficiency of deep networks, where\nclassification confidence of a given category can be linearly decided by the\nconfidence of a handful of other categories. The theoretical results of this\nwork, together with the empirical findings, may advance understanding of the\ninherent principles of deep neural networks.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2206.06072v1",
        "date": "2022-06-13 12:03:32+00:00"
    },
    {
        "title": "Artificial Neural Networks to Impute Rounded Zeros in Compositional Data",
        "authors": [
            "Matthias Templ"
        ],
        "abstract": "Methods of deep learning have become increasingly popular in recent years,\nbut they have not arrived in compositional data analysis. Imputation methods\nfor compositional data are typically applied on additive, centered or isometric\nlog-ratio representations of the data. Generally, methods for compositional\ndata analysis can only be applied to observed positive entries in a data\nmatrix. Therefore one tries to impute missing values or measurements that were\nbelow a detection limit. In this paper, a new method for imputing rounded zeros\nbased on artificial neural networks is shown and compared with conventional\nmethods. We are also interested in the question whether for ANNs, a\nrepresentation of the data in log-ratios for imputation purposes, is relevant.\nIt can be shown, that ANNs are competitive or even performing better when\nimputing rounded zeros of data sets with moderate size. They deliver better\nresults when data sets are big. Also, we can see that log-ratio transformations\nwithin the artificial neural network imputation procedure nevertheless help to\nimprove the results. This proves that the theory of compositional data analysis\nand the fulfillment of all properties of compositional data analysis is still\nvery important in the age of deep learning.",
        "categories": [
            "stat.ML",
            "cs.LG",
            "stat.ME"
        ],
        "link": "http://arxiv.org/pdf/2012.10300v1",
        "date": "2020-12-18 15:31:23+00:00"
    },
    {
        "title": "Gauge Equivariant Convolutional Networks and the Icosahedral CNN",
        "authors": [
            "Taco S. Cohen",
            "Maurice Weiler",
            "Berkay Kicanaoglu",
            "Max Welling"
        ],
        "abstract": "The principle of equivariance to symmetry transformations enables a\ntheoretically grounded approach to neural network architecture design.\nEquivariant networks have shown excellent performance and data efficiency on\nvision and medical imaging problems that exhibit symmetries. Here we show how\nthis principle can be extended beyond global symmetries to local gauge\ntransformations. This enables the development of a very general class of\nconvolutional neural networks on manifolds that depend only on the intrinsic\ngeometry, and which includes many popular methods from equivariant and\ngeometric deep learning. We implement gauge equivariant CNNs for signals\ndefined on the surface of the icosahedron, which provides a reasonable\napproximation of the sphere. By choosing to work with this very regular\nmanifold, we are able to implement the gauge equivariant convolution using a\nsingle conv2d call, making it a highly scalable and practical alternative to\nSpherical CNNs. Using this method, we demonstrate substantial improvements over\nprevious methods on the task of segmenting omnidirectional images and global\nclimate patterns.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "cs.NE",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1902.04615v3",
        "date": "2019-02-11 17:01:05+00:00"
    },
    {
        "title": "Self-supervised edge features for improved Graph Neural Network training",
        "authors": [
            "Arijit Sehanobish",
            "Neal G. Ravindra",
            "David van Dijk"
        ],
        "abstract": "Graph Neural Networks (GNN) have been extensively used to extract meaningful\nrepresentations from graph structured data and to perform predictive tasks such\nas node classification and link prediction. In recent years, there has been a\nlot of work incorporating edge features along with node features for prediction\ntasks. One of the main difficulties in using edge features is that they are\noften handcrafted, hard to get, specific to a particular domain, and may\ncontain redundant information. In this work, we present a framework for\ncreating new edge features, applicable to any domain, via a combination of\nself-supervised and unsupervised learning. In addition to this, we use\nForman-Ricci curvature as an additional edge feature to encapsulate the local\ngeometry of the graph. We then encode our edge features via a Set Transformer\nand combine them with node features extracted from popular GNN architectures\nfor node classification in an end-to-end training scheme. We validate our work\non three biological datasets comprising of single-cell RNA sequencing data of\nneurological disease, \\textit{in vitro} SARS-CoV-2 infection, and human\nCOVID-19 patients. We demonstrate that our method achieves better performance\non node classification tasks over baseline Graph Attention Network (GAT) and\nGraph Convolutional Network (GCN) models. Furthermore, given the attention\nmechanism on edge and node features, we are able to interpret the cell types\nand genes that determine the course and severity of COVID-19, contributing to a\ngrowing list of potential disease biomarkers and therapeutic targets.",
        "categories": [
            "eess.IV",
            "cs.LG",
            "q-bio.GN",
            "stat.ML",
            "I.2.4; J.3"
        ],
        "link": "http://arxiv.org/pdf/2007.04777v1",
        "date": "2020-06-23 20:18:22+00:00"
    },
    {
        "title": "A Generative Model for Sampling High-Performance and Diverse Weights for Neural Networks",
        "authors": [
            "Lior Deutsch",
            "Erik Nijkamp",
            "Yu Yang"
        ],
        "abstract": "Recent work on mode connectivity in the loss landscape of deep neural\nnetworks has demonstrated that the locus of (sub-)optimal weight vectors lies\non continuous paths. In this work, we train a neural network that serves as a\nhypernetwork, mapping a latent vector into high-performance (low-loss) weight\nvectors, generalizing recent findings of mode connectivity to higher\ndimensional manifolds. We formulate the training objective as a compromise\nbetween accuracy and diversity, where the diversity takes into account trivial\nsymmetry transformations of the target network. We demonstrate how to reduce\nthe number of parameters in the hypernetwork by parameter sharing. Once\nlearned, the hypernetwork allows for a computationally efficient, ancestral\nsampling of neural network weights, which we recruit to form large ensembles.\nThe improvement in classification accuracy obtained by this ensembling\nindicates that the generated manifold extends in dimensions other than\ndirections implied by trivial symmetries. For computational efficiency, we\ndistill an ensemble into a single classifier while retaining generalization.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1905.02898v1",
        "date": "2019-05-07 04:28:46+00:00"
    },
    {
        "title": "Examining the Benefits of Capsule Neural Networks",
        "authors": [
            "Arjun Punjabi",
            "Jonas Schmid",
            "Aggelos K. Katsaggelos"
        ],
        "abstract": "Capsule networks are a recently developed class of neural networks that\npotentially address some of the deficiencies with traditional convolutional\nneural networks. By replacing the standard scalar activations with vectors, and\nby connecting the artificial neurons in a new way, capsule networks aim to be\nthe next great development for computer vision applications. However, in order\nto determine whether these networks truly operate differently than traditional\nnetworks, one must look at the differences in the capsule features. To this\nend, we perform several analyses with the purpose of elucidating capsule\nfeatures and determining whether they perform as described in the initial\npublication. First, we perform a deep visualization analysis to visually\ncompare capsule features and convolutional neural network features. Then, we\nlook at the ability for capsule features to encode information across the\nvector components and address what changes in the capsule architecture provides\nthe most benefit. Finally, we look at how well the capsule features are able to\nencode instantiation parameters of class objects via visual transformations.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2001.10964v1",
        "date": "2020-01-29 17:18:43+00:00"
    },
    {
        "title": "Consistent Representation Learning for High Dimensional Data Analysis",
        "authors": [
            "Stan Z. Li",
            "Lirong Wu",
            "Zelin Zang"
        ],
        "abstract": "High dimensional data analysis for exploration and discovery includes three\nfundamental tasks: dimensionality reduction, clustering, and visualization.\nWhen the three associated tasks are done separately, as is often the case thus\nfar, inconsistencies can occur among the tasks in terms of data geometry and\nothers. This can lead to confusing or misleading data interpretation. In this\npaper, we propose a novel neural network-based method, called Consistent\nRepresentation Learning (CRL), to accomplish the three associated tasks\nend-to-end and improve the consistencies. The CRL network consists of two\nnonlinear dimensionality reduction (NLDR) transformations: (1) one from the\ninput data space to the latent feature space for clustering, and (2) the other\nfrom the clustering space to the final 2D or 3D space for visualization.\nImportantly, the two NLDR transformations are performed to best satisfy local\ngeometry preserving (LGP) constraints across the spaces or network layers, to\nimprove data consistencies along with the processing flow. Also, we propose a\nnovel metric, clustering-visualization inconsistency (CVI), for evaluating the\ninconsistencies. Extensive comparative results show that the proposed CRL\nneural network method outperforms the popular t-SNE and UMAP-based and other\ncontemporary clustering and visualization algorithms in terms of evaluation\nmetrics and visualization.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2012.00481v1",
        "date": "2020-12-01 13:39:50+00:00"
    },
    {
        "title": "On the Limits of Learning Representations with Label-Based Supervision",
        "authors": [
            "Jiaming Song",
            "Russell Stewart",
            "Shengjia Zhao",
            "Stefano Ermon"
        ],
        "abstract": "Advances in neural network based classifiers have transformed automatic\nfeature learning from a pipe dream of stronger AI to a routine and expected\nproperty of practical systems. Since the emergence of AlexNet every winning\nsubmission of the ImageNet challenge has employed end-to-end representation\nlearning, and due to the utility of good representations for transfer learning,\nrepresentation learning has become as an important and distinct task from\nsupervised learning. At present, this distinction is inconsequential, as\nsupervised methods are state-of-the-art in learning transferable\nrepresentations. But recent work has shown that generative models can also be\npowerful agents of representation learning. Will the representations learned\nfrom these generative methods ever rival the quality of those from their\nsupervised competitors? In this work, we argue in the affirmative, that from an\ninformation theoretic perspective, generative models have greater potential for\nrepresentation learning. Based on several experimentally validated assumptions,\nwe show that supervised learning is upper bounded in its capacity for\nrepresentation learning in ways that certain generative models, such as\nGenerative Adversarial Networks (GANs) are not. We hope that our analysis will\nprovide a rigorous motivation for further exploration of generative\nrepresentation learning.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1703.02156v1",
        "date": "2017-03-07 00:09:31+00:00"
    },
    {
        "title": "Deephys: Deep Electrophysiology, Debugging Neural Networks under Distribution Shifts",
        "authors": [
            "Anirban Sarkar",
            "Matthew Groth",
            "Ian Mason",
            "Tomotake Sasaki",
            "Xavier Boix"
        ],
        "abstract": "Deep Neural Networks (DNNs) often fail in out-of-distribution scenarios. In\nthis paper, we introduce a tool to visualize and understand such failures. We\ndraw inspiration from concepts from neural electrophysiology, which are based\non inspecting the internal functioning of a neural networks by analyzing the\nfeature tuning and invariances of individual units. Deep Electrophysiology, in\nshort Deephys, provides insights of the DNN's failures in out-of-distribution\nscenarios by comparative visualization of the neural activity in\nin-distribution and out-of-distribution datasets. Deephys provides seamless\nanalyses of individual neurons, individual images, and a set of set of images\nfrom a category, and it is capable of revealing failures due to the presence of\nspurious features and novel features. We substantiate the validity of the\nqualitative visualizations of Deephys thorough quantitative analyses using\nconvolutional and transformers architectures, in several datasets and\ndistribution shifts (namely, colored MNIST, CIFAR-10 and ImageNet).",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2303.11912v1",
        "date": "2023-03-17 21:13:41+00:00"
    },
    {
        "title": "A Deep Learning based approach to VM behavior identification in cloud systems",
        "authors": [
            "Matteo Stefanini",
            "Riccardo Lancellotti",
            "Lorenzo Baraldi",
            "Simone Calderara"
        ],
        "abstract": "Cloud computing data centers are growing in size and complexity to the point\nwhere monitoring and management of the infrastructure become a challenge due to\nscalability issues. A possible approach to cope with the size of such data\ncenters is to identify VMs exhibiting a similar behavior. Existing literature\ndemonstrated that clustering together VMs that show a similar behavior may\nimprove the scalability of both monitoring andmanagement of a data center.\nHowever, available techniques suffer from a trade-off between accuracy and time\nto achieve this result. Throughout this paper we propose a different approach\nwhere, instead of an unsupervised clustering, we rely on classifiers based on\ndeep learning techniques to assigna newly deployed VMs to a cluster of\nalready-known VMs. The two proposed classifiers, namely DeepConv and DeepFFT\nuse a convolution neural network and (in the latter model) exploits Fast\nFourier Transformation to classify the VMs. Our proposal is validated using a\nset of traces describing the behavior of VMs from a realcloud data center. The\nexperiments compare our proposal with state-of-the-art solutions available in\nliterature, demonstrating that our proposal achieve better performance.\nFurthermore, we show that our solution issignificantly faster than the\nalternatives as it can produce a perfect classification even with just a few\nsamples of data, making our proposal viable also toclassify on-demand VMs that\nare characterized by a short life span.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1903.01930v1",
        "date": "2019-03-05 16:49:00+00:00"
    },
    {
        "title": "You are AllSet: A Multiset Function Framework for Hypergraph Neural Networks",
        "authors": [
            "Eli Chien",
            "Chao Pan",
            "Jianhao Peng",
            "Olgica Milenkovic"
        ],
        "abstract": "Hypergraphs are used to model higher-order interactions amongst agents and\nthere exist many practically relevant instances of hypergraph datasets. To\nenable efficient processing of hypergraph-structured data, several hypergraph\nneural network platforms have been proposed for learning hypergraph properties\nand structure, with a special focus on node classification. However, almost all\nexisting methods use heuristic propagation rules and offer suboptimal\nperformance on many datasets. We propose AllSet, a new hypergraph neural\nnetwork paradigm that represents a highly general framework for (hyper)graph\nneural networks and for the first time implements hypergraph neural network\nlayers as compositions of two multiset functions that can be efficiently\nlearned for each task and each dataset. Furthermore, AllSet draws on new\nconnections between hypergraph neural networks and recent advances in deep\nlearning of multiset functions. In particular, the proposed architecture\nutilizes Deep Sets and Set Transformer architectures that allow for significant\nmodeling flexibility and offer high expressive power. To evaluate the\nperformance of AllSet, we conduct the most extensive experiments to date\ninvolving ten known benchmarking datasets and three newly curated datasets that\nrepresent significant challenges for hypergraph node classification. The\nresults demonstrate that AllSet has the unique ability to consistently either\nmatch or outperform all other hypergraph neural networks across the tested\ndatasets.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2106.13264v4",
        "date": "2021-06-24 18:10:08+00:00"
    },
    {
        "title": "Fourier Spectrum Discrepancies in Deep Network Generated Images",
        "authors": [
            "Tarik Dzanic",
            "Karan Shah",
            "Freddie Witherden"
        ],
        "abstract": "Advancements in deep generative models such as generative adversarial\nnetworks and variational autoencoders have resulted in the ability to generate\nrealistic images that are visually indistinguishable from real images, which\nraises concerns about their potential malicious usage. In this paper, we\npresent an analysis of the high-frequency Fourier modes of real and deep\nnetwork generated images and show that deep network generated images share an\nobservable, systematic shortcoming in replicating the attributes of these\nhigh-frequency modes. Using this, we propose a detection method based on the\nfrequency spectrum of the images which is able to achieve an accuracy of up to\n99.2% in classifying real and deep network generated images from various GAN\nand VAE architectures on a dataset of 5000 images with as few as 8 training\nexamples. Furthermore, we show the impact of image transformations such as\ncompression, cropping, and resolution reduction on the classification accuracy\nand suggest a method for modifying the high-frequency attributes of deep\nnetwork generated images to mimic real images.",
        "categories": [
            "eess.IV",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1911.06465v3",
        "date": "2019-11-15 03:55:12+00:00"
    },
    {
        "title": "Hide and Speak: Towards Deep Neural Networks for Speech Steganography",
        "authors": [
            "Felix Kreuk",
            "Yossi Adi",
            "Bhiksha Raj",
            "Rita Singh",
            "Joseph Keshet"
        ],
        "abstract": "Steganography is the science of hiding a secret message within an ordinary\npublic message, which is referred to as Carrier. Traditionally, digital signal\nprocessing techniques, such as least significant bit encoding, were used for\nhiding messages. In this paper, we explore the use of deep neural networks as\nsteganographic functions for speech data. We showed that steganography models\nproposed for vision are less suitable for speech, and propose a new model that\nincludes the short-time Fourier transform and inverse-short-time Fourier\ntransform as differentiable layers within the network, thus imposing a vital\nconstraint on the network outputs. We empirically demonstrated the\neffectiveness of the proposed method comparing to deep learning based on\nseveral speech datasets and analyzed the results quantitatively and\nqualitatively. Moreover, we showed that the proposed approach could be applied\nto conceal multiple messages in a single carrier using multiple decoders or a\nsingle conditional decoder. Lastly, we evaluated our model under different\nchannel distortions. Qualitative experiments suggest that modifications to the\ncarrier are unnoticeable by human listeners and that the decoded messages are\nhighly intelligible.",
        "categories": [
            "cs.SD",
            "cs.CR",
            "cs.LG",
            "eess.AS",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1902.03083v2",
        "date": "2019-02-07 13:48:28+00:00"
    },
    {
        "title": "Complex-valued neural networks for machine learning on non-stationary physical data",
        "authors": [
            "Jesper S\u00f6ren Dramsch",
            "Mikael L\u00fcthje",
            "Anders Nymark Christensen"
        ],
        "abstract": "Deep learning has become an area of interest in most scientific areas,\nincluding physical sciences. Modern networks apply real-valued transformations\non the data. Particularly, convolutions in convolutional neural networks\ndiscard phase information entirely. Many deterministic signals, such as seismic\ndata or electrical signals, contain significant information in the phase of the\nsignal. We explore complex-valued deep convolutional networks to leverage\nnon-linear feature maps. Seismic data commonly has a lowcut filter applied, to\nattenuate noise from ocean waves and similar long wavelength contributions.\nDiscarding the phase information leads to low-frequency aliasing analogous to\nthe Nyquist-Shannon theorem for high frequencies. In non-stationary data, the\nphase content can stabilize training and improve the generalizability of neural\nnetworks. While it has been shown that phase content can be restored in deep\nneural networks, we show how including phase information in feature maps\nimproves both training and inference from deterministic physical data.\nFurthermore, we show that the reduction of parameters in a complex network\noutperforms larger real-valued networks.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "physics.comp-ph",
            "physics.geo-ph",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1905.12321v2",
        "date": "2019-05-29 10:47:42+00:00"
    },
    {
        "title": "Efficient Architecture Search by Network Transformation",
        "authors": [
            "Han Cai",
            "Tianyao Chen",
            "Weinan Zhang",
            "Yong Yu",
            "Jun Wang"
        ],
        "abstract": "Techniques for automatically designing deep neural network architectures such\nas reinforcement learning based approaches have recently shown promising\nresults. However, their success is based on vast computational resources (e.g.\nhundreds of GPUs), making them difficult to be widely used. A noticeable\nlimitation is that they still design and train each network from scratch during\nthe exploration of the architecture space, which is highly inefficient. In this\npaper, we propose a new framework toward efficient architecture search by\nexploring the architecture space based on the current network and reusing its\nweights. We employ a reinforcement learning agent as the meta-controller, whose\naction is to grow the network depth or layer width with function-preserving\ntransformations. As such, the previously validated networks can be reused for\nfurther exploration, thus saves a large amount of computational cost. We apply\nour method to explore the architecture space of the plain convolutional neural\nnetworks (no skip-connections, branching etc.) on image benchmark datasets\n(CIFAR-10, SVHN) with restricted computational resources (5 GPUs). Our method\ncan design highly competitive networks that outperform existing networks using\nthe same design scheme. On CIFAR-10, our model without skip-connections\nachieves 4.23\\% test error rate, exceeding a vast majority of modern\narchitectures and approaching DenseNet. Furthermore, by applying our method to\nexplore the DenseNet architecture space, we are able to achieve more accurate\nnetworks with fewer parameters.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/1707.04873v2",
        "date": "2017-07-16 12:39:02+00:00"
    },
    {
        "title": "Transformative Machine Learning",
        "authors": [
            "Ivan Olier",
            "Oghenejokpeme I. Orhobor",
            "Joaquin Vanschoren",
            "Ross D. King"
        ],
        "abstract": "The key to success in machine learning (ML) is the use of effective data\nrepresentations. Traditionally, data representations were hand-crafted.\nRecently it has been demonstrated that, given sufficient data, deep neural\nnetworks can learn effective implicit representations from simple input\nrepresentations. However, for most scientific problems, the use of deep\nlearning is not appropriate as the amount of available data is limited, and/or\nthe output models must be explainable. Nevertheless, many scientific problems\ndo have significant amounts of data available on related tasks, which makes\nthem amenable to multi-task learning, i.e. learning many related problems\nsimultaneously. Here we propose a novel and general representation learning\napproach for multi-task learning that works successfully with small amounts of\ndata. The fundamental new idea is to transform an input intrinsic data\nrepresentation (i.e., handcrafted features), to an extrinsic representation\nbased on what a pre-trained set of models predict about the examples. This\ntransformation has the dual advantages of producing significantly more accurate\npredictions, and providing explainable models. To demonstrate the utility of\nthis transformative learning approach, we have applied it to three real-world\nscientific problems: drug-design (quantitative structure activity relationship\nlearning), predicting human gene expression (across different tissue types and\ndrug treatments), and meta-learning for machine learning (predicting which\nmachine learning methods work best for a given problem). In all three problems,\ntransformative machine learning significantly outperforms the best intrinsic\nrepresentation.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1811.03392v1",
        "date": "2018-11-08 13:09:05+00:00"
    },
    {
        "title": "Learning with Pseudo-Ensembles",
        "authors": [
            "Philip Bachman",
            "Ouais Alsharif",
            "Doina Precup"
        ],
        "abstract": "We formalize the notion of a pseudo-ensemble, a (possibly infinite)\ncollection of child models spawned from a parent model by perturbing it\naccording to some noise process. E.g., dropout (Hinton et. al, 2012) in a deep\nneural network trains a pseudo-ensemble of child subnetworks generated by\nrandomly masking nodes in the parent network. We present a novel regularizer\nbased on making the behavior of a pseudo-ensemble robust with respect to the\nnoise process generating it. In the fully-supervised setting, our regularizer\nmatches the performance of dropout. But, unlike dropout, our regularizer\nnaturally extends to the semi-supervised setting, where it produces\nstate-of-the-art results. We provide a case study in which we transform the\nRecursive Neural Tensor Network of (Socher et. al, 2013) into a\npseudo-ensemble, which significantly improves its performance on a real-world\nsentiment analysis benchmark.",
        "categories": [
            "stat.ML",
            "cs.LG",
            "cs.NE"
        ],
        "link": "http://arxiv.org/pdf/1412.4864v1",
        "date": "2014-12-16 02:55:05+00:00"
    },
    {
        "title": "Adaptive Laplace Mechanism: Differential Privacy Preservation in Deep Learning",
        "authors": [
            "NhatHai Phan",
            "Xintao Wu",
            "Han Hu",
            "Dejing Dou"
        ],
        "abstract": "In this paper, we focus on developing a novel mechanism to preserve\ndifferential privacy in deep neural networks, such that: (1) The privacy budget\nconsumption is totally independent of the number of training steps; (2) It has\nthe ability to adaptively inject noise into features based on the contribution\nof each to the output; and (3) It could be applied in a variety of different\ndeep neural networks. To achieve this, we figure out a way to perturb affine\ntransformations of neurons, and loss functions used in deep neural networks. In\naddition, our mechanism intentionally adds \"more noise\" into features which are\n\"less relevant\" to the model output, and vice-versa. Our theoretical analysis\nfurther derives the sensitivities and error bounds of our mechanism. Rigorous\nexperiments conducted on MNIST and CIFAR-10 datasets show that our mechanism is\nhighly effective and outperforms existing solutions.",
        "categories": [
            "cs.CR",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1709.05750v2",
        "date": "2017-09-18 02:37:40+00:00"
    },
    {
        "title": "Transfer Learning in Deep Learning Models for Building Load Forecasting: Case of Limited Data",
        "authors": [
            "Menna Nawar",
            "Moustafa Shomer",
            "Samy Faddel",
            "Huangjie Gong"
        ],
        "abstract": "Precise load forecasting in buildings could increase the bill savings\npotential and facilitate optimized strategies for power generation planning.\nWith the rapid evolution of computer science, data-driven techniques, in\nparticular the Deep Learning models, have become a promising solution for the\nload forecasting problem. These models have showed accurate forecasting\nresults; however, they need abundance amount of historical data to maintain the\nperformance. Considering the new buildings and buildings with low resolution\nmeasuring equipment, it is difficult to get enough historical data from them,\nleading to poor forecasting performance. In order to adapt Deep Learning models\nfor buildings with limited and scarce data, this paper proposes a\nBuilding-to-Building Transfer Learning framework to overcome the problem and\nenhance the performance of Deep Learning models. The transfer learning approach\nwas applied to a new technique known as Transformer model due to its efficacy\nin capturing data trends. The performance of the algorithm was tested on a\nlarge commercial building with limited data. The result showed that the\nproposed approach improved the forecasting accuracy by 56.8% compared to the\ncase of conventional deep learning where training from scratch is used. The\npaper also compared the proposed Transformer model to other sequential deep\nlearning models such as Long-short Term Memory (LSTM) and Recurrent Neural\nNetwork (RNN). The accuracy of the transformer model outperformed other models\nby reducing the root mean square error to 0.009, compared to LSTM with 0.011\nand RNN with 0.051.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2301.10663v2",
        "date": "2023-01-25 16:05:47+00:00"
    },
    {
        "title": "Invariance Learning in Deep Neural Networks with Differentiable Laplace Approximations",
        "authors": [
            "Alexander Immer",
            "Tycho F. A. van der Ouderaa",
            "Gunnar R\u00e4tsch",
            "Vincent Fortuin",
            "Mark van der Wilk"
        ],
        "abstract": "Data augmentation is commonly applied to improve performance of deep learning\nby enforcing the knowledge that certain transformations on the input preserve\nthe output. Currently, the data augmentation parameters are chosen by human\neffort and costly cross-validation, which makes it cumbersome to apply to new\ndatasets. We develop a convenient gradient-based method for selecting the data\naugmentation without validation data during training of a deep neural network.\nOur approach relies on phrasing data augmentation as an invariance in the prior\ndistribution on the functions of a neural network, which allows us to learn it\nusing Bayesian model selection. This has been shown to work in Gaussian\nprocesses, but not yet for deep neural networks. We propose a differentiable\nKronecker-factored Laplace approximation to the marginal likelihood as our\nobjective, which can be optimised without human supervision or validation data.\nWe show that our method can successfully recover invariances present in the\ndata, and that this improves generalisation and data efficiency on image\ndatasets.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2202.10638v3",
        "date": "2022-02-22 02:51:11+00:00"
    },
    {
        "title": "Compression of Deep Learning Models for Text: A Survey",
        "authors": [
            "Manish Gupta",
            "Puneet Agrawal"
        ],
        "abstract": "In recent years, the fields of natural language processing (NLP) and\ninformation retrieval (IR) have made tremendous progress thanksto deep learning\nmodels like Recurrent Neural Networks (RNNs), Gated Recurrent Units (GRUs) and\nLong Short-Term Memory (LSTMs)networks, and Transformer [120] based models like\nBidirectional Encoder Representations from Transformers (BERT) [24],\nGenerativePre-training Transformer (GPT-2) [94], Multi-task Deep Neural Network\n(MT-DNN) [73], Extra-Long Network (XLNet) [134], Text-to-text transfer\ntransformer (T5) [95], T-NLG [98] and GShard [63]. But these models are\nhumongous in size. On the other hand,real world applications demand small model\nsize, low response times and low computational power wattage. In this survey,\nwediscuss six different types of methods (Pruning, Quantization, Knowledge\nDistillation, Parameter Sharing, Tensor Decomposition, andSub-quadratic\nTransformer based methods) for compression of such models to enable their\ndeployment in real industry NLP projects.Given the critical need of building\napplications with efficient and small models, and the large amount of recently\npublished work inthis area, we believe that this survey organizes the plethora\nof work done by the 'deep learning for NLP' community in the past fewyears and\npresents it as a coherent story.",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2008.05221v4",
        "date": "2020-08-12 10:42:14+00:00"
    },
    {
        "title": "Stochastic Feedforward Neural Networks: Universal Approximation",
        "authors": [
            "Thomas Merkh",
            "Guido Mont\u00fafar"
        ],
        "abstract": "In this chapter we take a look at the universal approximation question for\nstochastic feedforward neural networks. In contrast to deterministic networks,\nwhich represent mappings from a set of inputs to a set of outputs, stochastic\nnetworks represent mappings from a set of inputs to a set of probability\ndistributions over the set of outputs. In particular, even if the sets of\ninputs and outputs are finite, the class of stochastic mappings in question is\nnot finite. Moreover, while for a deterministic function the values of all\noutput variables can be computed independently of each other given the values\nof the inputs, in the stochastic setting the values of the output variables may\nneed to be correlated, which requires that their values are computed jointly. A\nprominent class of stochastic feedforward networks which has played a key role\nin the resurgence of deep learning are deep belief networks. The\nrepresentational power of these networks has been studied mainly in the\ngenerative setting, as models of probability distributions without an input, or\nin the discriminative setting for the special case of deterministic mappings.\nWe study the representational power of deep sigmoid belief networks in terms of\ncompositions of linear transformations of probability distributions, Markov\nkernels, that can be expressed by the layers of the network. We investigate\ndifferent types of shallow and deep architectures, and the minimal number of\nlayers and units per layer that are sufficient and necessary in order for the\nnetwork to be able to approximate any given stochastic mapping from the set of\ninputs to the set of outputs arbitrarily well.",
        "categories": [
            "cs.LG",
            "cs.NE",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1910.09763v1",
        "date": "2019-10-22 04:49:43+00:00"
    },
    {
        "title": "MDP Homomorphic Networks: Group Symmetries in Reinforcement Learning",
        "authors": [
            "Elise van der Pol",
            "Daniel E. Worrall",
            "Herke van Hoof",
            "Frans A. Oliehoek",
            "Max Welling"
        ],
        "abstract": "This paper introduces MDP homomorphic networks for deep reinforcement\nlearning. MDP homomorphic networks are neural networks that are equivariant\nunder symmetries in the joint state-action space of an MDP. Current approaches\nto deep reinforcement learning do not usually exploit knowledge about such\nstructure. By building this prior knowledge into policy and value networks\nusing an equivariance constraint, we can reduce the size of the solution space.\nWe specifically focus on group-structured symmetries (invertible\ntransformations). Additionally, we introduce an easy method for constructing\nequivariant network layers numerically, so the system designer need not solve\nthe constraints by hand, as is typically done. We construct MDP homomorphic\nMLPs and CNNs that are equivariant under either a group of reflections or\nrotations. We show that such networks converge faster than unstructured\nbaselines on CartPole, a grid world and Pong.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2006.16908v2",
        "date": "2020-06-30 15:38:37+00:00"
    },
    {
        "title": "ProdSumNet: reducing model parameters in deep neural networks via product-of-sums matrix decompositions",
        "authors": [
            "Chai Wah Wu"
        ],
        "abstract": "We consider a general framework for reducing the number of trainable model\nparameters in deep learning networks by decomposing linear operators as a\nproduct of sums of simpler linear operators. Recently proposed deep learning\narchitectures such as CNN, KFC, Dilated CNN, etc. are all subsumed in this\nframework and we illustrate other types of neural network architectures within\nthis framework. We show that good accuracy on MNIST and Fashion MNIST can be\nobtained using a relatively small number of trainable parameters. In addition,\nsince implementation of the convolutional layer is resource-heavy, we consider\nan approach in the transform domain that obviates the need for convolutional\nlayers. One of the advantages of this general framework over prior approaches\nis that the number of trainable parameters is not fixed and can be varied\narbitrarily. In particular, we illustrate the tradeoff of varying the number of\ntrainable variables and the corresponding error rate. As an example, by using\nthis decomposition on a reference CNN architecture for MNIST with over 3x10^6\ntrainable parameters, we are able to obtain an accuracy of 98.44% using only\n3554 trainable parameters.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1809.02209v2",
        "date": "2018-09-06 20:50:40+00:00"
    },
    {
        "title": "CubeNet: Equivariance to 3D Rotation and Translation",
        "authors": [
            "Daniel Worrall",
            "Gabriel Brostow"
        ],
        "abstract": "3D Convolutional Neural Networks are sensitive to transformations applied to\ntheir input. This is a problem because a voxelized version of a 3D object, and\nits rotated clone, will look unrelated to each other after passing through to\nthe last layer of a network. Instead, an idealized model would preserve a\nmeaningful representation of the voxelized object, while explaining the\npose-difference between the two inputs. An equivariant representation vector\nhas two components: the invariant identity part, and a discernable encoding of\nthe transformation. Models that can't explain pose-differences risk \"diluting\"\nthe representation, in pursuit of optimizing a classification or regression\nloss function.\n  We introduce a Group Convolutional Neural Network with linear equivariance to\ntranslations and right angle rotations in three dimensions. We call this\nnetwork CubeNet, reflecting its cube-like symmetry. By construction, this\nnetwork helps preserve a 3D shape's global and local signature, as it is\ntransformed through successive layers. We apply this network to a variety of 3D\ninference problems, achieving state-of-the-art on the ModelNet10 classification\nchallenge, and comparable performance on the ISBI 2012 Connectome Segmentation\nBenchmark. To the best of our knowledge, this is the first 3D rotation\nequivariant CNN for voxel representations.",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1804.04458v1",
        "date": "2018-04-12 12:14:18+00:00"
    },
    {
        "title": "A Structural Approach to the Design of Domain Specific Neural Network Architectures",
        "authors": [
            "Gerrit Nolte"
        ],
        "abstract": "This is a master's thesis concerning the theoretical ideas of geometric deep\nlearning. Geometric deep learning aims to provide a structured characterization\nof neural network architectures, specifically focused on the ideas of\ninvariance and equivariance of data with respect to given transformations.\n  This thesis aims to provide a theoretical evaluation of geometric deep\nlearning, compiling theoretical results that characterize the properties of\ninvariant neural networks with respect to learning performance.",
        "categories": [
            "cs.LG",
            "cs.NE"
        ],
        "link": "http://arxiv.org/pdf/2301.09381v1",
        "date": "2023-01-23 11:50:57+00:00"
    },
    {
        "title": "Scalable Out-of-Sample Extension of Graph Embeddings Using Deep Neural Networks",
        "authors": [
            "Aren Jansen",
            "Gregory Sell",
            "Vince Lyzinski"
        ],
        "abstract": "Several popular graph embedding techniques for representation learning and\ndimensionality reduction rely on performing computationally expensive\neigendecompositions to derive a nonlinear transformation of the input data\nspace. The resulting eigenvectors encode the embedding coordinates for the\ntraining samples only, and so the embedding of novel data samples requires\nfurther costly computation. In this paper, we present a method for the\nout-of-sample extension of graph embeddings using deep neural networks (DNN) to\nparametrically approximate these nonlinear maps. Compared with traditional\nnonparametric out-of-sample extension methods, we demonstrate that the DNNs can\ngeneralize with equal or better fidelity and require orders of magnitude less\ncomputation at test time. Moreover, we find that unsupervised pretraining of\nthe DNNs improves optimization for larger network sizes, thus removing\nsensitivity to model selection.",
        "categories": [
            "stat.ML",
            "cs.LG",
            "cs.NE",
            "stat.ME"
        ],
        "link": "http://arxiv.org/pdf/1508.04422v3",
        "date": "2015-08-18 19:47:31+00:00"
    },
    {
        "title": "An Empirical Survey of Data Augmentation for Time Series Classification with Neural Networks",
        "authors": [
            "Brian Kenji Iwana",
            "Seiichi Uchida"
        ],
        "abstract": "In recent times, deep artificial neural networks have achieved many successes\nin pattern recognition. Part of this success can be attributed to the reliance\non big data to increase generalization. However, in the field of time series\nrecognition, many datasets are often very small. One method of addressing this\nproblem is through the use of data augmentation. In this paper, we survey data\naugmentation techniques for time series and their application to time series\nclassification with neural networks. We propose a taxonomy and outline the four\nfamilies in time series data augmentation, including transformation-based\nmethods, pattern mixing, generative models, and decomposition methods.\nFurthermore, we empirically evaluate 12 time series data augmentation methods\non 128 time series classification datasets with six different types of neural\nnetworks. Through the results, we are able to analyze the characteristics,\nadvantages and disadvantages, and recommendations of each data augmentation\nmethod. This survey aims to help in the selection of time series data\naugmentation for neural network applications.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2007.15951v4",
        "date": "2020-07-31 10:33:54+00:00"
    },
    {
        "title": "Enforcing Linearity in DNN succours Robustness and Adversarial Image Generation",
        "authors": [
            "Anindya Sarkar",
            "Nikhil Kumar Gupta",
            "Raghu Iyengar"
        ],
        "abstract": "Recent studies on the adversarial vulnerability of neural networks have shown\nthat models trained with the objective of minimizing an upper bound on the\nworst-case loss over all possible adversarial perturbations improve robustness\nagainst adversarial attacks. Beside exploiting adversarial training framework,\nwe show that by enforcing a Deep Neural Network (DNN) to be linear in\ntransformed input and feature space improves robustness significantly. We also\ndemonstrate that by augmenting the objective function with Local Lipschitz\nregularizer boost robustness of the model further. Our method outperforms most\nsophisticated adversarial training methods and achieves state of the art\nadversarial accuracy on MNIST, CIFAR10 and SVHN dataset. In this paper, we also\npropose a novel adversarial image generation method by leveraging Inverse\nRepresentation Learning and Linearity aspect of an adversarially trained deep\nneural network classifier.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1910.08108v2",
        "date": "2019-10-17 18:38:40+00:00"
    },
    {
        "title": "Equivalent and Approximate Transformations of Deep Neural Networks",
        "authors": [
            "Abhinav Kumar",
            "Thiago Serra",
            "Srikumar Ramalingam"
        ],
        "abstract": "Two networks are equivalent if they produce the same output for any given\ninput. In this paper, we study the possibility of transforming a deep neural\nnetwork to another network with a different number of units or layers, which\ncan be either equivalent, a local exact approximation, or a global linear\napproximation of the original network. On the practical side, we show that\ncertain rectified linear units (ReLUs) can be safely removed from a network if\nthey are always active or inactive for any valid input. If we only need an\nequivalent network for a smaller domain, then more units can be removed and\nsome layers collapsed. On the theoretical side, we constructively show that for\nany feed-forward ReLU network, there exists a global linear approximation to a\n2-hidden-layer shallow network with a fixed number of units. This result is a\nbalance between the increasing number of units for arbitrary approximation with\na single layer and the known upper bound of $\\lceil log(n_0+1)\\rceil +1$ layers\nfor exact representation, where $n_0$ is the input dimension. While the\ntransformed network may require an exponential number of units to capture the\nactivation patterns of the original network, we show that it can be made\nsubstantially smaller by only accounting for the patterns that define linear\nregions. Based on experiments with ReLU networks on the MNIST dataset, we found\nthat $l_1$-regularization and adversarial training reduces the number of linear\nregions significantly as the number of stable units increases due to weight\nsparsity. Therefore, we can also intentionally train ReLU networks to allow for\neffective loss-less compression and approximation.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1905.11428v1",
        "date": "2019-05-27 18:05:28+00:00"
    },
    {
        "title": "Transformer Neural Networks Attending to Both Sequence and Structure for Protein Prediction Tasks",
        "authors": [
            "Anowarul Kabir",
            "Amarda Shehu"
        ],
        "abstract": "The increasing number of protein sequences decoded from genomes is opening up\nnew avenues of research on linking protein sequence to function with\ntransformer neural networks. Recent research has shown that the number of known\nprotein sequences supports learning useful, task-agnostic sequence\nrepresentations via transformers. In this paper, we posit that learning joint\nsequence-structure representations yields better representations for\nfunction-related prediction tasks. We propose a transformer neural network that\nattends to both sequence and tertiary structure. We show that such joint\nrepresentations are more powerful than sequence-based representations only, and\nthey yield better performance on superfamily membership across various metrics.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "q-bio.QM"
        ],
        "link": "http://arxiv.org/pdf/2206.11057v1",
        "date": "2022-06-17 18:40:19+00:00"
    },
    {
        "title": "A Projection-Based K-space Transformer Network for Undersampled Radial MRI Reconstruction with Limited Training Subjects",
        "authors": [
            "Chang Gao",
            "Shu-Fu Shih",
            "J. Paul Finn",
            "Xiaodong Zhong"
        ],
        "abstract": "The recent development of deep learning combined with compressed sensing\nenables fast reconstruction of undersampled MR images and has achieved\nstate-of-the-art performance for Cartesian k-space trajectories. However,\nnon-Cartesian trajectories such as the radial trajectory need to be transformed\nonto a Cartesian grid in each iteration of the network training, slowing down\nthe training process and posing inconvenience and delay during training.\nMultiple iterations of nonuniform Fourier transform in the networks offset the\ndeep learning advantage of fast inference. Current approaches typically either\nwork on image-to-image networks or grid the non-Cartesian trajectories before\nthe network training to avoid the repeated gridding process. However, the\nimage-to-image networks cannot ensure the k-space data consistency in the\nreconstructed images and the pre-processing of non-Cartesian k-space leads to\ngridding errors which cannot be compensated by the network training. Inspired\nby the Transformer network to handle long-range dependencies in sequence\ntransduction tasks, we propose to rearrange the radial spokes to sequential\ndata based on the chronological order of acquisition and use the Transformer to\npredict unacquired radial spokes from acquired ones. We propose novel data\naugmentation methods to generate a large amount of training data from a limited\nnumber of subjects. The network can be generated to different anatomical\nstructures. Experimental results show superior performance of the proposed\nframework compared to state-of-the-art deep neural networks.",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2206.07219v2",
        "date": "2022-06-15 00:20:22+00:00"
    },
    {
        "title": "Dynamically-Scaled Deep Canonical Correlation Analysis",
        "authors": [
            "Tomer Friedlander",
            "Lior Wolf"
        ],
        "abstract": "Canonical Correlation Analysis (CCA) is a method for feature extraction of\ntwo views by finding maximally correlated linear projections of them. Several\nvariants of CCA have been introduced in the literature, in particular, variants\nbased on deep neural networks for learning highly correlated nonlinear\ntransformations of two views. As these models are parameterized conventionally,\ntheir learnable parameters remain independent of the inputs after the training\nprocess, which may limit their capacity for learning highly correlated\nrepresentations. We introduce a novel dynamic scaling method for training an\ninput-dependent canonical correlation model. In our deep-CCA models, the\nparameters of the last layer are scaled by a second neural network that is\nconditioned on the model's input, resulting in a parameterization that is\ndependent on the input samples. We evaluate our model on multiple datasets and\ndemonstrate that the learned representations are more correlated in comparison\nto the conventionally-parameterized CCA-based models and also obtain preferable\nretrieval results. Our code is available at\nhttps://github.com/tomerfr/DynamicallyScaledDeepCCA.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2203.12377v2",
        "date": "2022-03-23 12:52:49+00:00"
    },
    {
        "title": "FFT-Based Deep Learning Deployment in Embedded Systems",
        "authors": [
            "Sheng Lin",
            "Ning Liu",
            "Mahdi Nazemi",
            "Hongjia Li",
            "Caiwen Ding",
            "Yanzhi Wang",
            "Massoud Pedram"
        ],
        "abstract": "Deep learning has delivered its powerfulness in many application domains,\nespecially in image and speech recognition. As the backbone of deep learning,\ndeep neural networks (DNNs) consist of multiple layers of various types with\nhundreds to thousands of neurons. Embedded platforms are now becoming essential\nfor deep learning deployment due to their portability, versatility, and energy\nefficiency. The large model size of DNNs, while providing excellent accuracy,\nalso burdens the embedded platforms with intensive computation and storage.\nResearchers have investigated on reducing DNN model size with negligible\naccuracy loss. This work proposes a Fast Fourier Transform (FFT)-based DNN\ntraining and inference model suitable for embedded platforms with reduced\nasymptotic complexity of both computation and storage, making our approach\ndistinguished from existing approaches. We develop the training and inference\nalgorithms based on FFT as the computing kernel and deploy the FFT-based\ninference model on embedded platforms achieving extraordinary processing speed.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1712.04910v1",
        "date": "2017-12-13 18:26:17+00:00"
    },
    {
        "title": "Do Vision Transformers See Like Convolutional Neural Networks?",
        "authors": [
            "Maithra Raghu",
            "Thomas Unterthiner",
            "Simon Kornblith",
            "Chiyuan Zhang",
            "Alexey Dosovitskiy"
        ],
        "abstract": "Convolutional neural networks (CNNs) have so far been the de-facto model for\nvisual data. Recent work has shown that (Vision) Transformer models (ViT) can\nachieve comparable or even superior performance on image classification tasks.\nThis raises a central question: how are Vision Transformers solving these\ntasks? Are they acting like convolutional networks, or learning entirely\ndifferent visual representations? Analyzing the internal representation\nstructure of ViTs and CNNs on image classification benchmarks, we find striking\ndifferences between the two architectures, such as ViT having more uniform\nrepresentations across all layers. We explore how these differences arise,\nfinding crucial roles played by self-attention, which enables early aggregation\nof global information, and ViT residual connections, which strongly propagate\nfeatures from lower to higher layers. We study the ramifications for spatial\nlocalization, demonstrating ViTs successfully preserve input spatial\ninformation, with noticeable effects from different classification methods.\nFinally, we study the effect of (pretraining) dataset scale on intermediate\nfeatures and transfer learning, and conclude with a discussion on connections\nto new architectures such as the MLP-Mixer.",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2108.08810v2",
        "date": "2021-08-19 17:27:03+00:00"
    },
    {
        "title": "Fairness-Aware Neural R\u00e9yni Minimization for Continuous Features",
        "authors": [
            "Vincent Grari",
            "Boris Ruf",
            "Sylvain Lamprier",
            "Marcin Detyniecki"
        ],
        "abstract": "The past few years have seen a dramatic rise of academic and societal\ninterest in fair machine learning. While plenty of fair algorithms have been\nproposed recently to tackle this challenge for discrete variables, only a few\nideas exist for continuous ones. The objective in this paper is to ensure some\nindependence level between the outputs of regression models and any given\ncontinuous sensitive variables. For this purpose, we use the\nHirschfeld-Gebelein-R\\'enyi (HGR) maximal correlation coefficient as a fairness\nmetric. We propose two approaches to minimize the HGR coefficient. First, by\nreducing an upper bound of the HGR with a neural network estimation of the\n$\\chi^{2}$ divergence. Second, by minimizing the HGR directly with an\nadversarial neural network architecture. The idea is to predict the output Y\nwhile minimizing the ability of an adversarial neural network to find the\nestimated transformations which are required to predict the HGR coefficient. We\nempirically assess and compare our approaches and demonstrate significant\nimprovements on previously presented work in the field.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CY",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1911.04929v1",
        "date": "2019-11-12 15:20:29+00:00"
    },
    {
        "title": "Why Learning of Large-Scale Neural Networks Behaves Like Convex Optimization",
        "authors": [
            "Hui Jiang"
        ],
        "abstract": "In this paper, we present some theoretical work to explain why simple\ngradient descent methods are so successful in solving non-convex optimization\nproblems in learning large-scale neural networks (NN). After introducing a\nmathematical tool called canonical space, we have proved that the objective\nfunctions in learning NNs are convex in the canonical model space. We further\nelucidate that the gradients between the original NN model space and the\ncanonical space are related by a pointwise linear transformation, which is\nrepresented by the so-called disparity matrix. Furthermore, we have proved that\ngradient descent methods surely converge to a global minimum of zero loss\nprovided that the disparity matrices maintain full rank. If this full-rank\ncondition holds, the learning of NNs behaves in the same way as normal convex\noptimization. At last, we have shown that the chance to have singular disparity\nmatrices is extremely slim in large NNs. In particular, when over-parameterized\nNNs are randomly initialized, the gradient decent algorithms converge to a\nglobal minimum of zero loss in probability.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.NE",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1903.02140v1",
        "date": "2019-03-06 02:21:37+00:00"
    },
    {
        "title": "Building Efficient Deep Neural Networks with Unitary Group Convolutions",
        "authors": [
            "Ritchie Zhao",
            "Yuwei Hu",
            "Jordan Dotzel",
            "Christopher De Sa",
            "Zhiru Zhang"
        ],
        "abstract": "We propose unitary group convolutions (UGConvs), a building block for CNNs\nwhich compose a group convolution with unitary transforms in feature space to\nlearn a richer set of representations than group convolution alone. UGConvs\ngeneralize two disparate ideas in CNN architecture, channel shuffling (i.e.\nShuffleNet) and block-circulant networks (i.e. CirCNN), and provide unifying\ninsights that lead to a deeper understanding of each technique. We\nexperimentally demonstrate that dense unitary transforms can outperform channel\nshuffling in DNN accuracy. On the other hand, different dense transforms\nexhibit comparable accuracy performance. Based on these observations we propose\nHadaNet, a UGConv network using Hadamard transforms. HadaNets achieve similar\naccuracy to circulant networks with lower computation complexity, and better\naccuracy than ShuffleNets with the same number of parameters and floating-point\nmultiplies.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1811.07755v2",
        "date": "2018-11-19 15:48:12+00:00"
    },
    {
        "title": "Compressing Deep Neural Networks via Layer Fusion",
        "authors": [
            "James O' Neill",
            "Greg Ver Steeg",
            "Aram Galstyan"
        ],
        "abstract": "This paper proposes \\textit{layer fusion} - a model compression technique\nthat discovers which weights to combine and then fuses weights of similar\nfully-connected, convolutional and attention layers. Layer fusion can\nsignificantly reduce the number of layers of the original network with little\nadditional computation overhead, while maintaining competitive performance.\nFrom experiments on CIFAR-10, we find that various deep convolution neural\nnetworks can remain within 2\\% accuracy points of the original networks up to a\ncompression ratio of 3.33 when iteratively retrained with layer fusion. For\nexperiments on the WikiText-2 language modelling dataset where pretrained\ntransformer models are used, we achieve compression that leads to a network\nthat is 20\\% of its original size while being within 5 perplexity points of the\noriginal network. We also find that other well-established compression\ntechniques can achieve competitive performance when compared to their original\nnetworks given a sufficient number of retraining steps. Generally, we observe a\nclear inflection point in performance as the amount of compression increases,\nsuggesting a bound on the amount of compression that can be achieved before an\nexponential degradation in performance.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2007.14917v1",
        "date": "2020-07-29 15:43:19+00:00"
    },
    {
        "title": "TorchIO: A Python library for efficient loading, preprocessing, augmentation and patch-based sampling of medical images in deep learning",
        "authors": [
            "Fernando P\u00e9rez-Garc\u00eda",
            "Rachel Sparks",
            "S\u00e9bastien Ourselin"
        ],
        "abstract": "Processing of medical images such as MRI or CT presents unique challenges\ncompared to RGB images typically used in computer vision. These include a lack\nof labels for large datasets, high computational costs, and metadata to\ndescribe the physical properties of voxels. Data augmentation is used to\nartificially increase the size of the training datasets. Training with image\npatches decreases the need for computational power. Spatial metadata needs to\nbe carefully taken into account in order to ensure a correct alignment of\nvolumes.\n  We present TorchIO, an open-source Python library to enable efficient\nloading, preprocessing, augmentation and patch-based sampling of medical images\nfor deep learning. TorchIO follows the style of PyTorch and integrates standard\nmedical image processing libraries to efficiently process images during\ntraining of neural networks. TorchIO transforms can be composed, reproduced,\ntraced and extended. We provide multiple generic preprocessing and augmentation\noperations as well as simulation of MRI-specific artifacts.\n  Source code, comprehensive tutorials and extensive documentation for TorchIO\ncan be found at https://torchio.rtfd.io/. The package can be installed from the\nPython Package Index running 'pip install torchio'. It includes a command-line\ninterface which allows users to apply transforms to image files without using\nPython. Additionally, we provide a graphical interface within a TorchIO\nextension in 3D Slicer to visualize the effects of transforms.\n  TorchIO was developed to help researchers standardize medical image\nprocessing pipelines and allow them to focus on the deep learning experiments.\nIt encourages open science, as it supports reproducibility and is version\ncontrolled so that the software can be cited precisely. Due to its modularity,\nthe library is compatible with other frameworks for deep learning with medical\nimages.",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2003.04696v5",
        "date": "2020-03-09 13:36:16+00:00"
    },
    {
        "title": "Deep kernel processes",
        "authors": [
            "Laurence Aitchison",
            "Adam X. Yang",
            "Sebastian W. Ober"
        ],
        "abstract": "We define deep kernel processes in which positive definite Gram matrices are\nprogressively transformed by nonlinear kernel functions and by sampling from\n(inverse) Wishart distributions. Remarkably, we find that deep Gaussian\nprocesses (DGPs), Bayesian neural networks (BNNs), infinite BNNs, and infinite\nBNNs with bottlenecks can all be written as deep kernel processes. For DGPs the\nequivalence arises because the Gram matrix formed by the inner product of\nfeatures is Wishart distributed, and as we show, standard isotropic kernels can\nbe written entirely in terms of this Gram matrix -- we do not need knowledge of\nthe underlying features. We define a tractable deep kernel process, the deep\ninverse Wishart process, and give a doubly-stochastic inducing-point\nvariational inference scheme that operates on the Gram matrices, not on the\nfeatures, as in DGPs. We show that the deep inverse Wishart process gives\nsuperior performance to DGPs and infinite BNNs on standard fully-connected\nbaselines.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2010.01590v2",
        "date": "2020-10-04 14:31:18+00:00"
    },
    {
        "title": "Partially Exchangeable Networks and Architectures for Learning Summary Statistics in Approximate Bayesian Computation",
        "authors": [
            "Samuel Wiqvist",
            "Pierre-Alexandre Mattei",
            "Umberto Picchini",
            "Jes Frellsen"
        ],
        "abstract": "We present a novel family of deep neural architectures, named partially\nexchangeable networks (PENs) that leverage probabilistic symmetries. By design,\nPENs are invariant to block-switch transformations, which characterize the\npartial exchangeability properties of conditionally Markovian processes.\nMoreover, we show that any block-switch invariant function has a PEN-like\nrepresentation. The DeepSets architecture is a special case of PEN and we can\ntherefore also target fully exchangeable data. We employ PENs to learn summary\nstatistics in approximate Bayesian computation (ABC). When comparing PENs to\nprevious deep learning methods for learning summary statistics, our results are\nhighly competitive, both considering time series and static models. Indeed,\nPENs provide more reliable posterior samples even when using less training\ndata.",
        "categories": [
            "stat.ML",
            "cs.LG",
            "stat.CO"
        ],
        "link": "http://arxiv.org/pdf/1901.10230v2",
        "date": "2019-01-29 11:31:31+00:00"
    },
    {
        "title": "Multiscale Hierarchical Convolutional Networks",
        "authors": [
            "J\u00f6rn-Henrik Jacobsen",
            "Edouard Oyallon",
            "St\u00e9phane Mallat",
            "Arnold W. M. Smeulders"
        ],
        "abstract": "Deep neural network algorithms are difficult to analyze because they lack\nstructure allowing to understand the properties of underlying transforms and\ninvariants. Multiscale hierarchical convolutional networks are structured deep\nconvolutional networks where layers are indexed by progressively higher\ndimensional attributes, which are learned from training data. Each new layer is\ncomputed with multidimensional convolutions along spatial and attribute\nvariables. We introduce an efficient implementation of such networks where the\ndimensionality is progressively reduced by averaging intermediate layers along\nattribute indices. Hierarchical networks are tested on CIFAR image data bases\nwhere they obtain comparable precisions to state of the art networks, with much\nfewer parameters. We study some properties of the attributes learned from these\ndatabases.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1703.04140v1",
        "date": "2017-03-12 16:29:44+00:00"
    },
    {
        "title": "Transformers are Deep Infinite-Dimensional Non-Mercer Binary Kernel Machines",
        "authors": [
            "Matthew A. Wright",
            "Joseph E. Gonzalez"
        ],
        "abstract": "Despite their ubiquity in core AI fields like natural language processing,\nthe mechanics of deep attention-based neural networks like the Transformer\nmodel are not fully understood. In this article, we present a new perspective\ntowards understanding how Transformers work. In particular, we show that the\n\"dot-product attention\" that is the core of the Transformer's operation can be\ncharacterized as a kernel learning method on a pair of Banach spaces. In\nparticular, the Transformer's kernel is characterized as having an infinite\nfeature dimension. Along the way we consider an extension of the standard\nkernel learning problem to a binary setting, where data come from two input\ndomains and a response is defined for every cross-domain pair. We prove a new\nrepresenter theorem for these binary kernel machines with non-Mercer\n(indefinite, asymmetric) kernels (implying that the functions learned are\nelements of reproducing kernel Banach spaces rather than Hilbert spaces), and\nalso prove a new universal approximation theorem showing that the Transformer\ncalculation can learn any binary non-Mercer reproducing kernel Banach space\npair. We experiment with new kernels in Transformers, and obtain results that\nsuggest the infinite dimensionality of the standard Transformer kernel is\npartially responsible for its performance. This paper's results provide a new\ntheoretical understanding of a very important but poorly understood model in\nmodern machine~learning.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2106.01506v1",
        "date": "2021-06-02 23:24:06+00:00"
    },
    {
        "title": "Are Nearby Neighbors Relatives?: Testing Deep Music Embeddings",
        "authors": [
            "Jaehun Kim",
            "Juli\u00e1n Urbano",
            "Cynthia C. S. Liem",
            "Alan Hanjalic"
        ],
        "abstract": "Deep neural networks have frequently been used to directly learn\nrepresentations useful for a given task from raw input data. In terms of\noverall performance metrics, machine learning solutions employing deep\nrepresentations frequently have been reported to greatly outperform those using\nhand-crafted feature representations. At the same time, they may pick up on\naspects that are predominant in the data, yet not actually meaningful or\ninterpretable. In this paper, we therefore propose a systematic way to test the\ntrustworthiness of deep music representations, considering musical semantics.\nThe underlying assumption is that in case a deep representation is to be\ntrusted, distance consistency between known related points should be maintained\nboth in the input audio space and corresponding latent deep space. We generate\nknown related points through semantically meaningful transformations, both\nconsidering imperceptible and graver transformations. Then, we examine within-\nand between-space distance consistencies, both considering audio space and\nlatent embedded space, the latter either being a result of a conventional\nfeature extractor or a deep encoder. We illustrate how our method, as a\ncomplement to task-specific performance, provides interpretable insight into\nwhat a network may have captured from training data signals.",
        "categories": [
            "cs.LG",
            "cs.SD",
            "eess.AS",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1904.07154v3",
        "date": "2019-04-15 16:08:41+00:00"
    },
    {
        "title": "Image Embedding of PMU Data for Deep Learning towards Transient Disturbance Classification",
        "authors": [
            "Yongli Zhu",
            "Chengxi Liu",
            "Kai Sun"
        ],
        "abstract": "This paper presents a study on power grid disturbance classification by Deep\nLearning (DL). A real synchrophasor set composing of three different types of\ndisturbance events from the Frequency Monitoring Network (FNET) is used. An\nimage embedding technique called Gramian Angular Field is applied to transform\neach time series of event data to a two-dimensional image for learning. Two\nmain DL algorithms, i.e. CNN (Convolutional Neural Network) and RNN (Recurrent\nNeural Network) are tested and compared with two widely used data mining tools,\nthe Support Vector Machine and Decision Tree. The test results demonstrate the\nsuperiority of the both DL algorithms over other methods in the application of\npower system transient disturbance classification.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1812.09427v1",
        "date": "2018-12-22 01:08:47+00:00"
    },
    {
        "title": "Vector Field Based Neural Networks",
        "authors": [
            "Daniel Vieira",
            "Fabio Rangel",
            "Fabricio Firmino",
            "Joao Paixao"
        ],
        "abstract": "A novel Neural Network architecture is proposed using the mathematically and\nphysically rich idea of vector fields as hidden layers to perform nonlinear\ntransformations in the data. The data points are interpreted as particles\nmoving along a flow defined by the vector field which intuitively represents\nthe desired movement to enable classification. The architecture moves the data\npoints from their original configuration to anew one following the streamlines\nof the vector field with the objective of achieving a final configuration where\nclasses are separable. An optimization problem is solved through gradient\ndescent to learn this vector field.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1802.08235v1",
        "date": "2018-02-22 18:46:15+00:00"
    },
    {
        "title": "Entangled Residual Mappings",
        "authors": [
            "Mathias Lechner",
            "Ramin Hasani",
            "Zahra Babaiee",
            "Radu Grosu",
            "Daniela Rus",
            "Thomas A. Henzinger",
            "Sepp Hochreiter"
        ],
        "abstract": "Residual mappings have been shown to perform representation learning in the\nfirst layers and iterative feature refinement in higher layers. This interplay,\ncombined with their stabilizing effect on the gradient norms, enables them to\ntrain very deep networks. In this paper, we take a step further and introduce\nentangled residual mappings to generalize the structure of the residual\nconnections and evaluate their role in iterative learning representations. An\nentangled residual mapping replaces the identity skip connections with\nspecialized entangled mappings such as orthogonal, sparse, and structural\ncorrelation matrices that share key attributes (eigenvalues, structure, and\nJacobian norm) with identity mappings. We show that while entangled mappings\ncan preserve the iterative refinement of features across various deep models,\nthey influence the representation learning process in convolutional networks\ndifferently than attention-based models and recurrent neural networks. In\ngeneral, we find that for CNNs and Vision Transformers entangled sparse mapping\ncan help generalization while orthogonal mappings hurt performance. For\nrecurrent networks, orthogonal residual mappings form an inductive bias for\ntime-variant sequences, which degrades accuracy on time-invariant tasks.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.NE"
        ],
        "link": "http://arxiv.org/pdf/2206.01261v1",
        "date": "2022-06-02 19:36:03+00:00"
    },
    {
        "title": "Density Compensated Unrolled Networks for Non-Cartesian MRI Reconstruction",
        "authors": [
            "Zaccharie Ramzi",
            "Jean-Luc Starck",
            "Philippe Ciuciu"
        ],
        "abstract": "Deep neural networks have recently been thoroughly investigated as a powerful\ntool for MRI reconstruction. There is a lack of research, however, regarding\ntheir use for a specific setting of MRI, namely non-Cartesian acquisitions. In\nthis work, we introduce a novel kind of deep neural networks to tackle this\nproblem, namely density compensated unrolled neural networks, which rely on\nDensity Compensation to correct the uneven weighting of the k-space. We assess\ntheir efficiency on the publicly available fastMRI dataset, and perform a small\nablation study. Our results show that the density-compensated unrolled neural\nnetworks outperform the different baselines, and that all parts of the design\nare needed. We also open source our code, in particular a Non-Uniform Fast\nFourier transform for TensorFlow.",
        "categories": [
            "eess.IV",
            "cs.CV",
            "cs.LG",
            "physics.med-ph",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2101.01570v2",
        "date": "2021-01-05 15:03:38+00:00"
    },
    {
        "title": "Extraction of digital wavefront sets using applied harmonic analysis and deep neural networks",
        "authors": [
            "H\u00e9ctor Andrade-Loarca",
            "Gitta Kutyniok",
            "Ozan \u00d6ktem",
            "Philipp Petersen"
        ],
        "abstract": "Microlocal analysis provides deep insight into singularity structures and is\noften crucial for solving inverse problems, predominately, in imaging sciences.\nOf particular importance is the analysis of wavefront sets and the correct\nextraction of those. In this paper, we introduce the first algorithmic approach\nto extract the wavefront set of images, which combines data-based and\nmodel-based methods. Based on a celebrated property of the shearlet transform\nto unravel information on the wavefront set, we extract the wavefront set of an\nimage by first applying a discrete shearlet transform and then feeding local\npatches of this transform to a deep convolutional neural network trained on\nlabeled data. The resulting algorithm outperforms all competing algorithms in\nedge-orientation and ramp-orientation detection.",
        "categories": [
            "eess.IV",
            "cs.LG",
            "eess.SP",
            "stat.ML",
            "35A18, 65T60, 68T10"
        ],
        "link": "http://arxiv.org/pdf/1901.01388v2",
        "date": "2019-01-05 09:27:46+00:00"
    },
    {
        "title": "Natural-Logarithm-Rectified Activation Function in Convolutional Neural Networks",
        "authors": [
            "Yang Liu",
            "Jianpeng Zhang",
            "Chao Gao",
            "Jinghua Qu",
            "Lixin Ji"
        ],
        "abstract": "Activation functions play a key role in providing remarkable performance in\ndeep neural networks, and the rectified linear unit (ReLU) is one of the most\nwidely used activation functions. Various new activation functions and\nimprovements on ReLU have been proposed, but each carry performance drawbacks.\nIn this paper, we propose an improved activation function, which we name the\nnatural-logarithm-rectified linear unit (NLReLU). This activation function uses\nthe parametric natural logarithmic transform to improve ReLU and is simply\ndefined as. NLReLU not only retains the sparse activation characteristic of\nReLU, but it also alleviates the \"dying ReLU\" and vanishing gradient problems\nto some extent. It also reduces the bias shift effect and heteroscedasticity of\nneuron data distributions among network layers in order to accelerate the\nlearning process. The proposed method was verified across ten convolutional\nneural networks with different depths for two essential datasets. Experiments\nillustrate that convolutional neural networks with NLReLU exhibit higher\naccuracy than those with ReLU, and that NLReLU is comparable to other\nwell-known activation functions. NLReLU provides 0.16% and 2.04% higher\nclassification accuracy on average compared to ReLU when used in shallow\nconvolutional neural networks with the MNIST and CIFAR-10 datasets,\nrespectively. The average accuracy of deep convolutional neural networks with\nNLReLU is 1.35% higher on average with the CIFAR-10 dataset.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1908.03682v2",
        "date": "2019-08-10 03:51:36+00:00"
    },
    {
        "title": "Graph Convolutional Networks with EigenPooling",
        "authors": [
            "Yao Ma",
            "Suhang Wang",
            "Charu C. Aggarwal",
            "Jiliang Tang"
        ],
        "abstract": "Graph neural networks, which generalize deep neural network models to graph\nstructured data, have attracted increasing attention in recent years. They\nusually learn node representations by transforming, propagating and aggregating\nnode features and have been proven to improve the performance of many graph\nrelated tasks such as node classification and link prediction. To apply graph\nneural networks for the graph classification task, approaches to generate the\n\\textit{graph representation} from node representations are demanded. A common\nway is to globally combine the node representations. However, rich structural\ninformation is overlooked. Thus a hierarchical pooling procedure is desired to\npreserve the graph structure during the graph representation learning. There\nare some recent works on hierarchically learning graph representation analogous\nto the pooling step in conventional convolutional neural (CNN) networks.\nHowever, the local structural information is still largely neglected during the\npooling process. In this paper, we introduce a pooling operator $\\pooling$\nbased on graph Fourier transform, which can utilize the node features and local\nstructures during the pooling process. We then design pooling layers based on\nthe pooling operator, which are further combined with traditional GCN\nconvolutional layers to form a graph neural network framework $\\m$ for graph\nclassification. Theoretical analysis is provided to understand $\\pooling$ from\nboth local and global perspectives. Experimental results of the graph\nclassification task on $6$ commonly used benchmarks demonstrate the\neffectiveness of the proposed framework.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1904.13107v2",
        "date": "2019-04-30 08:57:54+00:00"
    },
    {
        "title": "SE3-Nets: Learning Rigid Body Motion using Deep Neural Networks",
        "authors": [
            "Arunkumar Byravan",
            "Dieter Fox"
        ],
        "abstract": "We introduce SE3-Nets, which are deep neural networks designed to model and\nlearn rigid body motion from raw point cloud data. Based only on sequences of\ndepth images along with action vectors and point wise data associations,\nSE3-Nets learn to segment effected object parts and predict their motion\nresulting from the applied force. Rather than learning point wise flow vectors,\nSE3-Nets predict SE3 transformations for different parts of the scene. Using\nsimulated depth data of a table top scene and a robot manipulator, we show that\nthe structure underlying SE3-Nets enables them to generate a far more\nconsistent prediction of object motion than traditional flow based networks.\nAdditional experiments with a depth camera observing a Baxter robot pushing\nobjects on a table show that SE3-Nets also work well on real data.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "cs.RO"
        ],
        "link": "http://arxiv.org/pdf/1606.02378v3",
        "date": "2016-06-08 02:36:11+00:00"
    },
    {
        "title": "Sparse GPU Kernels for Deep Learning",
        "authors": [
            "Trevor Gale",
            "Matei Zaharia",
            "Cliff Young",
            "Erich Elsen"
        ],
        "abstract": "Scientific workloads have traditionally exploited high levels of sparsity to\naccelerate computation and reduce memory requirements. While deep neural\nnetworks can be made sparse, achieving practical speedups on GPUs is difficult\nbecause these applications have relatively moderate levels of sparsity that are\nnot sufficient for existing sparse kernels to outperform their dense\ncounterparts. In this work, we study sparse matrices from deep learning\napplications and identify favorable properties that can be exploited to\naccelerate computation. Based on these insights, we develop high-performance\nGPU kernels for two sparse matrix operations widely applicable in neural\nnetworks: sparse matrix-dense matrix multiplication and sampled dense-dense\nmatrix multiplication. Our kernels reach 27% of single-precision peak on Nvidia\nV100 GPUs. Using our kernels, we demonstrate sparse Transformer and MobileNet\nmodels that achieve 1.2-2.1x speedups and up to 12.8x memory savings without\nsacrificing accuracy.",
        "categories": [
            "cs.LG",
            "cs.DC",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2006.10901v2",
        "date": "2020-06-18 23:59:11+00:00"
    },
    {
        "title": "Universal Regular Conditional Distributions",
        "authors": [
            "Anastasis Kratsios"
        ],
        "abstract": "We introduce a deep learning model that can universally approximate regular\nconditional distributions (RCDs). The proposed model operates in three phases:\nfirst, it linearizes inputs from a given metric space $\\mathcal{X}$ to\n$\\mathbb{R}^d$ via a feature map, then a deep feedforward neural network\nprocesses these linearized features, and then the network's outputs are then\ntransformed to the $1$-Wasserstein space $\\mathcal{P}_1(\\mathbb{R}^D)$ via a\nprobabilistic extension of the attention mechanism of Bahdanau et al.\\ (2014).\nOur model, called the \\textit{probabilistic transformer (PT)}, can approximate\nany continuous function from $\\mathbb{R}^d $ to $\\mathcal{P}_1(\\mathbb{R}^D)$\nuniformly on compact sets, quantitatively. We identify two ways in which the PT\navoids the curse of dimensionality when approximating\n$\\mathcal{P}_1(\\mathbb{R}^D)$-valued functions. The first strategy builds\nfunctions in $C(\\mathbb{R}^d,\\mathcal{P}_1(\\mathbb{R}^D))$ which can be\nefficiently approximated by a PT, uniformly on any given compact subset of\n$\\mathbb{R}^d$. In the second approach, given any function $f$ in\n$C(\\mathbb{R}^d,\\mathcal{P}_1(\\mathbb{R}^D))$, we build compact subsets of\n$\\mathbb{R}^d$ whereon $f$ can be efficiently approximated by a PT.",
        "categories": [
            "cs.LG",
            "cs.NE",
            "math.MG",
            "math.PR",
            "stat.ML",
            "68T07, 28A50, 49Q22, 54C65"
        ],
        "link": "http://arxiv.org/pdf/2105.07743v5",
        "date": "2021-05-17 11:34:09+00:00"
    },
    {
        "title": "Deep Diffeomorphic Normalizing Flows",
        "authors": [
            "Hadi Salman",
            "Payman Yadollahpour",
            "Tom Fletcher",
            "Kayhan Batmanghelich"
        ],
        "abstract": "The Normalizing Flow (NF) models a general probability density by estimating\nan invertible transformation applied on samples drawn from a known\ndistribution. We introduce a new type of NF, called Deep Diffeomorphic\nNormalizing Flow (DDNF). A diffeomorphic flow is an invertible function where\nboth the function and its inverse are smooth. We construct the flow using an\nordinary differential equation (ODE) governed by a time-varying smooth vector\nfield. We use a neural network to parametrize the smooth vector field and a\nrecursive neural network (RNN) for approximating the solution of the ODE. Each\ncell in the RNN is a residual network implementing one Euler integration step.\nThe architecture of our flow enables efficient likelihood evaluation,\nstraightforward flow inversion, and results in highly flexible density\nestimation. An end-to-end trained DDNF achieves competitive results with\nstate-of-the-art methods on a suite of density estimation and variational\ninference tasks. Finally, our method brings concepts from Riemannian geometry\nthat, we believe, can open a new research direction for neural density\nestimation.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1810.03256v2",
        "date": "2018-10-08 03:09:41+00:00"
    },
    {
        "title": "A Mathematical Principle of Deep Learning: Learn the Geodesic Curve in the Wasserstein Space",
        "authors": [
            "Kuo Gai",
            "Shihua Zhang"
        ],
        "abstract": "Recent studies revealed the mathematical connection of deep neural network\n(DNN) and dynamic system. However, the fundamental principle of DNN has not\nbeen fully characterized with dynamic system in terms of optimization and\ngeneralization. To this end, we build the connection of DNN and continuity\nequation where the measure is conserved to model the forward propagation\nprocess of DNN which has not been addressed before. DNN learns the\ntransformation of the input distribution to the output one. However, in the\nmeasure space, there are infinite curves connecting two distributions. Which\none can lead to good optimization and generaliztion for DNN? By diving the\noptimal transport theory, we find DNN with weight decay attempts to learn the\ngeodesic curve in the Wasserstein space, which is induced by the optimal\ntransport map. Compared with plain network, ResNet is a better approximation to\nthe geodesic curve, which explains why ResNet can be optimized and generalize\nbetter. Numerical experiments show that the data tracks of both plain network\nand ResNet tend to be line-shape in term of line-shape score (LSS), and the map\nlearned by ResNet is closer to the optimal transport map in term of optimal\ntransport score (OTS). In a word, we conclude a mathematical principle of deep\nlearning is to learn the geodesic curve in the Wasserstein space; and deep\nlearning is a great engineering realization of continuous transformation in\nhigh-dimensional space.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2102.09235v2",
        "date": "2021-02-18 09:37:49+00:00"
    },
    {
        "title": "Graph Wavelet Neural Network",
        "authors": [
            "Bingbing Xu",
            "Huawei Shen",
            "Qi Cao",
            "Yunqi Qiu",
            "Xueqi Cheng"
        ],
        "abstract": "We present graph wavelet neural network (GWNN), a novel graph convolutional\nneural network (CNN), leveraging graph wavelet transform to address the\nshortcomings of previous spectral graph CNN methods that depend on graph\nFourier transform. Different from graph Fourier transform, graph wavelet\ntransform can be obtained via a fast algorithm without requiring matrix\neigendecomposition with high computational cost. Moreover, graph wavelets are\nsparse and localized in vertex domain, offering high efficiency and good\ninterpretability for graph convolution. The proposed GWNN significantly\noutperforms previous spectral graph CNNs in the task of graph-based\nsemi-supervised classification on three benchmark datasets: Cora, Citeseer and\nPubmed.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1904.07785v1",
        "date": "2019-04-12 08:20:08+00:00"
    },
    {
        "title": "Coloring Big Graphs with AlphaGoZero",
        "authors": [
            "Jiayi Huang",
            "Mostofa Patwary",
            "Gregory Diamos"
        ],
        "abstract": "We show that recent innovations in deep reinforcement learning can\neffectively color very large graphs -- a well-known NP-hard problem with clear\ncommercial applications. Because the Monte Carlo Tree Search with Upper\nConfidence Bound algorithm used in AlphaGoZero can improve the performance of a\ngiven heuristic, our approach allows deep neural networks trained using high\nperformance computing (HPC) technologies to transform computation into improved\nheuristics with zero prior knowledge. Key to our approach is the introduction\nof a novel deep neural network architecture (FastColorNet) that has access to\nthe full graph context and requires $O(V)$ time and space to color a graph with\n$V$ vertices, which enables scaling to very large graphs that arise in real\napplications like parallel computing, compilers, numerical solvers, and design\nautomation, among others. As a result, we are able to learn new state of the\nart heuristics for graph coloring.",
        "categories": [
            "cs.AI",
            "cs.DM",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1902.10162v3",
        "date": "2019-02-26 19:05:30+00:00"
    },
    {
        "title": "Neural Coarse-Graining: Extracting slowly-varying latent degrees of freedom with neural networks",
        "authors": [
            "Nicholas Guttenberg",
            "Martin Biehl",
            "Ryota Kanai"
        ],
        "abstract": "We present a loss function for neural networks that encompasses an idea of\ntrivial versus non-trivial predictions, such that the network jointly\ndetermines its own prediction goals and learns to satisfy them. This permits\nthe network to choose sub-sets of a problem which are most amenable to its\nabilities to focus on solving, while discarding 'distracting' elements that\ninterfere with its learning. To do this, the network first transforms the raw\ndata into a higher-level categorical representation, and then trains a\npredictor from that new time series to its future. To prevent a trivial\nsolution of mapping the signal to zero, we introduce a measure of\nnon-triviality via a contrast between the prediction error of the learned model\nwith a naive model of the overall signal statistics. The transform can learn to\ndiscard uninformative and unpredictable components of the signal in favor of\nthe features which are both highly predictive and highly predictable. This\ncreates a coarse-grained model of the time-series dynamics, focusing on\npredicting the slowly varying latent parameters which control the statistics of\nthe time-series, rather than predicting the fast details directly. The result\nis a semi-supervised algorithm which is capable of extracting latent\nparameters, segmenting sections of time-series with differing statistics, and\nbuilding a higher-level representation of the underlying dynamics from\nunlabeled data.",
        "categories": [
            "cs.AI",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1609.00116v1",
        "date": "2016-09-01 05:34:23+00:00"
    },
    {
        "title": "Two-Step Sound Source Separation: Training on Learned Latent Targets",
        "authors": [
            "Efthymios Tzinis",
            "Shrikant Venkataramani",
            "Zhepei Wang",
            "Cem Subakan",
            "Paris Smaragdis"
        ],
        "abstract": "In this paper, we propose a two-step training procedure for source separation\nvia a deep neural network. In the first step we learn a transform (and it's\ninverse) to a latent space where masking-based separation performance using\noracles is optimal. For the second step, we train a separation module that\noperates on the previously learned space. In order to do so, we also make use\nof a scale-invariant signal to distortion ratio (SI-SDR) loss function that\nworks in the latent space, and we prove that it lower-bounds the SI-SDR in the\ntime domain. We run various sound separation experiments that show how this\napproach can obtain better performance as compared to systems that learn the\ntransform and the separation module jointly. The proposed methodology is\ngeneral enough to be applicable to a large class of neural network end-to-end\nseparation systems.",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.SD",
            "eess.AS",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1910.09804v2",
        "date": "2019-10-22 07:49:21+00:00"
    },
    {
        "title": "Geometric Scattering on Manifolds",
        "authors": [
            "Michael Perlmutter",
            "Guy Wolf",
            "Matthew Hirn"
        ],
        "abstract": "The Euclidean scattering transform was introduced nearly a decade ago to\nimprove the mathematical understanding of the success of convolutional neural\nnetworks (ConvNets) in image data analysis and other tasks. Inspired by recent\ninterest in geometric deep learning, which aims to generalize ConvNets to\nmanifold and graph-structured domains, we generalize the scattering transform\nto compact manifolds. Similar to the Euclidean scattering transform, our\ngeometric scattering transform is based on a cascade of designed filters and\npointwise nonlinearities, which enables rigorous analysis of the feature\nextraction provided by scattering layers. Our main focus here is on theoretical\nunderstanding of this geometric scattering network, while setting aside\nimplementation aspects, although we remark that application of similar\ntransforms to graph data analysis has been studied recently in related work.\nOur results establish conditions under which geometric scattering provides\nlocalized isometry invariant descriptions of manifold signals, which are also\nstable to families of diffeomorphisms formulated in intrinsic manifolds terms.\nThese results not only generalize the deformation stability and local\nroto-translation invariance of Euclidean scattering, but also demonstrate the\nimportance of linking the used filter structures (e.g., in geometric deep\nlearning) to the underlying manifold geometry, or the data geometry it\nrepresents.",
        "categories": [
            "stat.ML",
            "cs.CV",
            "cs.LG",
            "math.FA"
        ],
        "link": "http://arxiv.org/pdf/1812.06968v4",
        "date": "2018-12-15 23:13:59+00:00"
    },
    {
        "title": "Intra Order-preserving Functions for Calibration of Multi-Class Neural Networks",
        "authors": [
            "Amir Rahimi",
            "Amirreza Shaban",
            "Ching-An Cheng",
            "Richard Hartley",
            "Byron Boots"
        ],
        "abstract": "Predicting calibrated confidence scores for multi-class deep networks is\nimportant for avoiding rare but costly mistakes. A common approach is to learn\na post-hoc calibration function that transforms the output of the original\nnetwork into calibrated confidence scores while maintaining the network's\naccuracy. However, previous post-hoc calibration techniques work only with\nsimple calibration functions, potentially lacking sufficient representation to\ncalibrate the complex function landscape of deep networks. In this work, we aim\nto learn general post-hoc calibration functions that can preserve the top-k\npredictions of any deep network. We call this family of functions intra\norder-preserving functions. We propose a new neural network architecture that\nrepresents a class of intra order-preserving functions by combining common\nneural network components. Additionally, we introduce order-invariant and\ndiagonal sub-families, which can act as regularization for better\ngeneralization when the training data size is small. We show the effectiveness\nof the proposed method across a wide range of datasets and classifiers. Our\nmethod outperforms state-of-the-art post-hoc calibration methods, namely\ntemperature scaling and Dirichlet calibration, in several evaluation metrics\nfor the task.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2003.06820v2",
        "date": "2020-03-15 12:57:21+00:00"
    },
    {
        "title": "Global Optimality in Tensor Factorization, Deep Learning, and Beyond",
        "authors": [
            "Benjamin D. Haeffele",
            "Rene Vidal"
        ],
        "abstract": "Techniques involving factorization are found in a wide range of applications\nand have enjoyed significant empirical success in many fields. However, common\nto a vast majority of these problems is the significant disadvantage that the\nassociated optimization problems are typically non-convex due to a multilinear\nform or other convexity destroying transformation. Here we build on ideas from\nconvex relaxations of matrix factorizations and present a very general\nframework which allows for the analysis of a wide range of non-convex\nfactorization problems - including matrix factorization, tensor factorization,\nand deep neural network training formulations. We derive sufficient conditions\nto guarantee that a local minimum of the non-convex optimization problem is a\nglobal minimum and show that if the size of the factorized variables is large\nenough then from any initialization it is possible to find a global minimizer\nusing a purely local descent algorithm. Our framework also provides a partial\ntheoretical justification for the increasingly common use of Rectified Linear\nUnits (ReLUs) in deep neural networks and offers guidance on deep network\narchitectures and regularization strategies to facilitate efficient\noptimization.",
        "categories": [
            "cs.NA",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1506.07540v1",
        "date": "2015-06-24 20:08:47+00:00"
    },
    {
        "title": "STFNets: Learning Sensing Signals from the Time-Frequency Perspective with Short-Time Fourier Neural Networks",
        "authors": [
            "Shuochao Yao",
            "Ailing Piao",
            "Wenjun Jiang",
            "Yiran Zhao",
            "Huajie Shao",
            "Shengzhong Liu",
            "Dongxin Liu",
            "Jinyang Li",
            "Tianshi Wang",
            "Shaohan Hu",
            "Lu Su",
            "Jiawei Han",
            "Tarek Abdelzaher"
        ],
        "abstract": "Recent advances in deep learning motivate the use of deep neural networks in\nInternet-of-Things (IoT) applications. These networks are modelled after signal\nprocessing in the human brain, thereby leading to significant advantages at\nperceptual tasks such as vision and speech recognition. IoT applications,\nhowever, often measure physical phenomena, where the underlying physics (such\nas inertia, wireless signal propagation, or the natural frequency of\noscillation) are fundamentally a function of signal frequencies, offering\nbetter features in the frequency domain. This observation leads to a\nfundamental question: For IoT applications, can one develop a new brand of\nneural network structures that synthesize features inspired not only by the\nbiology of human perception but also by the fundamental nature of physics?\nHence, in this paper, instead of using conventional building blocks (e.g.,\nconvolutional and recurrent layers), we propose a new foundational neural\nnetwork building block, the Short-Time Fourier Neural Network (STFNet). It\nintegrates a widely-used time-frequency analysis method, the Short-Time Fourier\nTransform, into data processing to learn features directly in the frequency\ndomain, where the physics of underlying phenomena leave better foot-prints.\nSTFNets bring additional flexibility to time-frequency analysis by offering\nnovel nonlinear learnable operations that are spectral-compatible. Moreover,\nSTFNets show that transforming signals to a domain that is more connected to\nthe underlying physics greatly simplifies the learning process. We demonstrate\nthe effectiveness of STFNets with extensive experiments. STFNets significantly\noutperform the state-of-the-art deep learning models in all experiments. A\nSTFNet, therefore, demonstrates superior capability as the fundamental building\nblock of deep neural networks for IoT applications for various sensor inputs.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1902.07849v1",
        "date": "2019-02-21 02:44:41+00:00"
    },
    {
        "title": "The Lipschitz Constant of Self-Attention",
        "authors": [
            "Hyunjik Kim",
            "George Papamakarios",
            "Andriy Mnih"
        ],
        "abstract": "Lipschitz constants of neural networks have been explored in various contexts\nin deep learning, such as provable adversarial robustness, estimating\nWasserstein distance, stabilising training of GANs, and formulating invertible\nneural networks. Such works have focused on bounding the Lipschitz constant of\nfully connected or convolutional networks, composed of linear maps and\npointwise non-linearities. In this paper, we investigate the Lipschitz constant\nof self-attention, a non-linear neural network module widely used in sequence\nmodelling. We prove that the standard dot-product self-attention is not\nLipschitz for unbounded input domain, and propose an alternative L2\nself-attention that is Lipschitz. We derive an upper bound on the Lipschitz\nconstant of L2 self-attention and provide empirical evidence for its asymptotic\ntightness. To demonstrate the practical relevance of our theoretical work, we\nformulate invertible self-attention and use it in a Transformer-based\narchitecture for a character-level language modelling task.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2006.04710v2",
        "date": "2020-06-08 16:08:38+00:00"
    },
    {
        "title": "Deep Neural Networks for the Correction of Mie Scattering in Fourier-Transformed Infrared Spectra of Biological Samples",
        "authors": [
            "Arne P. Raulf",
            "Joshua Butke",
            "Lukas Menzen",
            "Claus K\u00fcpper",
            "Frederik Gro\u00dferueschkamp",
            "Klaus Gerwert",
            "Axel Mosig"
        ],
        "abstract": "Infrared spectra obtained from cell or tissue specimen have commonly been\nobserved to involve a significant degree of (resonant) Mie scattering, which\noften overshadows biochemically relevant spectral information by a non-linear,\nnon-additive spectral component in Fourier transformed infrared (FTIR)\nspectroscopic measurements. Correspondingly, many successful machine learning\napproaches for FTIR spectra have relied on preprocessing procedures that\ncomputationally remove the scattering components from an infrared spectrum. We\npropose an approach to approximate this complex preprocessing function using\ndeep neural networks. As we demonstrate, the resulting model is not just\nseveral orders of magnitudes faster, which is important for real-time clinical\napplications, but also generalizes strongly across different tissue types.\nFurthermore, our proposed method overcomes the trade-off between computation\ntime and the corrected spectrum being biased towards an artificial reference\nspectrum.",
        "categories": [
            "cs.LG",
            "eess.IV",
            "q-bio.TO",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2002.07681v1",
        "date": "2020-02-18 16:07:07+00:00"
    },
    {
        "title": "From Fully Supervised to Zero Shot Settings for Twitter Hashtag Recommendation",
        "authors": [
            "Abhay Kumar",
            "Nishant Jain",
            "Suraj Tripathi",
            "Chirag Singh"
        ],
        "abstract": "We propose a comprehensive end-to-end pipeline for Twitter hashtags\nrecommendation system including data collection, supervised training setting\nand zero shot training setting. In the supervised training setting, we have\nproposed and compared the performance of various deep learning architectures,\nnamely Convolutional Neural Network (CNN), Recurrent Neural Network (RNN) and\nTransformer Network. However, it is not feasible to collect data for all\npossible hashtag labels and train a classifier model on them. To overcome this\nlimitation, we propose a Zero Shot Learning (ZSL) paradigm for predicting\nunseen hashtag labels by learning the relationship between the semantic space\nof tweets and the embedding space of hashtag labels. We evaluated various\nstate-of-the-art ZSL methods like Convex combination of Semantic Embedding\n(ConSE), Embarrassingly Simple Zero-Shot Learning (ESZSL) and Deep Embedding\nModel for Zero-Shot Learning (DEM-ZSL) for the hashtag recommendation task. We\ndemonstrate the effectiveness and scalability of ZSL methods for the\nrecommendation of unseen hashtags. To the best of our knowledge, this is the\nfirst quantitative evaluation of ZSL methods to date for unseen hashtags\nrecommendations from tweet text.",
        "categories": [
            "cs.IR",
            "cs.CL",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1906.04914v1",
        "date": "2019-06-11 17:38:28+00:00"
    },
    {
        "title": "Interferometric Graph Transform: a Deep Unsupervised Graph Representation",
        "authors": [
            "Edouard Oyallon"
        ],
        "abstract": "We propose the Interferometric Graph Transform (IGT), which is a new class of\ndeep unsupervised graph convolutional neural network for building graph\nrepresentations. Our first contribution is to propose a generic, complex-valued\nspectral graph architecture obtained from a generalization of the Euclidean\nFourier transform. We show that our learned representation consists of both\ndiscriminative and invariant features, thanks to a novel greedy concave\nobjective. From our experiments, we conclude that our learning procedure\nexploits the topology of the spectral domain, which is normally a flaw of\nspectral methods, and in particular our method can recover an analytic operator\nfor vision tasks. We test our algorithm on various and challenging tasks such\nas image classification (MNIST, CIFAR-10), community detection (Authorship,\nFacebook graph) and action recognition from 3D skeletons videos (SBU, NTU),\nexhibiting a new state-of-the-art in spectral graph unsupervised settings.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2006.05722v1",
        "date": "2020-06-10 08:27:53+00:00"
    },
    {
        "title": "Framelet Pooling Aided Deep Learning Network : The Method to Process High Dimensional Medical Data",
        "authors": [
            "Chang Min Hyun",
            "Kang Cheol Kim",
            "Hyun Cheol Cho",
            "Jae Kyu Choi",
            "Jin Keun Seo"
        ],
        "abstract": "Machine learning-based analysis of medical images often faces several\nhurdles, such as the lack of training data, the curse of dimensionality\nproblem, and the generalization issues. One of the main difficulties is that\nthere exists computational cost problem in dealing with input data of large\nsize matrices which represent medical images. The purpose of this paper is to\nintroduce a framelet-pooling aided deep learning method for mitigating\ncomputational bundle, caused by large dimensionality. By transforming high\ndimensional data into low dimensional components by filter banks with\npreserving detailed information, the proposed method aims to reduce the\ncomplexity of the neural network and computational costs significantly during\nthe learning process. Various experiments show that our method is comparable to\nthe standard unreduced learning method, while reducing computational burdens by\ndecomposing large-sized learning tasks into several small-scale learning tasks.",
        "categories": [
            "cs.LG",
            "cs.NA",
            "eess.IV",
            "math.NA",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1907.10834v1",
        "date": "2019-07-25 04:40:16+00:00"
    },
    {
        "title": "Fix Bugs with Transformer through a Neural-Symbolic Edit Grammar",
        "authors": [
            "Yaojie Hu",
            "Xingjian Shi",
            "Qiang Zhou",
            "Lee Pike"
        ],
        "abstract": "We introduce NSEdit (neural-symbolic edit), a novel Transformer-based code\nrepair method. Given only the source code that contains bugs, NSEdit predicts\nan editing sequence that can fix the bugs. The edit grammar is formulated as a\nregular language, and the Transformer uses it as a neural-symbolic scripting\ninterface to generate editing programs. We modify the Transformer and add a\npointer network to select the edit locations. An ensemble of rerankers are\ntrained to re-rank the editing sequences generated by beam search. We fine-tune\nthe rerankers on the validation set to reduce over-fitting. NSEdit is evaluated\non various code repair datasets and achieved a new state-of-the-art accuracy\n($24.04\\%$) on the Tufano small dataset of the CodeXGLUE benchmark. NSEdit\nperforms robustly when programs vary from packages to packages and when buggy\nprograms are concrete. We conduct detailed analysis on our methods and\ndemonstrate the effectiveness of each component.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.PL",
            "cs.SE"
        ],
        "link": "http://arxiv.org/pdf/2204.06643v1",
        "date": "2022-04-13 21:39:01+00:00"
    },
    {
        "title": "Scaling Up Computer Vision Neural Networks Using Fast Fourier Transform",
        "authors": [
            "Siddharth Agrawal"
        ],
        "abstract": "Deep Learning-based Computer Vision field has recently been trying to explore\nlarger kernels for convolution to effectively scale up Convolutional Neural\nNetworks. Simultaneously, new paradigm of models such as Vision Transformers\nfind it difficult to scale up to larger higher resolution images due to their\nquadratic complexity in terms of input sequence. In this report, Fast Fourier\nTransform is utilised in various ways to provide some solutions to these\nissues.",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2302.12185v1",
        "date": "2023-02-02 19:19:10+00:00"
    },
    {
        "title": "Shapley Explanation Networks",
        "authors": [
            "Rui Wang",
            "Xiaoqian Wang",
            "David I. Inouye"
        ],
        "abstract": "Shapley values have become one of the most popular feature attribution\nexplanation methods. However, most prior work has focused on post-hoc Shapley\nexplanations, which can be computationally demanding due to its exponential\ntime complexity and preclude model regularization based on Shapley explanations\nduring training. Thus, we propose to incorporate Shapley values themselves as\nlatent representations in deep models thereby making Shapley explanations\nfirst-class citizens in the modeling paradigm. This intrinsic explanation\napproach enables layer-wise explanations, explanation regularization of the\nmodel during training, and fast explanation computation at test time. We define\nthe Shapley transform that transforms the input into a Shapley representation\ngiven a specific function. We operationalize the Shapley transform as a neural\nnetwork module and construct both shallow and deep networks, called ShapNets,\nby composing Shapley modules. We prove that our Shallow ShapNets compute the\nexact Shapley values and our Deep ShapNets maintain the missingness and\naccuracy properties of Shapley values. We demonstrate on synthetic and\nreal-world datasets that our ShapNets enable layer-wise Shapley explanations,\nnovel Shapley regularizations during training, and fast computation while\nmaintaining reasonable performance. Code is available at\nhttps://github.com/inouye-lab/ShapleyExplanationNetworks.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2104.02297v1",
        "date": "2021-04-06 05:42:12+00:00"
    },
    {
        "title": "Engineering flexible machine learning systems by traversing functionally invariant paths in weight space",
        "authors": [
            "Guruprasad Raghavan",
            "Matt Thomson"
        ],
        "abstract": "Deep neural networks achieve human-like performance on a variety of\nperceptual and decision-making tasks. However, networks perform poorly when\nconfronted with changing tasks or goals, and broadly fail to match the\nflexibility and robustness of human intelligence. Here, we develop a\nmathematical and algorithmic framework that enables flexible and continuous\ntraining of neural networks on a range of objectives by constructing path\nconnected sets of networks that achieve equivalent functional performance on a\ngiven machine learning task. We view the weight space of a neural network as a\ncurved Riemannian manifold and move a network along a functionally invariant\npath in weight space while searching for networks that satisfy secondary\nobjectives. A path-sampling algorithm trains computer vision and natural\nlanguage processing networks with millions of weight parameters to learn a\nseries of classification tasks without performance loss while accommodating\nsecondary objectives including network sparsification, incremental task\nlearning, and increased adversarial robustness. Broadly, we conceptualize a\nneural network as a mathematical object that can be iteratively transformed\ninto distinct configurations by the path-sampling algorithm to define a\nsub-manifold of networks that can be harnessed to achieve user goals.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "math.DG"
        ],
        "link": "http://arxiv.org/pdf/2205.00334v3",
        "date": "2022-04-30 19:44:56+00:00"
    },
    {
        "title": "A hybrid quantum-classical neural network with deep residual learning",
        "authors": [
            "Yanying Liang",
            "Wei Peng",
            "Zhu-Jun Zheng",
            "Olli Silv\u00e9n",
            "Guoying Zhao"
        ],
        "abstract": "Inspired by the success of classical neural networks, there has been\ntremendous effort to develop classical effective neural networks into quantum\nconcept. In this paper, a novel hybrid quantum-classical neural network with\ndeep residual learning (Res-HQCNN) is proposed. We firstly analysis how to\nconnect residual block structure with a quantum neural network, and give the\ncorresponding training algorithm. At the same time, the advantages and\ndisadvantages of transforming deep residual learning into quantum concept are\nprovided. As a result, the model can be trained in an end-to-end fashion,\nanalogue to the backpropagation in classical neural networks.\n  To explore the effectiveness of Res-HQCNN , we perform extensive experiments\nfor quantum data with or without noisy on classical computer. The experimental\nresults show the Res-HQCNN performs better to learn an unknown unitary\ntransformation and has stronger robustness for noisy data, when compared to\nstate of the arts. Moreover, the possible methods of combining residual\nlearning with quantum neural networks are also discussed.",
        "categories": [
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2012.07772v3",
        "date": "2020-12-14 18:11:07+00:00"
    },
    {
        "title": "Semi-supervised deep learning for high-dimensional uncertainty quantification",
        "authors": [
            "Zequn Wang",
            "Mingyang Li"
        ],
        "abstract": "Conventional uncertainty quantification methods usually lacks the capability\nof dealing with high-dimensional problems due to the curse of dimensionality.\nThis paper presents a semi-supervised learning framework for dimension\nreduction and reliability analysis. An autoencoder is first adopted for mapping\nthe high-dimensional space into a low-dimensional latent space, which contains\na distinguishable failure surface. Then a deep feedforward neural network (DFN)\nis utilized to learn the mapping relationship and reconstruct the latent space,\nwhile the Gaussian process (GP) modeling technique is used to build the\nsurrogate model of the transformed limit state function. During the training\nprocess of the DFN, the discrepancy between the actual and reconstructed latent\nspace is minimized through semi-supervised learning for ensuring the accuracy.\nBoth labeled and unlabeled samples are utilized for defining the loss function\nof the DFN. Evolutionary algorithm is adopted to train the DFN, then the Monte\nCarlo simulation method is used for uncertainty quantification and reliability\nanalysis based on the proposed framework. The effectiveness is demonstrated\nthrough a mathematical example.",
        "categories": [
            "stat.ML",
            "cs.LG",
            "cs.NE"
        ],
        "link": "http://arxiv.org/pdf/2006.01010v1",
        "date": "2020-06-01 15:15:42+00:00"
    },
    {
        "title": "Not Just Privacy: Improving Performance of Private Deep Learning in Mobile Cloud",
        "authors": [
            "Ji Wang",
            "Jianguo Zhang",
            "Weidong Bao",
            "Xiaomin Zhu",
            "Bokai Cao",
            "Philip S. Yu"
        ],
        "abstract": "The increasing demand for on-device deep learning services calls for a highly\nefficient manner to deploy deep neural networks (DNNs) on mobile devices with\nlimited capacity. The cloud-based solution is a promising approach to enabling\ndeep learning applications on mobile devices where the large portions of a DNN\nare offloaded to the cloud. However, revealing data to the cloud leads to\npotential privacy risk. To benefit from the cloud data center without the\nprivacy risk, we design, evaluate, and implement a cloud-based framework ARDEN\nwhich partitions the DNN across mobile devices and cloud data centers. A simple\ndata transformation is performed on the mobile device, while the\nresource-hungry training and the complex inference rely on the cloud data\ncenter. To protect the sensitive information, a lightweight privacy-preserving\nmechanism consisting of arbitrary data nullification and random noise addition\nis introduced, which provides strong privacy guarantee. A rigorous privacy\nbudget analysis is given. Nonetheless, the private perturbation to the original\ndata inevitably has a negative impact on the performance of further inference\non the cloud side. To mitigate this influence, we propose a noisy training\nmethod to enhance the cloud-side network robustness to perturbed data. Through\nthe sophisticated design, ARDEN can not only preserve privacy but also improve\nthe inference performance. To validate the proposed ARDEN, a series of\nexperiments based on three image datasets and a real mobile application are\nconducted. The experimental results demonstrate the effectiveness of ARDEN.\nFinally, we implement ARDEN on a demo system to verify its practicality.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1809.03428v3",
        "date": "2018-09-10 16:09:58+00:00"
    },
    {
        "title": "Learning Sparse Wavelet Representations",
        "authors": [
            "Daniel Recoskie",
            "Richard Mann"
        ],
        "abstract": "In this work we propose a method for learning wavelet filters directly from\ndata. We accomplish this by framing the discrete wavelet transform as a\nmodified convolutional neural network. We introduce an autoencoder wavelet\ntransform network that is trained using gradient descent. We show that the\nmodel is capable of learning structured wavelet filters from synthetic and real\ndata. The learned wavelets are shown to be similar to traditional wavelets that\nare derived using Fourier methods. Our method is simple to implement and easily\nincorporated into neural network architectures. A major advantage to our model\nis that we can learn from raw audio data.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1802.02961v1",
        "date": "2018-02-08 16:49:00+00:00"
    },
    {
        "title": "On the Importance of Critical Period in Multi-stage Reinforcement Learning",
        "authors": [
            "Junseok Park",
            "Inwoo Hwang",
            "Min Whoo Lee",
            "Hyunseok Oh",
            "Minsu Lee",
            "Youngki Lee",
            "Byoung-Tak Zhang"
        ],
        "abstract": "The initial years of an infant's life are known as the critical period,\nduring which the overall development of learning performance is significantly\nimpacted due to neural plasticity. In recent studies, an AI agent, with a deep\nneural network mimicking mechanisms of actual neurons, exhibited a learning\nperiod similar to human's critical period. Especially during this initial\nperiod, the appropriate stimuli play a vital role in developing learning\nability. However, transforming human cognitive bias into an appropriate shaping\nreward is quite challenging, and prior works on critical period do not focus on\nfinding the appropriate stimulus. To take a step further, we propose\nmulti-stage reinforcement learning to emphasize finding ``appropriate stimulus\"\naround the critical period. Inspired by humans' early cognitive-developmental\nstage, we use multi-stage guidance near the critical period, and demonstrate\nthe appropriate shaping reward (stage-2 guidance) in terms of the AI agent's\nperformance, efficiency, and stability.",
        "categories": [
            "cs.AI",
            "cs.LG",
            "cs.NE"
        ],
        "link": "http://arxiv.org/pdf/2208.04832v1",
        "date": "2022-08-09 15:17:22+00:00"
    },
    {
        "title": "Decision Explanation and Feature Importance for Invertible Networks",
        "authors": [
            "Juntang Zhuang",
            "Nicha C. Dvornek",
            "Xiaoxiao Li",
            "Junlin Yang",
            "James S. Duncan"
        ],
        "abstract": "Deep neural networks are vulnerable to adversarial attacks and hard to\ninterpret because of their black-box nature. The recently proposed invertible\nnetwork is able to accurately reconstruct the inputs to a layer from its\noutputs, thus has the potential to unravel the black-box model. An invertible\nnetwork classifier can be viewed as a two-stage model: (1) invertible\ntransformation from input space to the feature space; (2) a linear classifier\nin the feature space. We can determine the decision boundary of a linear\nclassifier in the feature space; since the transform is invertible, we can\ninvert the decision boundary from the feature space to the input space.\nFurthermore, we propose to determine the projection of a data point onto the\ndecision boundary, and define explanation as the difference between data and\nits projection. Finally, we propose to locally approximate a neural network\nwith its first-order Taylor expansion, and define feature importance using a\nlocal linear model. We provide the implementation of our method:\n\\url{https://github.com/juntang-zhuang/explain_invertible}.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1910.00406v2",
        "date": "2019-09-30 01:01:58+00:00"
    },
    {
        "title": "Mixed Precision Training With 8-bit Floating Point",
        "authors": [
            "Naveen Mellempudi",
            "Sudarshan Srinivasan",
            "Dipankar Das",
            "Bharat Kaul"
        ],
        "abstract": "Reduced precision computation for deep neural networks is one of the key\nareas addressing the widening compute gap driven by an exponential growth in\nmodel size. In recent years, deep learning training has largely migrated to\n16-bit precision, with significant gains in performance and energy efficiency.\nHowever, attempts to train DNNs at 8-bit precision have met with significant\nchallenges because of the higher precision and dynamic range requirements of\nback-propagation. In this paper, we propose a method to train deep neural\nnetworks using 8-bit floating point representation for weights, activations,\nerrors, and gradients. In addition to reducing compute precision, we also\nreduced the precision requirements for the master copy of weights from 32-bit\nto 16-bit. We demonstrate state-of-the-art accuracy across multiple data sets\n(imagenet-1K, WMT16) and a broader set of workloads (Resnet-18/34/50, GNMT,\nTransformer) than previously reported. We propose an enhanced loss scaling\nmethod to augment the reduced subnormal range of 8-bit floating point for\nimproved error propagation. We also examine the impact of quantization noise on\ngeneralization and propose a stochastic rounding technique to address gradient\nnoise. As a result of applying all these techniques, we report slightly higher\nvalidation accuracy compared to full precision baseline.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1905.12334v1",
        "date": "2019-05-29 11:25:09+00:00"
    },
    {
        "title": "Adaptively Solving the Local-Minimum Problem for Deep Neural Networks",
        "authors": [
            "Huachuan Wang",
            "James Ting-Ho Lo"
        ],
        "abstract": "This paper aims to overcome a fundamental problem in the theory and\napplication of deep neural networks (DNNs). We propose a method to solve the\nlocal minimum problem in training DNNs directly. Our method is based on the\ncross-entropy loss criterion's convexification by transforming the\ncross-entropy loss into a risk averting error (RAE) criterion. To alleviate\nnumerical difficulties, a normalized RAE (NRAE) is employed. The convexity\nregion of the cross-entropy loss expands as its risk sensitivity index (RSI)\nincreases. Making the best use of the convexity region, our method starts\ntraining with an extensive RSI, gradually reduces it, and switches to the RAE\nas soon as the RAE is numerically feasible. After training converges, the\nresultant deep learning machine is expected to be inside the attraction basin\nof a global minimum of the cross-entropy loss. Numerical results are provided\nto show the effectiveness of the proposed method.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2012.13632v1",
        "date": "2020-12-25 21:51:48+00:00"
    },
    {
        "title": "PARNN: A Probabilistic Autoregressive Neural Network Framework for Accurate Forecasting",
        "authors": [
            "Madhurima Panja",
            "Tanujit Chakraborty",
            "Uttam Kumar",
            "Abdenour Hadid"
        ],
        "abstract": "Forecasting time series data represents an emerging field of research in data\nscience and knowledge discovery with vast applications ranging from stock price\nand energy demand prediction to the early prediction of epidemics. Numerous\nstatistical and machine learning methods have been proposed in the last five\ndecades with the demand for high-quality and reliable forecasts. However, in\nreal-life prediction problems, situations exist in which a model based on one\nof the above paradigms is preferable. Therefore, hybrid solutions are needed to\nbridge the gap between classical forecasting methods and modern neural network\nmodels. In this context, we introduce a Probabilistic AutoRegressive Neural\nNetwork (PARNN) model that can handle a wide variety of complex time series\ndata (e.g., nonlinearity, non-seasonal, long-range dependence, and\nnon-stationarity). The proposed PARNN model is built by creating a fusion of an\nintegrated moving average and autoregressive neural network to preserve the\nexplainability, scalability, and ``white-box-like'' prediction behavior of the\nindividuals. Sufficient conditions for asymptotic stationarity and geometric\nergodicity are obtained by considering the asymptotic behavior of the\nassociated Markov chain. Unlike advanced deep learning tools, the uncertainty\nquantification of the PARNN model based on prediction intervals is obtained.\nDuring computational experiments, PARNN outperforms standard statistical,\nmachine learning, and deep learning models (e.g., Transformers, NBeats, DeepAR,\netc.) on a diverse collection of real-world datasets from macroeconomics,\ntourism, energy, epidemiology, and others for short-term, medium-term, and\nlong-term forecasting. Multiple comparisons with the best method are carried\nout to showcase the superiority of the proposal in comparison with the\nstate-of-the-art forecasters over different forecast horizons.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2204.09640v2",
        "date": "2022-04-01 17:57:36+00:00"
    },
    {
        "title": "Data Augmentation with Manifold Exploring Geometric Transformations for Increased Performance and Robustness",
        "authors": [
            "Magdalini Paschali",
            "Walter Simson",
            "Abhijit Guha Roy",
            "Muhammad Ferjad Naeem",
            "R\u00fcdiger G\u00f6bl",
            "Christian Wachinger",
            "Nassir Navab"
        ],
        "abstract": "In this paper we propose a novel augmentation technique that improves not\nonly the performance of deep neural networks on clean test data, but also\nsignificantly increases their robustness to random transformations, both affine\nand projective. Inspired by ManiFool, the augmentation is performed by a\nline-search manifold-exploration method that learns affine geometric\ntransformations that lead to the misclassification on an image, while ensuring\nthat it remains on the same manifold as the training data.\n  This augmentation method populates any training dataset with images that lie\non the border of the manifolds between two-classes and maximizes the variance\nthe network is exposed to during training. Our method was thoroughly evaluated\non the challenging tasks of fine-grained skin lesion classification from\nlimited data, and breast tumor classification of mammograms. Compared with\ntraditional augmentation methods, and with images synthesized by Generative\nAdversarial Networks our method not only achieves state-of-the-art performance\nbut also significantly improves the network's robustness.",
        "categories": [
            "cs.LG",
            "eess.IV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1901.04420v1",
        "date": "2019-01-14 17:30:46+00:00"
    },
    {
        "title": "A Note on Learning Rare Events in Molecular Dynamics using LSTM and Transformer",
        "authors": [
            "Wenqi Zeng",
            "Siqin Cao",
            "Xuhui Huang",
            "Yuan Yao"
        ],
        "abstract": "Recurrent neural networks for language models like long short-term memory\n(LSTM) have been utilized as a tool for modeling and predicting long term\ndynamics of complex stochastic molecular systems. Recently successful examples\non learning slow dynamics by LSTM are given with simulation data of low\ndimensional reaction coordinate. However, in this report we show that the\nfollowing three key factors significantly affect the performance of language\nmodel learning, namely dimensionality of reaction coordinates, temporal\nresolution and state partition. When applying recurrent neural networks to\nmolecular dynamics simulation trajectories of high dimensionality, we find that\nrare events corresponding to the slow dynamics might be obscured by other\nfaster dynamics of the system, and cannot be efficiently learned. Under such\nconditions, we find that coarse graining the conformational space into\nmetastable states and removing recrossing events when estimating transition\nprobabilities between states could greatly help improve the accuracy of slow\ndynamics learning in molecular dynamics. Moreover, we also explore other models\nlike Transformer, which do not show superior performance than LSTM in\novercoming these issues. Therefore, to learn rare events of slow molecular\ndynamics by LSTM and Transformer, it is critical to choose proper temporal\nresolution (i.e., saving intervals of MD simulation trajectories) and state\npartition in high resolution data, since deep neural network models might not\nautomatically disentangle slow dynamics from fast dynamics when both are\npresent in data influencing each other.",
        "categories": [
            "cs.AI",
            "cs.LG",
            "physics.comp-ph"
        ],
        "link": "http://arxiv.org/pdf/2107.06573v1",
        "date": "2021-07-14 09:26:36+00:00"
    },
    {
        "title": "Recursive input and state estimation: A general framework for learning from time series with missing data",
        "authors": [
            "Alberto Garc\u00eda-Dur\u00e1n",
            "Robert West"
        ],
        "abstract": "Time series with missing data are signals encountered in important settings\nfor machine learning. Some of the most successful prior approaches for modeling\nsuch time series are based on recurrent neural networks that transform the\ninput and previous state to account for the missing observations, and then\ntreat the transformed signal in a standard manner.\n  In this paper, we introduce a single unifying framework, Recursive Input and\nState Estimation (RISE), for this general approach and reformulate existing\nmodels as specific instances of this framework. We then explore additional\nnovel variations within the RISE framework to improve the performance of any\ninstance. We exploit representation learning techniques to learn latent\nrepresentations of the signals used by RISE instances. We discuss and develop\nvarious encoding techniques to learn latent signal representations. We\nbenchmark instances of the framework with various encoding functions on three\ndata imputation datasets, observing that RISE instances always benefit from\nencoders that learn representations for numerical values from the digits into\nwhich they can be decomposed.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2104.08556v1",
        "date": "2021-04-17 14:43:33+00:00"
    },
    {
        "title": "Spectral Cross-Domain Neural Network with Soft-adaptive Threshold Spectral Enhancement",
        "authors": [
            "Che Liu",
            "Sibo Cheng",
            "Weiping Ding",
            "Rossella Arcucci"
        ],
        "abstract": "Electrocardiography (ECG) signals can be considered as multi-variable\ntime-series. The state-of-the-art ECG data classification approaches, based on\neither feature engineering or deep learning techniques, treat separately\nspectral and time domains in machine learning systems. No spectral-time domain\ncommunication mechanism inside the classifier model can be found in current\napproaches, leading to difficulties in identifying complex ECG forms. In this\npaper, we proposed a novel deep learning model named Spectral Cross-domain\nneural network (SCDNN) with a new block called Soft-adaptive threshold spectral\nenhancement (SATSE), to simultaneously reveal the key information embedded in\nspectral and time domains inside the neural network. More precisely, the\ndomain-cross information is captured by a general Convolutional neural network\n(CNN) backbone, and different information sources are merged by a self-adaptive\nmechanism to mine the connection between time and spectral domains. In SATSE,\nthe knowledge from time and spectral domains is extracted via the Fast Fourier\nTransformation (FFT) with soft trainable thresholds in modified Sigmoid\nfunctions. The proposed SCDNN is tested with several classification tasks\nimplemented on the public ECG databases \\textit{PTB-XL} and \\textit{MIT-BIH}.\nSCDNN outperforms the state-of-the-art approaches with a low computational cost\nregarding a variety of metrics in all classification tasks on both databases,\nby finding appropriate domains from the infinite spectral mapping. The\nconvergence of the trainable thresholds in the spectral domain is also\nnumerically investigated in this paper. The robust performance of SCDNN\nprovides a new perspective to exploit knowledge across deep learning models\nfrom time and spectral domains. The repository can be found:\nhttps://github.com/DL-WG/SCDNN-TS",
        "categories": [
            "cs.LG",
            "cs.AI",
            "eess.SP"
        ],
        "link": "http://arxiv.org/pdf/2301.10171v1",
        "date": "2023-01-10 14:23:43+00:00"
    },
    {
        "title": "Time Series Data Augmentation for Neural Networks by Time Warping with a Discriminative Teacher",
        "authors": [
            "Brian Kenji Iwana",
            "Seiichi Uchida"
        ],
        "abstract": "Neural networks have become a powerful tool in pattern recognition and part\nof their success is due to generalization from using large datasets. However,\nunlike other domains, time series classification datasets are often small. In\norder to address this problem, we propose a novel time series data augmentation\ncalled guided warping. While many data augmentation methods are based on random\ntransformations, guided warping exploits the element alignment properties of\nDynamic Time Warping (DTW) and shapeDTW, a high-level DTW method based on shape\ndescriptors, to deterministically warp sample patterns. In this way, the time\nseries are mixed by warping the features of a sample pattern to match the time\nsteps of a reference pattern. Furthermore, we introduce a discriminative\nteacher in order to serve as a directed reference for the guided warping. We\nevaluate the method on all 85 datasets in the 2015 UCR Time Series Archive with\na deep convolutional neural network (CNN) and a recurrent neural network (RNN).\nThe code with an easy to use implementation can be found at\nhttps://github.com/uchidalab/time_series_augmentation .",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2004.08780v1",
        "date": "2020-04-19 06:33:44+00:00"
    },
    {
        "title": "Retro*: Learning Retrosynthetic Planning with Neural Guided A* Search",
        "authors": [
            "Binghong Chen",
            "Chengtao Li",
            "Hanjun Dai",
            "Le Song"
        ],
        "abstract": "Retrosynthetic planning is a critical task in organic chemistry which\nidentifies a series of reactions that can lead to the synthesis of a target\nproduct. The vast number of possible chemical transformations makes the size of\nthe search space very big, and retrosynthetic planning is challenging even for\nexperienced chemists. However, existing methods either require expensive return\nestimation by rollout with high variance, or optimize for search speed rather\nthan the quality. In this paper, we propose Retro*, a neural-based A*-like\nalgorithm that finds high-quality synthetic routes efficiently. It maintains\nthe search as an AND-OR tree, and learns a neural search bias with off-policy\ndata. Then guided by this neural network, it performs best-first search\nefficiently during new planning episodes. Experiments on benchmark USPTO\ndatasets show that, our proposed method outperforms existing state-of-the-art\nwith respect to both the success rate and solution quality, while being more\nefficient at the same time.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2006.15820v1",
        "date": "2020-06-29 05:53:33+00:00"
    },
    {
        "title": "Weightless: Lossy Weight Encoding For Deep Neural Network Compression",
        "authors": [
            "Brandon Reagen",
            "Udit Gupta",
            "Robert Adolf",
            "Michael M. Mitzenmacher",
            "Alexander M. Rush",
            "Gu-Yeon Wei",
            "David Brooks"
        ],
        "abstract": "The large memory requirements of deep neural networks limit their deployment\nand adoption on many devices. Model compression methods effectively reduce the\nmemory requirements of these models, usually through applying transformations\nsuch as weight pruning or quantization. In this paper, we present a novel\nscheme for lossy weight encoding which complements conventional compression\ntechniques. The encoding is based on the Bloomier filter, a probabilistic data\nstructure that can save space at the cost of introducing random errors.\nLeveraging the ability of neural networks to tolerate these imperfections and\nby re-training around the errors, the proposed technique, Weightless, can\ncompress DNN weights by up to 496x with the same model accuracy. This results\nin up to a 1.51x improvement over the state-of-the-art.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1711.04686v1",
        "date": "2017-11-13 16:28:37+00:00"
    },
    {
        "title": "Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the Neural Tangent Kernel",
        "authors": [
            "Stanislav Fort",
            "Gintare Karolina Dziugaite",
            "Mansheej Paul",
            "Sepideh Kharaghani",
            "Daniel M. Roy",
            "Surya Ganguli"
        ],
        "abstract": "In suitably initialized wide networks, small learning rates transform deep\nneural networks (DNNs) into neural tangent kernel (NTK) machines, whose\ntraining dynamics is well-approximated by a linear weight expansion of the\nnetwork at initialization. Standard training, however, diverges from its\nlinearization in ways that are poorly understood. We study the relationship\nbetween the training dynamics of nonlinear deep networks, the geometry of the\nloss landscape, and the time evolution of a data-dependent NTK. We do so\nthrough a large-scale phenomenological analysis of training, synthesizing\ndiverse measures characterizing loss landscape geometry and NTK dynamics. In\nmultiple neural architectures and datasets, we find these diverse measures\nevolve in a highly correlated manner, revealing a universal picture of the deep\nlearning process. In this picture, deep network training exhibits a highly\nchaotic rapid initial transient that within 2 to 3 epochs determines the final\nlinearly connected basin of low loss containing the end point of training.\nDuring this chaotic transient, the NTK changes rapidly, learning useful\nfeatures from the training data that enables it to outperform the standard\ninitial NTK by a factor of 3 in less than 3 to 4 epochs. After this rapid\nchaotic transient, the NTK changes at constant velocity, and its performance\nmatches that of full network training in 15% to 45% of training time. Overall,\nour analysis reveals a striking correlation between a diverse set of metrics\nover training time, governed by a rapid chaotic to stable transition in the\nfirst few epochs, that together poses challenges and opportunities for the\ndevelopment of more accurate theories of deep learning.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2010.15110v1",
        "date": "2020-10-28 17:53:01+00:00"
    },
    {
        "title": "Deciding How to Decide: Dynamic Routing in Artificial Neural Networks",
        "authors": [
            "Mason McGill",
            "Pietro Perona"
        ],
        "abstract": "We propose and systematically evaluate three strategies for training\ndynamically-routed artificial neural networks: graphs of learned\ntransformations through which different input signals may take different paths.\nThough some approaches have advantages over others, the resulting networks are\noften qualitatively similar. We find that, in dynamically-routed networks\ntrained to classify images, layers and branches become specialized to process\ndistinct categories of images. Additionally, given a fixed computational\nbudget, dynamically-routed networks tend to perform better than comparable\nstatically-routed networks.",
        "categories": [
            "stat.ML",
            "cs.CV",
            "cs.LG",
            "cs.NE"
        ],
        "link": "http://arxiv.org/pdf/1703.06217v2",
        "date": "2017-03-17 23:52:14+00:00"
    },
    {
        "title": "BatchFormer: Learning to Explore Sample Relationships for Robust Representation Learning",
        "authors": [
            "Zhi Hou",
            "Baosheng Yu",
            "Dacheng Tao"
        ],
        "abstract": "Despite the success of deep neural networks, there are still many challenges\nin deep representation learning due to the data scarcity issues such as data\nimbalance, unseen distribution, and domain shift. To address the\nabove-mentioned issues, a variety of methods have been devised to explore the\nsample relationships in a vanilla way (i.e., from the perspectives of either\nthe input or the loss function), failing to explore the internal structure of\ndeep neural networks for learning with sample relationships. Inspired by this,\nwe propose to enable deep neural networks themselves with the ability to learn\nthe sample relationships from each mini-batch. Specifically, we introduce a\nbatch transformer module or BatchFormer, which is then applied into the batch\ndimension of each mini-batch to implicitly explore sample relationships during\ntraining. By doing this, the proposed method enables the collaboration of\ndifferent samples, e.g., the head-class samples can also contribute to the\nlearning of the tail classes for long-tailed recognition. Furthermore, to\nmitigate the gap between training and testing, we share the classifier between\nwith or without the BatchFormer during training, which can thus be removed\nduring testing. We perform extensive experiments on over ten datasets and the\nproposed method achieves significant improvements on different data scarcity\napplications without any bells and whistles, including the tasks of long-tailed\nrecognition, compositional zero-shot learning, domain generalization, and\ncontrastive learning. Code will be made publicly available at\nhttps://github.com/zhihou7/BatchFormer.",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2203.01522v2",
        "date": "2022-03-03 05:31:33+00:00"
    },
    {
        "title": "Improving Deep Neural Networks with Probabilistic Maxout Units",
        "authors": [
            "Jost Tobias Springenberg",
            "Martin Riedmiller"
        ],
        "abstract": "We present a probabilistic variant of the recently introduced maxout unit.\nThe success of deep neural networks utilizing maxout can partly be attributed\nto favorable performance under dropout, when compared to rectified linear\nunits. It however also depends on the fact that each maxout unit performs a\npooling operation over a group of linear transformations and is thus partially\ninvariant to changes in its input. Starting from this observation we ask the\nquestion: Can the desirable properties of maxout units be preserved while\nimproving their invariance properties ? We argue that our probabilistic maxout\n(probout) units successfully achieve this balance. We quantitatively verify\nthis claim and report classification performance matching or exceeding the\ncurrent state of the art on three challenging image classification benchmarks\n(CIFAR-10, CIFAR-100 and SVHN).",
        "categories": [
            "stat.ML",
            "cs.LG",
            "cs.NE"
        ],
        "link": "http://arxiv.org/pdf/1312.6116v2",
        "date": "2013-12-20 20:59:15+00:00"
    },
    {
        "title": "Graph Neural Network based Agent in Google Research Football",
        "authors": [
            "Yizhan Niu",
            "Jinglong Liu",
            "Yuhao Shi",
            "Jiren Zhu"
        ],
        "abstract": "Deep neural networks (DNN) can approximate value functions or policies for\nreinforcement learning, which makes the reinforcement learning algorithms more\npowerful. However, some DNNs, such as convolutional neural networks (CNN),\ncannot extract enough information or take too long to obtain enough features\nfrom the inputs under specific circumstances of reinforcement learning. For\nexample, the input data of Google Research Football, a reinforcement learning\nenvironment which trains agents to play football, is the small map of players'\nlocations. The information is contained not only in the coordinates of players,\nbut also in the relationships between different players. CNNs can neither\nextract enough information nor take too long to train. To address this issue,\nthis paper proposes a deep q-learning network (DQN) with a graph neural network\n(GNN) as its model. The GNN transforms the input data into a graph which better\nrepresents the football players' locations so that it extracts more information\nof the interactions between different players. With two GNNs to approximate its\nlocal and target value functions, this DQN allows players to learn from their\nexperience by using value functions to see the prospective value of each\nintended action. The proposed model demonstrated the power of GNN in the\nfootball game by outperforming other DRL models with significantly fewer steps.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2204.11142v1",
        "date": "2022-04-23 21:26:00+00:00"
    },
    {
        "title": "Parametrizing filters of a CNN with a GAN",
        "authors": [
            "Yannic Kilcher",
            "Gary Becigneul",
            "Thomas Hofmann"
        ],
        "abstract": "It is commonly agreed that the use of relevant invariances as a good\nstatistical bias is important in machine-learning. However, most approaches\nthat explicitly incorporate invariances into a model architecture only make use\nof very simple transformations, such as translations and rotations. Hence,\nthere is a need for methods to model and extract richer transformations that\ncapture much higher-level invariances. To that end, we introduce a tool\nallowing to parametrize the set of filters of a trained convolutional neural\nnetwork with the latent space of a generative adversarial network. We then show\nthat the method can capture highly non-linear invariances of the data by\nvisualizing their effect in the data space.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.NE",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1710.11386v1",
        "date": "2017-10-31 09:24:39+00:00"
    },
    {
        "title": "Deep Time-Frequency Representation and Progressive Decision Fusion for ECG Classification",
        "authors": [
            "Jing Zhang",
            "Jing Tian",
            "Yang Cao",
            "Yuxiang Yang",
            "Xiaobin Xu"
        ],
        "abstract": "Early recognition of abnormal rhythms in ECG signals is crucial for\nmonitoring and diagnosing patients' cardiac conditions, increasing the success\nrate of the treatment. Classifying abnormal rhythms into exact categories is\nvery challenging due to the broad taxonomy of rhythms, noises and lack of\nlarge-scale real-world annotated data. Different from previous methods that\nutilize hand-crafted features or learn features from the original signal\ndomain, we propose a novel ECG classification method by learning deep\ntime-frequency representation and progressive decision fusion at different\ntemporal scales in an end-to-end manner. First, the ECG wave signal is\ntransformed into the time-frequency domain by using the Short-Time Fourier\nTransform. Next, several scale-specific deep convolutional neural networks are\ntrained on ECG samples of a specific length. Finally, a progressive online\ndecision fusion method is proposed to fuse decisions from the scale-specific\nmodels into a more accurate and stable one. Extensive experiments on both\nsynthetic and real-world ECG datasets demonstrate the effectiveness and\nefficiency of the proposed method.",
        "categories": [
            "cs.LG",
            "eess.SP",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1901.06469v3",
        "date": "2019-01-19 05:25:24+00:00"
    },
    {
        "title": "T4PdM: a Deep Neural Network based on the Transformer Architecture for Fault Diagnosis of Rotating Machinery",
        "authors": [
            "Erick Giovani Sperandio Nascimento",
            "Julian Santana Liang",
            "Ilan Sousa Figueiredo",
            "Lilian Lefol Nani Guarieiro"
        ],
        "abstract": "Deep learning and big data algorithms have become widely used in industrial\napplications to optimize several tasks in many complex systems. Particularly,\ndeep learning model for diagnosing and prognosing machinery health has\nleveraged predictive maintenance (PdM) to be more accurate and reliable in\ndecision making, in this way avoiding unnecessary interventions, machinery\naccidents, and environment catastrophes. Recently, Transformer Neural Networks\nhave gained notoriety and have been increasingly the favorite choice for\nNatural Language Processing (NLP) tasks. Thus, given their recent major\nachievements in NLP, this paper proposes the development of an automatic fault\nclassifier model for predictive maintenance based on a modified version of the\nTransformer architecture, namely T4PdM, to identify multiple types of faults in\nrotating machinery. Experimental results are developed and presented for the\nMaFaulDa and CWRU databases. T4PdM was able to achieve an overall accuracy of\n99.98% and 98% for both datasets, respectively. In addition, the performance of\nthe proposed model is compared to other previously published works. It has\ndemonstrated the superiority of the model in detecting and classifying faults\nin rotating industrial machinery. Therefore, the proposed Transformer-based\nmodel can improve the performance of machinery fault analysis and diagnostic\nprocesses and leverage companies to a new era of the Industry 4.0. In addition,\nthis methodology can be adapted to any other task of time series\nclassification.",
        "categories": [
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2204.03725v1",
        "date": "2022-04-07 20:31:45+00:00"
    },
    {
        "title": "Sequence Modeling of Temporal Credit Assignment for Episodic Reinforcement Learning",
        "authors": [
            "Yang Liu",
            "Yunan Luo",
            "Yuanyi Zhong",
            "Xi Chen",
            "Qiang Liu",
            "Jian Peng"
        ],
        "abstract": "Recent advances in deep reinforcement learning algorithms have shown great\npotential and success for solving many challenging real-world problems,\nincluding Go game and robotic applications. Usually, these algorithms need a\ncarefully designed reward function to guide training in each time step.\nHowever, in real world, it is non-trivial to design such a reward function, and\nthe only signal available is usually obtained at the end of a trajectory, also\nknown as the episodic reward or return. In this work, we introduce a new\nalgorithm for temporal credit assignment, which learns to decompose the\nepisodic return back to each time-step in the trajectory using deep neural\nnetworks. With this learned reward signal, the learning efficiency can be\nsubstantially improved for episodic reinforcement learning. In particular, we\nfind that expressive language models such as the Transformer can be adopted for\nlearning the importance and the dependency of states in the trajectory,\ntherefore providing high-quality and interpretable learned reward signals. We\nhave performed extensive experiments on a set of MuJoCo continuous locomotive\ncontrol tasks with only episodic returns and demonstrated the effectiveness of\nour algorithm.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1905.13420v1",
        "date": "2019-05-31 05:20:12+00:00"
    },
    {
        "title": "Reparameterization through Spatial Gradient Scaling",
        "authors": [
            "Alexander Detkov",
            "Mohammad Salameh",
            "Muhammad Fetrat Qharabagh",
            "Jialin Zhang",
            "Wei Lui",
            "Shangling Jui",
            "Di Niu"
        ],
        "abstract": "Reparameterization aims to improve the generalization of deep neural networks\nby transforming convolutional layers into equivalent multi-branched structures\nduring training. However, there exists a gap in understanding how\nreparameterization may change and benefit the learning process of neural\nnetworks. In this paper, we present a novel spatial gradient scaling method to\nredistribute learning focus among weights in convolutional networks. We prove\nthat spatial gradient scaling achieves the same learning dynamics as a branched\nreparameterization yet without introducing structural changes into the network.\nWe further propose an analytical approach that dynamically learns scalings for\neach convolutional layer based on the spatial characteristics of its input\nfeature map gauged by mutual information. Experiments on CIFAR-10, CIFAR-100,\nand ImageNet show that without searching for reparameterized structures, our\nproposed scaling method outperforms the state-of-the-art reparameterization\nstrategies at a lower computational cost.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "link": "http://arxiv.org/pdf/2303.02733v2",
        "date": "2023-03-05 17:57:33+00:00"
    },
    {
        "title": "Linearly Constrained Neural Networks",
        "authors": [
            "Johannes Hendriks",
            "Carl Jidling",
            "Adrian Wills",
            "Thomas Sch\u00f6n"
        ],
        "abstract": "We present a novel approach to modelling and learning vector fields from\nphysical systems using neural networks that explicitly satisfy known linear\noperator constraints. To achieve this, the target function is modelled as a\nlinear transformation of an underlying potential field, which is in turn\nmodelled by a neural network. This transformation is chosen such that any\nprediction of the target function is guaranteed to satisfy the constraints. The\napproach is demonstrated on both simulated and real data examples.",
        "categories": [
            "stat.ML",
            "cs.LG",
            "physics.comp-ph"
        ],
        "link": "http://arxiv.org/pdf/2002.01600v4",
        "date": "2020-02-05 01:27:29+00:00"
    },
    {
        "title": "Towards Deeper Graph Neural Networks",
        "authors": [
            "Meng Liu",
            "Hongyang Gao",
            "Shuiwang Ji"
        ],
        "abstract": "Graph neural networks have shown significant success in the field of graph\nrepresentation learning. Graph convolutions perform neighborhood aggregation\nand represent one of the most important graph operations. Nevertheless, one\nlayer of these neighborhood aggregation methods only consider immediate\nneighbors, and the performance decreases when going deeper to enable larger\nreceptive fields. Several recent studies attribute this performance\ndeterioration to the over-smoothing issue, which states that repeated\npropagation makes node representations of different classes indistinguishable.\nIn this work, we study this observation systematically and develop new insights\ntowards deeper graph neural networks. First, we provide a systematical analysis\non this issue and argue that the key factor compromising the performance\nsignificantly is the entanglement of representation transformation and\npropagation in current graph convolution operations. After decoupling these two\noperations, deeper graph neural networks can be used to learn graph node\nrepresentations from larger receptive fields. We further provide a theoretical\nanalysis of the above observation when building very deep models, which can\nserve as a rigorous and gentle description of the over-smoothing issue. Based\non our theoretical and empirical analysis, we propose Deep Adaptive Graph\nNeural Network (DAGNN) to adaptively incorporate information from large\nreceptive fields. A set of experiments on citation, co-authorship, and\nco-purchase datasets have confirmed our analysis and insights and demonstrated\nthe superiority of our proposed methods.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2007.09296v1",
        "date": "2020-07-18 01:11:14+00:00"
    },
    {
        "title": "StrokeCoder: Path-Based Image Generation from Single Examples using Transformers",
        "authors": [
            "Sabine Wieluch",
            "Friedhelm Schwenker"
        ],
        "abstract": "This paper demonstrates how a Transformer Neural Network can be used to learn\na Generative Model from a single path-based example image. We further show how\na data set can be generated from the example image and how the model can be\nused to generate a large set of deviated images, which still represent the\noriginal image's style and concept.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2003.11958v2",
        "date": "2020-03-26 14:55:16+00:00"
    },
    {
        "title": "Achieving Generalizable Robustness of Deep Neural Networks by Stability Training",
        "authors": [
            "Jan Laermann",
            "Wojciech Samek",
            "Nils Strodthoff"
        ],
        "abstract": "We study the recently introduced stability training as a general-purpose\nmethod to increase the robustness of deep neural networks against input\nperturbations. In particular, we explore its use as an alternative to data\naugmentation and validate its performance against a number of distortion types\nand transformations including adversarial examples. In our image classification\nexperiments using ImageNet data stability training performs on a par or even\noutperforms data augmentation for specific transformations, while consistently\noffering improved robustness against a broader range of distortion strengths\nand types unseen during training, a considerably smaller hyperparameter\ndependence and less potentially negative side effects compared to data\naugmentation.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1906.00735v2",
        "date": "2019-06-03 12:20:43+00:00"
    },
    {
        "title": "A Low-cost Fault Corrector for Deep Neural Networks through Range Restriction",
        "authors": [
            "Zitao Chen",
            "Guanpeng Li",
            "Karthik Pattabiraman"
        ],
        "abstract": "The adoption of deep neural networks (DNNs) in safety-critical domains has\nengendered serious reliability concerns. A prominent example is hardware\ntransient faults that are growing in frequency due to the progressive\ntechnology scaling, and can lead to failures in DNNs.\n  This work proposes Ranger, a low-cost fault corrector, which directly\nrectifies the faulty output due to transient faults without re-computation.\nDNNs are inherently resilient to benign faults (which will not cause output\ncorruption), but not to critical faults (which can result in erroneous output).\nRanger is an automated transformation to selectively restrict the value ranges\nin DNNs, which reduces the large deviations caused by critical faults and\ntransforms them to benign faults that can be tolerated by the inherent\nresilience of the DNNs. Our evaluation on 8 DNNs demonstrates Ranger\nsignificantly increases the error resilience of the DNNs (by 3x to 50x), with\nno loss in accuracy, and with negligible overheads.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2003.13874v4",
        "date": "2020-03-30 23:53:55+00:00"
    },
    {
        "title": "ADMM-based Decoder for Binary Linear Codes Aided by Deep Learning",
        "authors": [
            "Yi Wei",
            "Ming-Min Zhao",
            "Min-Jian Zhao",
            "Ming Lei"
        ],
        "abstract": "Inspired by the recent advances in deep learning (DL), this work presents a\ndeep neural network aided decoding algorithm for binary linear codes. Based on\nthe concept of deep unfolding, we design a decoding network by unfolding the\nalternating direction method of multipliers (ADMM)-penalized decoder. In\naddition, we propose two improved versions of the proposed network. The first\none transforms the penalty parameter into a set of iteration-dependent ones,\nand the second one adopts a specially designed penalty function, which is based\non a piecewise linear function with adjustable slopes. Numerical results show\nthat the resulting DL-aided decoders outperform the original ADMM-penalized\ndecoder for various low density parity check (LDPC) codes with similar\ncomputational complexity.",
        "categories": [
            "cs.IT",
            "cs.LG",
            "eess.SP",
            "math.IT",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2002.07601v1",
        "date": "2020-02-14 03:32:14+00:00"
    },
    {
        "title": "Robustness to Programmable String Transformations via Augmented Abstract Training",
        "authors": [
            "Yuhao Zhang",
            "Aws Albarghouthi",
            "Loris D'Antoni"
        ],
        "abstract": "Deep neural networks for natural language processing tasks are vulnerable to\nadversarial input perturbations. In this paper, we present a versatile language\nfor programmatically specifying string transformations -- e.g., insertions,\ndeletions, substitutions, swaps, etc. -- that are relevant to the task at hand.\nWe then present an approach to adversarially training models that are robust to\nsuch user-defined string transformations. Our approach combines the advantages\nof search-based techniques for adversarial training with abstraction-based\ntechniques. Specifically, we show how to decompose a set of user-defined string\ntransformations into two component specifications, one that benefits from\nsearch and another from abstraction. We use our technique to train models on\nthe AG and SST2 datasets and show that the resulting models are robust to\ncombinations of user-defined transformations mimicking spelling mistakes and\nother meaning-preserving transformations.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2002.09579v4",
        "date": "2020-02-22 00:06:09+00:00"
    },
    {
        "title": "One Network to Solve All ROIs: Deep Learning CT for Any ROI using Differentiated Backprojection",
        "authors": [
            "Yoseob Han",
            "Jong Chul Ye"
        ],
        "abstract": "Computed tomography for region-of-interest (ROI) reconstruction has\nadvantages of reducing X-ray radiation dose and using a small detector.\nHowever, standard analytic reconstruction methods suffer from severe cupping\nartifacts, and existing model-based iterative reconstruction methods require\nextensive computations. Recently, we proposed a deep neural network to learn\nthe cupping artifact, but the network is not well generalized for different\nROIs due to the singularities in the corrupted images. Therefore, there is an\nincreasing demand for a neural network that works well for any ROI sizes. In\nthis paper, two types of neural networks are designed. The first type learns\nROI size-specific cupping artifacts from the analytic reconstruction images,\nwhereas the second type network is to learn to invert the finite Hilbert\ntransform from the truncated differentiated backprojection (DBP) data. Their\ngeneralizability for any ROI sizes is then examined. Experimental results show\nthat the new type of neural network significantly outperforms the existing\niterative methods for any ROI size in spite of significantly reduced run-time\ncomplexity. Since the proposed method consistently surpasses existing methods\nfor any ROIs, it can be used as a general CT reconstruction engine for many\npractical applications without compromising possible detector truncation.",
        "categories": [
            "cs.CV",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1810.00500v2",
        "date": "2018-10-01 01:51:33+00:00"
    },
    {
        "title": "Convolution Aware Initialization",
        "authors": [
            "Armen Aghajanyan"
        ],
        "abstract": "Initialization of parameters in deep neural networks has been shown to have a\nbig impact on the performance of the networks (Mishkin & Matas, 2015). The\ninitialization scheme devised by He et al, allowed convolution activations to\ncarry a constrained mean which allowed deep networks to be trained effectively\n(He et al., 2015a). Orthogonal initializations and more generally orthogonal\nmatrices in standard recurrent networks have been proved to eradicate the\nvanishing and exploding gradient problem (Pascanu et al., 2012). Majority of\ncurrent initialization schemes do not take fully into account the intrinsic\nstructure of the convolution operator. Using the duality of the Fourier\ntransform and the convolution operator, Convolution Aware Initialization builds\northogonal filters in the Fourier space, and using the inverse Fourier\ntransform represents them in the standard space. With Convolution Aware\nInitialization we noticed not only higher accuracy and lower loss, but faster\nconvergence. We achieve new state of the art on the CIFAR10 dataset, and\nachieve close to state of the art on various other tasks.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1702.06295v3",
        "date": "2017-02-21 09:01:46+00:00"
    },
    {
        "title": "A Framework and Benchmark for Deep Batch Active Learning for Regression",
        "authors": [
            "David Holzm\u00fcller",
            "Viktor Zaverkin",
            "Johannes K\u00e4stner",
            "Ingo Steinwart"
        ],
        "abstract": "The acquisition of labels for supervised learning can be expensive. In order\nto improve the sample-efficiency of neural network regression, we study active\nlearning methods that adaptively select batches of unlabeled data for labeling.\nWe present a framework for constructing such methods out of (network-dependent)\nbase kernels, kernel transformations and selection methods. Our framework\nencompasses many existing Bayesian methods based on Gaussian Process\napproximations of neural networks as well as non-Bayesian methods.\nAdditionally, we propose to replace the commonly used last-layer features with\nsketched finite-width Neural Tangent Kernels, and to combine them with a novel\nclustering method. To evaluate different methods, we introduce an open-source\nbenchmark consisting of 15 large tabular regression data sets. Our proposed\nmethod outperforms the state-of-the-art on our benchmark, scales to large data\nsets, and works out-of-the-box without adjusting the network architecture or\ntraining code. We provide open-source code that includes efficient\nimplementations of all kernels, kernel transformations, and selection methods,\nand can be used for reproducing our results.",
        "categories": [
            "stat.ML",
            "cs.LG",
            "cs.NE"
        ],
        "link": "http://arxiv.org/pdf/2203.09410v3",
        "date": "2022-03-17 16:11:36+00:00"
    },
    {
        "title": "Exploiting Invariance in Training Deep Neural Networks",
        "authors": [
            "Chengxi Ye",
            "Xiong Zhou",
            "Tristan McKinney",
            "Yanfeng Liu",
            "Qinggang Zhou",
            "Fedor Zhdanov"
        ],
        "abstract": "Inspired by two basic mechanisms in animal visual systems, we introduce a\nfeature transform technique that imposes invariance properties in the training\nof deep neural networks. The resulting algorithm requires less parameter\ntuning, trains well with an initial learning rate 1.0, and easily generalizes\nto different tasks. We enforce scale invariance with local statistics in the\ndata to align similar samples at diverse scales. To accelerate convergence, we\nenforce a GL(n)-invariance property with global statistics extracted from a\nbatch such that the gradient descent solution should remain invariant under\nbasis change. Profiling analysis shows our proposed modifications takes 5% of\nthe computations of the underlying convolution layer. Tested on convolutional\nnetworks and transformer networks, our proposed technique requires fewer\niterations to train, surpasses all baselines by a large margin, seamlessly\nworks on both small and large batch size training, and applies to different\ncomputer vision and language tasks.",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2103.16634v2",
        "date": "2021-03-30 19:18:31+00:00"
    },
    {
        "title": "On Extensions of CLEVER: A Neural Network Robustness Evaluation Algorithm",
        "authors": [
            "Tsui-Wei Weng",
            "Huan Zhang",
            "Pin-Yu Chen",
            "Aurelie Lozano",
            "Cho-Jui Hsieh",
            "Luca Daniel"
        ],
        "abstract": "CLEVER (Cross-Lipschitz Extreme Value for nEtwork Robustness) is an Extreme\nValue Theory (EVT) based robustness score for large-scale deep neural networks\n(DNNs). In this paper, we propose two extensions on this robustness score.\nFirst, we provide a new formal robustness guarantee for classifier functions\nthat are twice differentiable. We apply extreme value theory on the new formal\nrobustness guarantee and the estimated robustness is called second-order CLEVER\nscore. Second, we discuss how to handle gradient masking, a common defensive\ntechnique, using CLEVER with Backward Pass Differentiable Approximation (BPDA).\nWith BPDA applied, CLEVER can evaluate the intrinsic robustness of neural\nnetworks of a broader class -- networks with non-differentiable input\ntransformations. We demonstrate the effectiveness of CLEVER with BPDA in\nexperiments on a 121-layer Densenet model trained on the ImageNet dataset.",
        "categories": [
            "cs.LG",
            "cs.CR",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1810.08640v1",
        "date": "2018-10-19 18:44:58+00:00"
    },
    {
        "title": "A Hybrid Approach Between Adversarial Generative Networks and Actor-Critic Policy Gradient for Low Rate High-Resolution Image Compression",
        "authors": [
            "Nicol\u00f3 Savioli"
        ],
        "abstract": "Image compression is an essential approach for decreasing the size in bytes\nof the image without deteriorating the quality of it. Typically, classic\nalgorithms are used but recently deep-learning has been successfully applied.\nIn this work, is presented a deep super-resolution work-flow for image\ncompression that maps low-resolution JPEG image to the high-resolution. The\npipeline consists of two components: first, an encoder-decoder neural network\nlearns how to transform the downsampling JPEG images to high resolution.\nSecond, a combination between Generative Adversarial Networks (GANs) and\nreinforcement learning Actor-Critic (A3C) loss pushes the encoder-decoder to\nindirectly maximize High Peak Signal-to-Noise Ratio (PSNR). Although PSNR is a\nfully differentiable metric, this work opens the doors to new solutions for\nmaximizing non-differential metrics through an end-to-end approach between\nencoder-decoder networks and reinforcement learning policy gradient methods.",
        "categories": [
            "eess.IV",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1906.04681v2",
        "date": "2019-06-11 16:27:51+00:00"
    },
    {
        "title": "Vulcan: Solving the Steiner Tree Problem with Graph Neural Networks and Deep Reinforcement Learning",
        "authors": [
            "Haizhou Du",
            "Zong Yan",
            "Qiao Xiang",
            "Qinqing Zhan"
        ],
        "abstract": "Steiner Tree Problem (STP) in graphs aims to find a tree of minimum weight in\nthe graph that connects a given set of vertices. It is a classic NP-hard\ncombinatorial optimization problem and has many real-world applications (e.g.,\nVLSI chip design, transportation network planning and wireless sensor\nnetworks). Many exact and approximate algorithms have been developed for STP,\nbut they suffer from high computational complexity and weak worst-case solution\nguarantees, respectively. Heuristic algorithms are also developed. However,\neach of them requires application domain knowledge to design and is only\nsuitable for specific scenarios. Motivated by the recently reported observation\nthat instances of the same NP-hard combinatorial problem may maintain the same\nor similar combinatorial structure but mainly differ in their data, we\ninvestigate the feasibility and benefits of applying machine learning\ntechniques to solving STP. To this end, we design a novel model Vulcan based on\nnovel graph neural networks and deep reinforcement learning. The core of Vulcan\nis a novel, compact graph embedding that transforms highdimensional graph\nstructure data (i.e., path-changed information) into a low-dimensional vector\nrepresentation. Given an STP instance, Vulcan uses this embedding to encode its\npathrelated information and sends the encoded graph to a deep reinforcement\nlearning component based on a double deep Q network (DDQN) to find solutions.\nIn addition to STP, Vulcan can also find solutions to a wide range of NP-hard\nproblems (e.g., SAT, MVC and X3C) by reducing them to STP. We implement a\nprototype of Vulcan and demonstrate its efficacy and efficiency with extensive\nexperiments using real-world and synthetic datasets.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "I.2.8"
        ],
        "link": "http://arxiv.org/pdf/2111.10810v1",
        "date": "2021-11-21 12:53:50+00:00"
    },
    {
        "title": "Learning Polynomial Transformations",
        "authors": [
            "Sitan Chen",
            "Jerry Li",
            "Yuanzhi Li",
            "Anru R. Zhang"
        ],
        "abstract": "We consider the problem of learning high dimensional polynomial\ntransformations of Gaussians. Given samples of the form $p(x)$, where $x\\sim\nN(0, \\mathrm{Id}_r)$ is hidden and $p: \\mathbb{R}^r \\to \\mathbb{R}^d$ is a\nfunction where every output coordinate is a low-degree polynomial, the goal is\nto learn the distribution over $p(x)$. This problem is natural in its own\nright, but is also an important special case of learning deep generative\nmodels, namely pushforwards of Gaussians under two-layer neural networks with\npolynomial activations. Understanding the learnability of such generative\nmodels is crucial to understanding why they perform so well in practice.\n  Our first main result is a polynomial-time algorithm for learning quadratic\ntransformations of Gaussians in a smoothed setting. Our second main result is a\npolynomial-time algorithm for learning constant-degree polynomial\ntransformations of Gaussian in a smoothed setting, when the rank of the\nassociated tensors is small. In fact our results extend to any\nrotation-invariant input distribution, not just Gaussian. These are the first\nend-to-end guarantees for learning a pushforward under a neural network with\nmore than one layer.\n  Along the way, we also give the first polynomial-time algorithms with\nprovable guarantees for tensor ring decomposition, a popular generalization of\ntensor decomposition that is used in practice to implicitly store large\ntensors.",
        "categories": [
            "cs.LG",
            "cs.DS",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2204.04209v1",
        "date": "2022-04-08 17:59:31+00:00"
    },
    {
        "title": "Generating Neural Networks with Neural Networks",
        "authors": [
            "Lior Deutsch"
        ],
        "abstract": "Hypernetworks are neural networks that generate weights for another neural\nnetwork. We formulate the hypernetwork training objective as a compromise\nbetween accuracy and diversity, where the diversity takes into account trivial\nsymmetry transformations of the target network. We explain how this simple\nformulation generalizes variational inference. We use multi-layered perceptrons\nto form the mapping from the low dimensional input random vector to the high\ndimensional weight space, and demonstrate how to reduce the number of\nparameters in this mapping by parameter sharing. We perform experiments and\nshow that the generated weights are diverse and lie on a non-trivial manifold.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1801.01952v4",
        "date": "2018-01-06 01:27:16+00:00"
    },
    {
        "title": "Variational Linearized Laplace Approximation for Bayesian Deep Learning",
        "authors": [
            "Luis A. Ortega",
            "Sim\u00f3n Rodr\u00edguez Santana",
            "Daniel Hern\u00e1ndez-Lobato"
        ],
        "abstract": "Pre-trained deep neural networks can be adapted to perform uncertainty\nestimation by transforming them into Bayesian neural networks via methods such\nas Laplace approximation (LA) or its linearized form (LLA), among others. To\nmake these methods more tractable, the generalized Gauss-Newton (GGN)\napproximation is often used. However, due to complex inefficiency difficulties,\nboth LA and LLA rely on further approximations, such as Kronecker-factored or\ndiagonal approximate GGN matrices, which can affect the results. To address\nthese issues, we propose a new method for scaling LLA using a variational\nsparse Gaussian Process (GP) approximation based on the dual RKHS of GPs. Our\nmethod retains the predictive mean of the original model while allowing for\nefficient stochastic optimization and scalability in both the number of\nparameters and the size of the training dataset. Moreover, its training cost is\nindependent of the number of training points, improving over previously\nexisting methods. Our preliminary experiments indicate that it outperforms\nalready existing efficient variants of LLA, such as accelerated LLA (ELLA),\nbased on the Nystr\\\"om approximation.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2302.12565v1",
        "date": "2023-02-24 10:32:30+00:00"
    },
    {
        "title": "Revisiting Transformation Invariant Geometric Deep Learning: Are Initial Representations All You Need?",
        "authors": [
            "Ziwei Zhang",
            "Xin Wang",
            "Zeyang Zhang",
            "Peng Cui",
            "Wenwu Zhu"
        ],
        "abstract": "Geometric deep learning, i.e., designing neural networks to handle the\nubiquitous geometric data such as point clouds and graphs, have achieved great\nsuccesses in the last decade. One critical inductive bias is that the model can\nmaintain invariance towards various transformations such as translation,\nrotation, and scaling. The existing graph neural network (GNN) approaches can\nonly maintain permutation-invariance, failing to guarantee invariance with\nrespect to other transformations. Besides GNNs, other works design\nsophisticated transformation-invariant layers, which are computationally\nexpensive and difficult to be extended. To solve this problem, we revisit why\nthe existing neural networks cannot maintain transformation invariance when\nhandling geometric data. Our findings show that transformation-invariant and\ndistance-preserving initial representations are sufficient to achieve\ntransformation invariance rather than needing sophisticated neural layer\ndesigns. Motivated by these findings, we propose Transformation Invariant\nNeural Networks (TinvNN), a straightforward and general framework for geometric\ndata. Specifically, we realize transformation-invariant and distance-preserving\ninitial point representations by modifying multi-dimensional scaling before\nfeeding the representations into neural networks. We prove that TinvNN can\nstrictly guarantee transformation invariance, being general and flexible enough\nto be combined with the existing neural networks. Extensive experimental\nresults on point cloud analysis and combinatorial optimization demonstrate the\neffectiveness and general applicability of our proposed method. Based on the\nexperimental results, we advocate that TinvNN should be considered a new\nstarting point and an essential baseline for further studies of\ntransformation-invariant geometric deep learning.",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2112.12345v1",
        "date": "2021-12-23 03:52:33+00:00"
    },
    {
        "title": "Constructive Universal High-Dimensional Distribution Generation through Deep ReLU Networks",
        "authors": [
            "Dmytro Perekrestenko",
            "Stephan M\u00fcller",
            "Helmut B\u00f6lcskei"
        ],
        "abstract": "We present an explicit deep neural network construction that transforms\nuniformly distributed one-dimensional noise into an arbitrarily close\napproximation of any two-dimensional Lipschitz-continuous target distribution.\nThe key ingredient of our design is a generalization of the \"space-filling\"\nproperty of sawtooth functions discovered in (Bailey & Telgarsky, 2018). We\nelicit the importance of depth - in our neural network construction - in\ndriving the Wasserstein distance between the target distribution and the\napproximation realized by the network to zero. An extension to output\ndistributions of arbitrary dimension is outlined. Finally, we show that the\nproposed construction does not incur a cost - in terms of error measured in\nWasserstein-distance - relative to generating $d$-dimensional target\ndistributions from $d$ independent random variables.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2006.16664v2",
        "date": "2020-06-30 10:36:15+00:00"
    },
    {
        "title": "On the capacity of deep generative networks for approximating distributions",
        "authors": [
            "Yunfei Yang",
            "Zhen Li",
            "Yang Wang"
        ],
        "abstract": "We study the efficacy and efficiency of deep generative networks for\napproximating probability distributions. We prove that neural networks can\ntransform a low-dimensional source distribution to a distribution that is\narbitrarily close to a high-dimensional target distribution, when the closeness\nare measured by Wasserstein distances and maximum mean discrepancy. Upper\nbounds of the approximation error are obtained in terms of the width and depth\nof neural network. Furthermore, it is shown that the approximation error in\nWasserstein distance grows at most linearly on the ambient dimension and that\nthe approximation order only depends on the intrinsic dimension of the target\ndistribution. On the contrary, when $f$-divergences are used as metrics of\ndistributions, the approximation property is different. We show that in order\nto approximate the target distribution in $f$-divergences, the dimension of the\nsource distribution cannot be smaller than the intrinsic dimension of the\ntarget distribution.",
        "categories": [
            "cs.LG",
            "math.PR",
            "math.ST",
            "stat.ML",
            "stat.TH"
        ],
        "link": "http://arxiv.org/pdf/2101.12353v3",
        "date": "2021-01-29 01:45:02+00:00"
    },
    {
        "title": "Deep Multi-View Spatiotemporal Virtual Graph Neural Network for Significant Citywide Ride-hailing Demand Prediction",
        "authors": [
            "Guangyin Jin",
            "Zhexu Xi",
            "Hengyu Sha",
            "Yanghe Feng",
            "Jincai Huang"
        ],
        "abstract": "Urban ride-hailing demand prediction is a crucial but challenging task for\nintelligent transportation system construction. Predictable ride-hailing demand\ncan facilitate more reasonable vehicle scheduling and online car-hailing\nplatform dispatch. Conventional deep learning methods with no external\nstructured data can be accomplished via hybrid models of CNNs and RNNs by\nmeshing plentiful pixel-level labeled data, but spatial data sparsity and\nlimited learning capabilities on temporal long-term dependencies are still two\nstriking bottlenecks. To address these limitations, we propose a new virtual\ngraph modeling method to focus on significant demand regions and a novel Deep\nMulti-View Spatiotemporal Virtual Graph Neural Network (DMVST-VGNN) to\nstrengthen learning capabilities of spatial dynamics and temporal long-term\ndependencies. Specifically, DMVST-VGNN integrates the structures of 1D\nConvolutional Neural Network, Multi Graph Attention Neural Network and\nTransformer layer, which correspond to short-term temporal dynamics view,\nspatial dynamics view and long-term temporal dynamics view respectively. In\nthis paper, experiments are conducted on two large-scale New York City datasets\nin fine-grained prediction scenes. And the experimental results demonstrate\neffectiveness and superiority of DMVST-VGNN framework in significant citywide\nride-hailing demand prediction.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2007.15189v5",
        "date": "2020-07-30 02:37:05+00:00"
    },
    {
        "title": "Deep Reinforcement Learning with Swin Transformer",
        "authors": [
            "Li Meng",
            "Morten Goodwin",
            "Anis Yazidi",
            "Paal Engelstad"
        ],
        "abstract": "Transformers are neural network models that utilize multiple layers of\nself-attention heads. Attention is implemented in transformers as the\ncontextual embeddings of the 'key' and 'query'. Transformers allow the\nre-combination of attention information from different layers and the\nprocessing of all inputs at once, which are more convenient than recurrent\nneural networks when dealt with a large number of data. Transformers have\nexhibited great performances on natural language processing tasks in recent\nyears. Meanwhile, there have been tremendous efforts to adapt transformers into\nother fields of machine learning, such as Swin Transformer and Decision\nTransformer. Swin Transformer is a promising neural network architecture that\nsplits image pixels into small patches and applies local self-attention\noperations inside the (shifted) windows of fixed sizes. Decision Transformer\nhas successfully applied transformers to off-line reinforcement learning and\nshowed that random-walk samples from Atari games are sufficient to let an agent\nlearn optimized behaviors. However, it is considerably more challenging to\ncombine online reinforcement learning with transformers. In this article, we\nfurther explore the possibility of not modifying the reinforcement learning\npolicy, but only replacing the convolutional neural network architecture with\nthe self-attention architecture from Swin Transformer. Namely, we target at\nchanging how an agent views the world, but not how an agent plans about the\nworld. We conduct our experiment on 49 games in Arcade Learning Environment.\nThe results show that using Swin Transformer in reinforcement learning achieves\nsignificantly higher evaluation scores across the majority of games in Arcade\nLearning Environment. Thus, we conclude that online reinforcement learning can\nbenefit from exploiting self-attentions with spatial token embeddings.",
        "categories": [
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2206.15269v1",
        "date": "2022-06-30 13:20:48+00:00"
    },
    {
        "title": "Improving Deep Learning using Generic Data Augmentation",
        "authors": [
            "Luke Taylor",
            "Geoff Nitschke"
        ],
        "abstract": "Deep artificial neural networks require a large corpus of training data in\norder to effectively learn, where collection of such training data is often\nexpensive and laborious. Data augmentation overcomes this issue by artificially\ninflating the training set with label preserving transformations. Recently\nthere has been extensive use of generic data augmentation to improve\nConvolutional Neural Network (CNN) task performance. This study benchmarks\nvarious popular data augmentation schemes to allow researchers to make informed\ndecisions as to which training methods are most appropriate for their data\nsets. Various geometric and photometric schemes are evaluated on a\ncoarse-grained data set using a relatively simple CNN. Experimental results,\nrun using 4-fold cross-validation and reported in terms of Top-1 and Top-5\naccuracy, indicate that cropping in geometric augmentation significantly\nincreases CNN task performance.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1708.06020v1",
        "date": "2017-08-20 21:16:59+00:00"
    },
    {
        "title": "The State of Sparsity in Deep Neural Networks",
        "authors": [
            "Trevor Gale",
            "Erich Elsen",
            "Sara Hooker"
        ],
        "abstract": "We rigorously evaluate three state-of-the-art techniques for inducing\nsparsity in deep neural networks on two large-scale learning tasks: Transformer\ntrained on WMT 2014 English-to-German, and ResNet-50 trained on ImageNet.\nAcross thousands of experiments, we demonstrate that complex techniques\n(Molchanov et al., 2017; Louizos et al., 2017b) shown to yield high compression\nrates on smaller datasets perform inconsistently, and that simple magnitude\npruning approaches achieve comparable or better results. Additionally, we\nreplicate the experiments performed by (Frankle & Carbin, 2018) and (Liu et\nal., 2018) at scale and show that unstructured sparse architectures learned\nthrough pruning cannot be trained from scratch to the same test set performance\nas a model trained with joint sparsification and optimization. Together, these\nresults highlight the need for large-scale benchmarks in the field of model\ncompression. We open-source our code, top performing model checkpoints, and\nresults of all hyperparameter configurations to establish rigorous baselines\nfor future work on compression and sparsification.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1902.09574v1",
        "date": "2019-02-25 19:18:40+00:00"
    },
    {
        "title": "A theory of representation learning in deep neural networks gives a deep generalisation of kernel methods",
        "authors": [
            "Adam X. Yang",
            "Maxime Robeyns",
            "Edward Milsom",
            "Nandi Schoots",
            "Laurence Aitchison"
        ],
        "abstract": "The successes of modern deep machine learning methods are founded on their\nability to transform inputs across multiple layers to build good high-level\nrepresentations. It is therefore critical to understand this process of\nrepresentation learning. However, standard theoretical approaches (formally\nNNGPs) involving infinite width limits eliminate representation learning. We\ntherefore develop a new infinite width limit, the Bayesian representation\nlearning limit, that exhibits representation learning mirroring that in\nfinite-width models, yet at the same time, retains some of the simplicity of\nstandard infinite-width limits. In particular, we show that Deep Gaussian\nprocesses (DGPs) in the Bayesian representation learning limit have exactly\nmultivariate Gaussian posteriors, and the posterior covariances can be obtained\nby optimizing an interpretable objective combining a log-likelihood to improve\nperformance with a series of KL-divergences which keep the posteriors close to\nthe prior. We confirm these results experimentally in wide but finite DGPs.\nNext, we introduce the possibility of using this limit and objective as a\nflexible, deep generalisation of kernel methods, that we call deep kernel\nmachines (DKMs). Like most naive kernel methods, DKMs scale cubically in the\nnumber of datapoints. We therefore use methods from the Gaussian process\ninducing point literature to develop a sparse DKM that scales linearly in the\nnumber of datapoints. Finally, we extend these approaches to NNs (which have\nnon-Gaussian posteriors) in the Appendices.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2108.13097v5",
        "date": "2021-08-30 10:07:37+00:00"
    },
    {
        "title": "Multi-lingual Dialogue Act Recognition with Deep Learning Methods",
        "authors": [
            "Ji\u0159\u00ed Mart\u00ednek",
            "Pavel Kr\u00e1l",
            "Ladislav Lenc",
            "Christophe Cerisara"
        ],
        "abstract": "This paper deals with multi-lingual dialogue act (DA) recognition. The\nproposed approaches are based on deep neural networks and use word2vec\nembeddings for word representation. Two multi-lingual models are proposed for\nthis task. The first approach uses one general model trained on the embeddings\nfrom all available languages. The second method trains the model on a single\npivot language and a linear transformation method is used to project other\nlanguages onto the pivot language. The popular convolutional neural network and\nLSTM architectures with different set-ups are used as classifiers. To the best\nof our knowledge this is the first attempt at multi-lingual DA recognition\nusing neural networks. The multi-lingual models are validated experimentally on\ntwo languages from the Verbmobil corpus.",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.LG",
            "cs.NE"
        ],
        "link": "http://arxiv.org/pdf/1904.05606v1",
        "date": "2019-04-11 09:55:41+00:00"
    },
    {
        "title": "Net-Trim: Convex Pruning of Deep Neural Networks with Performance Guarantee",
        "authors": [
            "Alireza Aghasi",
            "Afshin Abdi",
            "Nam Nguyen",
            "Justin Romberg"
        ],
        "abstract": "We introduce and analyze a new technique for model reduction for deep neural\nnetworks. While large networks are theoretically capable of learning\narbitrarily complex models, overfitting and model redundancy negatively affects\nthe prediction accuracy and model variance. Our Net-Trim algorithm prunes\n(sparsifies) a trained network layer-wise, removing connections at each layer\nby solving a convex optimization program. This program seeks a sparse set of\nweights at each layer that keeps the layer inputs and outputs consistent with\nthe originally trained model. The algorithms and associated analysis are\napplicable to neural networks operating with the rectified linear unit (ReLU)\nas the nonlinear activation. We present both parallel and cascade versions of\nthe algorithm. While the latter can achieve slightly simpler models with the\nsame generalization performance, the former can be computed in a distributed\nmanner. In both cases, Net-Trim significantly reduces the number of connections\nin the network, while also providing enough regularization to slightly reduce\nthe generalization error. We also provide a mathematical analysis of the\nconsistency between the initial network and the retrained model. To analyze the\nmodel sample complexity, we derive the general sufficient conditions for the\nrecovery of a sparse transform matrix. For a single layer taking independent\nGaussian random vectors of length $N$ as inputs, we show that if the network\nresponse can be described using a maximum number of $s$ non-zero weights per\nnode, these weights can be learned from $\\mathcal{O}(s\\log N)$ samples.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1611.05162v4",
        "date": "2016-11-16 06:34:41+00:00"
    },
    {
        "title": "Deep Transformer Model with Pre-Layer Normalization for COVID-19 Growth Prediction",
        "authors": [
            "Rizki Ramadhan Fitra",
            "Novanto Yudistira",
            "Wayan Firdaus Mahmudy"
        ],
        "abstract": "Coronavirus disease or COVID-19 is an infectious disease caused by the\nSARS-CoV-2 virus. The first confirmed case caused by this virus was found at\nthe end of December 2019 in Wuhan City, China. This case then spread throughout\nthe world, including Indonesia. Therefore, the COVID-19 case was designated as\na global pandemic by WHO. The growth of COVID-19 cases, especially in\nIndonesia, can be predicted using several approaches, such as the Deep Neural\nNetwork (DNN). One of the DNN models that can be used is Deep Transformer which\ncan predict time series. The model is trained with several test scenarios to\nget the best model. The evaluation is finding the best hyperparameters. Then,\nfurther evaluation was carried out using the best hyperparameters setting of\nthe number of prediction days, the optimizer, the number of features, and\ncomparison with the former models of the Long Short-Term Memory (LSTM) and\nRecurrent Neural Network (RNN). All evaluations used metric of the Mean\nAbsolute Percentage Error (MAPE). Based on the results of the evaluations, Deep\nTransformer produces the best results when using the Pre-Layer Normalization\nand predicting one day ahead with a MAPE value of 18.83. Furthermore, the model\ntrained with the Adamax optimizer obtains the best performance among other\ntested optimizers. The performance of the Deep Transformer also exceeds other\ntest models, which are LSTM and RNN.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2207.06356v1",
        "date": "2022-07-10 03:48:45+00:00"
    },
    {
        "title": "Zero Initialization of modified Gated Recurrent Encoder-Decoder Network for Short Term Load Forecasting",
        "authors": [
            "Vedanshu",
            "M M Tripathi"
        ],
        "abstract": "Single layer Feedforward Neural Network(FNN) is used many a time as a last\nlayer in models such as seq2seq or could be a simple RNN network. The\nimportance of such layer is to transform the output to our required dimensions.\nWhen it comes to weights and biases initialization, there is no such specific\ntechnique that could speed up the learning process. We could depend on deep\nnetwork initialization techniques such as Xavier or He initialization. But such\ninitialization fails to show much improvement in learning speed or accuracy. In\nthis paper we propose Zero Initialization (ZI) for weights of a single layer\nnetwork. We first test this technique with on a simple RNN network and compare\nthe results against Xavier, He and Identity initialization. As a final test we\nimplement it on a seq2seq network. It was found that ZI considerably reduces\nthe number of epochs used and improve the accuracy. The developed model has\nbeen applied for short-term load forecasting using the load data of Australian\nEnergy Market. The model is able to forecast the day ahead load accurately with\nerror of 0.94%.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1812.03425v1",
        "date": "2018-12-09 04:29:41+00:00"
    },
    {
        "title": "NAT: Neural Architecture Transformer for Accurate and Compact Architectures",
        "authors": [
            "Yong Guo",
            "Yin Zheng",
            "Mingkui Tan",
            "Qi Chen",
            "Jian Chen",
            "Peilin Zhao",
            "Junzhou Huang"
        ],
        "abstract": "Designing effective architectures is one of the key factors behind the\nsuccess of deep neural networks. Existing deep architectures are either\nmanually designed or automatically searched by some Neural Architecture Search\n(NAS) methods. However, even a well-searched architecture may still contain\nmany non-significant or redundant modules or operations (e.g., convolution or\npooling), which may not only incur substantial memory consumption and\ncomputation cost but also deteriorate the performance. Thus, it is necessary to\noptimize the operations inside an architecture to improve the performance\nwithout introducing extra computation cost. Unfortunately, such a constrained\noptimization problem is NP-hard. To make the problem feasible, we cast the\noptimization problem into a Markov decision process (MDP) and seek to learn a\nNeural Architecture Transformer (NAT) to replace the redundant operations with\nthe more computationally efficient ones (e.g., skip connection or directly\nremoving the connection). Based on MDP, we learn NAT by exploiting\nreinforcement learning to obtain the optimization policies w.r.t. different\narchitectures. To verify the effectiveness of the proposed strategies, we apply\nNAT on both hand-crafted architectures and NAS based architectures. Extensive\nexperiments on two benchmark datasets, i.e., CIFAR-10 and ImageNet, demonstrate\nthat the transformed architecture by NAT significantly outperforms both its\noriginal form and those architectures optimized by existing methods.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1910.14488v5",
        "date": "2019-10-31 14:29:09+00:00"
    },
    {
        "title": "Stacked Boosters Network Architecture for Short Term Load Forecasting in Buildings",
        "authors": [
            "Tuukka Salmi",
            "Jussi Kiljander",
            "Daniel Pakkala"
        ],
        "abstract": "This paper presents a novel deep learning architecture for short term load\nforecasting of building energy loads. The architecture is based on a simple\nbase learner and multiple boosting systems that are modelled as a single deep\nneural network. The architecture transforms the original multivariate time\nseries into multiple cascading univariate time series. Together with sparse\ninteractions, parameter sharing and equivariant representations, this approach\nmakes it possible to combat against overfitting while still achieving good\npresentation power with a deep network architecture. The architecture is\nevaluated in several short-term load forecasting tasks with energy data from an\noffice building in Finland. The proposed architecture outperforms\nstate-of-the-art load forecasting model in all the tasks.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2001.08406v2",
        "date": "2020-01-23 08:35:36+00:00"
    },
    {
        "title": "Bayesian Layers: A Module for Neural Network Uncertainty",
        "authors": [
            "Dustin Tran",
            "Michael W. Dusenberry",
            "Mark van der Wilk",
            "Danijar Hafner"
        ],
        "abstract": "We describe Bayesian Layers, a module designed for fast experimentation with\nneural network uncertainty. It extends neural network libraries with drop-in\nreplacements for common layers. This enables composition via a unified\nabstraction over deterministic and stochastic functions and allows for\nscalability via the underlying system. These layers capture uncertainty over\nweights (Bayesian neural nets), pre-activation units (dropout), activations\n(\"stochastic output layers\"), or the function itself (Gaussian processes). They\ncan also be reversible to propagate uncertainty from input to output. We\ninclude code examples for common architectures such as Bayesian LSTMs, deep\nGPs, and flow-based models. As demonstration, we fit a 5-billion parameter\n\"Bayesian Transformer\" on 512 TPUv2 cores for uncertainty in machine\ntranslation and a Bayesian dynamics model for model-based planning. Finally, we\nshow how Bayesian Layers can be used within the Edward2 probabilistic\nprogramming language for probabilistic programs with stochastic processes.",
        "categories": [
            "cs.LG",
            "cs.PL",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1812.03973v3",
        "date": "2018-12-10 18:46:21+00:00"
    },
    {
        "title": "CirCNN: Accelerating and Compressing Deep Neural Networks Using Block-CirculantWeight Matrices",
        "authors": [
            "Caiwen Ding",
            "Siyu Liao",
            "Yanzhi Wang",
            "Zhe Li",
            "Ning Liu",
            "Youwei Zhuo",
            "Chao Wang",
            "Xuehai Qian",
            "Yu Bai",
            "Geng Yuan",
            "Xiaolong Ma",
            "Yipeng Zhang",
            "Jian Tang",
            "Qinru Qiu",
            "Xue Lin",
            "Bo Yuan"
        ],
        "abstract": "Large-scale deep neural networks (DNNs) are both compute and memory\nintensive. As the size of DNNs continues to grow, it is critical to improve the\nenergy efficiency and performance while maintaining accuracy. For DNNs, the\nmodel size is an important factor affecting performance, scalability and energy\nefficiency. Weight pruning achieves good compression ratios but suffers from\nthree drawbacks: 1) the irregular network structure after pruning; 2) the\nincreased training complexity; and 3) the lack of rigorous guarantee of\ncompression ratio and inference accuracy. To overcome these limitations, this\npaper proposes CirCNN, a principled approach to represent weights and process\nneural networks using block-circulant matrices. CirCNN utilizes the Fast\nFourier Transform (FFT)-based fast multiplication, simultaneously reducing the\ncomputational complexity (both in inference and training) from O(n2) to\nO(nlogn) and the storage complexity from O(n2) to O(n), with negligible\naccuracy loss. Compared to other approaches, CirCNN is distinct due to its\nmathematical rigor: it can converge to the same effectiveness as DNNs without\ncompression. The CirCNN architecture, a universal DNN inference engine that can\nbe implemented on various hardware/software platforms with configurable network\narchitecture. To demonstrate the performance and energy efficiency, we test\nCirCNN in FPGA, ASIC and embedded processors. Our results show that CirCNN\narchitecture achieves very high energy efficiency and performance with a small\nhardware footprint. Based on the FPGA implementation and ASIC synthesis\nresults, CirCNN achieves 6-102X energy efficiency improvements compared with\nthe best state-of-the-art results.",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1708.08917v1",
        "date": "2017-08-29 04:18:57+00:00"
    },
    {
        "title": "Molecule Attention Transformer",
        "authors": [
            "\u0141ukasz Maziarka",
            "Tomasz Danel",
            "S\u0142awomir Mucha",
            "Krzysztof Rataj",
            "Jacek Tabor",
            "Stanis\u0142aw Jastrz\u0119bski"
        ],
        "abstract": "Designing a single neural network architecture that performs competitively\nacross a range of molecule property prediction tasks remains largely an open\nchallenge, and its solution may unlock a widespread use of deep learning in the\ndrug discovery industry. To move towards this goal, we propose Molecule\nAttention Transformer (MAT). Our key innovation is to augment the attention\nmechanism in Transformer using inter-atomic distances and the molecular graph\nstructure. Experiments show that MAT performs competitively on a diverse set of\nmolecular prediction tasks. Most importantly, with a simple self-supervised\npretraining, MAT requires tuning of only a few hyperparameter values to achieve\nstate-of-the-art performance on downstream tasks. Finally, we show that\nattention weights learned by MAT are interpretable from the chemical point of\nview.",
        "categories": [
            "cs.LG",
            "physics.comp-ph",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2002.08264v1",
        "date": "2020-02-19 16:14:48+00:00"
    },
    {
        "title": "Generalized Batch Normalization: Towards Accelerating Deep Neural Networks",
        "authors": [
            "Xiaoyong Yuan",
            "Zheng Feng",
            "Matthew Norton",
            "Xiaolin Li"
        ],
        "abstract": "Utilizing recently introduced concepts from statistics and quantitative risk\nmanagement, we present a general variant of Batch Normalization (BN) that\noffers accelerated convergence of Neural Network training compared to\nconventional BN. In general, we show that mean and standard deviation are not\nalways the most appropriate choice for the centering and scaling procedure\nwithin the BN transformation, particularly if ReLU follows the normalization\nstep. We present a Generalized Batch Normalization (GBN) transformation, which\ncan utilize a variety of alternative deviation measures for scaling and\nstatistics for centering, choices which naturally arise from the theory of\ngeneralized deviation measures and risk theory in general. When used in\nconjunction with the ReLU non-linearity, the underlying risk theory suggests\nnatural, arguably optimal choices for the deviation measure and statistic.\nUtilizing the suggested deviation measure and statistic, we show experimentally\nthat training is accelerated more so than with conventional BN, often with\nimproved error rate as well. Overall, we propose a more flexible BN\ntransformation supported by a complimentary theoretical framework that can\npotentially guide design choices.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1812.03271v1",
        "date": "2018-12-08 06:53:48+00:00"
    },
    {
        "title": "Large-Margin kNN Classification Using a Deep Encoder Network",
        "authors": [
            "Martin Renqiang Min",
            "David A. Stanley",
            "Zineng Yuan",
            "Anthony Bonner",
            "Zhaolei Zhang"
        ],
        "abstract": "KNN is one of the most popular classification methods, but it often fails to\nwork well with inappropriate choice of distance metric or due to the presence\nof numerous class-irrelevant features. Linear feature transformation methods\nhave been widely applied to extract class-relevant information to improve kNN\nclassification, which is very limited in many applications. Kernels have been\nused to learn powerful non-linear feature transformations, but these methods\nfail to scale to large datasets. In this paper, we present a scalable\nnon-linear feature mapping method based on a deep neural network pretrained\nwith restricted boltzmann machines for improving kNN classification in a\nlarge-margin framework, which we call DNet-kNN. DNet-kNN can be used for both\nclassification and for supervised dimensionality reduction. The experimental\nresults on two benchmark handwritten digit datasets show that DNet-kNN has much\nbetter performance than large-margin kNN using a linear mapping and kNN based\non a deep autoencoder pretrained with retricted boltzmann machines.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/0906.1814v1",
        "date": "2009-06-09 20:06:45+00:00"
    },
    {
        "title": "Not All Features Are Equal: Feature Leveling Deep Neural Networks for Better Interpretation",
        "authors": [
            "Yingjing Lu",
            "Runde Yang"
        ],
        "abstract": "Self-explaining models are models that reveal decision making parameters in\nan interpretable manner so that the model reasoning process can be directly\nunderstood by human beings. General Linear Models (GLMs) are self-explaining\nbecause the model weights directly show how each feature contributes to the\noutput value. However, deep neural networks (DNNs) are in general not\nself-explaining due to the non-linearity of the activation functions, complex\narchitectures, obscure feature extraction and transformation process. In this\nwork, we illustrate the fact that existing deep architectures are hard to\ninterpret because each hidden layer carries a mix of low level features and\nhigh level features. As a solution, we propose a novel feature leveling\narchitecture that isolates low level features from high level features on a\nper-layer basis to better utilize the GLM layer in the proposed architecture\nfor interpretation. Experimental results show that our modified models are able\nto achieve competitive results comparing to main-stream architectures on\nstandard datasets while being more self-explainable. Our implementations and\nconfigurations are publicly available for reproductions",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1905.10009v2",
        "date": "2019-05-24 02:53:45+00:00"
    },
    {
        "title": "DCT-SNN: Using DCT to Distribute Spatial Information over Time for Learning Low-Latency Spiking Neural Networks",
        "authors": [
            "Isha Garg",
            "Sayeed Shafayet Chowdhury",
            "Kaushik Roy"
        ],
        "abstract": "Spiking Neural Networks (SNNs) offer a promising alternative to traditional\ndeep learning frameworks, since they provide higher computational efficiency\ndue to event-driven information processing. SNNs distribute the analog values\nof pixel intensities into binary spikes over time. However, the most widely\nused input coding schemes, such as Poisson based rate-coding, do not leverage\nthe additional temporal learning capability of SNNs effectively. Moreover,\nthese SNNs suffer from high inference latency which is a major bottleneck to\ntheir deployment. To overcome this, we propose a scalable time-based encoding\nscheme that utilizes the Discrete Cosine Transform (DCT) to reduce the number\nof timesteps required for inference. DCT decomposes an image into a weighted\nsum of sinusoidal basis images. At each time step, the Hadamard product of the\nDCT coefficients and a single frequency base, taken in order, is given to an\naccumulator that generates spikes upon crossing a threshold. We use the\nproposed scheme to learn DCT-SNN, a low-latency deep SNN with\nleaky-integrate-and-fire neurons, trained using surrogate gradient descent\nbased backpropagation. We achieve top-1 accuracy of 89.94%, 68.3% and 52.43% on\nCIFAR-10, CIFAR-100 and TinyImageNet, respectively using VGG architectures.\nNotably, DCT-SNN performs inference with 2-14X reduced latency compared to\nother state-of-the-art SNNs, while achieving comparable accuracy to their\nstandard deep learning counterparts. The dimension of the transform allows us\nto control the number of timesteps required for inference. Additionally, we can\ntrade-off accuracy with latency in a principled manner by dropping the highest\nfrequency components during inference.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2010.01795v1",
        "date": "2020-10-05 05:55:34+00:00"
    },
    {
        "title": "Single-Cell Multimodal Prediction via Transformers",
        "authors": [
            "Wenzhuo Tang",
            "Hongzhi Wen",
            "Renming Liu",
            "Jiayuan Ding",
            "Wei Jin",
            "Yuying Xie",
            "Hui Liu",
            "Jiliang Tang"
        ],
        "abstract": "The recent development of multimodal single-cell technology has made the\npossibility of acquiring multiple omics data from individual cells, thereby\nenabling a deeper understanding of cellular states and dynamics. Nevertheless,\nthe proliferation of multimodal single-cell data also introduces tremendous\nchallenges in modeling the complex interactions among different modalities. The\nrecently advanced methods focus on constructing static interaction graphs and\napplying graph neural networks (GNNs) to learn from multimodal data. However,\nsuch static graphs can be suboptimal as they do not take advantage of the\ndownstream task information; meanwhile GNNs also have some inherent limitations\nwhen deeply stacking GNN layers. To tackle these issues, in this work, we\ninvestigate how to leverage transformers for multimodal single-cell data in an\nend-to-end manner while exploiting downstream task information. In particular,\nwe propose a scMoFormer framework which can readily incorporate external domain\nknowledge and model the interactions within each modality and cross modalities.\nExtensive experiments demonstrate that scMoFormer achieves superior performance\non various benchmark datasets. Note that scMoFormer won a Kaggle silver medal\nwith the rank of $24\\ /\\ 1221$ (Top 2%) without ensemble in a NeurIPS 2022\ncompetition. Our implementation is publicly available at Github.",
        "categories": [
            "q-bio.GN",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2303.00233v1",
        "date": "2023-03-01 05:03:23+00:00"
    },
    {
        "title": "Sample Compression, Support Vectors, and Generalization in Deep Learning",
        "authors": [
            "Christopher Snyder",
            "Sriram Vishwanath"
        ],
        "abstract": "Even though Deep Neural Networks (DNNs) are widely celebrated for their\npractical performance, they possess many intriguing properties related to depth\nthat are difficult to explain both theoretically and intuitively. Understanding\nhow weights in deep networks coordinate together across layers to form useful\nlearners has proven challenging, in part because the repeated composition of\nnonlinearities has proved intractable. This paper presents a reparameterization\nof DNNs as a linear function of a feature map that is locally independent of\nthe weights. This feature map transforms depth-dependencies into simple tensor\nproducts and maps each input to a discrete subset of the feature space. Then,\nusing a max-margin assumption, the paper develops a sample compression\nrepresentation of the neural network in terms of the discrete activation state\nof neurons induced by s ``support vectors\". The paper shows that the number of\nsupport vectors s relates with learning guarantees for neural networks through\nsample compression bounds, yielding a sample complexity of O(ns/epsilon) for\nnetworks with n neurons. Finally, the number of support vectors s is found to\nmonotonically increase with width and label noise but decrease with depth.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1811.02067v4",
        "date": "2018-11-05 22:32:15+00:00"
    },
    {
        "title": "Multi-Agent Motion Planning using Deep Learning for Space Applications",
        "authors": [
            "Kyongsik Yun",
            "Changrak Choi",
            "Ryan Alimo",
            "Anthony Davis",
            "Linda Forster",
            "Amir Rahmani",
            "Muhammad Adil",
            "Ramtin Madani"
        ],
        "abstract": "State-of-the-art motion planners cannot scale to a large number of systems.\nMotion planning for multiple agents is an NP (non-deterministic\npolynomial-time) hard problem, so the computation time increases exponentially\nwith each addition of agents. This computational demand is a major stumbling\nblock to the motion planner's application to future NASA missions involving the\nswarm of space vehicles. We applied a deep neural network to transform\ncomputationally demanding mathematical motion planning problems into deep\nlearning-based numerical problems. We showed optimal motion trajectories can be\naccurately replicated using deep learning-based numerical models in several 2D\nand 3D systems with multiple agents. The deep learning-based numerical model\ndemonstrates superior computational efficiency with plans generated 1000 times\nfaster than the mathematical model counterpart.",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2010.07935v1",
        "date": "2020-10-15 06:42:47+00:00"
    },
    {
        "title": "Do We Really Need Deep Learning Models for Time Series Forecasting?",
        "authors": [
            "Shereen Elsayed",
            "Daniela Thyssens",
            "Ahmed Rashed",
            "Hadi Samer Jomaa",
            "Lars Schmidt-Thieme"
        ],
        "abstract": "Time series forecasting is a crucial task in machine learning, as it has a\nwide range of applications including but not limited to forecasting electricity\nconsumption, traffic, and air quality. Traditional forecasting models rely on\nrolling averages, vector auto-regression and auto-regressive integrated moving\naverages. On the other hand, deep learning and matrix factorization models have\nbeen recently proposed to tackle the same problem with more competitive\nperformance. However, one major drawback of such models is that they tend to be\noverly complex in comparison to traditional techniques. In this paper, we\nreport the results of prominent deep learning models with respect to a\nwell-known machine learning baseline, a Gradient Boosting Regression Tree\n(GBRT) model. Similar to the deep neural network (DNN) models, we transform the\ntime series forecasting task into a window-based regression problem.\nFurthermore, we feature-engineered the input and output structure of the GBRT\nmodel, such that, for each training window, the target values are concatenated\nwith external features, and then flattened to form one input instance for a\nmulti-output GBRT model. We conducted a comparative study on nine datasets for\neight state-of-the-art deep-learning models that were presented at top-level\nconferences in the last years. The results demonstrate that the window-based\ninput transformation boosts the performance of a simple GBRT model to levels\nthat outperform all state-of-the-art DNN models evaluated in this paper.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2101.02118v2",
        "date": "2021-01-06 16:18:04+00:00"
    },
    {
        "title": "Application of Deep Learning in Generating Structured Radiology Reports: A Transformer-Based Technique",
        "authors": [
            "Seyed Ali Reza Moezzi",
            "Abdolrahman Ghaedi",
            "Mojdeh Rahmanian",
            "Seyedeh Zahra Mousavi",
            "Ashkan Sami"
        ],
        "abstract": "Since radiology reports needed for clinical practice and research are written\nand stored in free-text narrations, extraction of relative information for\nfurther analysis is difficult. In these circumstances, natural language\nprocessing (NLP) techniques can facilitate automatic information extraction and\ntransformation of free-text formats to structured data. In recent years, deep\nlearning (DL)-based models have been adapted for NLP experiments with promising\nresults. Despite the significant potential of DL models based on artificial\nneural networks (ANN) and convolutional neural networks (CNN), the models face\nsome limitations to implement in clinical practice. Transformers, another new\nDL architecture, have been increasingly applied to improve the process.\nTherefore, in this study, we propose a transformer-based fine-grained named\nentity recognition (NER) architecture for clinical information extraction. We\ncollected 88 abdominopelvic sonography reports in free-text formats and\nannotated them based on our developed information schema. The text-to-text\ntransfer transformer model (T5) and Scifive, a pre-trained domain-specific\nadaptation of the T5 model, were applied for fine-tuning to extract entities\nand relations and transform the input into a structured format. Our\ntransformer-based model in this study outperformed previously applied\napproaches such as ANN and CNN models based on ROUGE-1, ROUGE-2, ROUGE-L, and\nBLEU scores of 0.816, 0.668, 0.528, and 0.743, respectively, while providing an\ninterpretable structured report.",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2209.12177v1",
        "date": "2022-09-25 08:03:15+00:00"
    },
    {
        "title": "Adaptive wavelet distillation from neural networks through interpretations",
        "authors": [
            "Wooseok Ha",
            "Chandan Singh",
            "Francois Lanusse",
            "Srigokul Upadhyayula",
            "Bin Yu"
        ],
        "abstract": "Recent deep-learning models have achieved impressive prediction performance,\nbut often sacrifice interpretability and computational efficiency.\nInterpretability is crucial in many disciplines, such as science and medicine,\nwhere models must be carefully vetted or where interpretation is the goal\nitself. Moreover, interpretable models are concise and often yield\ncomputational efficiency. Here, we propose adaptive wavelet distillation (AWD),\na method which aims to distill information from a trained neural network into a\nwavelet transform. Specifically, AWD penalizes feature attributions of a neural\nnetwork in the wavelet domain to learn an effective multi-resolution wavelet\ntransform. The resulting model is highly predictive, concise, computationally\nefficient, and has properties (such as a multi-scale structure) which make it\neasy to interpret. In close collaboration with domain experts, we showcase how\nAWD addresses challenges in two real-world settings: cosmological parameter\ninference and molecular-partner prediction. In both cases, AWD yields a\nscientifically interpretable and concise model which gives predictive\nperformance better than state-of-the-art neural networks. Moreover, AWD\nidentifies predictive features that are scientifically meaningful in the\ncontext of respective domains. All code and models are released in a\nfull-fledged package available on Github\n(https://github.com/Yu-Group/adaptive-wavelets).",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2107.09145v2",
        "date": "2021-07-19 20:40:35+00:00"
    },
    {
        "title": "HashTran-DNN: A Framework for Enhancing Robustness of Deep Neural Networks against Adversarial Malware Samples",
        "authors": [
            "Deqiang Li",
            "Ramesh Baral",
            "Tao Li",
            "Han Wang",
            "Qianmu Li",
            "Shouhuai Xu"
        ],
        "abstract": "Adversarial machine learning in the context of image processing and related\napplications has received a large amount of attention. However, adversarial\nmachine learning, especially adversarial deep learning, in the context of\nmalware detection has received much less attention despite its apparent\nimportance. In this paper, we present a framework for enhancing the robustness\nof Deep Neural Networks (DNNs) against adversarial malware samples, dubbed\nHashing Transformation Deep Neural Networks} (HashTran-DNN). The core idea is\nto use hash functions with a certain locality-preserving property to transform\nsamples to enhance the robustness of DNNs in malware classification. The\nframework further uses a Denoising Auto-Encoder (DAE) regularizer to\nreconstruct the hash representations of samples, making the resulting DNN\nclassifiers capable of attaining the locality information in the latent space.\nWe experiment with two concrete instantiations of the HashTran-DNN framework to\nclassify Android malware. Experimental results show that four known attacks can\nrender standard DNNs useless in classifying Android malware, that known\ndefenses can at most defend three of the four attacks, and that HashTran-DNN\ncan effectively defend against all of the four attacks.",
        "categories": [
            "cs.CR",
            "cs.LG",
            "stat.ML",
            "62"
        ],
        "link": "http://arxiv.org/pdf/1809.06498v1",
        "date": "2018-09-18 01:39:04+00:00"
    },
    {
        "title": "EEG-ITNet: An Explainable Inception Temporal Convolutional Network for Motor Imagery Classification",
        "authors": [
            "Abbas Salami",
            "Javier Andreu-Perez",
            "Helge Gillmeister"
        ],
        "abstract": "In recent years, neural networks and especially deep architectures have\nreceived substantial attention for EEG signal analysis in the field of\nbrain-computer interfaces (BCIs). In this ongoing research area, the end-to-end\nmodels are more favoured than traditional approaches requiring signal\ntransformation pre-classification. They can eliminate the need for prior\ninformation from experts and the extraction of handcrafted features. However,\nalthough several deep learning algorithms have been already proposed in the\nliterature, achieving high accuracies for classifying motor movements or mental\ntasks, they often face a lack of interpretability and therefore are not quite\nfavoured by the neuroscience community. The reasons behind this issue can be\nthe high number of parameters and the sensitivity of deep neural networks to\ncapture tiny yet unrelated discriminative features. We propose an end-to-end\ndeep learning architecture called EEG-ITNet and a more comprehensible method to\nvisualise the network learned patterns. Using inception modules and causal\nconvolutions with dilation, our model can extract rich spectral, spatial, and\ntemporal information from multi-channel EEG signals with less complexity (in\nterms of the number of trainable parameters) than other existing end-to-end\narchitectures, such as EEG-Inception and EEG-TCNet. By an exhaustive evaluation\non dataset 2a from BCI competition IV and OpenBMI motor imagery dataset,\nEEG-ITNet shows up to 5.9\\% improvement in the classification accuracy in\ndifferent scenarios with statistical significance compared to its competitors.\nWe also comprehensively explain and support the validity of network\nillustration from a neuroscientific perspective. We have also made our code\nopen at https://github.com/AbbasSalami/EEG-ITNet",
        "categories": [
            "cs.LG",
            "cs.AI",
            "q-bio.NC"
        ],
        "link": "http://arxiv.org/pdf/2204.06947v1",
        "date": "2022-04-14 13:18:43+00:00"
    },
    {
        "title": "DeepGate: Learning Neural Representations of Logic Gates",
        "authors": [
            "Min Li",
            "Sadaf Khan",
            "Zhengyuan Shi",
            "Naixing Wang",
            "Yu Huang",
            "Qiang Xu"
        ],
        "abstract": "Applying deep learning (DL) techniques in the electronic design automation\n(EDA) field has become a trending topic. Most solutions apply well-developed DL\nmodels to solve specific EDA problems. While demonstrating promising results,\nthey require careful model tuning for every problem. The fundamental question\non \"How to obtain a general and effective neural representation of circuits?\"\nhas not been answered yet. In this work, we take the first step towards solving\nthis problem. We propose DeepGate, a novel representation learning solution\nthat effectively embeds both logic function and structural information of a\ncircuit as vectors on each gate. Specifically, we propose transforming circuits\ninto unified and-inverter graph format for learning and using signal\nprobabilities as the supervision task in DeepGate. We then introduce a novel\ngraph neural network that uses strong inductive biases in practical circuits as\nlearning priors for signal probability prediction. Our experimental results\nshow the efficacy and generalization capability of DeepGate.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2111.14616v3",
        "date": "2021-11-26 05:57:05+00:00"
    },
    {
        "title": "Roundtrip: A Deep Generative Neural Density Estimator",
        "authors": [
            "Qiao Liu",
            "Jiaze Xu",
            "Rui Jiang",
            "Wing Hung Wong"
        ],
        "abstract": "Density estimation is a fundamental problem in both statistics and machine\nlearning. In this study, we proposed Roundtrip as a general-purpose neural\ndensity estimator based on deep generative models. Roundtrip retains the\ngenerative power of generative adversarial networks (GANs) but also provides\nestimates of density values. Unlike previous neural density estimators that put\nstringent conditions on the transformation from the latent space to the data\nspace, Roundtrip enables the use of much more general mappings. In a series of\nexperiments, Roundtrip achieves state-of-the-art performance in a diverse range\nof density estimation tasks.",
        "categories": [
            "cs.LG",
            "stat.ME",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2004.09017v4",
        "date": "2020-04-20 01:47:00+00:00"
    },
    {
        "title": "Learning Compressed Transforms with Low Displacement Rank",
        "authors": [
            "Anna T. Thomas",
            "Albert Gu",
            "Tri Dao",
            "Atri Rudra",
            "Christopher R\u00e9"
        ],
        "abstract": "The low displacement rank (LDR) framework for structured matrices represents\na matrix through two displacement operators and a low-rank residual. Existing\nuse of LDR matrices in deep learning has applied fixed displacement operators\nencoding forms of shift invariance akin to convolutions. We introduce a class\nof LDR matrices with more general displacement operators, and explicitly learn\nover both the operators and the low-rank component. This class generalizes\nseveral previous constructions while preserving compression and efficient\ncomputation. We prove bounds on the VC dimension of multi-layer neural networks\nwith structured weight matrices and show empirically that our compact\nparameterization can reduce the sample complexity of learning. When replacing\nweight layers in fully-connected, convolutional, and recurrent neural networks\nfor image classification and language modeling tasks, our new classes exceed\nthe accuracy of existing compression approaches, and on some tasks also\noutperform general unstructured layers while using more than 20x fewer\nparameters.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1810.02309v3",
        "date": "2018-10-04 16:44:16+00:00"
    },
    {
        "title": "Neural networks for post-processing ensemble weather forecasts",
        "authors": [
            "Stephan Rasp",
            "Sebastian Lerch"
        ],
        "abstract": "Ensemble weather predictions require statistical post-processing of\nsystematic errors to obtain reliable and accurate probabilistic forecasts.\nTraditionally, this is accomplished with distributional regression models in\nwhich the parameters of a predictive distribution are estimated from a training\nperiod. We propose a flexible alternative based on neural networks that can\nincorporate nonlinear relationships between arbitrary predictor variables and\nforecast distribution parameters that are automatically learned in a\ndata-driven way rather than requiring pre-specified link functions. In a case\nstudy of 2-meter temperature forecasts at surface stations in Germany, the\nneural network approach significantly outperforms benchmark post-processing\nmethods while being computationally more affordable. Key components to this\nimprovement are the use of auxiliary predictor variables and station-specific\ninformation with the help of embeddings. Furthermore, the trained neural\nnetwork can be used to gain insight into the importance of meteorological\nvariables thereby challenging the notion of neural networks as uninterpretable\nblack boxes. Our approach can easily be extended to other statistical\npost-processing and forecasting problems. We anticipate that recent advances in\ndeep learning combined with the ever-increasing amounts of model and\nobservation data will transform the post-processing of numerical weather\nforecasts in the coming decade.",
        "categories": [
            "stat.ML",
            "cs.LG",
            "physics.ao-ph",
            "stat.AP",
            "stat.ME"
        ],
        "link": "http://arxiv.org/pdf/1805.09091v1",
        "date": "2018-05-23 12:30:28+00:00"
    },
    {
        "title": "Transform Once: Efficient Operator Learning in Frequency Domain",
        "authors": [
            "Michael Poli",
            "Stefano Massaroli",
            "Federico Berto",
            "Jinykoo Park",
            "Tri Dao",
            "Christopher R\u00e9",
            "Stefano Ermon"
        ],
        "abstract": "Spectral analysis provides one of the most effective paradigms for\ninformation-preserving dimensionality reduction, as simple descriptions of\nnaturally occurring signals are often obtained via few terms of periodic basis\nfunctions. In this work, we study deep neural networks designed to harness the\nstructure in frequency domain for efficient learning of long-range correlations\nin space or time: frequency-domain models (FDMs). Existing FDMs are based on\ncomplex-valued transforms i.e. Fourier Transforms (FT), and layers that perform\ncomputation on the spectrum and input data separately. This design introduces\nconsiderable computational overhead: for each layer, a forward and inverse FT.\nInstead, this work introduces a blueprint for frequency domain learning through\na single transform: transform once (T1). To enable efficient, direct learning\nin the frequency domain we derive a variance-preserving weight initialization\nscheme and investigate methods for frequency selection in reduced-order FDMs.\nOur results noticeably streamline the design process of FDMs, pruning redundant\ntransforms, and leading to speedups of 3x to 10x that increase with data\nresolution and model size. We perform extensive experiments on learning the\nsolution operator of spatio-temporal dynamics, including incompressible\nNavier-Stokes, turbulent flows around airfoils and high-resolution video of\nsmoke. T1 models improve on the test performance of FDMs while requiring\nsignificantly less computation (5 hours instead of 32 for our large-scale\nexperiment), with over 20% reduction in average predictive error across tasks.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.SY",
            "eess.SY"
        ],
        "link": "http://arxiv.org/pdf/2211.14453v1",
        "date": "2022-11-26 01:56:05+00:00"
    },
    {
        "title": "Transformation Importance with Applications to Cosmology",
        "authors": [
            "Chandan Singh",
            "Wooseok Ha",
            "Francois Lanusse",
            "Vanessa Boehm",
            "Jia Liu",
            "Bin Yu"
        ],
        "abstract": "Machine learning lies at the heart of new possibilities for scientific\ndiscovery, knowledge generation, and artificial intelligence. Its potential\nbenefits to these fields requires going beyond predictive accuracy and focusing\non interpretability. In particular, many scientific problems require\ninterpretations in a domain-specific interpretable feature space (e.g. the\nfrequency domain) whereas attributions to the raw features (e.g. the pixel\nspace) may be unintelligible or even misleading. To address this challenge, we\npropose TRIM (TRansformation IMportance), a novel approach which attributes\nimportances to features in a transformed space and can be applied post-hoc to a\nfully trained model. TRIM is motivated by a cosmological parameter estimation\nproblem using deep neural networks (DNNs) on simulated data, but it is\ngenerally applicable across domains/models and can be combined with any local\ninterpretation method. In our cosmology example, combining TRIM with contextual\ndecomposition shows promising results for identifying which frequencies a DNN\nuses, helping cosmologists to understand and validate that the model learns\nappropriate physical features rather than simulation artifacts.",
        "categories": [
            "stat.ML",
            "astro-ph.IM",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2003.01926v2",
        "date": "2020-03-04 07:50:49+00:00"
    },
    {
        "title": "Gradient-based Filter Design for the Dual-tree Wavelet Transform",
        "authors": [
            "Daniel Recoskie",
            "Richard Mann"
        ],
        "abstract": "The wavelet transform has seen success when incorporated into neural network\narchitectures, such as in wavelet scattering networks. More recently, it has\nbeen shown that the dual-tree complex wavelet transform can provide better\nrepresentations than the standard transform. With this in mind, we extend our\nprevious method for learning filters for the 1D and 2D wavelet transforms into\nthe dual-tree domain. We show that with few modifications to our original\nmodel, we can learn directional filters that leverage the properties of the\ndual-tree wavelet transform.",
        "categories": [
            "eess.SP",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1806.01793v1",
        "date": "2018-06-04 16:29:23+00:00"
    },
    {
        "title": "Topology of deep neural networks",
        "authors": [
            "Gregory Naitzat",
            "Andrey Zhitnikov",
            "Lek-Heng Lim"
        ],
        "abstract": "We study how the topology of a data set $M = M_a \\cup M_b \\subseteq\n\\mathbb{R}^d$, representing two classes $a$ and $b$ in a binary classification\nproblem, changes as it passes through the layers of a well-trained neural\nnetwork, i.e., with perfect accuracy on training set and near-zero\ngeneralization error ($\\approx 0.01\\%$). The goal is to shed light on two\nmysteries in deep neural networks: (i) a nonsmooth activation function like\nReLU outperforms a smooth one like hyperbolic tangent; (ii) successful neural\nnetwork architectures rely on having many layers, even though a shallow network\ncan approximate any function arbitrary well. We performed extensive experiments\non the persistent homology of a wide range of point cloud data sets, both real\nand simulated. The results consistently demonstrate the following: (1) Neural\nnetworks operate by changing topology, transforming a topologically complicated\ndata set into a topologically simple one as it passes through the layers. No\nmatter how complicated the topology of $M$ we begin with, when passed through a\nwell-trained neural network $f : \\mathbb{R}^d \\to \\mathbb{R}^p$, there is a\nvast reduction in the Betti numbers of both components $M_a$ and $M_b$; in fact\nthey nearly always reduce to their lowest possible values:\n$\\beta_k\\bigl(f(M_i)\\bigr) = 0$ for $k \\ge 1$ and $\\beta_0\\bigl(f(M_i)\\bigr) =\n1$, $i =a, b$. Furthermore, (2) the reduction in Betti numbers is significantly\nfaster for ReLU activation than hyperbolic tangent activation as the former\ndefines nonhomeomorphic maps that change topology, whereas the latter defines\nhomeomorphic maps that preserve topology. Lastly, (3) shallow and deep networks\ntransform data sets differently -- a shallow network operates mainly through\nchanging geometry and changes topology only in its final layers, a deep one\nspreads topological changes more evenly across all layers.",
        "categories": [
            "cs.LG",
            "math.AT",
            "stat.ML",
            "I.2.6"
        ],
        "link": "http://arxiv.org/pdf/2004.06093v1",
        "date": "2020-04-13 17:53:36+00:00"
    },
    {
        "title": "Cell Type Identification from Single-Cell Transcriptomic Data via Semi-supervised Learning",
        "authors": [
            "Xishuang Dong",
            "Shanta Chowdhury",
            "Uboho Victor",
            "Xiangfang Li",
            "Lijun Qian"
        ],
        "abstract": "Cell type identification from single-cell transcriptomic data is a common\ngoal of single-cell RNA sequencing (scRNAseq) data analysis. Neural networks\nhave been employed to identify cell types from scRNAseq data with high\nperformance. However, it requires a large mount of individual cells with\naccurate and unbiased annotated types to build the identification models.\nUnfortunately, labeling the scRNAseq data is cumbersome and time-consuming as\nit involves manual inspection of marker genes. To overcome this challenge, we\npropose a semi-supervised learning model to use unlabeled scRNAseq cells and\nlimited amount of labeled scRNAseq cells to implement cell identification.\nFirstly, we transform the scRNAseq cells to \"gene sentences\", which is inspired\nby similarities between natural language system and gene system. Then genes in\nthese sentences are represented as gene embeddings to reduce data sparsity.\nWith these embeddings, we implement a semi-supervised learning model based on\nrecurrent convolutional neural networks (RCNN), which includes a shared\nnetwork, a supervised network and an unsupervised network. The proposed model\nis evaluated on macosko2015, a large scale single-cell transcriptomic dataset\nwith ground truth of individual cell types. It is observed that the proposed\nmodel is able to achieve encouraging performance by learning on very limited\namount of labeled scRNAseq cells together with a large number of unlabeled\nscRNAseq cells.",
        "categories": [
            "q-bio.GN",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2005.03994v1",
        "date": "2020-05-06 19:15:43+00:00"
    },
    {
        "title": "Efficient Softmax Approximation for Deep Neural Networks with Attention Mechanism",
        "authors": [
            "Ihor Vasyltsov",
            "Wooseok Chang"
        ],
        "abstract": "There has been a rapid advance of custom hardware (HW) for accelerating the\ninference speed of deep neural networks (DNNs). Previously, the softmax layer\nwas not a main concern of DNN accelerating HW, because its portion is\nrelatively small in multi-layer perceptron or convolutional neural networks.\nHowever, as the attention mechanisms are widely used in various modern DNNs, a\ncost-efficient implementation of softmax layer is becoming very important. In\nthis paper, we propose two methods to approximate softmax computation, which\nare based on the usage of LookUp Tables (LUTs). The required size of LUT is\nquite small (about 700 Bytes) because ranges of numerators and denominators of\nsoftmax are stable if normalization is applied to the input. We have validated\nthe proposed technique over different AI tasks (object detection, machine\ntranslation, sentiment analysis, and semantic equivalence) and DNN models\n(DETR, Transformer, BERT) by a variety of benchmarks (COCO17, WMT14, WMT17,\nGLUE). We showed that 8-bit approximation allows to obtain acceptable accuracy\nloss below $1.0\\%$.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2111.10770v1",
        "date": "2021-11-21 08:56:29+00:00"
    },
    {
        "title": "A trans-disciplinary review of deep learning research for water resources scientists",
        "authors": [
            "Chaopeng Shen"
        ],
        "abstract": "Deep learning (DL), a new-generation of artificial neural network research,\nhas transformed industries, daily lives and various scientific disciplines in\nrecent years. DL represents significant progress in the ability of neural\nnetworks to automatically engineer problem-relevant features and capture highly\ncomplex data distributions. I argue that DL can help address several major new\nand old challenges facing research in water sciences such as\ninter-disciplinarity, data discoverability, hydrologic scaling, equifinality,\nand needs for parameter regionalization. This review paper is intended to\nprovide water resources scientists and hydrologists in particular with a simple\ntechnical overview, trans-disciplinary progress update, and a source of\ninspiration about the relevance of DL to water. The review reveals that various\nphysical and geoscientific disciplines have utilized DL to address data\nchallenges, improve efficiency, and gain scientific insights. DL is especially\nsuited for information extraction from image-like data and sequential data.\nTechniques and experiences presented in other disciplines are of high relevance\nto water research. Meanwhile, less noticed is that DL may also serve as a\nscientific exploratory tool. A new area termed 'AI neuroscience,' where\nscientists interpret the decision process of deep networks and derive insights,\nhas been born. This budding sub-discipline has demonstrated methods including\ncorrelation-based analysis, inversion of network-extracted features,\nreduced-order approximations by interpretable models, and attribution of\nnetwork decisions to inputs. Moreover, DL can also use data to condition\nneurons that mimic problem-specific fundamental organizing units, thus\nrevealing emergent behaviors of these units. Vast opportunities exist for DL to\npropel advances in water sciences.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1712.02162v3",
        "date": "2017-12-06 12:44:27+00:00"
    },
    {
        "title": "Incorporating Symmetry into Deep Dynamics Models for Improved Generalization",
        "authors": [
            "Rui Wang",
            "Robin Walters",
            "Rose Yu"
        ],
        "abstract": "Recent work has shown deep learning can accelerate the prediction of physical\ndynamics relative to numerical solvers. However, limited physical accuracy and\nan inability to generalize under distributional shift limit its applicability\nto the real world. We propose to improve accuracy and generalization by\nincorporating symmetries into convolutional neural networks. Specifically, we\nemploy a variety of methods each tailored to enforce a different symmetry. Our\nmodels are both theoretically and experimentally robust to distributional shift\nby symmetry group transformations and enjoy favorable sample complexity. We\ndemonstrate the advantage of our approach on a variety of physical dynamics\nincluding Rayleigh B\\'enard convection and real-world ocean currents and\ntemperatures. Compared with image or text applications, our work is a\nsignificant step towards applying equivariant neural networks to\nhigh-dimensional systems with complex dynamics. We open-source our simulation,\ndata, and code at \\url{https://github.com/Rose-STL-Lab/Equivariant-Net}.",
        "categories": [
            "cs.LG",
            "math.RT",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2002.03061v4",
        "date": "2020-02-08 01:28:17+00:00"
    },
    {
        "title": "Multi-source Domain Adaptation in the Deep Learning Era: A Systematic Survey",
        "authors": [
            "Sicheng Zhao",
            "Bo Li",
            "Colorado Reed",
            "Pengfei Xu",
            "Kurt Keutzer"
        ],
        "abstract": "In many practical applications, it is often difficult and expensive to obtain\nenough large-scale labeled data to train deep neural networks to their full\ncapability. Therefore, transferring the learned knowledge from a separate,\nlabeled source domain to an unlabeled or sparsely labeled target domain becomes\nan appealing alternative. However, direct transfer often results in significant\nperformance decay due to domain shift. Domain adaptation (DA) addresses this\nproblem by minimizing the impact of domain shift between the source and target\ndomains. Multi-source domain adaptation (MDA) is a powerful extension in which\nthe labeled data may be collected from multiple sources with different\ndistributions. Due to the success of DA methods and the prevalence of\nmulti-source data, MDA has attracted increasing attention in both academia and\nindustry. In this survey, we define various MDA strategies and summarize\navailable datasets for evaluation. We also compare modern MDA methods in the\ndeep learning era, including latent space transformation and intermediate\ndomain generation. Finally, we discuss future research directions for MDA.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2002.12169v1",
        "date": "2020-02-26 08:07:58+00:00"
    },
    {
        "title": "HINT: Hierarchical Invertible Neural Transport for Density Estimation and Bayesian Inference",
        "authors": [
            "Jakob Kruse",
            "Gianluca Detommaso",
            "Ullrich K\u00f6the",
            "Robert Scheichl"
        ],
        "abstract": "Many recent invertible neural architectures are based on coupling block\ndesigns where variables are divided in two subsets which serve as inputs of an\neasily invertible (usually affine) triangular transformation. While such a\ntransformation is invertible, its Jacobian is very sparse and thus may lack\nexpressiveness. This work presents a simple remedy by noting that subdivision\nand (affine) coupling can be repeated recursively within the resulting subsets,\nleading to an efficiently invertible block with dense, triangular Jacobian. By\nformulating our recursive coupling scheme via a hierarchical architecture, HINT\nallows sampling from a joint distribution p(y,x) and the corresponding\nposterior p(x|y) using a single invertible network. We evaluate our method on\nsome standard data sets and benchmark its full power for density estimation and\nBayesian inference on a novel data set of 2D shapes in Fourier\nparameterization, which enables consistent visualization of samples for\ndifferent dimensionalities.",
        "categories": [
            "stat.ML",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1905.10687v4",
        "date": "2019-05-25 22:29:21+00:00"
    },
    {
        "title": "An Information-Theoretic View for Deep Learning",
        "authors": [
            "Jingwei Zhang",
            "Tongliang Liu",
            "Dacheng Tao"
        ],
        "abstract": "Deep learning has transformed computer vision, natural language processing,\nand speech recognition\\cite{badrinarayanan2017segnet, dong2016image,\nren2017faster, ji20133d}. However, two critical questions remain obscure: (1)\nwhy do deep neural networks generalize better than shallow networks; and (2)\ndoes it always hold that a deeper network leads to better performance?\nSpecifically, letting $L$ be the number of convolutional and pooling layers in\na deep neural network, and $n$ be the size of the training sample, we derive an\nupper bound on the expected generalization error for this network, i.e.,\n  \\begin{eqnarray*}\n  \\mathbb{E}[R(W)-R_S(W)] \\leq\n\\exp{\\left(-\\frac{L}{2}\\log{\\frac{1}{\\eta}}\\right)}\\sqrt{\\frac{2\\sigma^2}{n}I(S,W)\n}\n  \\end{eqnarray*} where $\\sigma >0$ is a constant depending on the loss\nfunction, $0<\\eta<1$ is a constant depending on the information loss for each\nconvolutional or pooling layer, and $I(S, W)$ is the mutual information between\nthe training sample $S$ and the output hypothesis $W$. This upper bound shows\nthat as the number of convolutional and pooling layers $L$ increases in the\nnetwork, the expected generalization error will decrease exponentially to zero.\nLayers with strict information loss, such as the convolutional layers, reduce\nthe generalization error for the whole network; this answers the first\nquestion. However, algorithms with zero expected generalization error does not\nimply a small test error or $\\mathbb{E}[R(W)]$. This is because\n$\\mathbb{E}[R_S(W)]$ is large when the information for fitting the data is lost\nas the number of layers increases. This suggests that the claim `the deeper the\nbetter' is conditioned on a small training error or $\\mathbb{E}[R_S(W)]$.\nFinally, we show that deep learning satisfies a weak notion of stability and\nthe sample complexity of deep neural networks will decrease as $L$ increases.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1804.09060v8",
        "date": "2018-04-24 14:13:19+00:00"
    },
    {
        "title": "Fourier Phase Retrieval with Extended Support Estimation via Deep Neural Network",
        "authors": [
            "Kyung-Su Kim",
            "Sae-Young Chung"
        ],
        "abstract": "We consider the problem of sparse phase retrieval from Fourier transform\nmagnitudes to recover the $k$-sparse signal vector and its support\n$\\mathcal{T}$. We exploit extended support estimate $\\mathcal{E}$ with size\nlarger than $k$ satisfying $\\mathcal{E} \\supseteq \\mathcal{T}$ and obtained by\na trained deep neural network (DNN). To make the DNN learnable, it provides\n$\\mathcal{E}$ as the union of equivalent solutions of $\\mathcal{T}$ by\nutilizing modulo Fourier invariances. Set $\\mathcal{E}$ can be estimated with\nshort running time via the DNN, and support $\\mathcal{T}$ can be determined\nfrom the DNN output rather than from the full index set by applying hard\nthresholding to $\\mathcal{E}$. Thus, the DNN-based extended support estimation\nimproves the reconstruction performance of the signal with a low complexity\nburden dependent on $k$. Numerical results verify that the proposed scheme has\na superior performance with lower complexity compared to local search-based\ngreedy sparse phase retrieval and a state-of-the-art variant of the Fienup\nmethod.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1904.01821v3",
        "date": "2019-04-03 07:55:22+00:00"
    },
    {
        "title": "Achieving Efficiency in Black Box Simulation of Distribution Tails with Self-structuring Importance Samplers",
        "authors": [
            "Anand Deo",
            "Karthyek Murthy"
        ],
        "abstract": "Motivated by the increasing adoption of models which facilitate greater\nautomation in risk management and decision-making, this paper presents a novel\nImportance Sampling (IS) scheme for measuring distribution tails of objectives\nmodelled with enabling tools such as feature-based decision rules, mixed\ninteger linear programs, deep neural networks, etc. Conventional efficient IS\napproaches suffer from feasibility and scalability concerns due to the need to\nintricately tailor the sampler to the underlying probability distribution and\nthe objective. This challenge is overcome in the proposed black-box scheme by\nautomating the selection of an effective IS distribution with a transformation\nthat implicitly learns and replicates the concentration properties observed in\nless rare samples. This novel approach is guided by a large deviations\nprinciple that brings out the phenomenon of self-similarity of optimal IS\ndistributions. The proposed sampler is the first to attain asymptotically\noptimal variance reduction across a spectrum of multivariate distributions\ndespite being oblivious to the underlying structure. The large deviations\nprinciple additionally results in new distribution tail asymptotics capable of\nyielding operational insights. The applicability is illustrated by considering\nproduct distribution networks and portfolio credit risk models informed by\nneural networks as examples.",
        "categories": [
            "stat.ML",
            "cs.LG",
            "math.PR",
            "stat.ME"
        ],
        "link": "http://arxiv.org/pdf/2102.07060v2",
        "date": "2021-02-14 03:37:22+00:00"
    },
    {
        "title": "DeepcomplexMRI: Exploiting deep residual network for fast parallel MR imaging with complex convolution",
        "authors": [
            "Shanshan Wang",
            "Huitao Cheng",
            "Leslie Ying",
            "Taohui Xiao",
            "Ziwen Ke",
            "Xin Liu",
            "Hairong Zheng",
            "Dong Liang"
        ],
        "abstract": "This paper proposes a multi-channel image reconstruction method, named\nDeepcomplexMRI, to accelerate parallel MR imaging with residual complex\nconvolutional neural network. Different from most existing works which rely on\nthe utilization of the coil sensitivities or prior information of predefined\ntransforms, DeepcomplexMRI takes advantage of the availability of a large\nnumber of existing multi-channel groudtruth images and uses them as labeled\ndata to train the deep residual convolutional neural network offline. In\nparticular, a complex convolutional network is proposed to take into account\nthe correlation between the real and imaginary parts of MR images. In addition,\nthe k space data consistency is further enforced repeatedly in between layers\nof the network. The evaluations on in vivo datasets show that the proposed\nmethod has the capability to recover the desired multi-channel images. Its\ncomparison with state-of-the-art method also demonstrates that the proposed\nmethod can reconstruct the desired MR images more accurately.",
        "categories": [
            "eess.IV",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1906.04359v2",
        "date": "2019-06-11 02:41:52+00:00"
    },
    {
        "title": "Data Movement Is All You Need: A Case Study on Optimizing Transformers",
        "authors": [
            "Andrei Ivanov",
            "Nikoli Dryden",
            "Tal Ben-Nun",
            "Shigang Li",
            "Torsten Hoefler"
        ],
        "abstract": "Transformers are one of the most important machine learning workloads today.\nTraining one is a very compute-intensive task, often taking days or weeks, and\nsignificant attention has been given to optimizing transformers. Despite this,\nexisting implementations do not efficiently utilize GPUs. We find that data\nmovement is the key bottleneck when training. Due to Amdahl's Law and massive\nimprovements in compute performance, training has now become memory-bound.\nFurther, existing frameworks use suboptimal data layouts. Using these insights,\nwe present a recipe for globally optimizing data movement in transformers. We\nreduce data movement by up to 22.91% and overall achieve a 1.30x performance\nimprovement over state-of-the-art frameworks when training a BERT encoder layer\nand 1.19x for the entire BERT. Our approach is applicable more broadly to\noptimizing deep neural networks, and offers insight into how to tackle emerging\nperformance bottlenecks.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2007.00072v3",
        "date": "2020-06-30 19:26:36+00:00"
    },
    {
        "title": "Deep Representational Similarity Learning for analyzing neural signatures in task-based fMRI dataset",
        "authors": [
            "Muhammad Yousefnezhad",
            "Jeffrey Sawalha",
            "Alessandro Selvitella",
            "Daoqiang Zhang"
        ],
        "abstract": "Similarity analysis is one of the crucial steps in most fMRI studies.\nRepresentational Similarity Analysis (RSA) can measure similarities of neural\nsignatures generated by different cognitive states. This paper develops Deep\nRepresentational Similarity Learning (DRSL), a deep extension of RSA that is\nappropriate for analyzing similarities between various cognitive tasks in fMRI\ndatasets with a large number of subjects, and high-dimensionality -- such as\nwhole-brain images. Unlike the previous methods, DRSL is not limited by a\nlinear transformation or a restricted fixed nonlinear kernel function -- such\nas Gaussian kernel. DRSL utilizes a multi-layer neural network for mapping\nneural responses to linear space, where this network can implement a customized\nnonlinear transformation for each subject separately. Furthermore, utilizing a\ngradient-based optimization in DRSL can significantly reduce runtime of\nanalysis on large datasets because it uses a batch of samples in each iteration\nrather than all neural responses to find an optimal solution. Empirical studies\non multi-subject fMRI datasets with various tasks -- including visual stimuli,\ndecision making, flavor, and working memory -- confirm that the proposed method\nachieves superior performance to other state-of-the-art RSA algorithms.",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.LG",
            "q-bio.NC"
        ],
        "link": "http://arxiv.org/pdf/2010.02012v1",
        "date": "2020-09-28 18:30:14+00:00"
    },
    {
        "title": "Rethinking Neural Operations for Diverse Tasks",
        "authors": [
            "Nicholas Roberts",
            "Mikhail Khodak",
            "Tri Dao",
            "Liam Li",
            "Christopher R\u00e9",
            "Ameet Talwalkar"
        ],
        "abstract": "An important goal of AutoML is to automate-away the design of neural networks\non new tasks in under-explored domains. Motivated by this goal, we study the\nproblem of enabling users to discover the right neural operations given data\nfrom their specific domain. We introduce a search space of operations called\nXD-Operations that mimic the inductive bias of standard multi-channel\nconvolutions while being much more expressive: we prove that it includes many\nnamed operations across multiple application areas. Starting with any standard\nbackbone such as ResNet, we show how to transform it into a search space over\nXD-operations and how to traverse the space using a simple weight-sharing\nscheme. On a diverse set of tasks -- solving PDEs, distance prediction for\nprotein folding, and music modeling -- our approach consistently yields models\nwith lower error than baseline networks and often even lower error than\nexpert-designed domain-specific approaches.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "cs.NA",
            "math.NA",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2103.15798v2",
        "date": "2021-03-29 17:50:39+00:00"
    },
    {
        "title": "Unsupervised Single Image Dehazing Using Dark Channel Prior Loss",
        "authors": [
            "Alona Golts",
            "Daniel Freedman",
            "Michael Elad"
        ],
        "abstract": "Single image dehazing is a critical stage in many modern-day autonomous\nvision applications. Early prior-based methods often involved a time-consuming\nminimization of a hand-crafted energy function. Recent learning-based\napproaches utilize the representational power of deep neural networks (DNNs) to\nlearn the underlying transformation between hazy and clear images. Due to\ninherent limitations in collecting matching clear and hazy images, these\nmethods resort to training on synthetic data; constructed from indoor images\nand corresponding depth information. This may result in a possible domain shift\nwhen treating outdoor scenes. We propose a completely unsupervised method of\ntraining via minimization of the well-known, Dark Channel Prior (DCP) energy\nfunction. Instead of feeding the network with synthetic data, we solely use\nreal-world outdoor images and tune the network's parameters by directly\nminimizing the DCP. Although our \"Deep DCP\" technique can be regarded as a fast\napproximator of DCP, it actually improves its results significantly. This\nsuggests an additional regularization obtained via the network and learning\nprocess. Experiments show that our method performs on par with large-scale\nsupervised methods.",
        "categories": [
            "cs.CV",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1812.07051v2",
        "date": "2018-12-06 11:29:38+00:00"
    },
    {
        "title": "Normalizing flows for novelty detection in industrial time series data",
        "authors": [
            "Maximilian Schmidt",
            "Marko Simic"
        ],
        "abstract": "Flow-based deep generative models learn data distributions by transforming a\nsimple base distribution into a complex distribution via a set of invertible\ntransformations. Due to the invertibility, such models can score unseen data\nsamples by computing their exact likelihood under the learned distribution.\nThis makes flow-based models a perfect tool for novelty detection, an anomaly\ndetection technique where unseen data samples are classified as normal or\nabnormal by scoring them against a learned model of normal data. We show that\nnormalizing flows can be used as novelty detectors in time series. Two\nflow-based models, Masked Autoregressive Flows and Free-form Jacobian of\nReversible Dynamics restricted by autoregressive MADE networks, are tested on\nsynthetic data and motor current data from an industrial machine and achieve\ngood results, outperforming a conventional novelty detection method, the Local\nOutlier Factor.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1906.06904v1",
        "date": "2019-06-17 08:52:22+00:00"
    },
    {
        "title": "Stochastic Shared Embeddings: Data-driven Regularization of Embedding Layers",
        "authors": [
            "Liwei Wu",
            "Shuqing Li",
            "Cho-Jui Hsieh",
            "James Sharpnack"
        ],
        "abstract": "In deep neural nets, lower level embedding layers account for a large portion\nof the total number of parameters. Tikhonov regularization, graph-based\nregularization, and hard parameter sharing are approaches that introduce\nexplicit biases into training in a hope to reduce statistical complexity.\nAlternatively, we propose stochastically shared embeddings (SSE), a data-driven\napproach to regularizing embedding layers, which stochastically transitions\nbetween embeddings during stochastic gradient descent (SGD). Because SSE\nintegrates seamlessly with existing SGD algorithms, it can be used with only\nminor modifications when training large scale neural networks. We develop two\nversions of SSE: SSE-Graph using knowledge graphs of embeddings; SSE-SE using\nno prior information. We provide theoretical guarantees for our method and show\nits empirical effectiveness on 6 distinct tasks, from simple neural networks\nwith one hidden layer in recommender systems, to the transformer and BERT in\nnatural languages. We find that when used along with widely-used regularization\nmethods such as weight decay and dropout, our proposed SSE can further reduce\noverfitting, which often leads to more favorable generalization results.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1905.10630v3",
        "date": "2019-05-25 16:55:36+00:00"
    },
    {
        "title": "AReLU: Attention-based Rectified Linear Unit",
        "authors": [
            "Dengsheng Chen",
            "Jun Li",
            "Kai Xu"
        ],
        "abstract": "Element-wise activation functions play a critical role in deep neural\nnetworks via affecting the expressivity power and the learning dynamics.\nLearning-based activation functions have recently gained increasing attention\nand success. We propose a new perspective of learnable activation function\nthrough formulating them with element-wise attention mechanism. In each network\nlayer, we devise an attention module which learns an element-wise, sign-based\nattention map for the pre-activation feature map. The attention map scales an\nelement based on its sign. Adding the attention module with a rectified linear\nunit (ReLU) results in an amplification of positive elements and a suppression\nof negative ones, both with learned, data-adaptive parameters. We coin the\nresulting activation function Attention-based Rectified Linear Unit (AReLU).\nThe attention module essentially learns an element-wise residue of the\nactivated part of the input, as ReLU can be viewed as an identity\ntransformation. This makes the network training more resistant to gradient\nvanishing. The learned attentive activation leads to well-focused activation of\nrelevant regions of a feature map. Through extensive evaluations, we show that\nAReLU significantly boosts the performance of most mainstream network\narchitectures with only two extra learnable parameters per layer introduced.\nNotably, AReLU facilitates fast network training under small learning rates,\nwhich makes it especially suited in the case of transfer learning and meta\nlearning. Our source code has been released (see\nhttps://github.com/densechen/AReLU).",
        "categories": [
            "cs.LG",
            "cs.NE",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2006.13858v2",
        "date": "2020-06-24 16:39:16+00:00"
    },
    {
        "title": "Secure Forward Aggregation for Vertical Federated Neural Networks",
        "authors": [
            "Shuowei Cai",
            "Di Chai",
            "Liu Yang",
            "Junxue Zhang",
            "Yilun Jin",
            "Leye Wang",
            "Kun Guo",
            "Kai Chen"
        ],
        "abstract": "Vertical federated learning (VFL) is attracting much attention because it\nenables cross-silo data cooperation in a privacy-preserving manner. While most\nresearch works in VFL focus on linear and tree models, deep models (e.g.,\nneural networks) are not well studied in VFL. In this paper, we focus on\nSplitNN, a well-known neural network framework in VFL, and identify a trade-off\nbetween data security and model performance in SplitNN. Briefly, SplitNN trains\nthe model by exchanging gradients and transformed data. On the one hand,\nSplitNN suffers from the loss of model performance since multiply parties\njointly train the model using transformed data instead of raw data, and a large\namount of low-level feature information is discarded. On the other hand, a\nnaive solution of increasing the model performance through aggregating at lower\nlayers in SplitNN (i.e., the data is less transformed and more low-level\nfeature is preserved) makes raw data vulnerable to inference attacks. To\nmitigate the above trade-off, we propose a new neural network protocol in VFL\ncalled Security Forward Aggregation (SFA). It changes the way of aggregating\nthe transformed data and adopts removable masks to protect the raw data.\nExperiment results show that networks with SFA achieve both data security and\nhigh model performance.",
        "categories": [
            "cs.CR",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2207.00165v1",
        "date": "2022-06-28 03:13:26+00:00"
    },
    {
        "title": "Transfer learning with affine model transformation",
        "authors": [
            "Shunya Minami",
            "Kenji Fukumizu",
            "Yoshihiro Hayashi",
            "Ryo Yoshida"
        ],
        "abstract": "Supervised transfer learning (TL) has received considerable attention because\nof its potential to boost the predictive power of machine learning in cases\nwith limited data. In a conventional scenario, cross-domain differences are\nmodeled and estimated using a given set of source models and samples from a\ntarget domain. For example, if there is a functional relationship between\nsource and target domains, only domain-specific factors are additionally\nlearned using target samples to shift the source models to the target. However,\nthe general methodology for modeling and estimating such cross-domain shifts\nhas been less studied. This study presents a TL framework that simultaneously\nand separately estimates domain shifts and domain-specific factors using given\ntarget samples. Assuming consistency and invertibility of the domain\ntransformation functions, we derive an optimal family of functions to represent\nthe cross-domain shift. The newly derived class of transformation functions\ntakes the same form as invertible neural networks using affine coupling layers,\nwhich are widely used in generative deep learning. We show that the proposed\nmethod encompasses a wide range of existing methods, including the most common\nTL procedure based on feature extraction using neural networks. We also clarify\nthe theoretical properties of the proposed method, such as the convergence rate\nof the generalization error, and demonstrate the practical benefits of\nseparately modeling and estimating domain-specific factors through several case\nstudies.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2210.09745v1",
        "date": "2022-10-18 10:50:24+00:00"
    },
    {
        "title": "Attention is All You Need? Good Embeddings with Statistics are enough:Large Scale Audio Understanding without Transformers/ Convolutions/ BERTs/ Mixers/ Attention/ RNNs or ....",
        "authors": [
            "Prateek Verma"
        ],
        "abstract": "This paper presents a way of doing large scale audio understanding without\ntraditional state of the art neural architectures. Ever since the introduction\nof deep learning for understanding audio signals in the past decade,\nconvolutional architectures have been able to achieve state of the art results\nsurpassing traditional hand-crafted features. In the recent past, there has\nbeen a similar shift away from traditional convolutional and recurrent neural\nnetworks towards purely end-to-end Transformer architectures. We, in this work,\nexplore an approach, based on Bag-of-Words model. Our approach does not have\nany convolutions, recurrence, attention, transformers or other approaches such\nas BERT. We utilize micro and macro level clustered vanilla embeddings, and use\na MLP head for classification. We only use feed-forward encoder-decoder models\nto get the bottlenecks of spectral envelops, spectral patches and slices as\nwell as multi-resolution spectra. A classification head (a feed-forward layer),\nsimilar to the approach in SimCLR is trained on a learned representation. Using\nsimple codes learned on latent representations, we show how we surpass\ntraditional convolutional neural network architectures, and come strikingly\nclose to outperforming powerful Transformer architectures. This work hopefully\nwould pave way for exciting advancements in the field of representation\nlearning without massive, end-to-end neural architectures.",
        "categories": [
            "cs.SD",
            "cs.AI",
            "cs.IR",
            "cs.LG",
            "cs.MM",
            "eess.AS"
        ],
        "link": "http://arxiv.org/pdf/2110.03183v5",
        "date": "2021-10-07 05:00:26+00:00"
    },
    {
        "title": "Graph Convolutional Neural Networks based on Quantum Vertex Saliency",
        "authors": [
            "Lu Bai",
            "Yuhang Jiao",
            "Luca Rossi",
            "Lixin Cui",
            "Jian Cheng",
            "Edwin R. Hancock"
        ],
        "abstract": "This paper proposes a new Quantum Spatial Graph Convolutional Neural Network\n(QSGCNN) model that can directly learn a classification function for graphs of\narbitrary sizes. Unlike state-of-the-art Graph Convolutional Neural Network\n(GCNN) models, the proposed QSGCNN model incorporates the process of\nidentifying transitive aligned vertices between graphs, and transforms\narbitrary sized graphs into fixed-sized aligned vertex grid structures. In\norder to learn representative graph characteristics, a new quantum spatial\ngraph convolution is proposed and employed to extract multi-scale vertex\nfeatures, in terms of quantum information propagation between grid vertices of\neach graph. Since the quantum spatial convolution preserves the grid structures\nof the input vertices (i.e., the convolution layer does not change the original\nspatial sequence of vertices), the proposed QSGCNN model allows to directly\nemploy the traditional convolutional neural network architecture to further\nlearn from the global graph topology, providing an end-to-end deep learning\narchitecture that integrates the graph representation and learning in the\nquantum spatial graph convolution layer and the traditional convolutional layer\nfor graph classifications. We demonstrate the effectiveness of the proposed\nQSGCNN model in relation to existing state-of-the-art methods. The proposed\nQSGCNN model addresses the shortcomings of information loss and imprecise\ninformation representation arising in existing GCN models associated with the\nuse of SortPooling or SumPooling layers. Experiments on benchmark graph\nclassification datasets demonstrate the effectiveness of the proposed QSGCNN\nmodel.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1809.01090v2",
        "date": "2018-09-04 16:53:04+00:00"
    },
    {
        "title": "Switched linear projections for neural network interpretability",
        "authors": [
            "Lech Szymanski",
            "Brendan McCane",
            "Craig Atkinson"
        ],
        "abstract": "We introduce switched linear projections for expressing the activity of a\nneuron in a deep neural network in terms of a single linear projection in the\ninput space. The method works by isolating the active subnetwork, a series of\nlinear transformations, that determine the entire computation of the network\nfor a given input instance. With these projections we can decompose activity in\nany hidden layer into patterns detected in a given input instance. We also\npropose that in ReLU networks it is instructive and meaningful to examine\npatterns that deactivate the neurons in a hidden layer, something that is\nimplicitly ignored by the existing interpretability methods tracking solely the\nactive aspect of the network's computation.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1909.11275v3",
        "date": "2019-09-25 03:43:37+00:00"
    },
    {
        "title": "Learning Compositional Koopman Operators for Model-Based Control",
        "authors": [
            "Yunzhu Li",
            "Hao He",
            "Jiajun Wu",
            "Dina Katabi",
            "Antonio Torralba"
        ],
        "abstract": "Finding an embedding space for a linear approximation of a nonlinear\ndynamical system enables efficient system identification and control synthesis.\nThe Koopman operator theory lays the foundation for identifying the\nnonlinear-to-linear coordinate transformations with data-driven methods.\nRecently, researchers have proposed to use deep neural networks as a more\nexpressive class of basis functions for calculating the Koopman operators.\nThese approaches, however, assume a fixed dimensional state space; they are\ntherefore not applicable to scenarios with a variable number of objects. In\nthis paper, we propose to learn compositional Koopman operators, using graph\nneural networks to encode the state into object-centric embeddings and using a\nblock-wise linear transition matrix to regularize the shared structure across\nobjects. The learned dynamics can quickly adapt to new environments of unknown\nphysical parameters and produce control signals to achieve a specified goal.\nOur experiments on manipulating ropes and controlling soft robots show that the\nproposed method has better efficiency and generalization ability than existing\nbaselines.",
        "categories": [
            "cs.LG",
            "cs.RO",
            "math.OC",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1910.08264v2",
        "date": "2019-10-18 05:11:16+00:00"
    },
    {
        "title": "Supervised Domain Adaptation using Graph Embedding",
        "authors": [
            "Lukas Hedegaard Morsing",
            "Omar Ali Sheikh-Omar",
            "Alexandros Iosifidis"
        ],
        "abstract": "Getting deep convolutional neural networks to perform well requires a large\namount of training data. When the available labelled data is small, it is often\nbeneficial to use transfer learning to leverage a related larger dataset\n(source) in order to improve the performance on the small dataset (target).\nAmong the transfer learning approaches, domain adaptation methods assume that\ndistributions between the two domains are shifted and attempt to realign them.\nIn this paper, we consider the domain adaptation problem from the perspective\nof dimensionality reduction and propose a generic framework based on graph\nembedding. Instead of solving the generalised eigenvalue problem, we formulate\nthe graph-preserving criterion as a loss in the neural network and learn a\ndomain-invariant feature transformation in an end-to-end fashion. We show that\nthe proposed approach leads to a powerful Domain Adaptation framework; a simple\nLDA-inspired instantiation of the framework leads to state-of-the-art\nperformance on two of the most widely used Domain Adaptation benchmarks,\nOffice31 and MNIST to USPS datasets.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2003.04063v2",
        "date": "2020-03-09 12:25:13+00:00"
    },
    {
        "title": "Rapid training of deep neural networks without skip connections or normalization layers using Deep Kernel Shaping",
        "authors": [
            "James Martens",
            "Andy Ballard",
            "Guillaume Desjardins",
            "Grzegorz Swirszcz",
            "Valentin Dalibard",
            "Jascha Sohl-Dickstein",
            "Samuel S. Schoenholz"
        ],
        "abstract": "Using an extended and formalized version of the Q/C map analysis of Poole et\nal. (2016), along with Neural Tangent Kernel theory, we identify the main\npathologies present in deep networks that prevent them from training fast and\ngeneralizing to unseen data, and show how these can be avoided by carefully\ncontrolling the \"shape\" of the network's initialization-time kernel function.\nWe then develop a method called Deep Kernel Shaping (DKS), which accomplishes\nthis using a combination of precise parameter initialization, activation\nfunction transformations, and small architectural tweaks, all of which preserve\nthe model class. In our experiments we show that DKS enables SGD training of\nresidual networks without normalization layers on Imagenet and CIFAR-10\nclassification tasks at speeds comparable to standard ResNetV2 and Wide-ResNet\nmodels, with only a small decrease in generalization performance. And when\nusing K-FAC as the optimizer, we achieve similar results for networks without\nskip connections. Our results apply for a large variety of activation\nfunctions, including those which traditionally perform very badly, such as the\nlogistic sigmoid. In addition to DKS, we contribute a detailed analysis of skip\nconnections, normalization layers, special activation functions like RELU and\nSELU, and various initialization schemes, explaining their effectiveness as\nalternative (and ultimately incomplete) ways of \"shaping\" the network's\ninitialization-time kernel.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.NE"
        ],
        "link": "http://arxiv.org/pdf/2110.01765v1",
        "date": "2021-10-05 00:49:36+00:00"
    },
    {
        "title": "Natural-Parameter Networks: A Class of Probabilistic Neural Networks",
        "authors": [
            "Hao Wang",
            "Xingjian Shi",
            "Dit-Yan Yeung"
        ],
        "abstract": "Neural networks (NN) have achieved state-of-the-art performance in various\napplications. Unfortunately in applications where training data is\ninsufficient, they are often prone to overfitting. One effective way to\nalleviate this problem is to exploit the Bayesian approach by using Bayesian\nneural networks (BNN). Another shortcoming of NN is the lack of flexibility to\ncustomize different distributions for the weights and neurons according to the\ndata, as is often done in probabilistic graphical models. To address these\nproblems, we propose a class of probabilistic neural networks, dubbed\nnatural-parameter networks (NPN), as a novel and lightweight Bayesian treatment\nof NN. NPN allows the usage of arbitrary exponential-family distributions to\nmodel the weights and neurons. Different from traditional NN and BNN, NPN takes\ndistributions as input and goes through layers of transformation before\nproducing distributions to match the target output distributions. As a Bayesian\ntreatment, efficient backpropagation (BP) is performed to learn the natural\nparameters for the distributions over both the weights and neurons. The output\ndistributions of each layer, as byproducts, may be used as second-order\nrepresentations for the associated tasks such as link prediction. Experiments\non real-world datasets show that NPN can achieve state-of-the-art performance.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1611.00448v1",
        "date": "2016-11-02 02:32:05+00:00"
    },
    {
        "title": "Search Intelligence: Deep Learning For Dominant Category Prediction",
        "authors": [
            "Zeeshan Khawar Malik",
            "Mo Kobrosli",
            "Peter Maas"
        ],
        "abstract": "Deep Neural Networks, and specifically fully-connected convolutional neural\nnetworks are achieving remarkable results across a wide variety of domains.\nThey have been trained to achieve state-of-the-art performance when applied to\nproblems such as speech recognition, image classification, natural language\nprocessing and bioinformatics. Most of these deep learning models when applied\nto classification employ the softmax activation function for prediction and aim\nto minimize cross-entropy loss. In this paper, we have proposed a supervised\nmodel for dominant category prediction to improve search recall across all eBay\nclassifieds platforms. The dominant category label for each query in the last\n90 days is first calculated by summing the total number of collaborative clicks\namong all categories. The category having the highest number of collaborative\nclicks for the given query will be considered its dominant category. Second,\neach query is transformed to a numeric vector by mapping each unique word in\nthe query document to a unique integer value; all padded to equal length based\non the maximum document length within the pre-defined vocabulary size. A\nfully-connected deep convolutional neural network (CNN) is then applied for\nclassification. The proposed model achieves very high classification accuracy\ncompared to other state-of-the-art machine learning techniques.",
        "categories": [
            "cs.IR",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1702.01717v1",
        "date": "2017-02-06 17:27:12+00:00"
    },
    {
        "title": "Deep Hidden Physics Models: Deep Learning of Nonlinear Partial Differential Equations",
        "authors": [
            "Maziar Raissi"
        ],
        "abstract": "A long-standing problem at the interface of artificial intelligence and\napplied mathematics is to devise an algorithm capable of achieving human level\nor even superhuman proficiency in transforming observed data into predictive\nmathematical models of the physical world. In the current era of abundance of\ndata and advanced machine learning capabilities, the natural question arises:\nHow can we automatically uncover the underlying laws of physics from\nhigh-dimensional data generated from experiments? In this work, we put forth a\ndeep learning approach for discovering nonlinear partial differential equations\nfrom scattered and potentially noisy observations in space and time.\nSpecifically, we approximate the unknown solution as well as the nonlinear\ndynamics by two deep neural networks. The first network acts as a prior on the\nunknown solution and essentially enables us to avoid numerical differentiations\nwhich are inherently ill-conditioned and unstable. The second network\nrepresents the nonlinear dynamics and helps us distill the mechanisms that\ngovern the evolution of a given spatiotemporal data-set. We test the\neffectiveness of our approach for several benchmark problems spanning a number\nof scientific domains and demonstrate how the proposed framework can help us\naccurately learn the underlying dynamics and forecast future states of the\nsystem. In particular, we study the Burgers', Korteweg-de Vries (KdV),\nKuramoto-Sivashinsky, nonlinear Schr\\\"{o}dinger, and Navier-Stokes equations.",
        "categories": [
            "stat.ML",
            "cs.LG",
            "cs.NA",
            "math.AP"
        ],
        "link": "http://arxiv.org/pdf/1801.06637v1",
        "date": "2018-01-20 08:02:09+00:00"
    },
    {
        "title": "e3nn: Euclidean Neural Networks",
        "authors": [
            "Mario Geiger",
            "Tess Smidt"
        ],
        "abstract": "We present e3nn, a generalized framework for creating E(3) equivariant\ntrainable functions, also known as Euclidean neural networks. e3nn naturally\noperates on geometry and geometric tensors that describe systems in 3D and\ntransform predictably under a change of coordinate system. The core of e3nn are\nequivariant operations such as the TensorProduct class or the spherical\nharmonics functions that can be composed to create more complex modules such as\nconvolutions and attention mechanisms. These core operations of e3nn can be\nused to efficiently articulate Tensor Field Networks, 3D Steerable CNNs,\nClebsch-Gordan Networks, SE(3) Transformers and other E(3) equivariant\nnetworks.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.NE"
        ],
        "link": "http://arxiv.org/pdf/2207.09453v1",
        "date": "2022-07-18 21:19:40+00:00"
    },
    {
        "title": "Improving Neural Language Modeling via Adversarial Training",
        "authors": [
            "Dilin Wang",
            "Chengyue Gong",
            "Qiang Liu"
        ],
        "abstract": "Recently, substantial progress has been made in language modeling by using\ndeep neural networks. However, in practice, large scale neural language models\nhave been shown to be prone to overfitting. In this paper, we present a simple\nyet highly effective adversarial training mechanism for regularizing neural\nlanguage models. The idea is to introduce adversarial noise to the output\nembedding layer while training the models. We show that the optimal adversarial\nnoise yields a simple closed-form solution, thus allowing us to develop a\nsimple and time efficient algorithm. Theoretically, we show that our\nadversarial mechanism effectively encourages the diversity of the embedding\nvectors, helping to increase the robustness of models. Empirically, we show\nthat our method improves on the single model state-of-the-art results for\nlanguage modeling on Penn Treebank (PTB) and Wikitext-2, achieving test\nperplexity scores of 46.01 and 38.07, respectively. When applied to machine\ntranslation, our method improves over various transformer-based translation\nbaselines in BLEU scores on the WMT14 English-German and IWSLT14 German-English\ntasks.",
        "categories": [
            "cs.LG",
            "cs.CL",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1906.03805v2",
        "date": "2019-06-10 05:55:08+00:00"
    },
    {
        "title": "Transformer Based Reinforcement Learning For Games",
        "authors": [
            "Uddeshya Upadhyay",
            "Nikunj Shah",
            "Sucheta Ravikanti",
            "Mayanka Medhe"
        ],
        "abstract": "Recent times have witnessed sharp improvements in reinforcement learning\ntasks using deep reinforcement learning techniques like Deep Q Networks, Policy\nGradients, Actor Critic methods which are based on deep learning based models\nand back-propagation of gradients to train such models. An active area of\nresearch in reinforcement learning is about training agents to play complex\nvideo games, which so far has been something accomplished only by human\nintelligence. Some state of the art performances in video game playing using\ndeep reinforcement learning are obtained by processing the sequence of frames\nfrom video games, passing them through a convolutional network to obtain\nfeatures and then using recurrent neural networks to figure out the action\nleading to optimal rewards. The recurrent neural network will learn to extract\nthe meaningful signal out of the sequence of such features. In this work, we\npropose a method utilizing a transformer network which have recently replaced\nRNNs in Natural Language Processing (NLP), and perform experiments to compare\nwith existing methods.",
        "categories": [
            "cs.LG",
            "cs.NE"
        ],
        "link": "http://arxiv.org/pdf/1912.03918v1",
        "date": "2019-12-09 09:35:48+00:00"
    },
    {
        "title": "Reservoir Computing meets Recurrent Kernels and Structured Transforms",
        "authors": [
            "Jonathan Dong",
            "Ruben Ohana",
            "Mushegh Rafayelyan",
            "Florent Krzakala"
        ],
        "abstract": "Reservoir Computing is a class of simple yet efficient Recurrent Neural\nNetworks where internal weights are fixed at random and only a linear output\nlayer is trained. In the large size limit, such random neural networks have a\ndeep connection with kernel methods. Our contributions are threefold: a) We\nrigorously establish the recurrent kernel limit of Reservoir Computing and\nprove its convergence. b) We test our models on chaotic time series prediction,\na classic but challenging benchmark in Reservoir Computing, and show how the\nRecurrent Kernel is competitive and computationally efficient when the number\nof data points remains moderate. c) When the number of samples is too large, we\nleverage the success of structured Random Features for kernel approximation by\nintroducing Structured Reservoir Computing. The two proposed methods, Recurrent\nKernel and Structured Reservoir Computing, turn out to be much faster and more\nmemory-efficient than conventional Reservoir Computing.",
        "categories": [
            "stat.ML",
            "cs.LG",
            "eess.SP"
        ],
        "link": "http://arxiv.org/pdf/2006.07310v2",
        "date": "2020-06-12 16:46:34+00:00"
    },
    {
        "title": "Quantum Ridgelet Transform: Winning Lottery Ticket of Neural Networks with Quantum Computation",
        "authors": [
            "Hayata Yamasaki",
            "Sathyawageeswar Subramanian",
            "Satoshi Hayakawa",
            "Sho Sonoda"
        ],
        "abstract": "Ridgelet transform has been a fundamental mathematical tool in the\ntheoretical studies of neural networks. However, the practical applicability of\nridgelet transform to conducting learning tasks was limited since its numerical\nimplementation by conventional classical computation requires an exponential\nruntime $\\exp(O(D))$ as data dimension $D$ increases. To address this problem,\nwe develop a quantum ridgelet transform (QRT), which implements the ridgelet\ntransform of a quantum state within a linear runtime $O(D)$ of quantum\ncomputation. As an application, we also show that one can use QRT as a\nfundamental subroutine for quantum machine learning (QML) to efficiently find a\nsparse trainable subnetwork of large shallow wide neural networks without\nconducting large-scale optimization of the original network. This application\ndiscovers an efficient way in this regime to demonstrate the lottery ticket\nhypothesis on finding such a sparse trainable neural network. These results\nopen an avenue of QML for accelerating learning tasks with commonly used\nclassical neural networks.",
        "categories": [
            "quant-ph",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2301.11936v1",
        "date": "2023-01-27 19:00:00+00:00"
    },
    {
        "title": "Domain Adaptation with Randomized Expectation Maximization",
        "authors": [
            "Twan van Laarhoven",
            "Elena Marchiori"
        ],
        "abstract": "Domain adaptation (DA) is the task of classifying an unlabeled dataset\n(target) using a labeled dataset (source) from a related domain. The majority\nof successful DA methods try to directly match the distributions of the source\nand target data by transforming the feature space. Despite their success, state\nof the art methods based on this approach are either involved or unable to\ndirectly scale to data with many features. This article shows that domain\nadaptation can be successfully performed by using a very simple randomized\nexpectation maximization (EM) method. We consider two instances of the method,\nwhich involve logistic regression and support vector machine, respectively. The\nunderlying assumption of the proposed method is the existence of a good single\nlinear classifier for both source and target domain. The potential limitations\nof this assumption are alleviated by the flexibility of the method, which can\ndirectly incorporate deep features extracted from a pre-trained deep neural\nnetwork. The resulting algorithm is strikingly easy to implement and apply. We\ntest its performance on 36 real-life adaptation tasks over text and image data\nwith diverse characteristics. The method achieves state-of-the-art results,\ncompetitive with those of involved end-to-end deep transfer-learning methods.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1803.07634v1",
        "date": "2018-03-20 20:13:09+00:00"
    },
    {
        "title": "Boltzmann Generators -- Sampling Equilibrium States of Many-Body Systems with Deep Learning",
        "authors": [
            "Frank No\u00e9",
            "Simon Olsson",
            "Jonas K\u00f6hler",
            "Hao Wu"
        ],
        "abstract": "Computing equilibrium states in condensed-matter many-body systems, such as\nsolvated proteins, is a long-standing challenge. Lacking methods for generating\nstatistically independent equilibrium samples in \"one shot\", vast computational\neffort is invested for simulating these system in small steps, e.g., using\nMolecular Dynamics. Combining deep learning and statistical mechanics, we here\ndevelop Boltzmann Generators, that are shown to generate unbiased one-shot\nequilibrium samples of representative condensed matter systems and proteins.\nBoltzmann Generators use neural networks to learn a coordinate transformation\nof the complex configurational equilibrium distribution to a distribution that\ncan be easily sampled. Accurate computation of free energy differences and\ndiscovery of new configurations are demonstrated, providing a statistical\nmechanics tool that can avoid rare events during sampling without prior\nknowledge of reaction coordinates.",
        "categories": [
            "stat.ML",
            "cond-mat.stat-mech",
            "cs.LG",
            "physics.chem-ph"
        ],
        "link": "http://arxiv.org/pdf/1812.01729v2",
        "date": "2018-12-04 22:41:33+00:00"
    },
    {
        "title": "On Feature Collapse and Deep Kernel Learning for Single Forward Pass Uncertainty",
        "authors": [
            "Joost van Amersfoort",
            "Lewis Smith",
            "Andrew Jesson",
            "Oscar Key",
            "Yarin Gal"
        ],
        "abstract": "Inducing point Gaussian process approximations are often considered a gold\nstandard in uncertainty estimation since they retain many of the properties of\nthe exact GP and scale to large datasets. A major drawback is that they have\ndifficulty scaling to high dimensional inputs. Deep Kernel Learning (DKL)\npromises a solution: a deep feature extractor transforms the inputs over which\nan inducing point Gaussian process is defined. However, DKL has been shown to\nprovide unreliable uncertainty estimates in practice. We study why, and show\nthat with no constraints, the DKL objective pushes \"far-away\" data points to be\nmapped to the same features as those of training-set points. With this insight\nwe propose to constrain DKL's feature extractor to approximately preserve\ndistances through a bi-Lipschitz constraint, resulting in a feature space\nfavorable to DKL. We obtain a model, DUE, which demonstrates uncertainty\nquality outperforming previous DKL and other single forward pass uncertainty\nmethods, while maintaining the speed and accuracy of standard neural networks.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2102.11409v3",
        "date": "2021-02-22 23:29:12+00:00"
    },
    {
        "title": "Initializing LSTM internal states via manifold learning",
        "authors": [
            "Felix P. Kemeth",
            "Tom Bertalan",
            "Nikolaos Evangelou",
            "Tianqi Cui",
            "Saurabh Malani",
            "Ioannis G. Kevrekidis"
        ],
        "abstract": "We present an approach, based on learning an intrinsic data manifold, for the\ninitialization of the internal state values of LSTM recurrent neural networks,\nensuring consistency with the initial observed input data. Exploiting the\ngeneralized synchronization concept, we argue that the converged, \"mature\"\ninternal states constitute a function on this learned manifold. The dimension\nof this manifold then dictates the length of observed input time series data\nrequired for consistent initialization. We illustrate our approach through a\npartially observed chemical model system, where initializing the internal LSTM\nstates in this fashion yields visibly improved performance. Finally, we show\nthat learning this data manifold enables the transformation of partially\nobserved dynamics into fully observed ones, facilitating alternative\nidentification paths for nonlinear dynamical systems.",
        "categories": [
            "stat.ML",
            "cs.AI",
            "cs.LG",
            "nlin.PS"
        ],
        "link": "http://arxiv.org/pdf/2104.13101v2",
        "date": "2021-04-27 10:54:53+00:00"
    },
    {
        "title": "Financial Time Series Forecasting using CNN and Transformer",
        "authors": [
            "Zhen Zeng",
            "Rachneet Kaur",
            "Suchetha Siddagangappa",
            "Saba Rahimi",
            "Tucker Balch",
            "Manuela Veloso"
        ],
        "abstract": "Time series forecasting is important across various domains for\ndecision-making. In particular, financial time series such as stock prices can\nbe hard to predict as it is difficult to model short-term and long-term\ntemporal dependencies between data points. Convolutional Neural Networks (CNN)\nare good at capturing local patterns for modeling short-term dependencies.\nHowever, CNNs cannot learn long-term dependencies due to the limited receptive\nfield. Transformers on the other hand are capable of learning global context\nand long-term dependencies. In this paper, we propose to harness the power of\nCNNs and Transformers to model both short-term and long-term dependencies\nwithin a time series, and forecast if the price would go up, down or remain the\nsame (flat) in the future. In our experiments, we demonstrated the success of\nthe proposed method in comparison to commonly adopted statistical and deep\nlearning methods on forecasting intraday stock price change of S&P 500\nconstituents.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "econ.EM",
            "q-fin.CP"
        ],
        "link": "http://arxiv.org/pdf/2304.04912v1",
        "date": "2023-04-11 00:56:57+00:00"
    },
    {
        "title": "Exploring Robust Architectures for Deep Artificial Neural Networks",
        "authors": [
            "Asim Waqas",
            "Ghulam Rasool",
            "Hamza Farooq",
            "Nidhal C. Bouaynaya"
        ],
        "abstract": "The architectures of deep artificial neural networks (DANNs) are routinely\nstudied to improve their predictive performance. However, the relationship\nbetween the architecture of a DANN and its robustness to noise and adversarial\nattacks is less explored. We investigate how the robustness of DANNs relates to\ntheir underlying graph architectures or structures. This study: (1) starts by\nexploring the design space of architectures of DANNs using graph-theoretic\nrobustness measures; (2) transforms the graphs to DANN architectures to\ntrain/validate/test on various image classification tasks; (3) explores the\nrelationship between the robustness of trained DANNs against noise and\nadversarial attacks and the robustness of their underlying architectures\nestimated via graph-theoretic measures. We show that the topological entropy\nand Olivier-Ricci curvature of the underlying graphs can quantify the\nrobustness performance of DANNs. The said relationship is stronger for complex\ntasks and large DANNs. Our work will allow autoML and neural architecture\nsearch community to explore design spaces of robust and accurate DANNs.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2106.15850v2",
        "date": "2021-06-30 07:12:19+00:00"
    },
    {
        "title": "Deep Transform and Metric Learning Networks",
        "authors": [
            "Wen Tang",
            "Emilie Chouzenoux",
            "Jean-Christophe Pesquet",
            "Hamid Krim"
        ],
        "abstract": "Based on its great successes in inference and denosing tasks, Dictionary\nLearning (DL) and its related sparse optimization formulations have garnered a\nlot of research interest. While most solutions have focused on single layer\ndictionaries, the recently improved Deep DL methods have also fallen short on a\nnumber of issues. We hence propose a novel Deep DL approach where each DL layer\ncan be formulated and solved as a combination of one linear layer and a\nRecurrent Neural Network, where the RNN is flexibly regraded as a\nlayer-associated learned metric. Our proposed work unveils new insights between\nthe Neural Networks and Deep DL, and provides a novel, efficient and\ncompetitive approach to jointly learn the deep transforms and metrics.\nExtensive experiments are carried out to demonstrate that the proposed method\ncan not only outperform existing Deep DL, but also state-of-the-art generic\nConvolutional Neural Networks.",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "link": "http://arxiv.org/pdf/2104.10329v1",
        "date": "2021-04-21 03:10:15+00:00"
    },
    {
        "title": "BUZz: BUffer Zones for defending adversarial examples in image classification",
        "authors": [
            "Kaleel Mahmood",
            "Phuong Ha Nguyen",
            "Lam M. Nguyen",
            "Thanh Nguyen",
            "Marten van Dijk"
        ],
        "abstract": "We propose a novel defense against all existing gradient based adversarial\nattacks on deep neural networks for image classification problems. Our defense\nis based on a combination of deep neural networks and simple image\ntransformations. While straightforward in implementation, this defense yields a\nunique security property which we term buffer zones. We argue that our defense\nbased on buffer zones offers significant improvements over state-of-the-art\ndefenses. We are able to achieve this improvement even when the adversary has\naccess to the {\\em entire} original training data set and unlimited query\naccess to the defense. We verify our claim through experimentation using\nFashion-MNIST and CIFAR-10: We demonstrate $<11\\%$ attack success rate --\nsignificantly lower than what other well-known state-of-the-art defenses offer\n-- at only a price of a $11-18\\%$ drop in clean accuracy. By using a new\nintuitive metric, we explain why this trade-off offers a significant\nimprovement over prior work.",
        "categories": [
            "cs.LG",
            "cs.CR",
            "eess.IV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1910.02785v2",
        "date": "2019-10-03 18:15:10+00:00"
    },
    {
        "title": "SFFDD: Deep Neural Network with Enriched Features for Failure Prediction with Its Application to Computer Disk Driver",
        "authors": [
            "Lanfa Frank Wang",
            "Danjue Li"
        ],
        "abstract": "A classification technique incorporating a novel feature derivation method is\nproposed for predicting failure of a system or device with multivariate time\nseries sensor data. We treat the multivariate time series sensor data as images\nfor both visualization and computation. Failure follows various patterns which\nare closely related to the root causes. Different predefined transformations\nare applied on the original sensors data to better characterize the failure\npatterns. In addition to feature derivation, ensemble method is used to further\nimprove the performance. In addition, a general algorithm architecture of deep\nneural network is proposed to handle multiple types of data with less manual\nfeature engineering. We apply the proposed method on the early predict failure\nof computer disk drive in order to improve storage systems availability and\navoid data loss. The classification accuracy is largely improved with the\nenriched features, named smart features.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2109.09856v1",
        "date": "2021-09-20 21:43:43+00:00"
    },
    {
        "title": "Deep adversarial neural decoding",
        "authors": [
            "Ya\u011fmur G\u00fc\u00e7l\u00fct\u00fcrk",
            "Umut G\u00fc\u00e7l\u00fc",
            "Katja Seeliger",
            "Sander Bosch",
            "Rob van Lier",
            "Marcel van Gerven"
        ],
        "abstract": "Here, we present a novel approach to solve the problem of reconstructing\nperceived stimuli from brain responses by combining probabilistic inference\nwith deep learning. Our approach first inverts the linear transformation from\nlatent features to brain responses with maximum a posteriori estimation and\nthen inverts the nonlinear transformation from perceived stimuli to latent\nfeatures with adversarial training of convolutional neural networks. We test\nour approach with a functional magnetic resonance imaging experiment and show\nthat it can generate state-of-the-art reconstructions of perceived faces from\nbrain activations.",
        "categories": [
            "q-bio.NC",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1705.07109v3",
        "date": "2017-05-19 17:43:01+00:00"
    },
    {
        "title": "Towards Verifying the Geometric Robustness of Large-scale Neural Networks",
        "authors": [
            "Fu Wang",
            "Peipei Xu",
            "Wenjie Ruan",
            "Xiaowei Huang"
        ],
        "abstract": "Deep neural networks (DNNs) are known to be vulnerable to adversarial\ngeometric transformation. This paper aims to verify the robustness of\nlarge-scale DNNs against the combination of multiple geometric transformations\nwith a provable guarantee. Given a set of transformations (e.g., rotation,\nscaling, etc.), we develop GeoRobust, a black-box robustness analyser built\nupon a novel global optimisation strategy, for locating the worst-case\ncombination of transformations that affect and even alter a network's output.\nGeoRobust can provide provable guarantees on finding the worst-case combination\nbased on recent advances in Lipschitzian theory. Due to its black-box nature,\nGeoRobust can be deployed on large-scale DNNs regardless of their\narchitectures, activation functions, and the number of neurons. In practice,\nGeoRobust can locate the worst-case geometric transformation with high\nprecision for the ResNet50 model on ImageNet in a few seconds on average. We\nexamined 18 ImageNet classifiers, including the ResNet family and vision\ntransformers, and found a positive correlation between the geometric robustness\nof the networks and the parameter numbers. We also observe that increasing the\ndepth of DNN is more beneficial than increasing its width in terms of improving\nits geometric robustness. Our tool GeoRobust is available at\nhttps://github.com/TrustAI/GeoRobust.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "link": "http://arxiv.org/pdf/2301.12456v2",
        "date": "2023-01-29 14:44:27+00:00"
    },
    {
        "title": "Multi-task Self-Supervised Learning for Human Activity Detection",
        "authors": [
            "Aaqib Saeed",
            "Tanir Ozcelebi",
            "Johan Lukkien"
        ],
        "abstract": "Deep learning methods are successfully used in applications pertaining to\nubiquitous computing, health, and well-being. Specifically, the area of human\nactivity recognition (HAR) is primarily transformed by the convolutional and\nrecurrent neural networks, thanks to their ability to learn semantic\nrepresentations from raw input. However, to extract generalizable features,\nmassive amounts of well-curated data are required, which is a notoriously\nchallenging task; hindered by privacy issues, and annotation costs. Therefore,\nunsupervised representation learning is of prime importance to leverage the\nvast amount of unlabeled data produced by smart devices. In this work, we\npropose a novel self-supervised technique for feature learning from sensory\ndata that does not require access to any form of semantic labels. We learn a\nmulti-task temporal convolutional network to recognize transformations applied\non an input signal. By exploiting these transformations, we demonstrate that\nsimple auxiliary tasks of the binary classification result in a strong\nsupervisory signal for extracting useful features for the downstream task. We\nextensively evaluate the proposed approach on several publicly available\ndatasets for smartphone-based HAR in unsupervised, semi-supervised, and\ntransfer learning settings. Our method achieves performance levels superior to\nor comparable with fully-supervised networks, and it performs significantly\nbetter than autoencoders. Notably, for the semi-supervised case, the\nself-supervised features substantially boost the detection rate by attaining a\nkappa score between 0.7-0.8 with only 10 labeled examples per class. We get\nsimilar impressive performance even if the features are transferred from a\ndifferent data source. While this paper focuses on HAR as the application\ndomain, the proposed technique is general and could be applied to a wide\nvariety of problems in other areas.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1907.11879v1",
        "date": "2019-07-27 09:14:43+00:00"
    },
    {
        "title": "Stein Variational Inference for Discrete Distributions",
        "authors": [
            "Jun Han",
            "Fan Ding",
            "Xianglong Liu",
            "Lorenzo Torresani",
            "Jian Peng",
            "Qiang Liu"
        ],
        "abstract": "Gradient-based approximate inference methods, such as Stein variational\ngradient descent (SVGD), provide simple and general-purpose inference engines\nfor differentiable continuous distributions. However, existing forms of SVGD\ncannot be directly applied to discrete distributions. In this work, we fill\nthis gap by proposing a simple yet general framework that transforms discrete\ndistributions to equivalent piecewise continuous distributions, on which the\ngradient-free SVGD is applied to perform efficient approximate inference. The\nempirical results show that our method outperforms traditional algorithms such\nas Gibbs sampling and discontinuous Hamiltonian Monte Carlo on various\nchallenging benchmarks of discrete graphical models. We demonstrate that our\nmethod provides a promising tool for learning ensembles of binarized neural\nnetwork (BNN), outperforming other widely used ensemble methods on learning\nbinarized AlexNet on CIFAR-10 dataset. In addition, such transform can be\nstraightforwardly employed in gradient-free kernelized Stein discrepancy to\nperform goodness-of-fit (GOF) test on discrete distributions. Our proposed\nmethod outperforms existing GOF test methods for intractable discrete\ndistributions.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2003.00605v1",
        "date": "2020-03-01 22:45:41+00:00"
    },
    {
        "title": "Analysis of Catastrophic Forgetting for Random Orthogonal Transformation Tasks in the Overparameterized Regime",
        "authors": [
            "Daniel Goldfarb",
            "Paul Hand"
        ],
        "abstract": "Overparameterization is known to permit strong generalization performance in\nneural networks. In this work, we provide an initial theoretical analysis of\nits effect on catastrophic forgetting in a continual learning setup. We show\nexperimentally that in permuted MNIST image classification tasks, the\ngeneralization performance of multilayer perceptrons trained by vanilla\nstochastic gradient descent can be improved by overparameterization, and the\nextent of the performance increase achieved by overparameterization is\ncomparable to that of state-of-the-art continual learning algorithms. We\nprovide a theoretical explanation of this effect by studying a qualitatively\nsimilar two-task linear regression problem, where each task is related by a\nrandom orthogonal transformation. We show that when a model is trained on the\ntwo tasks in sequence without any additional regularization, the risk gain on\nthe first task is small if the model is sufficiently overparameterized.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2207.06475v1",
        "date": "2022-06-01 18:04:33+00:00"
    },
    {
        "title": "Empowering Graph Representation Learning with Test-Time Graph Transformation",
        "authors": [
            "Wei Jin",
            "Tong Zhao",
            "Jiayuan Ding",
            "Yozen Liu",
            "Jiliang Tang",
            "Neil Shah"
        ],
        "abstract": "As powerful tools for representation learning on graphs, graph neural\nnetworks (GNNs) have facilitated various applications from drug discovery to\nrecommender systems. Nevertheless, the effectiveness of GNNs is immensely\nchallenged by issues related to data quality, such as distribution shift,\nabnormal features and adversarial attacks. Recent efforts have been made on\ntackling these issues from a modeling perspective which requires additional\ncost of changing model architectures or re-training model parameters. In this\nwork, we provide a data-centric view to tackle these issues and propose a graph\ntransformation framework named GTrans which adapts and refines graph data at\ntest time to achieve better performance. We provide theoretical analysis on the\ndesign of the framework and discuss why adapting graph data works better than\nadapting the model. Extensive experiments have demonstrated the effectiveness\nof GTrans on three distinct scenarios for eight benchmark datasets where\nsuboptimal data is presented. Remarkably, GTrans performs the best in most\ncases with improvements up to 2.8%, 8.2% and 3.8% over the best baselines on\nthree experimental settings. Code is released at\nhttps://github.com/ChandlerBang/GTrans.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2210.03561v2",
        "date": "2022-10-07 13:51:59+00:00"
    },
    {
        "title": "Hierarchical Architectures in Reservoir Computing Systems",
        "authors": [
            "John Moon",
            "Wei D. Lu"
        ],
        "abstract": "Reservoir computing (RC) offers efficient temporal data processing with a low\ntraining cost by separating recurrent neural networks into a fixed network with\nrecurrent connections and a trainable linear network. The quality of the fixed\nnetwork, called reservoir, is the most important factor that determines the\nperformance of the RC system. In this paper, we investigate the influence of\nthe hierarchical reservoir structure on the properties of the reservoir and the\nperformance of the RC system. Analogous to deep neural networks, stacking\nsub-reservoirs in series is an efficient way to enhance the nonlinearity of\ndata transformation to high-dimensional space and expand the diversity of\ntemporal information captured by the reservoir. These deep reservoir systems\noffer better performance when compared to simply increasing the size of the\nreservoir or the number of sub-reservoirs. Low frequency components are mainly\ncaptured by the sub-reservoirs in later stage of the deep reservoir structure,\nsimilar to observations that more abstract information can be extracted by\nlayers in the late stage of deep neural networks. When the total size of the\nreservoir is fixed, tradeoff between the number of sub-reservoirs and the size\nof each sub-reservoir needs to be carefully considered, due to the degraded\nability of individual sub-reservoirs at small sizes. Improved performance of\nthe deep reservoir structure alleviates the difficulty of implementing the RC\nsystem on hardware systems.",
        "categories": [
            "cs.ET",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2105.06923v1",
        "date": "2021-05-14 16:11:35+00:00"
    },
    {
        "title": "Transformers in Action Recognition: A Review on Temporal Modeling",
        "authors": [
            "Elham Shabaninia",
            "Hossein Nezamabadi-pour",
            "Fatemeh Shafizadegan"
        ],
        "abstract": "In vision-based action recognition, spatio-temporal features from different\nmodalities are used for recognizing activities. Temporal modeling is a long\nchallenge of action recognition. However, there are limited methods such as\npre-computed motion features, three-dimensional (3D) filters, and recurrent\nneural networks (RNN) for modeling motion information in deep-based approaches.\nRecently, transformers success in modeling long-range dependencies in natural\nlanguage processing (NLP) tasks has gotten great attention from other domains;\nincluding speech, image, and video, to rely entirely on self-attention without\nusing sequence-aligned RNNs or convolutions. Although the application of\ntransformers to action recognition is relatively new, the amount of research\nproposed on this topic within the last few years is astounding. This paper\nespecially reviews recent progress in deep learning methods for modeling\ntemporal variations. It focuses on action recognition methods that use\ntransformers for temporal modeling, discussing their main features, used\nmodalities, and identifying opportunities and challenges for future research.",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2302.01921v1",
        "date": "2022-12-29 11:03:19+00:00"
    },
    {
        "title": "Deep and interpretable regression models for ordinal outcomes",
        "authors": [
            "Lucas Kook",
            "Lisa Herzog",
            "Torsten Hothorn",
            "Oliver D\u00fcrr",
            "Beate Sick"
        ],
        "abstract": "Outcomes with a natural order commonly occur in prediction tasks and often\nthe available input data are a mixture of complex data like images and tabular\npredictors. Deep Learning (DL) models are state-of-the-art for image\nclassification tasks but frequently treat ordinal outcomes as unordered and\nlack interpretability. In contrast, classical ordinal regression models\nconsider the outcome's order and yield interpretable predictor effects but are\nlimited to tabular data. We present ordinal neural network transformation\nmodels (ONTRAMs), which unite DL with classical ordinal regression approaches.\nONTRAMs are a special case of transformation models and trade off flexibility\nand interpretability by additively decomposing the transformation function into\nterms for image and tabular data using jointly trained neural networks. The\nperformance of the most flexible ONTRAM is by definition equivalent to a\nstandard multi-class DL model trained with cross-entropy while being faster in\ntraining when facing ordinal outcomes. Lastly, we discuss how to interpret\nmodel components for both tabular and image data on two publicly available\ndatasets.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2010.08376v4",
        "date": "2020-10-16 13:27:34+00:00"
    },
    {
        "title": "DeepEvolution: A Search-Based Testing Approach for Deep Neural Networks",
        "authors": [
            "Houssem Ben Braiek",
            "Foutse khomh"
        ],
        "abstract": "The increasing inclusion of Deep Learning (DL) models in safety-critical\nsystems such as autonomous vehicles have led to the development of multiple\nmodel-based DL testing techniques. One common denominator of these testing\ntechniques is the automated generation of test cases, e.g., new inputs\ntransformed from the original training data with the aim to optimize some test\nadequacy criteria. So far, the effectiveness of these approaches has been\nhindered by their reliance on random fuzzing or transformations that do not\nalways produce test cases with a good diversity. To overcome these limitations,\nwe propose, DeepEvolution, a novel search-based approach for testing DL models\nthat relies on metaheuristics to ensure a maximum diversity in generated test\ncases. We assess the effectiveness of DeepEvolution in testing computer-vision\nDL models and found that it significantly increases the neuronal coverage of\ngenerated test cases. Moreover, using DeepEvolution, we could successfully find\nseveral corner-case behaviors. Finally, DeepEvolution outperformed Tensorfuzz\n(a coverage-guided fuzzing tool developed at Google Brain) in detecting latent\ndefects introduced during the quantization of the models. These results suggest\nthat search-based approaches can help build effective testing tools for DL\nsystems.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1909.02563v1",
        "date": "2019-09-05 13:42:08+00:00"
    },
    {
        "title": "Co-domain Symmetry for Complex-Valued Deep Learning",
        "authors": [
            "Utkarsh Singhal",
            "Yifei Xing",
            "Stella X. Yu"
        ],
        "abstract": "We study complex-valued scaling as a type of symmetry natural and unique to\ncomplex-valued measurements and representations. Deep Complex Networks (DCN)\nextends real-valued algebra to the complex domain without addressing\ncomplex-valued scaling. SurReal takes a restrictive manifold view of complex\nnumbers, adopting a distance metric to achieve complex-scaling invariance while\nlosing rich complex-valued information. We analyze complex-valued scaling as a\nco-domain transformation and design novel equivariant and invariant neural\nnetwork layer functions for this special transformation. We also propose novel\ncomplex-valued representations of RGB images, where complex-valued scaling\nindicates hue shift or correlated changes across color channels. Benchmarked on\nMSTAR, CIFAR10, CIFAR100, and SVHN, our co-domain symmetric (CDS) classifiers\ndeliver higher accuracy, better generalization, robustness to co-domain\ntransformations, and lower model bias and variance than DCN and SurReal with\nfar fewer parameters.",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2112.01525v1",
        "date": "2021-12-02 18:59:56+00:00"
    },
    {
        "title": "Ghost Units Yield Biologically Plausible Backprop in Deep Neural Networks",
        "authors": [
            "Thomas Mesnard",
            "Gaetan Vignoud",
            "Joao Sacramento",
            "Walter Senn",
            "Yoshua Bengio"
        ],
        "abstract": "In the past few years, deep learning has transformed artificial intelligence\nresearch and led to impressive performance in various difficult tasks. However,\nit is still unclear how the brain can perform credit assignment across many\nareas as efficiently as backpropagation does in deep neural networks. In this\npaper, we introduce a model that relies on a new role for a neuronal inhibitory\nmachinery, referred to as ghost units. By cancelling the feedback coming from\nthe upper layer when no target signal is provided to the top layer, the ghost\nunits enables the network to backpropagate errors and do efficient credit\nassignment in deep structures. While considering one-compartment neurons and\nrequiring very few biological assumptions, it is able to approximate the error\ngradient and achieve good performance on classification tasks. Error\nbackpropagation occurs through the recurrent dynamics of the network and thanks\nto biologically plausible local learning rules. In particular, it does not\nrequire separate feedforward and feedback circuits. Different mechanisms for\ncancelling the feedback were studied, ranging from complete duplication of the\nconnectivity by long term processes to online replication of the feedback\nactivity. This reduced system combines the essential elements to have a working\nbiologically abstracted analogue of backpropagation with a simple formulation\nand proofs of the associated results. Therefore, this model is a step towards\nunderstanding how learning and memory are implemented in cortical multilayer\nstructures, but it also raises interesting perspectives for neuromorphic\nhardware.",
        "categories": [
            "q-bio.NC",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1911.08585v1",
        "date": "2019-11-15 17:47:00+00:00"
    },
    {
        "title": "Translating Math Formula Images to LaTeX Sequences Using Deep Neural Networks with Sequence-level Training",
        "authors": [
            "Zelun Wang",
            "Jyh-Charn Liu"
        ],
        "abstract": "In this paper we propose a deep neural network model with an encoder-decoder\narchitecture that translates images of math formulas into their LaTeX markup\nsequences. The encoder is a convolutional neural network (CNN) that transforms\nimages into a group of feature maps. To better capture the spatial\nrelationships of math symbols, the feature maps are augmented with 2D\npositional encoding before being unfolded into a vector. The decoder is a\nstacked bidirectional long short-term memory (LSTM) model integrated with the\nsoft attention mechanism, which works as a language model to translate the\nencoder output into a sequence of LaTeX tokens. The neural network is trained\nin two steps. The first step is token-level training using the\nMaximum-Likelihood Estimation (MLE) as the objective function. At completion of\nthe token-level training, the sequence-level training objective function is\nemployed to optimize the overall model based on the policy gradient algorithm\nfrom reinforcement learning. Our design also overcomes the exposure bias\nproblem by closing the feedback loop in the decoder during sequence-level\ntraining, i.e., feeding in the predicted token instead of the ground truth\ntoken at every time step. The model is trained and evaluated on the\nIM2LATEX-100K dataset and shows state-of-the-art performance on both\nsequence-based and image-based evaluation metrics.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1908.11415v2",
        "date": "2019-08-29 18:33:21+00:00"
    },
    {
        "title": "Bypass Exponential Time Preprocessing: Fast Neural Network Training via Weight-Data Correlation Preprocessing",
        "authors": [
            "Josh Alman",
            "Jiehao Liang",
            "Zhao Song",
            "Ruizhe Zhang",
            "Danyang Zhuo"
        ],
        "abstract": "Over the last decade, deep neural networks have transformed our society, and\nthey are already widely applied in various machine learning applications.\nState-of-art deep neural networks are becoming larger in size every year to\ndeliver increasing model accuracy, and as a result, model training consumes\nsubstantial computing resources and will only consume more in the future. Using\ncurrent training methods, in each iteration, to process a data point $x \\in\n\\mathbb{R}^d$ in a layer, we need to spend $\\Theta(md)$ time to evaluate all\nthe $m$ neurons in the layer. This means processing the entire layer takes\n$\\Theta(nmd)$ time for $n$ data points. Recent work [Song, Yang and Zhang,\nNeurIPS 2021] reduces this time per iteration to $o(nmd)$, but requires\nexponential time to preprocess either the data or the neural network weights,\nmaking it unlikely to have practical usage.\n  In this work, we present a new preprocessing method that simply stores the\nweight-data correlation in a tree data structure in order to quickly,\ndynamically detect which neurons fire at each iteration. Our method requires\nonly $O(nmd)$ time in preprocessing and still achieves $o(nmd)$ time per\niteration. We complement our new algorithm with a lower bound, proving that\nassuming a popular conjecture from complexity theory, one could not\nsubstantially speed up our algorithm for dynamic detection of firing neurons.",
        "categories": [
            "cs.LG",
            "cs.DS",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2211.14227v1",
        "date": "2022-11-25 16:40:49+00:00"
    },
    {
        "title": "Spectral Representations for Convolutional Neural Networks",
        "authors": [
            "Oren Rippel",
            "Jasper Snoek",
            "Ryan P. Adams"
        ],
        "abstract": "Discrete Fourier transforms provide a significant speedup in the computation\nof convolutions in deep learning. In this work, we demonstrate that, beyond its\nadvantages for efficient computation, the spectral domain also provides a\npowerful representation in which to model and train convolutional neural\nnetworks (CNNs).\n  We employ spectral representations to introduce a number of innovations to\nCNN design. First, we propose spectral pooling, which performs dimensionality\nreduction by truncating the representation in the frequency domain. This\napproach preserves considerably more information per parameter than other\npooling strategies and enables flexibility in the choice of pooling output\ndimensionality. This representation also enables a new form of stochastic\nregularization by randomized modification of resolution. We show that these\nmethods achieve competitive results on classification and approximation tasks,\nwithout using any dropout or max-pooling.\n  Finally, we demonstrate the effectiveness of complex-coefficient spectral\nparameterization of convolutional filters. While this leaves the underlying\nmodel unchanged, it results in a representation that greatly facilitates\noptimization. We observe on a variety of popular CNN configurations that this\nleads to significantly faster convergence during training.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1506.03767v1",
        "date": "2015-06-11 18:23:18+00:00"
    },
    {
        "title": "A Group-Theoretic Framework for Data Augmentation",
        "authors": [
            "Shuxiao Chen",
            "Edgar Dobriban",
            "Jane H Lee"
        ],
        "abstract": "Data augmentation is a widely used trick when training deep neural networks:\nin addition to the original data, properly transformed data are also added to\nthe training set. However, to the best of our knowledge, a clear mathematical\nframework to explain the performance benefits of data augmentation is not\navailable. In this paper, we develop such a theoretical framework. We show data\naugmentation is equivalent to an averaging operation over the orbits of a\ncertain group that keeps the data distribution approximately invariant. We\nprove that it leads to variance reduction. We study empirical risk\nminimization, and the examples of exponential families, linear regression, and\ncertain two-layer neural networks. We also discuss how data augmentation could\nbe used in problems with symmetry where other approaches are prevalent, such as\nin cryo-electron microscopy (cryo-EM).",
        "categories": [
            "stat.ML",
            "cs.LG",
            "math.ST",
            "stat.TH"
        ],
        "link": "http://arxiv.org/pdf/1907.10905v4",
        "date": "2019-07-25 08:58:59+00:00"
    },
    {
        "title": "Principal Component Networks: Parameter Reduction Early in Training",
        "authors": [
            "Roger Waleffe",
            "Theodoros Rekatsinas"
        ],
        "abstract": "Recent works show that overparameterized networks contain small subnetworks\nthat exhibit comparable accuracy to the full model when trained in isolation.\nThese results highlight the potential to reduce training costs of deep neural\nnetworks without sacrificing generalization performance. However, existing\napproaches for finding these small networks rely on expensive multi-round\ntrain-and-prune procedures and are non-practical for large data sets and\nmodels. In this paper, we show how to find small networks that exhibit the same\nperformance as their overparameterized counterparts after only a few training\nepochs. We find that hidden layer activations in overparameterized networks\nexist primarily in subspaces smaller than the actual model width. Building on\nthis observation, we use PCA to find a basis of high variance for layer inputs\nand represent layer weights using these directions. We eliminate all weights\nnot relevant to the found PCA basis and term these network architectures\nPrincipal Component Networks. On CIFAR-10 and ImageNet, we show that PCNs train\nfaster and use less energy than overparameterized models, without accuracy\nloss. We find that our transformation leads to networks with up to 23.8x fewer\nparameters, with equal or higher end-model accuracy---in some cases we observe\nimprovements up to 3%. We also show that ResNet-20 PCNs outperform deep\nResNet-110 networks while training faster.",
        "categories": [
            "cs.LG",
            "cs.NE",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2006.13347v1",
        "date": "2020-06-23 21:40:24+00:00"
    },
    {
        "title": "SpotFast Networks with Memory Augmented Lateral Transformers for Lipreading",
        "authors": [
            "Peratham Wiriyathammabhum"
        ],
        "abstract": "This paper presents a novel deep learning architecture for word-level\nlipreading. Previous works suggest a potential for incorporating a pretrained\ndeep 3D Convolutional Neural Networks as a front-end feature extractor. We\nintroduce a SpotFast networks, a variant of the state-of-the-art SlowFast\nnetworks for action recognition, which utilizes a temporal window as a spot\npathway and all frames as a fast pathway. We further incorporate memory\naugmented lateral transformers to learn sequential features for classification.\nWe evaluate the proposed model on the LRW dataset. The experiments show that\nour proposed model outperforms various state-of-the-art models and\nincorporating the memory augmented lateral transformers makes a 3.7%\nimprovement to the SpotFast networks.",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2005.10903v1",
        "date": "2020-05-21 21:04:12+00:00"
    },
    {
        "title": "Context-sensitive neocortical neurons transform the effectiveness and efficiency of neural information processing",
        "authors": [
            "Ahsan Adeel",
            "Mario Franco",
            "Mohsin Raza",
            "Khubaib Ahmed"
        ],
        "abstract": "Deep learning (DL) has big-data processing capabilities that are as good, or\neven better, than those of humans in many real-world domains, but at the cost\nof high energy requirements that may be unsustainable in some applications and\nof errors, that, though infrequent, can be large. We hypothesise that a\nfundamental weakness of DL lies in its intrinsic dependence on\nintegrate-and-fire point neurons that maximise information transmission\nirrespective of whether it is relevant in the current context or not. This\nleads to unnecessary neural firing and to the feedforward transmission of\nconflicting messages, which makes learning difficult and processing energy\ninefficient. Here we show how to circumvent these limitations by mimicking the\ncapabilities of context-sensitive neocortical neurons that receive input from\ndiverse sources as a context to amplify and attenuate the transmission of\nrelevant and irrelevant information, respectively. We demonstrate that a deep\nnetwork composed of such local processors seeks to maximise agreement between\nthe active neurons, thus restricting the transmission of conflicting\ninformation to higher levels and reducing the neural activity required to\nprocess large amounts of heterogeneous real-world data. As shown to be far more\neffective and efficient than current forms of DL, this two-point neuron study\noffers a possible step-change in transforming the cellular foundations of deep\nnetwork architectures.",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2207.07338v6",
        "date": "2022-07-15 08:30:33+00:00"
    },
    {
        "title": "Scaling Laws Beyond Backpropagation",
        "authors": [
            "Matthew J. Filipovich",
            "Alessandro Cappelli",
            "Daniel Hesslow",
            "Julien Launay"
        ],
        "abstract": "Alternatives to backpropagation have long been studied to better understand\nhow biological brains may learn. Recently, they have also garnered interest as\na way to train neural networks more efficiently. By relaxing constraints\ninherent to backpropagation (e.g., symmetric feedforward and feedback weights,\nsequential updates), these methods enable promising prospects, such as local\nlearning. However, the tradeoffs between different methods in terms of final\ntask performance, convergence speed, and ultimately compute and data\nrequirements are rarely outlined. In this work, we use scaling laws to study\nthe ability of Direct Feedback Alignment~(DFA) to train causal decoder-only\nTransformers efficiently. Scaling laws provide an overview of the tradeoffs\nimplied by a modeling decision, up to extrapolating how it might transfer to\nincreasingly large models. We find that DFA fails to offer more efficient\nscaling than backpropagation: there is never a regime for which the degradation\nin loss incurred by using DFA is worth the potential reduction in compute\nbudget. Our finding comes at variance with previous beliefs in the alternative\ntraining methods community, and highlights the need for holistic empirical\napproaches to better understand modeling decisions.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.NE",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2210.14593v1",
        "date": "2022-10-26 10:09:14+00:00"
    },
    {
        "title": "Haar Graph Pooling",
        "authors": [
            "Yu Guang Wang",
            "Ming Li",
            "Zheng Ma",
            "Guido Montufar",
            "Xiaosheng Zhuang",
            "Yanan Fan"
        ],
        "abstract": "Deep Graph Neural Networks (GNNs) are useful models for graph classification\nand graph-based regression tasks. In these tasks, graph pooling is a critical\ningredient by which GNNs adapt to input graphs of varying size and structure.\nWe propose a new graph pooling operation based on compressive Haar transforms\n-- HaarPooling. HaarPooling implements a cascade of pooling operations; it is\ncomputed by following a sequence of clusterings of the input graph. A\nHaarPooling layer transforms a given input graph to an output graph with a\nsmaller node number and the same feature dimension; the compressive Haar\ntransform filters out fine detail information in the Haar wavelet domain. In\nthis way, all the HaarPooling layers together synthesize the features of any\ngiven input graph into a feature vector of uniform size. Such transforms\nprovide a sparse characterization of the data and preserve the structure\ninformation of the input graph. GNNs implemented with standard graph\nconvolution layers and HaarPooling layers achieve state of the art performance\non diverse graph classification and regression problems.",
        "categories": [
            "cs.LG",
            "stat.ML",
            "42C40, 05C85, 11Y16",
            "I.2.4; I.2.6"
        ],
        "link": "http://arxiv.org/pdf/1909.11580v3",
        "date": "2019-09-25 16:13:54+00:00"
    },
    {
        "title": "Fast object detection in compressed JPEG Images",
        "authors": [
            "Benjamin Deguerre",
            "Cl\u00e9ment Chatelain",
            "Gilles Gasso"
        ],
        "abstract": "Object detection in still images has drawn a lot of attention over past few\nyears, and with the advent of Deep Learning impressive performances have been\nachieved with numerous industrial applications. Most of these deep learning\nmodels rely on RGB images to localize and identify objects in the image.\nHowever in some application scenarii, images are compressed either for storage\nsavings or fast transmission. Therefore a time consuming image decompression\nstep is compulsory in order to apply the aforementioned deep models. To\nalleviate this drawback, we propose a fast deep architecture for object\ndetection in JPEG images, one of the most widespread compression format. We\ntrain a neural network to detect objects based on the blockwise DCT (discrete\ncosine transform) coefficients {issued from} the JPEG compression algorithm. We\nmodify the well-known Single Shot multibox Detector (SSD) by replacing its\nfirst layers with one convolutional layer dedicated to process the DCT inputs.\nExperimental evaluations on PASCAL VOC and industrial dataset comprising images\nof road traffic surveillance show that the model is about $2\\times$ faster than\nregular SSD with promising detection performances. To the best of our\nknowledge, this paper is the first to address detection in compressed JPEG\nimages.",
        "categories": [
            "cs.CV",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1904.08408v1",
        "date": "2019-04-16 22:10:53+00:00"
    },
    {
        "title": "Improving Deep Learning Interpretability by Saliency Guided Training",
        "authors": [
            "Aya Abdelsalam Ismail",
            "H\u00e9ctor Corrada Bravo",
            "Soheil Feizi"
        ],
        "abstract": "Saliency methods have been widely used to highlight important input features\nin model predictions. Most existing methods use backpropagation on a modified\ngradient function to generate saliency maps. Thus, noisy gradients can result\nin unfaithful feature attributions. In this paper, we tackle this issue and\nintroduce a {\\it saliency guided training}procedure for neural networks to\nreduce noisy gradients used in predictions while retaining the predictive\nperformance of the model. Our saliency guided training procedure iteratively\nmasks features with small and potentially noisy gradients while maximizing the\nsimilarity of model outputs for both masked and unmasked inputs. We apply the\nsaliency guided training procedure to various synthetic and real data sets from\ncomputer vision, natural language processing, and time series across diverse\nneural architectures, including Recurrent Neural Networks, Convolutional\nNetworks, and Transformers. Through qualitative and quantitative evaluations,\nwe show that saliency guided training procedure significantly improves model\ninterpretability across various domains while preserving its predictive\nperformance.",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2111.14338v1",
        "date": "2021-11-29 06:05:23+00:00"
    },
    {
        "title": "Group Invariance, Stability to Deformations, and Complexity of Deep Convolutional Representations",
        "authors": [
            "Alberto Bietti",
            "Julien Mairal"
        ],
        "abstract": "The success of deep convolutional architectures is often attributed in part\nto their ability to learn multiscale and invariant representations of natural\nsignals. However, a precise study of these properties and how they affect\nlearning guarantees is still missing. In this paper, we consider deep\nconvolutional representations of signals; we study their invariance to\ntranslations and to more general groups of transformations, their stability to\nthe action of diffeomorphisms, and their ability to preserve signal\ninformation. This analysis is carried by introducing a multilayer kernel based\non convolutional kernel networks and by studying the geometry induced by the\nkernel mapping. We then characterize the corresponding reproducing kernel\nHilbert space (RKHS), showing that it contains a large class of convolutional\nneural networks with homogeneous activation functions. This analysis allows us\nto separate data representation from learning, and to provide a canonical\nmeasure of model complexity, the RKHS norm, which controls both stability and\ngeneralization of any learned model. In addition to models in the constructed\nRKHS, our stability analysis also applies to convolutional networks with\ngeneric activations such as rectified linear units, and we discuss its\nrelationship with recent generalization bounds based on spectral norms.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1706.03078v4",
        "date": "2017-06-09 18:02:47+00:00"
    },
    {
        "title": "ReDMark: Framework for Residual Diffusion Watermarking on Deep Networks",
        "authors": [
            "Mahdi Ahmadi",
            "Alireza Norouzi",
            "S. M. Reza Soroushmehr",
            "Nader Karimi",
            "Kayvan Najarian",
            "Shadrokh Samavi",
            "Ali Emami"
        ],
        "abstract": "Due to the rapid growth of machine learning tools and specifically deep\nnetworks in various computer vision and image processing areas, application of\nConvolutional Neural Networks for watermarking have recently emerged. In this\npaper, we propose a deep end-to-end diffusion watermarking framework (ReDMark)\nwhich can be adapted for any desired transform space. The framework is composed\nof two Fully Convolutional Neural Networks with the residual structure for\nembedding and extraction. The whole deep network is trained end-to-end to\nconduct a blind secure watermarking. The framework is customizable for the\nlevel of robustness vs. imperceptibility. It is also adjustable for the\ntrade-off between capacity and robustness. The proposed framework simulates\nvarious attacks as a differentiable network layer to facilitate end-to-end\ntraining. For JPEG attack, a differentiable approximation is utilized, which\ndrastically improves the watermarking robustness to this attack. Another\nimportant characteristic of the proposed framework, which leads to improved\nsecurity and robustness, is its capability to diffuse watermark information\namong a relatively wide area of the image. Comparative results versus recent\nstate-of-the-art researches highlight the superiority of the proposed framework\nin terms of imperceptibility and robustness.",
        "categories": [
            "cs.MM",
            "cs.CR",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1810.07248v3",
        "date": "2018-10-16 23:07:15+00:00"
    },
    {
        "title": "Tree Decomposed Graph Neural Network",
        "authors": [
            "Yu Wang",
            "Tyler Derr"
        ],
        "abstract": "Graph Neural Networks (GNNs) have achieved significant success in learning\nbetter representations by performing feature propagation and transformation\niteratively to leverage neighborhood information. Nevertheless, iterative\npropagation restricts the information of higher-layer neighborhoods to be\ntransported through and fused with the lower-layer neighborhoods', which\nunavoidably results in feature smoothing between neighborhoods in different\nlayers and can thus compromise the performance, especially on heterophily\nnetworks. Furthermore, most deep GNNs only recognize the importance of\nhigher-layer neighborhoods while yet to fully explore the importance of\nmulti-hop dependency within the context of different layer neighborhoods in\nlearning better representations. In this work, we first theoretically analyze\nthe feature smoothing between neighborhoods in different layers and empirically\ndemonstrate the variance of the homophily level across neighborhoods at\ndifferent layers. Motivated by these analyses, we further propose a tree\ndecomposition method to disentangle neighborhoods in different layers to\nalleviate feature smoothing among these layers. Moreover, we characterize the\nmulti-hop dependency via graph diffusion within our tree decomposition\nformulation to construct Tree Decomposed Graph Neural Network (TDGNN), which\ncan flexibly incorporate information from large receptive fields and aggregate\nthis information utilizing the multi-hop dependency. Comprehensive experiments\ndemonstrate the superior performance of TDGNN on both homophily and heterophily\nnetworks under a variety of node classification settings. Extensive parameter\nanalysis highlights the ability of TDGNN to prevent over-smoothing and\nincorporate features from shallow layers with deeper multi-hop dependencies,\nwhich provides new insights towards deeper graph neural networks. Code of\nTDGNN: http://github.com/YuWVandy/TDGNN",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.SI"
        ],
        "link": "http://arxiv.org/pdf/2108.11022v1",
        "date": "2021-08-25 02:47:16+00:00"
    },
    {
        "title": "Image Captioning using Deep Stacked LSTMs, Contextual Word Embeddings and Data Augmentation",
        "authors": [
            "Sulabh Katiyar",
            "Samir Kumar Borgohain"
        ],
        "abstract": "Image Captioning, or the automatic generation of descriptions for images, is\none of the core problems in Computer Vision and has seen considerable progress\nusing Deep Learning Techniques. We propose to use Inception-ResNet\nConvolutional Neural Network as encoder to extract features from images,\nHierarchical Context based Word Embeddings for word representations and a Deep\nStacked Long Short Term Memory network as decoder, in addition to using Image\nData Augmentation to avoid over-fitting. For data Augmentation, we use\nHorizontal and Vertical Flipping in addition to Perspective Transformations on\nthe images. We evaluate our proposed methods with two image captioning\nframeworks- Encoder-Decoder and Soft Attention. Evaluation on widely used\nmetrics have shown that our approach leads to considerable improvement in model\nperformance.",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.MM",
            "cs.NE"
        ],
        "link": "http://arxiv.org/pdf/2102.11237v1",
        "date": "2021-02-22 18:15:39+00:00"
    },
    {
        "title": "Hexagonal Image Processing in the Context of Machine Learning: Conception of a Biologically Inspired Hexagonal Deep Learning Framework",
        "authors": [
            "Tobias Schlosser",
            "Michael Friedrich",
            "Danny Kowerko"
        ],
        "abstract": "Inspired by the human visual perception system, hexagonal image processing in\nthe context of machine learning deals with the development of image processing\nsystems that combine the advantages of evolutionary motivated structures based\non biological models. While conventional state-of-the-art image processing\nsystems of recording and output devices almost exclusively utilize square\narranged methods, their hexagonal counterparts offer a number of key advantages\nthat can benefit both researchers and users. This contribution serves as a\ngeneral application-oriented approach the synthesis of the therefore designed\nhexagonal image processing framework, called Hexnet, the processing steps of\nhexagonal image transformation, and dependent methods. The results of our\ncreated test environment show that the realized framework surpasses current\napproaches of hexagonal image processing systems, while hexagonal artificial\nneural networks can benefit from the implemented hexagonal architecture. As\nhexagonal lattice format based deep neural networks, also called H-DNN, can be\ncompared to their square counterparts by transforming classical square lattice\nbased data sets into their hexagonal representation, they can also result in a\nreduction of trainable parameters as well as result in increased training and\ntest rates.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "eess.IV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1911.11251v7",
        "date": "2019-11-25 21:58:31+00:00"
    },
    {
        "title": "GTrans: Spatiotemporal Autoregressive Transformer with Graph Embeddings for Nowcasting Extreme Events",
        "authors": [
            "Bo Feng",
            "Geoffrey Fox"
        ],
        "abstract": "Spatiotemporal time series nowcasting should preserve temporal and spatial\ndynamics in the sense that generated new sequences from models respect the\ncovariance relationship from history. Conventional feature extractors are built\nwith deep convolutional neural networks (CNN). However, CNN models have limits\nto image-like applications where data can be formed with high-dimensional\narrays. In contrast, applications in social networks, road traffic, physics,\nand chemical property prediction where data features can be organized with\nnodes and edges of graphs. Transformer architecture is an emerging method for\npredictive models, bringing high accuracy and efficiency due to attention\nmechanism design. This paper proposes a spatiotemporal model, namely GTrans,\nthat transforms data features into graph embeddings and predicts temporal\ndynamics with a transformer model. According to our experiments, we demonstrate\nthat GTrans can model spatial and temporal dynamics and nowcasts extreme events\nfor datasets. Furthermore, in all the experiments, GTrans can achieve the\nhighest F1 and F2 scores in binary-class prediction tests than the baseline\nmodels.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2201.06717v1",
        "date": "2022-01-18 03:26:24+00:00"
    },
    {
        "title": "Comparative evaluation of CNN architectures for Image Caption Generation",
        "authors": [
            "Sulabh Katiyar",
            "Samir Kumar Borgohain"
        ],
        "abstract": "Aided by recent advances in Deep Learning, Image Caption Generation has seen\ntremendous progress over the last few years. Most methods use transfer learning\nto extract visual information, in the form of image features, with the help of\npre-trained Convolutional Neural Network models followed by transformation of\nthe visual information using a Caption Generator module to generate the output\nsentences. Different methods have used different Convolutional Neural Network\nArchitectures and, to the best of our knowledge, there is no systematic study\nwhich compares the relative efficacy of different Convolutional Neural Network\narchitectures for extracting the visual information. In this work, we have\nevaluated 17 different Convolutional Neural Networks on two popular Image\nCaption Generation frameworks: the first based on Neural Image Caption (NIC)\ngeneration model and the second based on Soft-Attention framework. We observe\nthat model complexity of Convolutional Neural Network, as measured by number of\nparameters, and the accuracy of the model on Object Recognition task does not\nnecessarily co-relate with its efficacy on feature extraction for Image Caption\nGeneration task.",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.MM",
            "cs.NE"
        ],
        "link": "http://arxiv.org/pdf/2102.11506v1",
        "date": "2021-02-23 05:43:54+00:00"
    },
    {
        "title": "Scalable, Axiomatic Explanations of Deep Alzheimer's Diagnosis from Heterogeneous Data",
        "authors": [
            "Sebastian P\u00f6lsterl",
            "Christina Aigner",
            "Christian Wachinger"
        ],
        "abstract": "Deep Neural Networks (DNNs) have an enormous potential to learn from complex\nbiomedical data. In particular, DNNs have been used to seamlessly fuse\nheterogeneous information from neuroanatomy, genetics, biomarkers, and\nneuropsychological tests for highly accurate Alzheimer's disease diagnosis. On\nthe other hand, their black-box nature is still a barrier for the adoption of\nsuch a system in the clinic, where interpretability is absolutely essential. We\npropose Shapley Value Explanation of Heterogeneous Neural Networks (SVEHNN) for\nexplaining the Alzheimer's diagnosis made by a DNN from the 3D point cloud of\nthe neuroanatomy and tabular biomarkers. Our explanations are based on the\nShapley value, which is the unique method that satisfies all fundamental axioms\nfor local explanations previously established in the literature. Thus, SVEHNN\nhas many desirable characteristics that previous work on interpretability for\nmedical decision making is lacking. To avoid the exponential time complexity of\nthe Shapley value, we propose to transform a given DNN into a Lightweight\nProbabilistic Deep Network without re-training, thus achieving a complexity\nonly quadratic in the number of features. In our experiments on synthetic and\nreal data, we show that we can closely approximate the exact Shapley value with\na dramatically reduced runtime and can reveal the hidden knowledge the network\nhas learned from the data.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2107.05997v2",
        "date": "2021-07-13 11:25:54+00:00"
    },
    {
        "title": "Your Transformer May Not be as Powerful as You Expect",
        "authors": [
            "Shengjie Luo",
            "Shanda Li",
            "Shuxin Zheng",
            "Tie-Yan Liu",
            "Liwei Wang",
            "Di He"
        ],
        "abstract": "Relative Positional Encoding (RPE), which encodes the relative distance\nbetween any pair of tokens, is one of the most successful modifications to the\noriginal Transformer. As far as we know, theoretical understanding of the\nRPE-based Transformers is largely unexplored. In this work, we mathematically\nanalyze the power of RPE-based Transformers regarding whether the model is\ncapable of approximating any continuous sequence-to-sequence functions. One may\nnaturally assume the answer is in the affirmative -- RPE-based Transformers are\nuniversal function approximators. However, we present a negative result by\nshowing there exist continuous sequence-to-sequence functions that RPE-based\nTransformers cannot approximate no matter how deep and wide the neural network\nis. One key reason lies in that most RPEs are placed in the softmax attention\nthat always generates a right stochastic matrix. This restricts the network\nfrom capturing positional information in the RPEs and limits its capacity. To\novercome the problem and make the model more powerful, we first present\nsufficient conditions for RPE-based Transformers to achieve universal function\napproximation. With the theoretical guidance, we develop a novel attention\nmodule, called Universal RPE-based (URPE) Attention, which satisfies the\nconditions. Therefore, the corresponding URPE-based Transformers become\nuniversal function approximators. Extensive experiments covering typical\narchitectures and tasks demonstrate that our model is parameter-efficient and\ncan achieve superior performance to strong baselines in a wide range of\napplications. The code will be made publicly available at\nhttps://github.com/lsj2408/URPE.",
        "categories": [
            "cs.LG",
            "cs.CL",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2205.13401v2",
        "date": "2022-05-26 14:51:30+00:00"
    },
    {
        "title": "Align, then memorise: the dynamics of learning with feedback alignment",
        "authors": [
            "Maria Refinetti",
            "St\u00e9phane d'Ascoli",
            "Ruben Ohana",
            "Sebastian Goldt"
        ],
        "abstract": "Direct Feedback Alignment (DFA) is emerging as an efficient and biologically\nplausible alternative to the ubiquitous backpropagation algorithm for training\ndeep neural networks. Despite relying on random feedback weights for the\nbackward pass, DFA successfully trains state-of-the-art models such as\nTransformers. On the other hand, it notoriously fails to train convolutional\nnetworks. An understanding of the inner workings of DFA to explain these\ndiverging results remains elusive. Here, we propose a theory for the success of\nDFA. We first show that learning in shallow networks proceeds in two steps: an\nalignment phase, where the model adapts its weights to align the approximate\ngradient with the true gradient of the loss function, is followed by a\nmemorisation phase, where the model focuses on fitting the data. This two-step\nprocess has a degeneracy breaking effect: out of all the low-loss solutions in\nthe landscape, a network trained with DFA naturally converges to the solution\nwhich maximises gradient alignment. We also identify a key quantity underlying\nalignment in deep linear networks: the conditioning of the alignment matrices.\nThe latter enables a detailed understanding of the impact of data structure on\nalignment, and suggests a simple explanation for the well-known failure of DFA\nto train convolutional neural networks. Numerical experiments on MNIST and\nCIFAR10 clearly demonstrate degeneracy breaking in deep non-linear networks and\nshow that the align-then-memorise process occurs sequentially from the bottom\nlayers of the network to the top.",
        "categories": [
            "stat.ML",
            "cond-mat.dis-nn",
            "cs.LG",
            "cs.NE"
        ],
        "link": "http://arxiv.org/pdf/2011.12428v2",
        "date": "2020-11-24 22:21:27+00:00"
    },
    {
        "title": "An Introduction to Transformers",
        "authors": [
            "Richard E. Turner"
        ],
        "abstract": "The transformer is a neural network component that can be used to learn\nuseful representations of sequences or sets of datapoints. The transformer has\ndriven recent advances in natural language processing, computer vision, and\nspatio-temporal modelling. There are many introductions to transformers, but\nmost do not contain precise mathematical descriptions of the architecture and\nthe intuitions behind the design choices are often also missing. Moreover, as\nresearch takes a winding path, the explanations for the components of the\ntransformer can be idiosyncratic. In this note we aim for a mathematically\nprecise, intuitive, and clean description of the transformer architecture.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2304.10557v2",
        "date": "2023-04-20 14:54:19+00:00"
    },
    {
        "title": "Towards an Efficient Voice Identification Using Wav2Vec2.0 and HuBERT Based on the Quran Reciters Dataset",
        "authors": [
            "Aly Moustafa",
            "Salah A. Aly"
        ],
        "abstract": "Current authentication and trusted systems depend on classical and biometric\nmethods to recognize or authorize users. Such methods include audio speech\nrecognitions, eye, and finger signatures. Recent tools utilize deep learning\nand transformers to achieve better results. In this paper, we develop a deep\nlearning constructed model for Arabic speakers identification by using\nWav2Vec2.0 and HuBERT audio representation learning tools. The end-to-end\nWav2Vec2.0 paradigm acquires contextualized speech representations learnings by\nrandomly masking a set of feature vectors, and then applies a transformer\nneural network. We employ an MLP classifier that is able to differentiate\nbetween invariant labeled classes. We show several experimental results that\nsafeguard the high accuracy of the proposed model. The experiments ensure that\nan arbitrary wave signal for a certain speaker can be identified with 98% and\n97.1% accuracies in the cases of Wav2Vec2.0 and HuBERT, respectively.",
        "categories": [
            "cs.SD",
            "cs.AI",
            "cs.CL",
            "cs.LG",
            "eess.AS"
        ],
        "link": "http://arxiv.org/pdf/2111.06331v1",
        "date": "2021-11-11 17:44:50+00:00"
    },
    {
        "title": "DecisiveNets: Training Deep Associative Memories to Solve Complex Machine Learning Problems",
        "authors": [
            "Vincent Gripon",
            "Carlos Lassance",
            "Ghouthi Boukli Hacene"
        ],
        "abstract": "Learning deep representations to solve complex machine learning tasks has\nbecome the prominent trend in the past few years. Indeed, Deep Neural Networks\nare now the golden standard in domains as various as computer vision, natural\nlanguage processing or even playing combinatorial games. However, problematic\nlimitations are hidden behind this surprising universal capability. Among other\nthings, explainability of the decisions is a major concern, especially since\ndeep neural networks are made up of a very large number of trainable\nparameters. Moreover, computational complexity can quickly become a problem,\nespecially in contexts constrained by real time or limited resources.\nTherefore, understanding how information is stored and the impact this storage\ncan have on the system remains a major and open issue. In this chapter, we\nintroduce a method to transform deep neural network models into deep\nassociative memories, with simpler, more explicable and less expensive\noperations. We show through experiments that these transformations can be done\nwithout penalty on predictive performance. The resulting deep associative\nmemories are excellent candidates for artificial intelligence that is easier to\ntheorize and manipulate.",
        "categories": [
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2012.01509v1",
        "date": "2020-12-02 20:22:25+00:00"
    },
    {
        "title": "Theoretical Exploration of Solutions of Feedforward ReLU Networks",
        "authors": [
            "Changcun Huang"
        ],
        "abstract": "This paper aims to interpret the mechanism of feedforward ReLU networks by\nexploring their solutions for piecewise linear functions, through the deduction\nfrom basic rules. The constructed solution should be universal enough to\nexplain some network architectures of engineering; in order for that, several\nways are provided to enhance the solution universality. Some of the\nconsequences of our theories include: Under affine-geometry background, the\nsolutions of both three-layer networks and deep-layer networks are given,\nparticularly for those architectures applied in practice, such as multilayer\nfeedforward neural networks and decoders; We give clear and intuitive\ninterpretations of each component of network architectures; The\nparameter-sharing mechanism for multi-outputs is investigated; We provide an\nexplanation of overparameterization solutions in terms of affine transforms;\nUnder our framework, an advantage of deep layers compared to shallower ones is\nnatural to be obtained. Some intermediate results are the basic knowledge for\nthe modeling or understanding of neural networks, such as the classification of\ndata embedded in a higher-dimensional space, the generalization of affine\ntransforms, the probabilistic model of matrix ranks, and the concept of\ndistinguishable data sets.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "68T07(Primary), 65D05(Secondary)",
            "I.2.6; I.2.0; G.1.1; G.1.2"
        ],
        "link": "http://arxiv.org/pdf/2202.01919v9",
        "date": "2022-01-24 01:51:52+00:00"
    },
    {
        "title": "Invariant and Equivariant Graph Networks",
        "authors": [
            "Haggai Maron",
            "Heli Ben-Hamu",
            "Nadav Shamir",
            "Yaron Lipman"
        ],
        "abstract": "Invariant and equivariant networks have been successfully used for learning\nimages, sets, point clouds, and graphs. A basic challenge in developing such\nnetworks is finding the maximal collection of invariant and equivariant linear\nlayers. Although this question is answered for the first three examples (for\npopular transformations, at-least), a full characterization of invariant and\nequivariant linear layers for graphs is not known.\n  In this paper we provide a characterization of all permutation invariant and\nequivariant linear layers for (hyper-)graph data, and show that their\ndimension, in case of edge-value graph data, is 2 and 15, respectively. More\ngenerally, for graph data defined on k-tuples of nodes, the dimension is the\nk-th and 2k-th Bell numbers. Orthogonal bases for the layers are computed,\nincluding generalization to multi-graph data. The constant number of basis\nelements and their characteristics allow successfully applying the networks to\ndifferent size graphs. From the theoretical point of view, our results\ngeneralize and unify recent advancement in equivariant deep learning. In\nparticular, we show that our model is capable of approximating any message\npassing neural network\n  Applying these new linear layers in a simple deep neural network framework is\nshown to achieve comparable results to state-of-the-art and to have better\nexpressivity than previous invariant and equivariant bases.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1812.09902v2",
        "date": "2018-12-24 11:52:27+00:00"
    },
    {
        "title": "REST: Performance Improvement of a Black Box Model via RL-based Spatial Transformation",
        "authors": [
            "Jae Myung Kim",
            "Hyungjin Kim",
            "Chanwoo Park",
            "Jungwoo Lee"
        ],
        "abstract": "In recent years, deep neural networks (DNN) have become a highly active area\nof research, and shown remarkable achievements on a variety of computer vision\ntasks. DNNs, however, are known to often make overconfident yet incorrect\npredictions on out-of-distribution samples, which can be a major obstacle to\nreal-world deployments because the training dataset is always limited compared\nto diverse real-world samples. Thus, it is fundamental to provide guarantees of\nrobustness to the distribution shift between training and test time when we\nconstruct DNN models in practice. Moreover, in many cases, the deep learning\nmodels are deployed as black boxes and the performance has been already\noptimized for a training dataset, thus changing the black box itself can lead\nto performance degradation. We here study the robustness to the geometric\ntransformations in a specific condition where the black-box image classifier is\ngiven. We propose an additional learner, \\emph{REinforcement Spatial Transform\nlearner (REST)}, that transforms the warped input data into samples regarded as\nin-distribution by the black-box models. Our work aims to improve the\nrobustness by adding a REST module in front of any black boxes and training\nonly the REST module without retraining the original black box model in an\nend-to-end manner, i.e. we try to convert the real-world data into training\ndistribution which the performance of the black-box model is best suited for.\nWe use a confidence score that is obtained from the black-box model to\ndetermine whether the transformed input is drawn from in-distribution. We\nempirically show that our method has an advantage in generalization to\ngeometric transformations and sample efficiency.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2002.06610v1",
        "date": "2020-02-16 16:15:59+00:00"
    },
    {
        "title": "Efficient Certification of Spatial Robustness",
        "authors": [
            "Anian Ruoss",
            "Maximilian Baader",
            "Mislav Balunovi\u0107",
            "Martin Vechev"
        ],
        "abstract": "Recent work has exposed the vulnerability of computer vision models to vector\nfield attacks. Due to the widespread usage of such models in safety-critical\napplications, it is crucial to quantify their robustness against such spatial\ntransformations. However, existing work only provides empirical robustness\nquantification against vector field deformations via adversarial attacks, which\nlack provable guarantees. In this work, we propose novel convex relaxations,\nenabling us, for the first time, to provide a certificate of robustness against\nvector field transformations. Our relaxations are model-agnostic and can be\nleveraged by a wide range of neural network verifiers. Experiments on various\nnetwork architectures and different datasets demonstrate the effectiveness and\nscalability of our method.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2009.09318v2",
        "date": "2020-09-19 23:09:11+00:00"
    },
    {
        "title": "A Loss Function for Generative Neural Networks Based on Watson's Perceptual Model",
        "authors": [
            "Steffen Czolbe",
            "Oswin Krause",
            "Ingemar Cox",
            "Christian Igel"
        ],
        "abstract": "To train Variational Autoencoders (VAEs) to generate realistic imagery\nrequires a loss function that reflects human perception of image similarity. We\npropose such a loss function based on Watson's perceptual model, which computes\na weighted distance in frequency space and accounts for luminance and contrast\nmasking. We extend the model to color images, increase its robustness to\ntranslation by using the Fourier Transform, remove artifacts due to splitting\nthe image into blocks, and make it differentiable. In experiments, VAEs trained\nwith the new loss function generated realistic, high-quality image samples.\nCompared to using the Euclidean distance and the Structural Similarity Index,\nthe images were less blurry; compared to deep neural network based losses, the\nnew approach required less computational resources and generated images with\nless artifacts.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "eess.IV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2006.15057v3",
        "date": "2020-06-26 15:36:11+00:00"
    },
    {
        "title": "Federated Self-Supervised Learning of Multi-Sensor Representations for Embedded Intelligence",
        "authors": [
            "Aaqib Saeed",
            "Flora D. Salim",
            "Tanir Ozcelebi",
            "Johan Lukkien"
        ],
        "abstract": "Smartphones, wearables, and Internet of Things (IoT) devices produce a wealth\nof data that cannot be accumulated in a centralized repository for learning\nsupervised models due to privacy, bandwidth limitations, and the prohibitive\ncost of annotations. Federated learning provides a compelling framework for\nlearning models from decentralized data, but conventionally, it assumes the\navailability of labeled samples, whereas on-device data are generally either\nunlabeled or cannot be annotated readily through user interaction. To address\nthese issues, we propose a self-supervised approach termed\n\\textit{scalogram-signal correspondence learning} based on wavelet transform to\nlearn useful representations from unlabeled sensor inputs, such as\nelectroencephalography, blood volume pulse, accelerometer, and WiFi channel\nstate information. Our auxiliary task requires a deep temporal neural network\nto determine if a given pair of a signal and its complementary viewpoint (i.e.,\na scalogram generated with a wavelet transform) align with each other or not\nthrough optimizing a contrastive objective. We extensively assess the quality\nof learned features with our multi-view strategy on diverse public datasets,\nachieving strong performance in all domains. We demonstrate the effectiveness\nof representations learned from an unlabeled input collection on downstream\ntasks with training a linear classifier over pretrained network, usefulness in\nlow-data regime, transfer learning, and cross-validation. Our methodology\nachieves competitive performance with fully-supervised networks, and it\noutperforms pre-training with autoencoders in both central and federated\ncontexts. Notably, it improves the generalization in a semi-supervised setting\nas it reduces the volume of labeled data required through leveraging\nself-supervised learning.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2007.13018v1",
        "date": "2020-07-25 21:59:17+00:00"
    },
    {
        "title": "Optimal Input Gain: All You Need to Supercharge a Feed-Forward Neural Network",
        "authors": [
            "Chinmay Rane",
            "Kanishka Tyagi",
            "Sanjeev Malalur",
            "Yash Shinge",
            "Michael Manry"
        ],
        "abstract": "Linear transformation of the inputs alters the training performance of\nfeed-forward networks that are otherwise equivalent. However, most linear\ntransforms are viewed as a pre-processing operation separate from the actual\ntraining. Starting from equivalent networks, it is shown that pre-processing\ninputs using linear transformation are equivalent to multiplying the negative\ngradient matrix with an autocorrelation matrix per training iteration. Second\norder method is proposed to find the autocorrelation matrix that maximizes\nlearning in a given iteration. When the autocorrelation matrix is diagonal, the\nmethod optimizes input gains. This optimal input gain (OIG) approach is used to\nimprove two first-order two-stage training algorithms, namely back-propagation\n(BP) and hidden weight optimization (HWO), which alternately update the input\nweights and solve linear equations for output weights. Results show that the\nproposed OIG approach greatly enhances the performance of the first-order\nalgorithms, often allowing them to rival the popular Levenberg-Marquardt\napproach with far less computation. It is shown that HWO is equivalent to BP\nwith Whitening transformation applied to the inputs. HWO effectively combines\nWhitening transformation with learning. Thus, OIG improved HWO could be a\nsignificant building block to more complex deep learning architectures.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2303.17732v1",
        "date": "2023-03-30 22:20:16+00:00"
    },
    {
        "title": "The Lie Derivative for Measuring Learned Equivariance",
        "authors": [
            "Nate Gruver",
            "Marc Finzi",
            "Micah Goldblum",
            "Andrew Gordon Wilson"
        ],
        "abstract": "Equivariance guarantees that a model's predictions capture key symmetries in\ndata. When an image is translated or rotated, an equivariant model's\nrepresentation of that image will translate or rotate accordingly. The success\nof convolutional neural networks has historically been tied to translation\nequivariance directly encoded in their architecture. The rising success of\nvision transformers, which have no explicit architectural bias towards\nequivariance, challenges this narrative and suggests that augmentations and\ntraining data might also play a significant role in their performance. In order\nto better understand the role of equivariance in recent vision models, we\nintroduce the Lie derivative, a method for measuring equivariance with strong\nmathematical foundations and minimal hyperparameters. Using the Lie derivative,\nwe study the equivariance properties of hundreds of pretrained models, spanning\nCNNs, transformers, and Mixer architectures. The scale of our analysis allows\nus to separate the impact of architecture from other factors like model size or\ntraining method. Surprisingly, we find that many violations of equivariance can\nbe linked to spatial aliasing in ubiquitous network layers, such as pointwise\nnon-linearities, and that as models get larger and more accurate they tend to\ndisplay more equivariance, regardless of architecture. For example,\ntransformers can be more equivariant than convolutional neural networks after\ntraining.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2210.02984v1",
        "date": "2022-10-06 15:20:55+00:00"
    },
    {
        "title": "DataMUX: Data Multiplexing for Neural Networks",
        "authors": [
            "Vishvak Murahari",
            "Carlos E. Jimenez",
            "Runzhe Yang",
            "Karthik Narasimhan"
        ],
        "abstract": "In this paper, we introduce data multiplexing (DataMUX), a technique that\nenables deep neural networks to process multiple inputs simultaneously using a\nsingle compact representation. DataMUX demonstrates that neural networks are\ncapable of generating accurate predictions over mixtures of inputs, resulting\nin increased throughput with minimal extra memory requirements. Our approach\nuses two key components -- 1) a multiplexing layer that performs a fixed linear\ntransformation to each input before combining them to create a mixed\nrepresentation of the same size as a single input, which is then processed by\nthe base network, and 2) a demultiplexing layer that converts the base\nnetwork's output back into independent representations before producing\npredictions for each input. We show the viability of DataMUX for different\narchitectures (Transformers, and to a lesser extent MLPs and CNNs) across six\ndifferent tasks spanning sentence classification, named entity recognition and\nimage classification. For instance, DataMUX for Transformers can multiplex up\nto $20$x/$40$x inputs, achieving $11$x/$18$x increase in throughput with\nminimal absolute performance drops of $<2\\%$ and $<4\\%$ respectively on MNLI, a\nnatural language inference task. We also provide a theoretical construction for\nmultiplexing in self-attention networks and analyze the effect of various\ndesign elements in DataMUX.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2202.09318v2",
        "date": "2022-02-18 17:35:33+00:00"
    },
    {
        "title": "Lipschitz Normalization for Self-Attention Layers with Application to Graph Neural Networks",
        "authors": [
            "George Dasoulas",
            "Kevin Scaman",
            "Aladin Virmaux"
        ],
        "abstract": "Attention based neural networks are state of the art in a large range of\napplications. However, their performance tends to degrade when the number of\nlayers increases. In this work, we show that enforcing Lipschitz continuity by\nnormalizing the attention scores can significantly improve the performance of\ndeep attention models. First, we show that, for deep graph attention networks\n(GAT), gradient explosion appears during training, leading to poor performance\nof gradient-based training algorithms. To address this issue, we derive a\ntheoretical analysis of the Lipschitz continuity of attention modules and\nintroduce LipschitzNorm, a simple and parameter-free normalization for\nself-attention mechanisms that enforces the model to be Lipschitz continuous.\nWe then apply LipschitzNorm to GAT and Graph Transformers and show that their\nperformance is substantially improved in the deep setting (10 to 30 layers).\nMore specifically, we show that a deep GAT model with LipschitzNorm achieves\nstate of the art results for node label prediction tasks that exhibit\nlong-range dependencies, while showing consistent improvements over their\nunnormalized counterparts in benchmark node classification tasks.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2103.04886v3",
        "date": "2021-03-08 16:47:16+00:00"
    },
    {
        "title": "CHEETAH: An Ultra-Fast, Approximation-Free, and Privacy-Preserved Neural Network Framework based on Joint Obscure Linear and Nonlinear Computations",
        "authors": [
            "Qiao Zhang",
            "Cong Wang",
            "Chunsheng Xin",
            "Hongyi Wu"
        ],
        "abstract": "Machine Learning as a Service (MLaaS) is enabling a wide range of smart\napplications on end devices. However, such convenience comes with a cost of\nprivacy because users have to upload their private data to the cloud. This\nresearch aims to provide effective and efficient MLaaS such that the cloud\nserver learns nothing about user data and the users cannot infer the\nproprietary model parameters owned by the server. This work makes the following\ncontributions. First, it unveils the fundamental performance bottleneck of\nexisting schemes due to the heavy permutations in computing linear\ntransformation and the use of communication intensive Garbled Circuits for\nnonlinear transformation. Second, it introduces an ultra-fast secure MLaaS\nframework, CHEETAH, which features a carefully crafted secret sharing scheme\nthat runs significantly faster than existing schemes without accuracy loss.\nThird, CHEETAH is evaluated on the benchmark of well-known, practical deep\nnetworks such as AlexNet and VGG-16 on the MNIST and ImageNet datasets. The\nresults demonstrate more than 100x speedup over the fastest GAZELLE (Usenix\nSecurity'18), 2000x speedup over MiniONN (ACM CCS'17) and five orders of\nmagnitude speedup over CryptoNets (ICML'16). This significant speedup enables a\nwide range of practical applications based on privacy-preserved deep neural\nnetworks.",
        "categories": [
            "cs.LG",
            "cs.CR",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1911.05184v2",
        "date": "2019-11-12 23:08:45+00:00"
    },
    {
        "title": "Zero-bias Deep Neural Network for Quickest RF Signal Surveillance",
        "authors": [
            "Yongxin Liu",
            "Yingjie Chen",
            "Jian Wang",
            "Shuteng Niu",
            "Dahai Liu",
            "Houbing Song"
        ],
        "abstract": "The Internet of Things (IoT) is reshaping modern society by allowing a decent\nnumber of RF devices to connect and share information through RF channels.\nHowever, such an open nature also brings obstacles to surveillance. For\nalleviation, a surveillance oracle, or a cognitive communication entity needs\nto identify and confirm the appearance of known or unknown signal sources in\nreal-time. In this paper, we provide a deep learning framework for RF signal\nsurveillance. Specifically, we jointly integrate the Deep Neural Networks\n(DNNs) and Quickest Detection (QD) to form a sequential signal surveillance\nscheme. We first analyze the latent space characteristic of neural network\nclassification models, and then we leverage the response characteristics of DNN\nclassifiers and propose a novel method to transform existing DNN classifiers\ninto performance-assured binary abnormality detectors. In this way, we\nseamlessly integrate the DNNs with the parametric quickest detection. Finally,\nwe propose an enhanced Elastic Weight Consolidation (EWC) algorithm with better\nnumerical stability for DNNs in signal surveillance systems to evolve\nincrementally, we demonstrate that the zero-bias DNN is superior to regular DNN\nmodels considering incremental learning and decision fairness. We evaluated the\nproposed framework using real signal datasets and we believe this framework is\nhelpful in developing a trustworthy IoT ecosystem.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CR",
            "eess.SP"
        ],
        "link": "http://arxiv.org/pdf/2110.05797v1",
        "date": "2021-10-12 07:48:57+00:00"
    },
    {
        "title": "Tensor-based Sequential Learning via Hankel Matrix Representation for Next Item Recommendations",
        "authors": [
            "Evgeny Frolov",
            "Ivan Oseledets"
        ],
        "abstract": "Self-attentive transformer models have recently been shown to solve the next\nitem recommendation task very efficiently. The learned attention weights\ncapture sequential dynamics in user behavior and generalize well. Motivated by\nthe special structure of learned parameter space, we question if it is possible\nto mimic it with an alternative and more lightweight approach. We develop a new\ntensor factorization-based model that ingrains the structural knowledge about\nsequential data within the learning process. We demonstrate how certain\nproperties of a self-attention network can be reproduced with our approach\nbased on special Hankel matrix representation. The resulting model has a\nshallow linear architecture and compares competitively to its neural\ncounterpart.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.IR",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2212.05720v1",
        "date": "2022-12-12 05:55:40+00:00"
    },
    {
        "title": "Learning to recognize touch gestures: recurrent vs. convolutional features and dynamic sampling",
        "authors": [
            "Quentin Debard",
            "Christian Wolf",
            "St\u00e9phane Canu",
            "Julien Arn\u00e9"
        ],
        "abstract": "We propose a fully automatic method for learning gestures on big touch\ndevices in a potentially multi-user context. The goal is to learn general\nmodels capable of adapting to different gestures, user styles and hardware\nvariations (e.g. device sizes, sampling frequencies and regularities).\n  Based on deep neural networks, our method features a novel dynamic sampling\nand temporal normalization component, transforming variable length gestures\ninto fixed length representations while preserving finger/surface contact\ntransitions, that is, the topology of the signal. This sequential\nrepresentation is then processed with a convolutional model capable, unlike\nrecurrent networks, of learning hierarchical representations with different\nlevels of abstraction.\n  To demonstrate the interest of the proposed method, we introduce a new touch\ngestures dataset with 6591 gestures performed by 27 people, which is, up to our\nknowledge, the first of its kind: a publicly available multi-touch gesture\ndataset for interaction.\n  We also tested our method on a standard dataset of symbolic touch gesture\nrecognition, the MMG dataset, outperforming the state of the art and reporting\nclose to perfect performance.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1802.09901v1",
        "date": "2018-02-19 10:24:48+00:00"
    },
    {
        "title": "Transformer for Partial Differential Equations' Operator Learning",
        "authors": [
            "Zijie Li",
            "Kazem Meidani",
            "Amir Barati Farimani"
        ],
        "abstract": "Data-driven learning of partial differential equations' solution operators\nhas recently emerged as a promising paradigm for approximating the underlying\nsolutions. The solution operators are usually parameterized by deep learning\nmodels that are built upon problem-specific inductive biases. An example is a\nconvolutional or a graph neural network that exploits the local grid structure\nwhere functions' values are sampled. The attention mechanism, on the other\nhand, provides a flexible way to implicitly exploit the patterns within inputs,\nand furthermore, relationship between arbitrary query locations and inputs. In\nthis work, we present an attention-based framework for data-driven operator\nlearning, which we term Operator Transformer (OFormer). Our framework is built\nupon self-attention, cross-attention, and a set of point-wise multilayer\nperceptrons (MLPs), and thus it makes few assumptions on the sampling pattern\nof the input function or query locations. We show that the proposed framework\nis competitive on standard benchmark problems and can flexibly be adapted to\nrandomly sampled input.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2205.13671v2",
        "date": "2022-05-26 23:17:53+00:00"
    },
    {
        "title": "Prediction of severe thunderstorm events with ensemble deep learning and radar data",
        "authors": [
            "Sabrina Guastavino",
            "Michele Piana",
            "Marco Tizzi",
            "Federico Cassola",
            "Antonio Iengo",
            "Davide Sacchetti",
            "Enrico Solazzo",
            "Federico Benvenuto"
        ],
        "abstract": "The problem of nowcasting extreme weather events can be addressed by applying\neither numerical methods for the solution of dynamic model equations or\ndata-driven artificial intelligence algorithms. Within this latter framework,\nthe present paper illustrates how a deep learning method, exploiting videos of\nradar reflectivity frames as input, can be used to realize a warning machine\nable to sound timely alarms of possible severe thunderstorm events. From a\ntechnical viewpoint, the computational core of this approach is the use of a\nvalue-weighted skill score for both transforming the probabilistic outcomes of\nthe deep neural network into binary classification and assessing the\nforecasting performances. The warning machine has been validated against\nweather radar data recorded in the Liguria region, in Italy,",
        "categories": [
            "cs.LG",
            "cs.AI",
            "68T07, 86A10"
        ],
        "link": "http://arxiv.org/pdf/2109.09791v1",
        "date": "2021-09-20 18:43:13+00:00"
    },
    {
        "title": "High-contrast \"gaudy\" images improve the training of deep neural network models of visual cortex",
        "authors": [
            "Benjamin R. Cowley",
            "Jonathan W. Pillow"
        ],
        "abstract": "A key challenge in understanding the sensory transformations of the visual\nsystem is to obtain a highly predictive model of responses from visual cortical\nneurons. Deep neural networks (DNNs) provide a promising candidate for such a\nmodel. However, DNNs require orders of magnitude more training data than\nneuroscientists can collect from real neurons because experimental recording\ntime is severely limited. This motivates us to find images that train\nhighly-predictive DNNs with as little training data as possible. We propose\ngaudy images---high-contrast binarized versions of natural images---to\nefficiently train DNNs. In extensive simulation experiments, we find that\ntraining DNNs with gaudy images substantially reduces the number of training\nimages needed to accurately predict the simulated responses of visual cortical\nneurons. We also find that gaudy images, chosen before training, outperform\nimages chosen during training by active learning algorithms. Thus, gaudy images\noveremphasize features of natural images, especially edges, that are the most\nimportant for efficiently training DNNs. We believe gaudy images will aid in\nthe modeling of visual cortical neurons, potentially opening new scientific\nquestions about visual processing, as well as aid general practitioners that\nseek ways to improve the training of DNNs.",
        "categories": [
            "cs.CV",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2006.11412v1",
        "date": "2020-06-13 20:05:16+00:00"
    },
    {
        "title": "Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks",
        "authors": [
            "Lechao Xiao",
            "Yasaman Bahri",
            "Jascha Sohl-Dickstein",
            "Samuel S. Schoenholz",
            "Jeffrey Pennington"
        ],
        "abstract": "In recent years, state-of-the-art methods in computer vision have utilized\nincreasingly deep convolutional neural network architectures (CNNs), with some\nof the most successful models employing hundreds or even thousands of layers. A\nvariety of pathologies such as vanishing/exploding gradients make training such\ndeep networks challenging. While residual connections and batch normalization\ndo enable training at these depths, it has remained unclear whether such\nspecialized architecture designs are truly necessary to train deep CNNs. In\nthis work, we demonstrate that it is possible to train vanilla CNNs with ten\nthousand layers or more simply by using an appropriate initialization scheme.\nWe derive this initialization scheme theoretically by developing a mean field\ntheory for signal propagation and by characterizing the conditions for\ndynamical isometry, the equilibration of singular values of the input-output\nJacobian matrix. These conditions require that the convolution operator be an\northogonal transformation in the sense that it is norm-preserving. We present\nan algorithm for generating such random initial orthogonal convolution kernels\nand demonstrate empirically that they enable efficient training of extremely\ndeep architectures.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1806.05393v2",
        "date": "2018-06-14 07:04:15+00:00"
    },
    {
        "title": "Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models",
        "authors": [
            "Jesse Engel",
            "Matthew Hoffman",
            "Adam Roberts"
        ],
        "abstract": "Deep generative neural networks have proven effective at both conditional and\nunconditional modeling of complex data distributions. Conditional generation\nenables interactive control, but creating new controls often requires expensive\nretraining. In this paper, we develop a method to condition generation without\nretraining the model. By post-hoc learning latent constraints, value functions\nthat identify regions in latent space that generate outputs with desired\nattributes, we can conditionally sample from these regions with gradient-based\noptimization or amortized actor functions. Combining attribute constraints with\na universal \"realism\" constraint, which enforces similarity to the data\ndistribution, we generate realistic conditional images from an unconditional\nvariational autoencoder. Further, using gradient-based optimization, we\ndemonstrate identity-preserving transformations that make the minimal\nadjustment in latent space to modify the attributes of an image. Finally, with\ndiscrete sequences of musical notes, we demonstrate zero-shot conditional\ngeneration, learning latent constraints in the absence of labeled data or a\ndifferentiable reward function. Code with dedicated cloud instance has been\nmade publicly available (https://goo.gl/STGMGx).",
        "categories": [
            "cs.LG",
            "cs.NE",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1711.05772v2",
        "date": "2017-11-15 19:45:10+00:00"
    },
    {
        "title": "Randomly Initialized One-Layer Neural Networks Make Data Linearly Separable",
        "authors": [
            "Promit Ghosal",
            "Srinath Mahankali",
            "Yihang Sun"
        ],
        "abstract": "Recently, neural networks have been shown to perform exceptionally well in\ntransforming two arbitrary sets into two linearly separable sets. Doing this\nwith a randomly initialized neural network is of immense interest because the\nassociated computation is cheaper than using fully trained networks. In this\npaper, we show that, with sufficient width, a randomly initialized one-layer\nneural network transforms two sets into two linearly separable sets with high\nprobability. Furthermore, we provide explicit bounds on the required width of\nthe neural network for this to occur. Our first bound is exponential in the\ninput dimension and polynomial in all other parameters, while our second bound\nis independent of the input dimension, thereby overcoming the curse of\ndimensionality. We also perform an experimental study comparing the separation\ncapacity of randomly initialized one-layer and two-layer neural networks. With\ncorrectly chosen biases, our study shows for low-dimensional data, the\ntwo-layer neural network outperforms the one-layer network. However, the\nopposite is observed for higher-dimensional data.",
        "categories": [
            "cs.LG",
            "math.PR",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2205.11716v1",
        "date": "2022-05-24 01:38:43+00:00"
    },
    {
        "title": "Trainable Adaptive Window Switching for Speech Enhancement",
        "authors": [
            "Yuma Koizumi",
            "Noboru Harada",
            "Yoichi Haneda"
        ],
        "abstract": "This study proposes a trainable adaptive window switching (AWS) method and\napply it to a deep-neural-network (DNN) for speech enhancement in the modified\ndiscrete cosine transform domain. Time-frequency (T-F) mask processing in the\nshort-time Fourier transform (STFT)-domain is a typical speech enhancement\nmethod. To recover the target signal precisely, DNN-based short-time frequency\ntransforms have recently been investigated and used instead of the STFT.\nHowever, since such a fixed-resolution short-time frequency transform method\nhas a T-F resolution problem based on the uncertainty principle, not only the\nshort-time frequency transform but also the length of the windowing function\nshould be optimized. To overcome this problem, we incorporate AWS into the\nspeech enhancement procedure, and the windowing function of each time-frame is\nmanipulated using a DNN depending on the input signal. We confirmed that the\nproposed method achieved a higher signal-to-distortion ratio than conventional\nspeech enhancement methods in fixed-resolution frequency domains.",
        "categories": [
            "eess.AS",
            "cs.LG",
            "cs.SD",
            "eess.SP",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1811.02438v4",
        "date": "2018-11-05 12:25:42+00:00"
    },
    {
        "title": "Large Margin Deep Networks for Classification",
        "authors": [
            "Gamaleldin F. Elsayed",
            "Dilip Krishnan",
            "Hossein Mobahi",
            "Kevin Regan",
            "Samy Bengio"
        ],
        "abstract": "We present a formulation of deep learning that aims at producing a large\nmargin classifier. The notion of margin, minimum distance to a decision\nboundary, has served as the foundation of several theoretically profound and\nempirically successful results for both classification and regression tasks.\nHowever, most large margin algorithms are applicable only to shallow models\nwith a preset feature representation; and conventional margin methods for\nneural networks only enforce margin at the output layer. Such methods are\ntherefore not well suited for deep networks.\n  In this work, we propose a novel loss function to impose a margin on any\nchosen set of layers of a deep network (including input and hidden layers). Our\nformulation allows choosing any norm on the metric measuring the margin. We\ndemonstrate that the decision boundary obtained by our loss has nice properties\ncompared to standard classification loss functions. Specifically, we show\nimproved empirical results on the MNIST, CIFAR-10 and ImageNet datasets on\nmultiple tasks: generalization from small training sets, corrupted labels, and\nrobustness against adversarial perturbations. The resulting loss is general and\ncomplementary to existing data augmentation (such as random/adversarial input\ntransform) and regularization techniques (such as weight decay, dropout, and\nbatch norm).",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1803.05598v2",
        "date": "2018-03-15 05:33:13+00:00"
    },
    {
        "title": "Evaluating Deep Graph Neural Networks",
        "authors": [
            "Wentao Zhang",
            "Zeang Sheng",
            "Yuezihan Jiang",
            "Yikuan Xia",
            "Jun Gao",
            "Zhi Yang",
            "Bin Cui"
        ],
        "abstract": "Graph Neural Networks (GNNs) have already been widely applied in various\ngraph mining tasks. However, they suffer from the shallow architecture issue,\nwhich is the key impediment that hinders the model performance improvement.\nAlthough several relevant approaches have been proposed, none of the existing\nstudies provides an in-depth understanding of the root causes of performance\ndegradation in deep GNNs. In this paper, we conduct the first systematic\nexperimental evaluation to present the fundamental limitations of shallow\narchitectures. Based on the experimental results, we answer the following two\nessential questions: (1) what actually leads to the compromised performance of\ndeep GNNs; (2) when we need and how to build deep GNNs. The answers to the\nabove questions provide empirical insights and guidelines for researchers to\ndesign deep and well-performed GNNs. To show the effectiveness of our proposed\nguidelines, we present Deep Graph Multi-Layer Perceptron (DGMLP), a powerful\napproach (a paradigm in its own right) that helps guide deep GNN designs.\nExperimental results demonstrate three advantages of DGMLP: 1) high accuracy --\nit achieves state-of-the-art node classification performance on various\ndatasets; 2) high flexibility -- it can flexibly choose different propagation\nand transformation depths according to graph size and sparsity; 3) high\nscalability and efficiency -- it supports fast training on large-scale graphs.\nOur code is available in https://github.com/zwt233/DGMLP.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2108.00955v1",
        "date": "2021-08-02 14:55:10+00:00"
    },
    {
        "title": "Static Activation Function Normalization",
        "authors": [
            "Pierre H. Richemond",
            "Yike Guo"
        ],
        "abstract": "Recent seminal work at the intersection of deep neural networks practice and\nrandom matrix theory has linked the convergence speed and robustness of these\nnetworks with the combination of random weight initialization and nonlinear\nactivation function in use. Building on those principles, we introduce a\nprocess to transform an existing activation function into another one with\nbetter properties. We term such transform \\emph{static activation\nnormalization}. More specifically we focus on this normalization applied to the\nReLU unit, and show empirically that it significantly promotes convergence\nrobustness, maximum training depth, and anytime performance. We verify these\nclaims by examining empirical eigenvalue distributions of networks trained with\nthose activations. Our static activation normalization provides a first step\ntowards giving benefits similar in spirit to schemes like batch normalization,\nbut without computational cost.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1905.01369v1",
        "date": "2019-05-03 21:43:35+00:00"
    },
    {
        "title": "Formal Verification of CNN-based Perception Systems",
        "authors": [
            "Panagiotis Kouvaros",
            "Alessio Lomuscio"
        ],
        "abstract": "We address the problem of verifying neural-based perception systems\nimplemented by convolutional neural networks. We define a notion of local\nrobustness based on affine and photometric transformations. We show the notion\ncannot be captured by previously employed notions of robustness. The method\nproposed is based on reachability analysis for feed-forward neural networks and\nrelies on MILP encodings of both the CNNs and transformations under question.\nWe present an implementation and discuss the experimental results obtained for\na CNN trained from the MNIST data set.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1811.11373v1",
        "date": "2018-11-28 03:36:25+00:00"
    },
    {
        "title": "DeformRS: Certifying Input Deformations with Randomized Smoothing",
        "authors": [
            "Motasem Alfarra",
            "Adel Bibi",
            "Naeemullah Khan",
            "Philip H. S. Torr",
            "Bernard Ghanem"
        ],
        "abstract": "Deep neural networks are vulnerable to input deformations in the form of\nvector fields of pixel displacements and to other parameterized geometric\ndeformations e.g. translations, rotations, etc. Current input deformation\ncertification methods either 1. do not scale to deep networks on large input\ndatasets, or 2. can only certify a specific class of deformations, e.g. only\nrotations. We reformulate certification in randomized smoothing setting for\nboth general vector field and parameterized deformations and propose\nDeformRS-VF and DeformRS-Par, respectively. Our new formulation scales to large\nnetworks on large input datasets. For instance, DeformRS-Par certifies rich\ndeformations, covering translations, rotations, scaling, affine deformations,\nand other visually aligned deformations such as ones parameterized by\nDiscrete-Cosine-Transform basis. Extensive experiments on MNIST, CIFAR10, and\nImageNet show competitive performance of DeformRS-Par achieving a certified\naccuracy of $39\\%$ against perturbed rotations in the set\n$[-10\\degree,10\\degree]$ on ImageNet.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2107.00996v2",
        "date": "2021-07-02 12:20:15+00:00"
    },
    {
        "title": "Stacked Kernel Network",
        "authors": [
            "Shuai Zhang",
            "Jianxin Li",
            "Pengtao Xie",
            "Yingchun Zhang",
            "Minglai Shao",
            "Haoyi Zhou",
            "Mengyi Yan"
        ],
        "abstract": "Kernel methods are powerful tools to capture nonlinear patterns behind data.\nThey implicitly learn high (even infinite) dimensional nonlinear features in\nthe Reproducing Kernel Hilbert Space (RKHS) while making the computation\ntractable by leveraging the kernel trick. Classic kernel methods learn a single\nlayer of nonlinear features, whose representational power may be limited.\nMotivated by recent success of deep neural networks (DNNs) that learn\nmulti-layer hierarchical representations, we propose a Stacked Kernel Network\n(SKN) that learns a hierarchy of RKHS-based nonlinear features. SKN interleaves\nseveral layers of nonlinear transformations (from a linear space to a RKHS) and\nlinear transformations (from a RKHS to a linear space). Similar to DNNs, a SKN\nis composed of multiple layers of hidden units, but each parameterized by a\nRKHS function rather than a finite-dimensional vector. We propose three ways to\nrepresent the RKHS functions in SKN: (1)nonparametric representation,\n(2)parametric representation and (3)random Fourier feature representation.\nFurthermore, we expand SKN into CNN architecture called Stacked Kernel\nConvolutional Network (SKCN). SKCN learning a hierarchy of RKHS-based nonlinear\nfeatures by convolutional operation with each filter also parameterized by a\nRKHS function rather than a finite-dimensional matrix in CNN, which is suitable\nfor image inputs. Experiments on various datasets demonstrate the effectiveness\nof SKN and SKCN, which outperform the competitive methods.",
        "categories": [
            "stat.ML",
            "cs.CV",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1711.09219v1",
        "date": "2017-11-25 09:01:40+00:00"
    },
    {
        "title": "An Empirical Comparison of GANs and Normalizing Flows for Density Estimation",
        "authors": [
            "Tianci Liu",
            "Jeffrey Regier"
        ],
        "abstract": "Generative adversarial networks (GANs) and normalizing flows are both\napproaches to density estimation that use deep neural networks to transform\nsamples from an uninformative prior distribution to an approximation of the\ndata distribution. There is great interest in both for general-purpose\nstatistical modeling, but the two approaches have seldom been compared to each\nother for modeling non-image data. The difficulty of computing likelihoods with\nGANs, which are implicit models, makes conducting such a comparison\nchallenging. We work around this difficulty by considering several\nlow-dimensional synthetic datasets. An extensive grid search over GAN\narchitectures, hyperparameters, and training procedures suggests that no GAN is\ncapable of modeling our simple low-dimensional data well, a task we view as a\nprerequisite for an approach to be considered suitable for general-purpose\nstatistical modeling. Several normalizing flows, on the other hand, excelled at\nthese tasks, even substantially outperforming WGAN in terms of Wasserstein\ndistance -- the metric that WGAN alone targets. Scientists and other\npractitioners should be wary of relying on WGAN for applications that require\naccurate density estimation.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2006.10175v2",
        "date": "2020-06-17 21:56:58+00:00"
    },
    {
        "title": "Understanding Robustness of Transformers for Image Classification",
        "authors": [
            "Srinadh Bhojanapalli",
            "Ayan Chakrabarti",
            "Daniel Glasner",
            "Daliang Li",
            "Thomas Unterthiner",
            "Andreas Veit"
        ],
        "abstract": "Deep Convolutional Neural Networks (CNNs) have long been the architecture of\nchoice for computer vision tasks. Recently, Transformer-based architectures\nlike Vision Transformer (ViT) have matched or even surpassed ResNets for image\nclassification. However, details of the Transformer architecture -- such as the\nuse of non-overlapping patches -- lead one to wonder whether these networks are\nas robust. In this paper, we perform an extensive study of a variety of\ndifferent measures of robustness of ViT models and compare the findings to\nResNet baselines. We investigate robustness to input perturbations as well as\nrobustness to model perturbations. We find that when pre-trained with a\nsufficient amount of data, ViT models are at least as robust as the ResNet\ncounterparts on a broad range of perturbations. We also find that Transformers\nare robust to the removal of almost any single layer, and that while\nactivations from later layers are highly correlated with each other, they\nnevertheless play an important role in classification.",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2103.14586v2",
        "date": "2021-03-26 16:47:55+00:00"
    },
    {
        "title": "Transform Learning for Magnetic Resonance Image Reconstruction: From Model-based Learning to Building Neural Networks",
        "authors": [
            "Bihan Wen",
            "Saiprasad Ravishankar",
            "Luke Pfister",
            "Yoram Bresler"
        ],
        "abstract": "Magnetic resonance imaging (MRI) is widely used in clinical practice, but it\nhas been traditionally limited by its slow data acquisition. Recent advances in\ncompressed sensing (CS) techniques for MRI reduce acquisition time while\nmaintaining high image quality. Whereas classical CS assumes the images are\nsparse in known analytical dictionaries or transform domains, methods using\nlearned image models for reconstruction have become popular. The model could be\npre-learned from datasets, or learned simultaneously with the reconstruction,\ni.e., blind CS (BCS). Besides the well-known synthesis dictionary model, recent\nadvances in transform learning (TL) provide an efficient alternative framework\nfor sparse modeling in MRI. TL-based methods enjoy numerous advantages\nincluding exact sparse coding, transform update, and clustering solutions,\ncheap computation, and convergence guarantees, and provide high-quality results\nin MRI compared to popular competing methods. This paper provides a review of\nsome recent works in MRI reconstruction from limited data, with focus on the\nrecent TL-based methods. A unified framework for incorporating various TL-based\nmodels is presented. We discuss the connections between transform learning and\nconvolutional or filter bank models and corresponding multi-layer extensions,\nwith connections to deep learning. Finally, we discuss recent trends in MRI,\nopen problems, and future directions for the field.",
        "categories": [
            "eess.IV",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1903.11431v2",
        "date": "2019-03-25 03:13:18+00:00"
    },
    {
        "title": "A Correspondence Between Random Neural Networks and Statistical Field Theory",
        "authors": [
            "Samuel S. Schoenholz",
            "Jeffrey Pennington",
            "Jascha Sohl-Dickstein"
        ],
        "abstract": "A number of recent papers have provided evidence that practical design\nquestions about neural networks may be tackled theoretically by studying the\nbehavior of random networks. However, until now the tools available for\nanalyzing random neural networks have been relatively ad-hoc. In this work, we\nshow that the distribution of pre-activations in random neural networks can be\nexactly mapped onto lattice models in statistical physics. We argue that\nseveral previous investigations of stochastic networks actually studied a\nparticular factorial approximation to the full lattice model. For random linear\nnetworks and random rectified linear networks we show that the corresponding\nlattice models in the wide network limit may be systematically approximated by\na Gaussian distribution with covariance between the layers of the network. In\neach case, the approximate distribution can be diagonalized by Fourier\ntransformation. We show that this approximation accurately describes the\nresults of numerical simulations of wide random neural networks. Finally, we\ndemonstrate that in each case the large scale behavior of the random networks\ncan be approximated by an effective field theory.",
        "categories": [
            "stat.ML",
            "cond-mat.dis-nn",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1710.06570v1",
        "date": "2017-10-18 03:05:47+00:00"
    },
    {
        "title": "Multi-Scale Deep Compressive Sensing Network",
        "authors": [
            "Thuong Nguyen Canh",
            "Byeungwoo Jeon"
        ],
        "abstract": "With joint learning of sampling and recovery, the deep learning-based\ncompressive sensing (DCS) has shown significant improvement in performance and\nrunning time reduction. Its reconstructed image, however, losses high-frequency\ncontent especially at low subrates. This happens similarly in the multi-scale\nsampling scheme which also samples more low-frequency components. In this\npaper, we propose a multi-scale DCS convolutional neural network (MS-DCSNet) in\nwhich we convert image signal using multiple scale-based wavelet transform,\nthen capture it through convolution block by block across scales. The initial\nreconstructed image is directly recovered from multi-scale measurements.\nMulti-scale wavelet convolution is utilized to enhance the final reconstruction\nquality. The network is able to learn both multi-scale sampling and multi-scale\nreconstruction, thus results in better reconstruction quality.",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1809.05717v2",
        "date": "2018-09-15 14:05:27+00:00"
    },
    {
        "title": "Adversarial Pretraining of Self-Supervised Deep Networks: Past, Present and Future",
        "authors": [
            "Guo-Jun Qi",
            "Mubarak Shah"
        ],
        "abstract": "In this paper, we review adversarial pretraining of self-supervised deep\nnetworks including both convolutional neural networks and vision transformers.\nUnlike the adversarial training with access to labeled examples, adversarial\npretraining is complicated as it only has access to unlabeled examples. To\nincorporate adversaries into pretraining models on either input or feature\nlevel, we find that existing approaches are largely categorized into two\ngroups: memory-free instance-wise attacks imposing worst-case perturbations on\nindividual examples, and memory-based adversaries shared across examples over\niterations. In particular, we review several representative adversarial\npretraining models based on Contrastive Learning (CL) and Masked Image Modeling\n(MIM), respectively, two popular self-supervised pretraining methods in\nliterature. We also review miscellaneous issues about computing overheads,\ninput-/feature-level adversaries, as well as other adversarial pretraining\napproaches beyond the above two groups. Finally, we discuss emerging trends and\nfuture directions about the relations between adversarial and cooperative\npretraining, unifying adversarial CL and MIM pretraining, and the trade-off\nbetween accuracy and robustness in adversarial pretraining.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "link": "http://arxiv.org/pdf/2210.13463v1",
        "date": "2022-10-23 13:14:06+00:00"
    },
    {
        "title": "Multi-Task Learning for Automotive Foggy Scene Understanding via Domain Adaptation to an Illumination-Invariant Representation",
        "authors": [
            "Naif Alshammari",
            "Samet Ak\u00e7ay",
            "Toby P. Breckon"
        ],
        "abstract": "Joint scene understanding and segmentation for automotive applications is a\nchallenging problem in two key aspects:- (1) classifying every pixel in the\nentire scene and (2) performing this task under unstable weather and\nillumination changes (e.g. foggy weather), which results in poor outdoor scene\nvisibility. This poor outdoor scene visibility leads to a non-optimal\nperformance of deep convolutional neural network-based scene understanding and\nsegmentation. In this paper, we propose an efficient end-to-end contemporary\nautomotive semantic scene understanding approach under foggy weather\nconditions, employing domain adaptation and illumination-invariant image\nper-transformation. As a multi-task pipeline, our proposed model provides:- (1)\ntransferring images from extreme to clear-weather condition using domain\ntransfer approach and (2) semantically segmenting a scene using a competitive\nencoder-decoder convolutional neural network (CNN) with dense connectivity,\nskip connections and fusion-based techniques. We evaluate our approach on\nchallenging foggy datasets, including synthetic dataset (Foggy Cityscapes) as\nwell as real-world datasets (Foggy Zurich and Foggy Driving). By incorporating\nRGB, depth, and illumination-invariant information, our approach outperforms\nthe state-of-the-art within automotive scene understanding, under foggy weather\ncondition.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1909.07697v1",
        "date": "2019-09-17 10:18:14+00:00"
    },
    {
        "title": "MaxUp: A Simple Way to Improve Generalization of Neural Network Training",
        "authors": [
            "Chengyue Gong",
            "Tongzheng Ren",
            "Mao Ye",
            "Qiang Liu"
        ],
        "abstract": "We propose \\emph{MaxUp}, an embarrassingly simple, highly effective technique\nfor improving the generalization performance of machine learning models,\nespecially deep neural networks. The idea is to generate a set of augmented\ndata with some random perturbations or transforms and minimize the maximum, or\nworst case loss over the augmented data. By doing so, we implicitly introduce a\nsmoothness or robustness regularization against the random perturbations, and\nhence improve the generation performance. For example, in the case of Gaussian\nperturbation,\n  \\emph{MaxUp} is asymptotically equivalent to using the gradient norm of the\nloss as a penalty to encourage smoothness. We test \\emph{MaxUp} on a range of\ntasks, including image classification, language modeling, and adversarial\ncertification, on which \\emph{MaxUp} consistently outperforms the existing best\nbaseline methods, without introducing substantial computational overhead. In\nparticular, we improve ImageNet classification from the state-of-the-art top-1\naccuracy $85.5\\%$ without extra data to $85.8\\%$. Code will be released soon.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2002.09024v1",
        "date": "2020-02-20 21:20:28+00:00"
    },
    {
        "title": "SE(3) Equivariant Graph Neural Networks with Complete Local Frames",
        "authors": [
            "Weitao Du",
            "He Zhang",
            "Yuanqi Du",
            "Qi Meng",
            "Wei Chen",
            "Bin Shao",
            "Tie-Yan Liu"
        ],
        "abstract": "Group equivariance (e.g. SE(3) equivariance) is a critical physical symmetry\nin science, from classical and quantum physics to computational biology. It\nenables robust and accurate prediction under arbitrary reference\ntransformations. In light of this, great efforts have been put on encoding this\nsymmetry into deep neural networks, which has been shown to improve the\ngeneralization performance and data efficiency for downstream tasks.\nConstructing an equivariant neural network generally brings high computational\ncosts to ensure expressiveness. Therefore, how to better trade-off the\nexpressiveness and computational efficiency plays a core role in the design of\nthe equivariant deep learning models. In this paper, we propose a framework to\nconstruct SE(3) equivariant graph neural networks that can approximate the\ngeometric quantities efficiently. Inspired by differential geometry and\nphysics, we introduce equivariant local complete frames to graph neural\nnetworks, such that tensor information at given orders can be projected onto\nthe frames. The local frame is constructed to form an orthonormal basis that\navoids direction degeneration and ensure completeness. Since the frames are\nbuilt only by cross product operations, our method is computationally\nefficient. We evaluate our method on two tasks: Newton mechanics modeling and\nequilibrium molecule conformation generation. Extensive experimental results\ndemonstrate that our model achieves the best or competitive performance in two\ntypes of datasets.",
        "categories": [
            "cs.CE",
            "cs.AI",
            "cs.LG",
            "math.DG",
            "physics.app-ph"
        ],
        "link": "http://arxiv.org/pdf/2110.14811v2",
        "date": "2021-10-26 14:26:25+00:00"
    },
    {
        "title": "Rotation Equivariance and Invariance in Convolutional Neural Networks",
        "authors": [
            "Benjamin Chidester",
            "Minh N. Do",
            "Jian Ma"
        ],
        "abstract": "Performance of neural networks can be significantly improved by encoding\nknown invariance for particular tasks. Many image classification tasks, such as\nthose related to cellular imaging, exhibit invariance to rotation. We present a\nnovel scheme using the magnitude response of the 2D-discrete-Fourier transform\n(2D-DFT) to encode rotational invariance in neural networks, along with a new,\nefficient convolutional scheme for encoding rotational equivariance throughout\nconvolutional layers. We implemented this scheme for several image\nclassification tasks and demonstrated improved performance, in terms of\nclassification accuracy, time required to train the model, and robustness to\nhyperparameter selection, over a standard CNN and another state-of-the-art\nmethod.",
        "categories": [
            "stat.ML",
            "cs.CV",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1805.12301v1",
        "date": "2018-05-31 03:13:41+00:00"
    },
    {
        "title": "Exploring Deep Registration Latent Spaces",
        "authors": [
            "Th\u00e9o Estienne",
            "Maria Vakalopoulou",
            "Stergios Christodoulidis",
            "Enzo Battistella",
            "Th\u00e9ophraste Henry",
            "Marvin Lerousseau",
            "Amaury Leroy",
            "Guillaume Chassagnon",
            "Marie-Pierre Revel",
            "Nikos Paragios",
            "Eric Deutsch"
        ],
        "abstract": "Explainability of deep neural networks is one of the most challenging and\ninteresting problems in the field. In this study, we investigate the topic\nfocusing on the interpretability of deep learning-based registration methods.\nIn particular, with the appropriate model architecture and using a simple\nlinear projection, we decompose the encoding space, generating a new basis, and\nwe empirically show that this basis captures various decomposed anatomically\naware geometrical transformations. We perform experiments using two different\ndatasets focusing on lungs and hippocampus MRI. We show that such an approach\ncan decompose the highly convoluted latent spaces of registration pipelines in\nan orthogonal space with several interesting properties. We hope that this work\ncould shed some light on a better understanding of deep learning-based\nregistration methods.",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2107.11238v1",
        "date": "2021-07-23 13:54:21+00:00"
    },
    {
        "title": "On the Continuity of Rotation Representations in Neural Networks",
        "authors": [
            "Yi Zhou",
            "Connelly Barnes",
            "Jingwan Lu",
            "Jimei Yang",
            "Hao Li"
        ],
        "abstract": "In neural networks, it is often desirable to work with various\nrepresentations of the same space. For example, 3D rotations can be represented\nwith quaternions or Euler angles. In this paper, we advance a definition of a\ncontinuous representation, which can be helpful for training deep neural\nnetworks. We relate this to topological concepts such as homeomorphism and\nembedding. We then investigate what are continuous and discontinuous\nrepresentations for 2D, 3D, and n-dimensional rotations. We demonstrate that\nfor 3D rotations, all representations are discontinuous in the real Euclidean\nspaces of four or fewer dimensions. Thus, widely used representations such as\nquaternions and Euler angles are discontinuous and difficult for neural\nnetworks to learn. We show that the 3D rotations have continuous\nrepresentations in 5D and 6D, which are more suitable for learning. We also\npresent continuous representations for the general case of the n-dimensional\nrotation group SO(n). While our main focus is on rotations, we also show that\nour constructions apply to other groups such as the orthogonal group and\nsimilarity transforms. We finally present empirical results, which show that\nour continuous rotation representations outperform discontinuous ones for\nseveral practical problems in graphics and vision, including a simple\nautoencoder sanity test, a rotation estimator for 3D point clouds, and an\ninverse kinematics solver for 3D human poses.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1812.07035v4",
        "date": "2018-12-17 20:13:17+00:00"
    },
    {
        "title": "Rank consistent ordinal regression for neural networks with application to age estimation",
        "authors": [
            "Wenzhi Cao",
            "Vahid Mirjalili",
            "Sebastian Raschka"
        ],
        "abstract": "In many real-world prediction tasks, class labels include information about\nthe relative ordering between labels, which is not captured by commonly-used\nloss functions such as multi-category cross-entropy. Recently, the deep\nlearning community adopted ordinal regression frameworks to take such ordering\ninformation into account. Neural networks were equipped with ordinal regression\ncapabilities by transforming ordinal targets into binary classification\nsubtasks. However, this method suffers from inconsistencies among the different\nbinary classifiers. To resolve these inconsistencies, we propose the COnsistent\nRAnk Logits (CORAL) framework with strong theoretical guarantees for\nrank-monotonicity and consistent confidence scores. Moreover, the proposed\nmethod is architecture-agnostic and can extend arbitrary state-of-the-art deep\nneural network classifiers for ordinal regression tasks. The empirical\nevaluation of the proposed rank-consistent method on a range of face-image\ndatasets for age prediction shows a substantial reduction of the prediction\nerror compared to the reference ordinal regression network.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1901.07884v7",
        "date": "2019-01-20 08:02:25+00:00"
    },
    {
        "title": "Causal Deep Learning: Causal Capsules and Tensor Transformers",
        "authors": [
            "M. Alex O. Vasilescu"
        ],
        "abstract": "We derive a set of causal deep neural networks whose architectures are a\nconsequence of tensor (multilinear) factor analysis. Forward causal questions\nare addressed with a neural network architecture composed of causal capsules\nand a tensor transformer. The former estimate a set of latent variables that\nrepresent the causal factors, and the latter governs their interaction. Causal\ncapsules and tensor transformers may be implemented using shallow autoencoders,\nbut for a scalable architecture we employ block algebra and derive a deep\nneural network composed of a hierarchy of autoencoders. An interleaved kernel\nhierarchy preprocesses the data resulting in a hierarchy of kernel tensor\nfactor models. Inverse causal questions are addressed with a neural network\nthat implements multilinear projection and estimates the causes of effects. As\nan alternative to aggressive bottleneck dimension reduction or regularized\nregression that may camouflage an inherently underdetermined inverse problem,\nwe prescribe modeling different aspects of the mechanism of data formation with\npiecewise tensor models whose multilinear projections are well-defined and\nproduce multiple candidate solutions. Our forward and inverse neural network\narchitectures are suitable for asynchronous parallel computation.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "I.4.10; I.2.4; I.2.10; I.5"
        ],
        "link": "http://arxiv.org/pdf/2301.00314v1",
        "date": "2023-01-01 00:47:03+00:00"
    },
    {
        "title": "RLFlow: Optimising Neural Network Subgraph Transformation with World Models",
        "authors": [
            "Sean Parker",
            "Sami Alabed",
            "Eiko Yoneki"
        ],
        "abstract": "Training deep learning models takes an extremely long execution time and\nconsumes large amounts of computing resources. At the same time, recent\nresearch proposed systems and compilers that are expected to decrease deep\nlearning models runtime. An effective optimisation methodology in data\nprocessing is desirable, and the reduction of compute requirements of deep\nlearning models is the focus of extensive research.\n  In this paper, we address the neural network sub-graph transformation by\nexploring reinforcement learning (RL) agents to achieve performance\nimprovement. Our proposed approach RLFlow can learn to perform neural network\nsubgraph transformations, without the need for expertly designed heuristics to\nachieve a high level of performance.\n  Recent work has aimed at applying RL to computer systems with some success,\nespecially using model-free RL techniques. Model-based reinforcement learning\nmethods have seen an increased focus in research as they can be used to learn\nthe transition dynamics of the environment; this can be leveraged to train an\nagent using a hallucinogenic environment such as World Model (WM), thereby\nincreasing sample efficiency compared to model-free approaches. WM uses\nvariational auto-encoders and it builds a model of the system and allows\nexploring the model in an inexpensive way.\n  In RLFlow, we propose a design for a model-based agent with WM which learns\nto optimise the architecture of neural networks by performing a sequence of\nsub-graph transformations to reduce model runtime. We show that our approach\ncan match the state-of-the-art performance on common convolutional networks and\noutperforms by up to 5% those based on transformer-style architectures",
        "categories": [
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2205.01435v2",
        "date": "2022-05-03 11:52:54+00:00"
    },
    {
        "title": "Deep Variational Inference Without Pixel-Wise Reconstruction",
        "authors": [
            "Siddharth Agrawal",
            "Ambedkar Dukkipati"
        ],
        "abstract": "Variational autoencoders (VAEs), that are built upon deep neural networks\nhave emerged as popular generative models in computer vision. Most of the work\ntowards improving variational autoencoders has focused mainly on making the\napproximations to the posterior flexible and accurate, leading to tremendous\nprogress. However, there have been limited efforts to replace pixel-wise\nreconstruction, which have known shortcomings. In this work, we use real-valued\nnon-volume preserving transformations (real NVP) to exactly compute the\nconditional likelihood of the data given the latent distribution. We show that\na simple VAE with this form of reconstruction is competitive with complicated\nVAE structures, on image modeling tasks. As part of our model, we develop\npowerful conditional coupling layers that enable real NVP to learn with fewer\nintermediate layers.",
        "categories": [
            "stat.ML",
            "cs.CV",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1611.05209v1",
        "date": "2016-11-16 10:20:10+00:00"
    },
    {
        "title": "Deep Distribution Regression",
        "authors": [
            "Rui Li",
            "Howard D. Bondell",
            "Brian J. Reich"
        ],
        "abstract": "Due to their flexibility and predictive performance, machine-learning based\nregression methods have become an important tool for predictive modeling and\nforecasting. However, most methods focus on estimating the conditional mean or\nspecific quantiles of the target quantity and do not provide the full\nconditional distribution, which contains uncertainty information that might be\ncrucial for decision making. In this article, we provide a general solution by\ntransforming a conditional distribution estimation problem into a constrained\nmulti-class classification problem, in which tools such as deep neural\nnetworks. We propose a novel joint binary cross-entropy loss function to\naccomplish this goal. We demonstrate its performance in various simulation\nstudies comparing to state-of-the-art competing methods. Additionally, our\nmethod shows improved accuracy in a probabilistic solar energy forecasting\nproblem.",
        "categories": [
            "stat.ML",
            "cs.LG",
            "stat.ME"
        ],
        "link": "http://arxiv.org/pdf/1903.06023v1",
        "date": "2019-03-14 14:19:39+00:00"
    },
    {
        "title": "Reinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook",
        "authors": [
            "Baihan Lin"
        ],
        "abstract": "In recent years, reinforcement learning and bandits have transformed a wide\nrange of real-world applications including healthcare, finance, recommendation\nsystems, robotics, and last but not least, the speech and natural language\nprocessing. While most speech and language applications of reinforcement\nlearning algorithms are centered around improving the training of deep neural\nnetworks with its flexible optimization properties, there are still many\ngrounds to explore to utilize the benefits of reinforcement learning, such as\nits reward-driven adaptability, state representations, temporal structures and\ngeneralizability. In this survey, we present an overview of recent advancements\nof reinforcement learning and bandits, and discuss how they can be effectively\nemployed to solve speech and natural language processing problems with models\nthat are adaptive, interactive and scalable.",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.LG",
            "cs.SD",
            "eess.AS"
        ],
        "link": "http://arxiv.org/pdf/2210.13623v2",
        "date": "2022-10-24 21:49:12+00:00"
    },
    {
        "title": "Training spiking neural networks using reinforcement learning",
        "authors": [
            "Sneha Aenugu"
        ],
        "abstract": "Neurons in the brain communicate with each other through discrete action\nspikes as opposed to continuous signal transmission in artificial neural\nnetworks. Therefore, the traditional techniques for optimization of parameters\nin neural networks which rely on the assumption of differentiability of\nactivation functions are no longer applicable to modeling the learning\nprocesses in the brain. In this project, we propose biologically-plausible\nalternatives to backpropagation to facilitate the training of spiking neural\nnetworks. We primarily focus on investigating the candidacy of reinforcement\nlearning (RL) rules in solving the spatial and temporal credit assignment\nproblems to enable decision-making in complex tasks. In one approach, we\nconsider each neuron in a multi-layer neural network as an independent RL agent\nforming a different representation of the feature space while the network as a\nwhole forms the representation of the complex policy to solve the task at hand.\nIn other approach, we apply the reparameterization trick to enable\ndifferentiation through stochastic transformations in spiking neural networks.\nWe compare and contrast the two approaches by applying them to traditional RL\ndomains such as gridworld, cartpole and mountain car. Further we also suggest\nvariations and enhancements to enable future research in this area.",
        "categories": [
            "cs.LG",
            "cs.NE",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2005.05941v1",
        "date": "2020-05-12 17:40:36+00:00"
    },
    {
        "title": "Solving Bilevel Knapsack Problem using Graph Neural Networks",
        "authors": [
            "Sunhyeon Kwon",
            "Hwayong Choi",
            "Sungsoo Park"
        ],
        "abstract": "The Bilevel Optimization Problem is a hierarchical optimization problem with\ntwo agents, a leader and a follower. The leader make their own decisions first,\nand the followers make the best choices accordingly. The leader knows the\ninformation of the followers, and the goal of the problem is to find the\noptimal solution by considering the reactions of the followers from the\nleader's point of view. For the Bilevel Optimization Problem, there are no\ngeneral and efficient algorithms or commercial solvers to get an optimal\nsolution, and it is very difficult to get a good solution even for a simple\nproblem. In this paper, we propose a deep learning approach using Graph Neural\nNetworks to solve the bilevel knapsack problem. We train the model to predict\nthe leader's solution and use it to transform the hierarchical optimization\nproblem into a single-level optimization problem to get the solution. Our model\nfound the feasible solution that was about 500 times faster than the exact\nalgorithm with $1.7\\%$ optimal gap. Also, our model performed well on problems\nof different size from the size it was trained on.",
        "categories": [
            "cs.AI",
            "cs.DM",
            "cs.LG",
            "math.OC",
            "68T07",
            "F.2.2"
        ],
        "link": "http://arxiv.org/pdf/2211.13436v2",
        "date": "2022-11-24 06:36:45+00:00"
    },
    {
        "title": "DeepPhaseCut: Deep Relaxation in Phase for Unsupervised Fourier Phase Retrieval",
        "authors": [
            "Eunju Cha",
            "Chanseok Lee",
            "Mooseok Jang",
            "Jong Chul Ye"
        ],
        "abstract": "Fourier phase retrieval is a classical problem of restoring a signal only\nfrom the measured magnitude of its Fourier transform. Although Fienup-type\nalgorithms, which use prior knowledge in both spatial and Fourier domains, have\nbeen widely used in practice, they can often stall in local minima. Modern\nmethods such as PhaseLift and PhaseCut may offer performance guarantees with\nthe help of convex relaxation. However, these algorithms are usually\ncomputationally intensive for practical use. To address this problem, we\npropose a novel, unsupervised, feed-forward neural network for Fourier phase\nretrieval which enables immediate high quality reconstruction. Unlike the\nexisting deep learning approaches that use a neural network as a regularization\nterm or an end-to-end blackbox model for supervised training, our algorithm is\na feed-forward neural network implementation of PhaseCut algorithm in an\nunsupervised learning framework. Specifically, our network is composed of two\ngenerators: one for the phase estimation using PhaseCut loss, followed by\nanother generator for image reconstruction, all of which are trained\nsimultaneously using a cycleGAN framework without matched data. The link to the\nclassical Fienup-type algorithms and the recent symmetry-breaking learning\napproach is also revealed. Extensive experiments demonstrate that the proposed\nmethod outperforms all existing approaches in Fourier phase retrieval problems.",
        "categories": [
            "cs.CV",
            "cs.LG",
            "eess.IV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2011.10475v1",
        "date": "2020-11-20 16:10:08+00:00"
    },
    {
        "title": "Layer Adaptive Deep Neural Networks for Out-of-distribution Detection",
        "authors": [
            "Haoliang Wang",
            "Chen Zhao",
            "Xujiang Zhao",
            "Feng Chen"
        ],
        "abstract": "During the forward pass of Deep Neural Networks (DNNs), inputs gradually\ntransformed from low-level features to high-level conceptual labels. While\nfeatures at different layers could summarize the important factors of the\ninputs at varying levels, modern out-of-distribution (OOD) detection methods\nmostly focus on utilizing their ending layer features. In this paper, we\nproposed a novel layer-adaptive OOD detection framework (LA-OOD) for DNNs that\ncan fully utilize the intermediate layers' outputs. Specifically, instead of\ntraining a unified OOD detector at a fixed ending layer, we train multiple\nOne-Class SVM OOD detectors simultaneously at the intermediate layers to\nexploit the full spectrum characteristics encoded at varying depths of DNNs. We\ndevelop a simple yet effective layer-adaptive policy to identify the best layer\nfor detecting each potential OOD example. LA-OOD can be applied to any existing\nDNNs and does not require access to OOD samples during the training. Using\nthree DNNs of varying depth and architectures, our experiments demonstrate that\nLA-OOD is robust against OODs of varying complexity and can outperform\nstate-of-the-art competitors by a large margin on some real-world datasets.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2203.00192v1",
        "date": "2022-03-01 02:47:33+00:00"
    },
    {
        "title": "OpenMix: Exploring Outlier Samples for Misclassification Detection",
        "authors": [
            "Fei Zhu",
            "Zhen Cheng",
            "Xu-Yao Zhang",
            "Cheng-Lin Liu"
        ],
        "abstract": "Reliable confidence estimation for deep neural classifiers is a challenging\nyet fundamental requirement in high-stakes applications. Unfortunately, modern\ndeep neural networks are often overconfident for their erroneous predictions.\nIn this work, we exploit the easily available outlier samples, i.e., unlabeled\nsamples coming from non-target classes, for helping detect misclassification\nerrors. Particularly, we find that the well-known Outlier Exposure, which is\npowerful in detecting out-of-distribution (OOD) samples from unknown classes,\ndoes not provide any gain in identifying misclassification errors. Based on\nthese observations, we propose a novel method called OpenMix, which\nincorporates open-world knowledge by learning to reject uncertain\npseudo-samples generated via outlier transformation. OpenMix significantly\nimproves confidence reliability under various scenarios, establishing a strong\nand unified framework for detecting both misclassified samples from known\nclasses and OOD samples from unknown classes. The code is publicly available at\nhttps://github.com/Impression2805/OpenMix.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2303.17093v1",
        "date": "2023-03-30 01:47:23+00:00"
    },
    {
        "title": "Transformers discover an elementary calculation system exploiting local attention and grid-like problem representation",
        "authors": [
            "Samuel Cognolato",
            "Alberto Testolin"
        ],
        "abstract": "Mathematical reasoning is one of the most impressive achievements of human\nintellect but remains a formidable challenge for artificial intelligence\nsystems. In this work we explore whether modern deep learning architectures can\nlearn to solve a symbolic addition task by discovering effective arithmetic\nprocedures. Although the problem might seem trivial at first glance,\ngeneralizing arithmetic knowledge to operations involving a higher number of\nterms, possibly composed by longer sequences of digits, has proven extremely\nchallenging for neural networks. Here we show that universal transformers\nequipped with local attention and adaptive halting mechanisms can learn to\nexploit an external, grid-like memory to carry out multi-digit addition. The\nproposed model achieves remarkable accuracy even when tested with problems\nrequiring extrapolation outside the training distribution; most notably, it\ndoes so by discovering human-like calculation strategies such as place value\nalignment.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2207.02536v1",
        "date": "2022-07-06 09:29:56+00:00"
    },
    {
        "title": "Generative Autoencoder Kernels on Deep Learning for Brain Activity Analysis",
        "authors": [
            "Gokhan Altan",
            "Yakup Kutlu"
        ],
        "abstract": "Deep Learning (DL) is a two-step classification model that consists feature\nlearning, generating feature representations using unsupervised ways and the\nsupervised learning stage at the last step of model using at least two hidden\nlayers on the proposed structures by fully connected layers depending on of the\nartificial neural networks. The optimization of the predefined classification\nparameters for the supervised models eases reaching the global optimality with\nexact zero training error. The autoencoder (AE) models are the highly\ngeneralized ways of the unsupervised stages for the DL to define the output\nweights of the hidden neurons with various representations. As alternatively to\nthe conventional Extreme Learning Machines (ELM) AE, Hessenberg\ndecomposition-based ELM autoencoder (HessELM-AE) is a novel kernel to generate\ndifferent presentations of the input data within the intended sizes of the\nmodels. The aim of the study is analyzing the performance of the novel Deep AE\nkernel for clinical availability on electroencephalogram (EEG) with stroke\npatients. The slow cortical potentials (SCP) training in stroke patients during\neight neurofeedback sessions were analyzed using Hilbert-Huang Transform. The\nstatistical features of different frequency modulations were fed into the Deep\nELM model for generative AE kernels. The novel Deep ELM-AE kernels have\ndiscriminated the brain activity with high classification performances for\npositivity and negativity tasks in stroke patients.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2101.10263v1",
        "date": "2021-01-21 08:19:47+00:00"
    },
    {
        "title": "Towards Consistent Predictive Confidence through Fitted Ensembles",
        "authors": [
            "Navid Kardan",
            "Ankit Sharma",
            "Kenneth O. Stanley"
        ],
        "abstract": "Deep neural networks are behind many of the recent successes in machine\nlearning applications. However, these models can produce overconfident\ndecisions while encountering out-of-distribution (OOD) examples or making a\nwrong prediction. This inconsistent predictive confidence limits the\nintegration of independently-trained learning models into a larger system. This\npaper introduces separable concept learning framework to realistically measure\nthe performance of classifiers in presence of OOD examples. In this setup,\nseveral instances of a classifier are trained on different parts of a partition\nof the set of classes. Later, the performance of the combination of these\nmodels is evaluated on a separate test set. Unlike current OOD detection\ntechniques, this framework does not require auxiliary OOD datasets and does not\nseparate classification from detection performance. Furthermore, we present a\nnew strong baseline for more consistent predictive confidence in deep models,\ncalled fitted ensembles, where overconfident predictions are rectified by\ntransformed versions of the original classification task. Fitted ensembles can\nnaturally detect OOD examples without requiring auxiliary data by observing\ncontradicting predictions among its components. Experiments on MNIST, SVHN,\nCIFAR-10/100, and ImageNet show fitted ensemble significantly outperform\nconventional ensembles on OOD examples and are possible to scale.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "link": "http://arxiv.org/pdf/2106.12070v1",
        "date": "2021-06-22 21:32:31+00:00"
    },
    {
        "title": "Do CNNs Encode Data Augmentations?",
        "authors": [
            "Eddie Yan",
            "Yanping Huang"
        ],
        "abstract": "Data augmentations are important ingredients in the recipe for training\nrobust neural networks, especially in computer vision. A fundamental question\nis whether neural network features encode data augmentation transformations. To\nanswer this question, we introduce a systematic approach to investigate which\nlayers of neural networks are the most predictive of augmentation\ntransformations. Our approach uses features in pre-trained vision models with\nminimal additional processing to predict common properties transformed by\naugmentation (scale, aspect ratio, hue, saturation, contrast, and brightness).\nSurprisingly, neural network features not only predict data augmentation\ntransformations, but they predict many transformations with high accuracy.\nAfter validating that neural networks encode features corresponding to\naugmentation transformations, we show that these features are encoded in the\nearly layers of modern CNNs, though the augmentation signal fades in deeper\nlayers.",
        "categories": [
            "cs.CV",
            "cs.LG",
            "eess.IV",
            "stat.ML",
            "68T45"
        ],
        "link": "http://arxiv.org/pdf/2003.08773v3",
        "date": "2020-02-29 00:42:23+00:00"
    },
    {
        "title": "Learning to Control using Image Feedback",
        "authors": [
            "Krishnan Raghavan",
            "Vignesh Narayanan",
            "Jagannathan Saraangapani"
        ],
        "abstract": "Learning to control complex systems using non-traditional feedback, e.g., in\nthe form of snapshot images, is an important task encountered in diverse\ndomains such as robotics, neuroscience, and biology (cellular systems). In this\npaper, we present a two neural-network (NN)-based feedback control framework to\ndesign control policies for systems that generate feedback in the form of\nimages. In particular, we develop a deep $Q$-network (DQN)-driven learning\ncontrol strategy to synthesize a sequence of control inputs from snapshot\nimages that encode the information pertaining to the current state and control\naction of the system. Further, to train the networks we employ a direct\nerror-driven learning (EDL) approach that utilizes a set of linear\ntransformations of the NN training error to update the NN weights in each\nlayer. We verify the efficacy of the proposed control strategy using numerical\nexamples.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.SY",
            "eess.SY"
        ],
        "link": "http://arxiv.org/pdf/2110.15290v1",
        "date": "2021-10-28 16:52:18+00:00"
    },
    {
        "title": "Unsupervised Domain Adaptation: from Simulation Engine to the RealWorld",
        "authors": [
            "Sicheng Zhao",
            "Bichen Wu",
            "Joseph Gonzalez",
            "Sanjit A. Seshia",
            "Kurt Keutzer"
        ],
        "abstract": "Large-scale labeled training datasets have enabled deep neural networks to\nexcel on a wide range of benchmark vision tasks. However, in many applications\nit is prohibitively expensive or time-consuming to obtain large quantities of\nlabeled data. To cope with limited labeled training data, many have attempted\nto directly apply models trained on a large-scale labeled source domain to\nanother sparsely labeled target domain. Unfortunately, direct transfer across\ndomains often performs poorly due to domain shift and dataset bias. Domain\nadaptation is the machine learning paradigm that aims to learn a model from a\nsource domain that can perform well on a different (but related) target domain.\nIn this paper, we summarize and compare the latest unsupervised domain\nadaptation methods in computer vision applications. We classify the non-deep\napproaches into sample re-weighting and intermediate subspace transformation\ncategories, while the deep strategy includes discrepancy-based methods,\nadversarial generative models, adversarial discriminative models and\nreconstruction-based methods. We also discuss some potential directions.",
        "categories": [
            "cs.CV",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1803.09180v1",
        "date": "2018-03-24 23:34:06+00:00"
    },
    {
        "title": "Conditional Mutual information-based Contrastive Loss for Financial Time Series Forecasting",
        "authors": [
            "Hanwei Wu",
            "Ather Gattami",
            "Markus Flierl"
        ],
        "abstract": "We present a representation learning framework for financial time series\nforecasting. One challenge of using deep learning models for finance\nforecasting is the shortage of available training data when using small\ndatasets. Direct trend classification using deep neural networks trained on\nsmall datasets is susceptible to the overfitting problem. In this paper, we\npropose to first learn compact representations from time series data, then use\nthe learned representations to train a simpler model for predicting time series\nmovements. We consider a class-conditioned latent variable model. We train an\nencoder network to maximize the mutual information between the latent variables\nand the trend information conditioned on the encoded observed variables. We\nshow that conditional mutual information maximization can be approximated by a\ncontrastive loss. Then, the problem is transformed into a classification task\nof determining whether two encoded representations are sampled from the same\nclass or not. This is equivalent to performing pairwise comparisons of the\ntraining datapoints, and thus, improves the generalization ability of the\nencoder network. We use deep autoregressive models as our encoder to capture\nlong-term dependencies of the sequence data. Empirical experiments indicate\nthat our proposed method has the potential to advance state-of-the-art\nperformance.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2002.07638v3",
        "date": "2020-02-18 15:24:33+00:00"
    },
    {
        "title": "Cross-Layer Strategic Ensemble Defense Against Adversarial Examples",
        "authors": [
            "Wenqi Wei",
            "Ling Liu",
            "Margaret Loper",
            "Ka-Ho Chow",
            "Emre Gursoy",
            "Stacey Truex",
            "Yanzhao Wu"
        ],
        "abstract": "Deep neural network (DNN) has demonstrated its success in multiple domains.\nHowever, DNN models are inherently vulnerable to adversarial examples, which\nare generated by adding adversarial perturbations to benign inputs to fool the\nDNN model to misclassify. In this paper, we present a cross-layer strategic\nensemble framework and a suite of robust defense algorithms, which are\nattack-independent, and capable of auto-repairing and auto-verifying the target\nmodel being attacked. Our strategic ensemble approach makes three original\ncontributions. First, we employ input-transformation diversity to design the\ninput-layer strategic transformation ensemble algorithms. Second, we utilize\nmodel-disagreement diversity to develop the output-layer strategic model\nensemble algorithms. Finally, we create an input-output cross-layer strategic\nensemble defense that strengthens the defensibility by combining diverse input\ntransformation based model ensembles with diverse output verification model\nensembles. Evaluated over 10 attacks on ImageNet dataset, we show that our\nstrategic ensemble defense algorithms can achieve high defense success rates\nand are more robust with high attack prevention success rates and low benign\nfalse negative rates, compared to existing representative defense methods.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1910.01742v1",
        "date": "2019-10-01 07:57:42+00:00"
    },
    {
        "title": "The Manifold Scattering Transform for High-Dimensional Point Cloud Data",
        "authors": [
            "Joyce Chew",
            "Holly R. Steach",
            "Siddharth Viswanath",
            "Hau-Tieng Wu",
            "Matthew Hirn",
            "Deanna Needell",
            "Smita Krishnaswamy",
            "Michael Perlmutter"
        ],
        "abstract": "The manifold scattering transform is a deep feature extractor for data\ndefined on a Riemannian manifold. It is one of the first examples of extending\nconvolutional neural network-like operators to general manifolds. The initial\nwork on this model focused primarily on its theoretical stability and\ninvariance properties but did not provide methods for its numerical\nimplementation except in the case of two-dimensional surfaces with predefined\nmeshes. In this work, we present practical schemes, based on the theory of\ndiffusion maps, for implementing the manifold scattering transform to datasets\narising in naturalistic systems, such as single cell genetics, where the data\nis a high-dimensional point cloud modeled as lying on a low-dimensional\nmanifold. We show that our methods are effective for signal classification and\nmanifold classification tasks.",
        "categories": [
            "cs.LG",
            "cs.NA",
            "eess.SP",
            "math.NA",
            "stat.ML",
            "68T07",
            "I.2.6"
        ],
        "link": "http://arxiv.org/pdf/2206.10078v1",
        "date": "2022-06-21 02:15:00+00:00"
    },
    {
        "title": "Beyond Transformers for Function Learning",
        "authors": [
            "Simon Segert",
            "Jonathan Cohen"
        ],
        "abstract": "The ability to learn and predict simple functions is a key aspect of human\nintelligence. Recent works have started to explore this ability using\ntransformer architectures, however it remains unclear whether this is\nsufficient to recapitulate the extrapolation abilities of people in this\ndomain. Here, we propose to address this gap by augmenting the transformer\narchitecture with two simple inductive learning biases, that are directly\nadapted from recent models of abstract reasoning in cognitive science. The\nresults we report demonstrate that these biases are helpful in the context of\nlarge neural network models, as well as shed light on the types of inductive\nlearning biases that may contribute to human abilities in extrapolation.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2304.09979v1",
        "date": "2023-04-19 21:33:06+00:00"
    },
    {
        "title": "Data-driven discoveries of B\u00e4cklund transforms and soliton evolution equations via deep neural network learning schemes",
        "authors": [
            "Zijian Zhou",
            "Li Wang",
            "Weifang Weng",
            "Zhenya Yan"
        ],
        "abstract": "We introduce a deep neural network learning scheme to learn the B\\\"acklund\ntransforms (BTs) of soliton evolution equations and an enhanced deep learning\nscheme for data-driven soliton equation discovery based on the known BTs,\nrespectively. The first scheme takes advantage of some solution (or soliton\nequation) information to study the data-driven BT of sine-Gordon equation, and\ncomplex and real Miura transforms between the defocusing (focusing) mKdV\nequation and KdV equation, as well as the data-driven mKdV equation discovery\nvia the Miura transforms. The second deep learning scheme uses the\nexplicit/implicit BTs generating the higher-order solitons to train the\ndata-driven discovery of mKdV and sine-Gordon equations, in which the\nhigh-order solution informations are more powerful for the enhanced leaning\nsoliton equations with higher accurates.",
        "categories": [
            "cs.LG",
            "math.AP",
            "nlin.PS",
            "nlin.SI"
        ],
        "link": "http://arxiv.org/pdf/2111.09489v2",
        "date": "2021-11-18 02:55:58+00:00"
    },
    {
        "title": "Co-Attentive Equivariant Neural Networks: Focusing Equivariance On Transformations Co-Occurring In Data",
        "authors": [
            "David W. Romero",
            "Mark Hoogendoorn"
        ],
        "abstract": "Equivariance is a nice property to have as it produces much more parameter\nefficient neural architectures and preserves the structure of the input through\nthe feature mapping. Even though some combinations of transformations might\nnever appear (e.g. an upright face with a horizontal nose), current equivariant\narchitectures consider the set of all possible transformations in a\ntransformation group when learning feature representations. Contrarily, the\nhuman visual system is able to attend to the set of relevant transformations\noccurring in the environment and utilizes this information to assist and\nimprove object recognition. Based on this observation, we modify conventional\nequivariant feature mappings such that they are able to attend to the set of\nco-occurring transformations in data and generalize this notion to act on\ngroups consisting of multiple symmetries. We show that our proposed\nco-attentive equivariant neural networks consistently outperform conventional\nrotation equivariant and rotation & reflection equivariant neural networks on\nrotated MNIST and CIFAR-10.",
        "categories": [
            "cs.CV",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1911.07849v2",
        "date": "2019-11-18 12:41:12+00:00"
    },
    {
        "title": "Unsupervised Sparse-view Backprojection via Convolutional and Spatial Transformer Networks",
        "authors": [
            "Xueqing Liu",
            "Paul Sajda"
        ],
        "abstract": "Many imaging technologies rely on tomographic reconstruction, which requires\nsolving a multidimensional inverse problem given a finite number of\nprojections. Backprojection is a popular class of algorithm for tomographic\nreconstruction, however it typically results in poor image reconstructions when\nthe projection angles are sparse and/or if the sensors characteristics are not\nuniform. Several deep learning based algorithms have been developed to solve\nthis inverse problem and reconstruct the image using a limited number of\nprojections. However these algorithms typically require examples of the\nground-truth (i.e. examples of reconstructed images) to yield good performance.\nIn this paper, we introduce an unsupervised sparse-view backprojection\nalgorithm, which does not require ground-truth. The algorithm consists of two\nmodules in a generator-projector framework; a convolutional neural network and\na spatial transformer network. We evaluated our algorithm using computed\ntomography (CT) images of the human chest. We show that our algorithm\nsignificantly out-performs filtered backprojection when the projection angles\nare very sparse, as well as when the sensor characteristics vary for different\nangles. Our approach has practical applications for medical imaging and other\nimaging modalities (e.g. radar) where sparse and/or non-uniform projections may\nbe acquired due to time or sampling constraints.",
        "categories": [
            "eess.IV",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2006.01658v1",
        "date": "2020-06-01 05:02:53+00:00"
    },
    {
        "title": "Predicting Deep Neural Network Generalization with Perturbation Response Curves",
        "authors": [
            "Yair Schiff",
            "Brian Quanz",
            "Payel Das",
            "Pin-Yu Chen"
        ],
        "abstract": "The field of Deep Learning is rich with empirical evidence of human-like\nperformance on a variety of prediction tasks. However, despite these successes,\nthe recent Predicting Generalization in Deep Learning (PGDL) NeurIPS 2020\ncompetition suggests that there is a need for more robust and efficient\nmeasures of network generalization. In this work, we propose a new framework\nfor evaluating the generalization capabilities of trained networks. We use\nperturbation response (PR) curves that capture the accuracy change of a given\nnetwork as a function of varying levels of training sample perturbation. From\nthese PR curves, we derive novel statistics that capture generalization\ncapability. Specifically, we introduce two new measures for accurately\npredicting generalization gaps: the Gi-score and Pal-score, which are inspired\nby the Gini coefficient and Palma ratio (measures of income inequality), that\naccurately predict generalization gaps. Using our framework applied to intra\nand inter-class sample mixup, we attain better predictive scores than the\ncurrent state-of-the-art measures on a majority of tasks in the PGDL\ncompetition. In addition, we show that our framework and the proposed\nstatistics can be used to capture to what extent a trained network is invariant\nto a given parametric input transformation, such as rotation or translation.\nTherefore, these generalization gap prediction statistics also provide a useful\nmeans for selecting optimal network architectures and hyperparameters that are\ninvariant to a certain perturbation.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2106.04765v2",
        "date": "2021-06-09 01:37:36+00:00"
    },
    {
        "title": "Transfer Deep Reinforcement Learning-enabled Energy Management Strategy for Hybrid Tracked Vehicle",
        "authors": [
            "Xiaowei Guo",
            "Teng Liu",
            "Bangbei Tang",
            "Xiaolin Tang",
            "Jinwei Zhang",
            "Wenhao Tan",
            "Shufeng Jin"
        ],
        "abstract": "This paper proposes an adaptive energy management strategy for hybrid\nelectric vehicles by combining deep reinforcement learning (DRL) and transfer\nlearning (TL). This work aims to address the defect of DRL in tedious training\ntime. First, an optimization control modeling of a hybrid tracked vehicle is\nbuilt, wherein the elaborate powertrain components are introduced. Then, a\nbi-level control framework is constructed to derive the energy management\nstrategies (EMSs). The upper-level is applying the particular deep\ndeterministic policy gradient (DDPG) algorithms for EMS training at different\nspeed intervals. The lower-level is employing the TL method to transform the\npre-trained neural networks for a novel driving cycle. Finally, a series of\nexperiments are executed to prove the effectiveness of the presented control\nframework. The optimality and adaptability of the formulated EMS are\nilluminated. The founded DRL and TL-enabled control policy is capable of\nenhancing energy efficiency and improving system performance.",
        "categories": [
            "eess.SP",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2007.08690v1",
        "date": "2020-07-16 23:39:34+00:00"
    },
    {
        "title": "An Interactive Interpretability System for Breast Cancer Screening with Deep Learning",
        "authors": [
            "Yuzhe Lu",
            "Adam Perer"
        ],
        "abstract": "Deep learning methods, in particular convolutional neural networks, have\nemerged as a powerful tool in medical image computing tasks. While these\ncomplex models provide excellent performance, their black-box nature may hinder\nreal-world adoption in high-stakes decision-making. In this paper, we propose\nan interactive system to take advantage of state-of-the-art interpretability\ntechniques to assist radiologists with breast cancer screening. Our system\nintegrates a deep learning model into the radiologists' workflow and provides\nnovel interactions to promote understanding of the model's decision-making\nprocess. Moreover, we demonstrate that our system can take advantage of user\ninteractions progressively to provide finer-grained explainability reports with\nlittle labeling overhead. Due to the generic nature of the adopted\ninterpretability technique, our system is domain-agnostic and can be used for\nmany different medical image computing tasks, presenting a novel perspective on\nhow we can leverage visual analytics to transform originally static\ninterpretability techniques to augment human decision making and promote the\nadoption of medical AI.",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV",
            "cs.HC",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2210.08979v1",
        "date": "2022-09-30 02:19:49+00:00"
    },
    {
        "title": "Agglomerative Attention",
        "authors": [
            "Matthew Spellings"
        ],
        "abstract": "Neural networks using transformer-based architectures have recently\ndemonstrated great power and flexibility in modeling sequences of many types.\nOne of the core components of transformer networks is the attention layer,\nwhich allows contextual information to be exchanged among sequence elements.\nWhile many of the prevalent network structures thus far have utilized full\nattention -- which operates on all pairs of sequence elements -- the quadratic\nscaling of this attention mechanism significantly constrains the size of models\nthat can be trained. In this work, we present an attention model that has only\nlinear requirements in memory and computation time. We show that, despite the\nsimpler attention model, networks using this attention mechanism can attain\ncomparable performance to full attention networks on language modeling tasks.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1907.06607v1",
        "date": "2019-07-15 17:11:05+00:00"
    },
    {
        "title": "gvnn: Neural Network Library for Geometric Computer Vision",
        "authors": [
            "Ankur Handa",
            "Michael Bloesch",
            "Viorica Patraucean",
            "Simon Stent",
            "John McCormac",
            "Andrew Davison"
        ],
        "abstract": "We introduce gvnn, a neural network library in Torch aimed towards bridging\nthe gap between classic geometric computer vision and deep learning. Inspired\nby the recent success of Spatial Transformer Networks, we propose several new\nlayers which are often used as parametric transformations on the data in\ngeometric computer vision. These layers can be inserted within a neural network\nmuch in the spirit of the original spatial transformers and allow\nbackpropagation to enable end-to-end learning of a network involving any domain\nknowledge in geometric computer vision. This opens up applications in learning\ninvariance to 3D geometric transformation for place recognition, end-to-end\nvisual odometry, depth estimation and unsupervised learning through warping\nwith a parametric transformation for image reconstruction error.",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1607.07405v3",
        "date": "2016-07-25 18:57:17+00:00"
    },
    {
        "title": "Structural Optimization Makes Graph Classification Simpler and Better",
        "authors": [
            "Junran Wu",
            "Jianhao Li",
            "Yicheng Pan",
            "Ke Xu"
        ],
        "abstract": "In deep neural networks, better results can often be obtained by increasing\nthe complexity of previously developed basic models. However, it is unclear\nwhether there is a way to boost performance by decreasing the complexity of\nsuch models. Here, based on an optimization method, we investigate the\nfeasibility of improving graph classification performance while simplifying the\nmodel learning process. Inspired by progress in structural information\nassessment, we optimize the given data sample from graphs to encoding trees. In\nparticular, we minimize the structural entropy of the transformed encoding tree\nto decode the key structure underlying a graph. This transformation is denoted\nas structural optimization. Furthermore, we propose a novel feature combination\nscheme, termed hierarchical reporting, for encoding trees. In this scheme,\nfeatures are transferred from leaf nodes to root nodes by following the\nhierarchical structures of encoding trees. We then present an implementation of\nthe scheme in a tree kernel and a convolutional network to perform graph\nclassification. The tree kernel follows label propagation in the\nWeisfeiler-Lehman (WL) subtree kernel, but it has a lower runtime complexity\n$O(n)$. The convolutional network is a special implementation of our tree\nkernel in the deep learning field and is called Encoding Tree Learning (ETL).\nWe empirically validate our tree kernel and convolutional network with several\ngraph classification benchmarks and demonstrate that our methods achieve better\nperformance and lower computational consumption than competing approaches.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2109.02027v1",
        "date": "2021-09-05 08:54:38+00:00"
    },
    {
        "title": "A writer-independent approach for offline signature verification using deep convolutional neural networks features",
        "authors": [
            "Victor L. F. Souza",
            "Adriano L. I. Oliveira",
            "Robert Sabourin"
        ],
        "abstract": "The use of features extracted using a deep convolutional neural network (CNN)\ncombined with a writer-dependent (WD) SVM classifier resulted in significant\nimprovement in performance of handwritten signature verification (HSV) when\ncompared to the previous state-of-the-art methods. In this work it is\ninvestigated whether the use of these CNN features provide good results in a\nwriter-independent (WI) HSV context, based on the dichotomy transformation\ncombined with the use of an SVM writer-independent classifier. The experiments\nperformed in the Brazilian and GPDS datasets show that (i) the proposed\napproach outperformed other WI-HSV methods from the literature, (ii) in the\nglobal threshold scenario, the proposed approach was able to outperform the\nwriter-dependent method with CNN features in the Brazilian dataset, (iii) in an\nuser threshold scenario, the results are similar to those obtained by the\nwriter-dependent method with CNN features.",
        "categories": [
            "cs.CV",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1807.10755v1",
        "date": "2018-07-26 22:09:45+00:00"
    },
    {
        "title": "Deep Transform: Cocktail Party Source Separation via Probabilistic Re-Synthesis",
        "authors": [
            "Andrew J. R. Simpson"
        ],
        "abstract": "In cocktail party listening scenarios, the human brain is able to separate\ncompeting speech signals. However, the signal processing implemented by the\nbrain to perform cocktail party listening is not well understood. Here, we\ntrained two separate convolutive autoencoder deep neural networks (DNN) to\nseparate monaural and binaural mixtures of two concurrent speech streams. We\nthen used these DNNs as convolutive deep transform (CDT) devices to perform\nprobabilistic re-synthesis. The CDTs operated directly in the time-domain. Our\nsimulations demonstrate that very simple neural networks are capable of\nexploiting monaural and binaural information available in a cocktail party\nlistening scenario.",
        "categories": [
            "cs.SD",
            "cs.LG",
            "cs.NE",
            "68Txx"
        ],
        "link": "http://arxiv.org/pdf/1503.06046v1",
        "date": "2015-03-20 12:00:44+00:00"
    },
    {
        "title": "Interpretable Feature Recommendation for Signal Analytics",
        "authors": [
            "Snehasis Banerjee",
            "Tanushyam Chattopadhyay",
            "Ayan Mukherjee"
        ],
        "abstract": "This paper presents an automated approach for interpretable feature\nrecommendation for solving signal data analytics problems. The method has been\ntested by performing experiments on datasets in the domain of prognostics where\ninterpretation of features is considered very important. The proposed approach\nis based on Wide Learning architecture and provides means for interpretation of\nthe recommended features. It is to be noted that such an interpretation is not\navailable with feature learning approaches like Deep Learning (such as\nConvolutional Neural Network) or feature transformation approaches like\nPrincipal Component Analysis. Results show that the feature recommendation and\ninterpretation techniques are quite effective for the problems at hand in terms\nof performance and drastic reduction in time to develop a solution. It is\nfurther shown by an example, how this human-in-loop interpretation system can\nbe used as a prescriptive system.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1711.01870v1",
        "date": "2017-11-06 13:03:37+00:00"
    },
    {
        "title": "GraphLIME: Local Interpretable Model Explanations for Graph Neural Networks",
        "authors": [
            "Qiang Huang",
            "Makoto Yamada",
            "Yuan Tian",
            "Dinesh Singh",
            "Dawei Yin",
            "Yi Chang"
        ],
        "abstract": "Graph structured data has wide applicability in various domains such as\nphysics, chemistry, biology, computer vision, and social networks, to name a\nfew. Recently, graph neural networks (GNN) were shown to be successful in\neffectively representing graph structured data because of their good\nperformance and generalization ability. GNN is a deep learning based method\nthat learns a node representation by combining specific nodes and the\nstructural/topological information of a graph. However, like other deep models,\nexplaining the effectiveness of GNN models is a challenging task because of the\ncomplex nonlinear transformations made over the iterations. In this paper, we\npropose GraphLIME, a local interpretable model explanation for graphs using the\nHilbert-Schmidt Independence Criterion (HSIC) Lasso, which is a nonlinear\nfeature selection method. GraphLIME is a generic GNN-model explanation\nframework that learns a nonlinear interpretable model locally in the subgraph\nof the node being explained. More specifically, to explain a node, we generate\na nonlinear interpretable model from its $N$-hop neighborhood and then compute\nthe K most representative features as the explanations of its prediction using\nHSIC Lasso. Through experiments on two real-world datasets, the explanations of\nGraphLIME are found to be of extraordinary degree and more descriptive in\ncomparison to the existing explanation methods.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2001.06216v2",
        "date": "2020-01-17 09:50:28+00:00"
    },
    {
        "title": "Fisher Information and Natural Gradient Learning of Random Deep Networks",
        "authors": [
            "Shun-ichi Amari",
            "Ryo Karakida",
            "Masafumi Oizumi"
        ],
        "abstract": "A deep neural network is a hierarchical nonlinear model transforming input\nsignals to output signals. Its input-output relation is considered to be\nstochastic, being described for a given input by a parameterized conditional\nprobability distribution of outputs. The space of parameters consisting of\nweights and biases is a Riemannian manifold, where the metric is defined by the\nFisher information matrix. The natural gradient method uses the steepest\ndescent direction in a Riemannian manifold, so it is effective in learning,\navoiding plateaus. It requires inversion of the Fisher information matrix,\nhowever, which is practically impossible when the matrix has a huge number of\ndimensions. Many methods for approximating the natural gradient have therefore\nbeen introduced. The present paper uses statistical neurodynamical method to\nreveal the properties of the Fisher information matrix in a net of random\nconnections under the mean field approximation. We prove that the Fisher\ninformation matrix is unit-wise block diagonal supplemented by small order\nterms of off-block-diagonal elements, which provides a justification for the\nquasi-diagonal natural gradient method by Y. Ollivier. A unitwise\nblock-diagonal Fisher metrix reduces to the tensor product of the Fisher\ninformation matrices of single units. We further prove that the Fisher\ninformation matrix of a single unit has a simple reduced form, a sum of a\ndiagonal matrix and a rank 2 matrix of weight-bias correlations. We obtain the\ninverse of Fisher information explicitly. We then have an explicit form of the\nnatural gradient, without relying on the numerical matrix inversion, which\ndrastically speeds up stochastic gradient learning.",
        "categories": [
            "cs.LG",
            "cond-mat.dis-nn",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1808.07172v1",
        "date": "2018-08-22 01:04:07+00:00"
    },
    {
        "title": "Semantic Representation and Inference for NLP",
        "authors": [
            "Dongsheng Wang"
        ],
        "abstract": "Semantic representation and inference is essential for Natural Language\nProcessing (NLP). The state of the art for semantic representation and\ninference is deep learning, and particularly Recurrent Neural Networks (RNNs),\nConvolutional Neural Networks (CNNs), and transformer Self-Attention models.\nThis thesis investigates the use of deep learning for novel semantic\nrepresentation and inference, and makes contributions in the following three\nareas: creating training data, improving semantic representations and extending\ninference learning. In terms of creating training data, we contribute the\nlargest publicly available dataset of real-life factual claims for the purpose\nof automatic claim verification (MultiFC), and we present a novel inference\nmodel composed of multi-scale CNNs with different kernel sizes that learn from\nexternal sources to infer fact checking labels. In terms of improving semantic\nrepresentations, we contribute a novel model that captures non-compositional\nsemantic indicators. By definition, the meaning of a non-compositional phrase\ncannot be inferred from the individual meanings of its composing words (e.g.,\nhot dog). Motivated by this, we operationalize the compositionality of a phrase\ncontextually by enriching the phrase representation with external word\nembeddings and knowledge graphs. Finally, in terms of inference learning, we\npropose a series of novel deep learning architectures that improve inference by\nusing syntactic dependencies, by ensembling role guided attention heads,\nincorporating gating layers, and concatenating multiple heads in novel and\neffective ways. This thesis consists of seven publications (five published and\ntwo under review).",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2106.08117v1",
        "date": "2021-06-15 13:22:48+00:00"
    },
    {
        "title": "Superpixel Image Classification with Graph Attention Networks",
        "authors": [
            "Pedro H. C. Avelar",
            "Anderson R. Tavares",
            "Thiago L. T. da Silveira",
            "Cl\u00e1udio R. Jung",
            "Lu\u00eds C. Lamb"
        ],
        "abstract": "This paper presents a methodology for image classification using Graph Neural\nNetwork (GNN) models. We transform the input images into region adjacency\ngraphs (RAGs), in which regions are superpixels and edges connect neighboring\nsuperpixels. Our experiments suggest that Graph Attention Networks (GATs),\nwhich combine graph convolutions with self-attention mechanisms, outperforms\nother GNN models. Although raw image classifiers perform better than GATs due\nto information loss during the RAG generation, our methodology opens an\ninteresting avenue of research on deep learning beyond rectangular-gridded\nimages, such as 360-degree field of view panoramas. Traditional convolutional\nkernels of current state-of-the-art methods cannot handle panoramas, whereas\nthe adapted superpixel algorithms and the resulting region adjacency graphs can\nnaturally feed a GNN, without topology issues.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2002.05544v2",
        "date": "2020-02-13 14:52:32+00:00"
    },
    {
        "title": "ReSet: Learning Recurrent Dynamic Routing in ResNet-like Neural Networks",
        "authors": [
            "Iurii Kemaev",
            "Daniil Polykovskiy",
            "Dmitry Vetrov"
        ],
        "abstract": "Neural Network is a powerful Machine Learning tool that shows outstanding\nperformance in Computer Vision, Natural Language Processing, and Artificial\nIntelligence. In particular, recently proposed ResNet architecture and its\nmodifications produce state-of-the-art results in image classification\nproblems. ResNet and most of the previously proposed architectures have a fixed\nstructure and apply the same transformation to all input images. In this work,\nwe develop a ResNet-based model that dynamically selects Computational Units\n(CU) for each input object from a learned set of transformations. Dynamic\nselection allows the network to learn a sequence of useful transformations and\napply only required units to predict the image label. We compare our model to\nResNet-38 architecture and achieve better results than the original ResNet on\nCIFAR-10.1 test set. While examining the produced paths, we discovered that the\nnetwork learned different routes for images from different classes and similar\nroutes for similar images.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1811.04380v1",
        "date": "2018-11-11 09:45:41+00:00"
    },
    {
        "title": "Disrupting Adversarial Transferability in Deep Neural Networks",
        "authors": [
            "Christopher Wiedeman",
            "Ge Wang"
        ],
        "abstract": "Adversarial attack transferability is well-recognized in deep learning. Prior\nwork has partially explained transferability by recognizing common adversarial\nsubspaces and correlations between decision boundaries, but little is known\nbeyond this. We propose that transferability between seemingly different models\nis due to a high linear correlation between the feature sets that different\nnetworks extract. In other words, two models trained on the same task that are\ndistant in the parameter space likely extract features in the same fashion,\njust with trivial affine transformations between the latent spaces.\nFurthermore, we show how applying a feature correlation loss, which\ndecorrelates the extracted features in a latent space, can reduce the\ntransferability of adversarial attacks between models, suggesting that the\nmodels complete tasks in semantically different ways. Finally, we propose a\nDual Neck Autoencoder (DNA), which leverages this feature correlation loss to\ncreate two meaningfully different encodings of input information with reduced\ntransferability.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CR"
        ],
        "link": "http://arxiv.org/pdf/2108.12492v3",
        "date": "2021-08-27 20:27:38+00:00"
    },
    {
        "title": "An adversarial learning framework for preserving users' anonymity in face-based emotion recognition",
        "authors": [
            "Vansh Narula",
            "Zhangyang",
            "Wang",
            "Theodora Chaspari"
        ],
        "abstract": "Image and video-capturing technologies have permeated our every-day life.\nSuch technologies can continuously monitor individuals' expressions in\nreal-life settings, affording us new insights into their emotional states and\ntransitions, thus paving the way to novel well-being and healthcare\napplications. Yet, due to the strong privacy concerns, the use of such\ntechnologies is met with strong skepticism, since current face-based emotion\nrecognition systems relying on deep learning techniques tend to preserve\nsubstantial information related to the identity of the user, apart from the\nemotion-specific information. This paper proposes an adversarial learning\nframework which relies on a convolutional neural network (CNN) architecture\ntrained through an iterative procedure for minimizing identity-specific\ninformation and maximizing emotion-dependent information. The proposed approach\nis evaluated through emotion classification and face identification metrics,\nand is compared against two CNNs, one trained solely for emotion recognition\nand the other trained solely for face identification. Experiments are performed\nusing the Yale Face Dataset and Japanese Female Facial Expression Database.\nResults indicate that the proposed approach can learn a convolutional\ntransformation for preserving emotion recognition accuracy and degrading face\nidentity recognition, providing a foundation toward privacy-aware emotion\nrecognition technologies.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2001.06103v1",
        "date": "2020-01-16 22:45:52+00:00"
    },
    {
        "title": "NanoFlow: Scalable Normalizing Flows with Sublinear Parameter Complexity",
        "authors": [
            "Sang-gil Lee",
            "Sungwon Kim",
            "Sungroh Yoon"
        ],
        "abstract": "Normalizing flows (NFs) have become a prominent method for deep generative\nmodels that allow for an analytic probability density estimation and efficient\nsynthesis. However, a flow-based network is considered to be inefficient in\nparameter complexity because of reduced expressiveness of bijective mapping,\nwhich renders the models unfeasibly expensive in terms of parameters. We\npresent an alternative parameterization scheme called NanoFlow, which uses a\nsingle neural density estimator to model multiple transformation stages. Hence,\nwe propose an efficient parameter decomposition method and the concept of flow\nindication embedding, which are key missing components that enable density\nestimation from a single neural network. Experiments performed on audio and\nimage models confirm that our method provides a new parameter-efficient\nsolution for scalable NFs with significant sublinear parameter complexity.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2006.06280v4",
        "date": "2020-06-11 09:35:00+00:00"
    },
    {
        "title": "AxoNN: An asynchronous, message-driven parallel framework for extreme-scale deep learning",
        "authors": [
            "Siddharth Singh",
            "Abhinav Bhatele"
        ],
        "abstract": "In the last few years, the memory requirements to train state-of-the-art\nneural networks have far exceeded the DRAM capacities of modern hardware\naccelerators. This has necessitated the development of efficient algorithms to\ntrain these neural networks in parallel on large-scale GPU-based clusters.\nSince computation is relatively inexpensive on modern GPUs, designing and\nimplementing extremely efficient communication in these parallel training\nalgorithms is critical for extracting the maximum performance. This paper\npresents AxoNN, a parallel deep learning framework that exploits asynchrony and\nmessage-driven execution to schedule neural network operations on each GPU,\nthereby reducing GPU idle time and maximizing hardware efficiency. By using the\nCPU memory as a scratch space for offloading data periodically during training,\nAxoNN is able to reduce GPU memory consumption by four times. This allows us to\nincrease the number of parameters per GPU by four times, thus reducing the\namount of communication and increasing performance by over 13%. When tested\nagainst large transformer models with 12-100 billion parameters on 48-384\nNVIDIA Tesla V100 GPUs, AxoNN achieves a per-GPU throughput of 49.4-54.78% of\ntheoretical peak and reduces the training time by 22-37 days (15-25% speedup)\nas compared to the state-of-the-art.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.DC",
            "cs.PF"
        ],
        "link": "http://arxiv.org/pdf/2110.13005v4",
        "date": "2021-10-25 14:43:36+00:00"
    },
    {
        "title": "Objective Variables for Probabilistic Revenue Maximization in Second-Price Auctions with Reserve",
        "authors": [
            "Maja R. Rudolph",
            "Joseph G. Ellis",
            "David M. Blei"
        ],
        "abstract": "Many online companies sell advertisement space in second-price auctions with\nreserve. In this paper, we develop a probabilistic method to learn a profitable\nstrategy to set the reserve price. We use historical auction data with features\nto fit a predictor of the best reserve price. This problem is delicate - the\nstructure of the auction is such that a reserve price set too high is much\nworse than a reserve price set too low. To address this we develop objective\nvariables, a new framework for combining probabilistic modeling with optimal\ndecision-making. Objective variables are \"hallucinated observations\" that\ntransform the revenue maximization task into a regularized maximum likelihood\nestimation problem, which we solve with an EM algorithm. This framework enables\na variety of prediction mechanisms to set the reserve price. As examples, we\nstudy objective variable methods with regression, kernelized regression, and\nneural networks on simulated and real data. Our methods outperform previous\napproaches both in terms of scalability and profit.",
        "categories": [
            "stat.ML",
            "cs.AI",
            "cs.GT",
            "cs.LG",
            "stat.AP"
        ],
        "link": "http://arxiv.org/pdf/1506.07504v1",
        "date": "2015-06-24 19:20:18+00:00"
    },
    {
        "title": "Learning Generalized Transformation Equivariant Representations via Autoencoding Transformations",
        "authors": [
            "Guo-Jun Qi",
            "Liheng Zhang",
            "Xiao Wang"
        ],
        "abstract": "Transformation Equivariant Representations (TERs) aim to capture the\nintrinsic visual structures that equivary to various transformations by\nexpanding the notion of {\\em translation} equivariance underlying the success\nof Convolutional Neural Networks (CNNs). For this purpose, we present both\ndeterministic AutoEncoding Transformations (AET) and probabilistic AutoEncoding\nVariational Transformations (AVT) models to learn visual representations from\ngeneric groups of transformations. While the AET is trained by directly\ndecoding the transformations from the learned representations, the AVT is\ntrained by maximizing the joint mutual information between the learned\nrepresentation and transformations. This results in Generalized TERs (GTERs)\nequivariant against transformations in a more general fashion by capturing\ncomplex patterns of visual structures beyond the conventional linear\nequivariance under a transformation group. The presented approach can be\nextended to (semi-)supervised models by jointly maximizing the mutual\ninformation of the learned representation with both labels and transformations.\nExperiments demonstrate the proposed models outperform the state-of-the-art\nmodels in both unsupervised and (semi-)supervised tasks.",
        "categories": [
            "cs.CV",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1906.08628v3",
        "date": "2019-06-19 06:17:56+00:00"
    },
    {
        "title": "Instance Enhancement Batch Normalization: an Adaptive Regulator of Batch Noise",
        "authors": [
            "Senwei Liang",
            "Zhongzhan Huang",
            "Mingfu Liang",
            "Haizhao Yang"
        ],
        "abstract": "Batch Normalization (BN)(Ioffe and Szegedy 2015) normalizes the features of\nan input image via statistics of a batch of images and hence BN will bring the\nnoise to the gradient of the training loss. Previous works indicate that the\nnoise is important for the optimization and generalization of deep neural\nnetworks, but too much noise will harm the performance of networks. In our\npaper, we offer a new point of view that self-attention mechanism can help to\nregulate the noise by enhancing instance-specific information to obtain a\nbetter regularization effect. Therefore, we propose an attention-based BN\ncalled Instance Enhancement Batch Normalization (IEBN) that recalibrates the\ninformation of each channel by a simple linear transformation. IEBN has a good\ncapacity of regulating noise and stabilizing network training to improve\ngeneralization even in the presence of two kinds of noise attacks during\ntraining. Finally, IEBN outperforms BN with only a light parameter increment in\nimage classification tasks for different network structures and benchmark\ndatasets.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1908.04008v2",
        "date": "2019-08-12 05:42:09+00:00"
    },
    {
        "title": "Deep Theory of Functional Connections: A New Method for Estimating the Solutions of PDEs",
        "authors": [
            "Carl Leake"
        ],
        "abstract": "This article presents a new methodology called deep Theory of Functional\nConnections (TFC) that estimates the solutions of partial differential\nequations (PDEs) by combining neural networks with TFC. TFC is used to\ntransform PDEs with boundary conditions into unconstrained optimization\nproblems by embedding the boundary conditions into a \"constrained expression.\"\nIn this work, a neural network is chosen as the free function, and used to\nsolve the now unconstrained optimization problem. The loss function is taken as\nthe square of the residual of the PDE. Then, the neural network is trained in\nan unsupervised manner to solve the unconstrained optimization problem. This\nmethodology has two major differences when compared with popular methods used\nto estimate the solutions of PDEs. First, this methodology does not need to\ndiscretize the domain into a grid, rather, this methodology randomly samples\npoints from the domain during the training phase. Second, after training, this\nmethodology represents a closed form, analytical, differentiable approximation\nof the solution throughout the entire training domain. In contrast, other\npopular methods require interpolation if the estimated solution is desired at\npoints that do not lie on the discretized grid. The deep TFC method for\nestimating the solution of PDEs is demonstrated on four problems with a variety\nof boundary conditions.",
        "categories": [
            "cs.NA",
            "cs.LG",
            "physics.comp-ph",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1812.08625v3",
        "date": "2018-12-20 15:05:25+00:00"
    },
    {
        "title": "Deep Convolutional Neural Networks Based on Semi-Discrete Frames",
        "authors": [
            "Thomas Wiatowski",
            "Helmut B\u00f6lcskei"
        ],
        "abstract": "Deep convolutional neural networks have led to breakthrough results in\npractical feature extraction applications. The mathematical analysis of these\nnetworks was pioneered by Mallat, 2012. Specifically, Mallat considered\nso-called scattering networks based on identical semi-discrete wavelet frames\nin each network layer, and proved translation-invariance as well as deformation\nstability of the resulting feature extractor. The purpose of this paper is to\ndevelop Mallat's theory further by allowing for different and, most\nimportantly, general semi-discrete frames (such as, e.g., Gabor frames,\nwavelets, curvelets, shearlets, ridgelets) in distinct network layers. This\nallows to extract wider classes of features than point singularities resolved\nby the wavelet transform. Our generalized feature extractor is proven to be\ntranslation-invariant, and we develop deformation stability results for a\nlarger class of deformations than those considered by Mallat. For Mallat's\nwavelet-based feature extractor, we get rid of a number of technical\nconditions. The mathematical engine behind our results is continuous frame\ntheory, which allows us to completely detach the invariance and deformation\nstability proofs from the particular algebraic structure of the underlying\nframes.",
        "categories": [
            "cs.LG",
            "cs.IT",
            "math.FA",
            "math.IT",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1504.05487v1",
        "date": "2015-04-21 16:01:00+00:00"
    },
    {
        "title": "Copula-Based Normalizing Flows",
        "authors": [
            "Mike Laszkiewicz",
            "Johannes Lederer",
            "Asja Fischer"
        ],
        "abstract": "Normalizing flows, which learn a distribution by transforming the data to\nsamples from a Gaussian base distribution, have proven powerful density\napproximations. But their expressive power is limited by this choice of the\nbase distribution. We, therefore, propose to generalize the base distribution\nto a more elaborate copula distribution to capture the properties of the target\ndistribution more accurately. In a first empirical analysis, we demonstrate\nthat this replacement can dramatically improve the vanilla normalizing flows in\nterms of flexibility, stability, and effectivity for heavy-tailed data. Our\nresults suggest that the improvements are related to an increased local\nLipschitz-stability of the learned flow.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2107.07352v1",
        "date": "2021-07-15 14:22:28+00:00"
    },
    {
        "title": "Riemannian batch normalization for SPD neural networks",
        "authors": [
            "Daniel Brooks",
            "Olivier Schwander",
            "Frederic Barbaresco",
            "Jean-Yves Schneider",
            "Matthieu Cord"
        ],
        "abstract": "Covariance matrices have attracted attention for machine learning\napplications due to their capacity to capture interesting structure in the\ndata. The main challenge is that one needs to take into account the particular\ngeometry of the Riemannian manifold of symmetric positive definite (SPD)\nmatrices they belong to. In the context of deep networks, several architectures\nfor these matrices have recently been proposed. In our article, we introduce a\nRiemannian batch normalization (batchnorm) algorithm, which generalizes the one\nused in Euclidean nets. This novel layer makes use of geometric operations on\nthe manifold, notably the Riemannian barycenter, parallel transport and\nnon-linear structured matrix transformations. We derive a new\nmanifold-constrained gradient descent algorithm working in the space of SPD\nmatrices, allowing to learn the batchnorm layer. We validate our proposed\napproach with experiments in three different contexts on diverse data types: a\ndrone recognition dataset from radar observations, and on emotion and action\nrecognition datasets from video and motion capture data. Experiments show that\nthe Riemannian batchnorm systematically gives better classification performance\ncompared with leading methods and a remarkable robustness to lack of data.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1909.02414v2",
        "date": "2019-09-03 23:03:50+00:00"
    },
    {
        "title": "Deep Tracking on the Move: Learning to Track the World from a Moving Vehicle using Recurrent Neural Networks",
        "authors": [
            "Julie Dequaire",
            "Dushyant Rao",
            "Peter Ondruska",
            "Dominic Wang",
            "Ingmar Posner"
        ],
        "abstract": "This paper presents an end-to-end approach for tracking static and dynamic\nobjects for an autonomous vehicle driving through crowded urban environments.\nUnlike traditional approaches to tracking, this method is learned end-to-end,\nand is able to directly predict a full unoccluded occupancy grid map from raw\nlaser input data. Inspired by the recently presented DeepTracking approach\n[Ondruska, 2016], we employ a recurrent neural network (RNN) to capture the\ntemporal evolution of the state of the environment, and propose to use Spatial\nTransformer modules to exploit estimates of the egomotion of the vehicle. Our\nresults demonstrate the ability to track a range of objects, including cars,\nbuses, pedestrians, and cyclists through occlusion, from both moving and\nstationary platforms, using a single learned model. Experimental results\ndemonstrate that the model can also predict the future states of objects from\ncurrent inputs, with greater accuracy than previous work.",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.RO"
        ],
        "link": "http://arxiv.org/pdf/1609.09365v3",
        "date": "2016-09-29 14:39:10+00:00"
    },
    {
        "title": "A Generative Model for Raw Audio Using Transformer Architectures",
        "authors": [
            "Prateek Verma",
            "Chris Chafe"
        ],
        "abstract": "This paper proposes a novel way of doing audio synthesis at the waveform\nlevel using Transformer architectures. We propose a deep neural network for\ngenerating waveforms, similar to wavenet. This is fully probabilistic,\nauto-regressive, and causal, i.e. each sample generated depends only on the\npreviously observed samples. Our approach outperforms a widely used wavenet\narchitecture by up to 9% on a similar dataset for predicting the next step.\nUsing the attention mechanism, we enable the architecture to learn which audio\nsamples are important for the prediction of the future sample. We show how\ncausal transformer generative models can be used for raw waveform synthesis. We\nalso show that this performance can be improved by another 2% by conditioning\nsamples over a wider context. The flexibility of the current model to\nsynthesize audio from latent representations suggests a large number of\npotential applications. The novel approach of using generative transformer\narchitectures for raw audio synthesis is, however, still far away from\ngenerating any meaningful music, without using latent codes/meta-data to aid\nthe generation process.",
        "categories": [
            "cs.SD",
            "cs.AI",
            "cs.LG",
            "cs.MM",
            "eess.AS"
        ],
        "link": "http://arxiv.org/pdf/2106.16036v3",
        "date": "2021-06-30 13:05:31+00:00"
    },
    {
        "title": "Stateful ODE-Nets using Basis Function Expansions",
        "authors": [
            "Alejandro Queiruga",
            "N. Benjamin Erichson",
            "Liam Hodgkinson",
            "Michael W. Mahoney"
        ],
        "abstract": "The recently-introduced class of ordinary differential equation networks\n(ODE-Nets) establishes a fruitful connection between deep learning and\ndynamical systems. In this work, we reconsider formulations of the weights as\ncontinuous-in-depth functions using linear combinations of basis functions\nwhich enables us to leverage parameter transformations such as function\nprojections. In turn, this view allows us to formulate a novel stateful\nODE-Block that handles stateful layers. The benefits of this new ODE-Block are\ntwofold: first, it enables incorporating meaningful continuous-in-depth batch\nnormalization layers to achieve state-of-the-art performance; second, it\nenables compressing the weights through a change of basis, without retraining,\nwhile maintaining near state-of-the-art performance and reducing both inference\ntime and memory footprint. Performance is demonstrated by applying our stateful\nODE-Block to (a) image classification tasks using convolutional units and (b)\nsentence-tagging tasks using transformer encoder units.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2106.10820v2",
        "date": "2021-06-21 03:04:51+00:00"
    },
    {
        "title": "Phase Collapse in Neural Networks",
        "authors": [
            "Florentin Guth",
            "John Zarka",
            "St\u00e9phane Mallat"
        ],
        "abstract": "Deep convolutional classifiers linearly separate image classes and improve\naccuracy as depth increases. They progressively reduce the spatial dimension\nwhereas the number of channels grows with depth. Spatial variability is\ntherefore transformed into variability along channels. A fundamental challenge\nis to understand the role of non-linearities together with convolutional\nfilters in this transformation. ReLUs with biases are often interpreted as\nthresholding operators that improve discrimination through sparsity. This paper\ndemonstrates that it is a different mechanism called phase collapse which\neliminates spatial variability while linearly separating classes. We show that\ncollapsing the phases of complex wavelet coefficients is sufficient to reach\nthe classification accuracy of ResNets of similar depths. However, replacing\nthe phase collapses with thresholding operators that enforce sparsity\nconsiderably degrades the performance. We explain these numerical results by\nshowing that the iteration of phase collapses progressively improves separation\nof classes, as opposed to thresholding non-linearities.",
        "categories": [
            "cs.LG",
            "eess.SP",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2110.05283v2",
        "date": "2021-10-11 13:58:01+00:00"
    },
    {
        "title": "PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning",
        "authors": [
            "Mehdi Jafarnia-Jahromi",
            "Tasmin Chowdhury",
            "Hsin-Tai Wu",
            "Sayandev Mukherjee"
        ],
        "abstract": "Deep neural networks have demonstrated cutting edge performance on various\ntasks including classification. However, it is well known that adversarially\ndesigned imperceptible perturbation of the input can mislead advanced\nclassifiers. In this paper, Permutation Phase Defense (PPD), is proposed as a\nnovel method to resist adversarial attacks. PPD combines random permutation of\nthe image with phase component of its Fourier transform. The basic idea behind\nthis approach is to turn adversarial defense problems analogously into\nsymmetric cryptography, which relies solely on safekeeping of the keys for\nsecurity. In PPD, safe keeping of the selected permutation ensures\neffectiveness against adversarial attacks. Testing PPD on MNIST and CIFAR-10\ndatasets yielded state-of-the-art robustness against the most powerful\nadversarial attacks currently available.",
        "categories": [
            "cs.LG",
            "cs.CR",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1812.10049v2",
        "date": "2018-12-25 06:17:54+00:00"
    },
    {
        "title": "Dual Averaging is Surprisingly Effective for Deep Learning Optimization",
        "authors": [
            "Samy Jelassi",
            "Aaron Defazio"
        ],
        "abstract": "First-order stochastic optimization methods are currently the most widely\nused class of methods for training deep neural networks. However, the choice of\nthe optimizer has become an ad-hoc rule that can significantly affect the\nperformance. For instance, SGD with momentum (SGD+M) is typically used in\ncomputer vision (CV) and Adam is used for training transformer models for\nNatural Language Processing (NLP). Using the wrong method can lead to\nsignificant performance degradation. Inspired by the dual averaging algorithm,\nwe propose Modernized Dual Averaging (MDA), an optimizer that is able to\nperform as well as SGD+M in CV and as Adam in NLP. Our method is not adaptive\nand is significantly simpler than Adam. We show that MDA induces a decaying\nuncentered $L_2$-regularization compared to vanilla SGD+M and hypothesize that\nthis may explain why it works on NLP problems where SGD+M fails.",
        "categories": [
            "cs.LG",
            "math.OC",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2010.10502v1",
        "date": "2020-10-20 17:55:11+00:00"
    },
    {
        "title": "SparseTrain:Leveraging Dynamic Sparsity in Training DNNs on General-Purpose SIMD Processors",
        "authors": [
            "Zhangxiaowen Gong",
            "Houxiang Ji",
            "Christopher Fletcher",
            "Christopher Hughes",
            "Josep Torrellas"
        ],
        "abstract": "Our community has greatly improved the efficiency of deep learning\napplications, including by exploiting sparsity in inputs. Most of that work,\nthough, is for inference, where weight sparsity is known statically, and/or for\nspecialized hardware. We propose a scheme to leverage dynamic sparsity during\ntraining. In particular, we exploit zeros introduced by the ReLU activation\nfunction to both feature maps and their gradients. This is challenging because\nthe sparsity degree is moderate and the locations of zeros change over time. We\nalso rely purely on software. We identify zeros in a dense data representation\nwithout transforming the data and performs conventional vectorized computation.\nVariations of the scheme are applicable to all major components of training:\nforward propagation, backward propagation by inputs, and backward propagation\nby weights. Our method significantly outperforms a highly-optimized dense\ndirect convolution on several popular deep neural networks. At realistic\nsparsity, we speed up the training of the non-initial convolutional layers in\nVGG16, ResNet-34, ResNet-50, and Fixup ResNet-50 by 2.19x, 1.37x, 1.31x, and\n1.51x respectively on an Intel Skylake-X CPU.",
        "categories": [
            "cs.LG",
            "cs.DC",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1911.10175v1",
        "date": "2019-11-22 18:19:32+00:00"
    },
    {
        "title": "Characterizing Inter-Layer Functional Mappings of Deep Learning Models",
        "authors": [
            "Donald Waagen",
            "Katie Rainey",
            "Jamie Gantert",
            "David Gray",
            "Megan King",
            "M. Shane Thompson",
            "Jonathan Barton",
            "Will Waldron",
            "Samantha Livingston",
            "Don Hulsey"
        ],
        "abstract": "Deep learning architectures have demonstrated state-of-the-art performance\nfor object classification and have become ubiquitous in commercial products.\nThese methods are often applied without understanding (a) the difficulty of a\nclassification task given the input data, and (b) how a specific deep learning\narchitecture transforms that data. To answer (a) and (b), we illustrate the\nutility of a multivariate nonparametric estimator of class separation, the\nHenze-Penrose (HP) statistic, in the original as well as layer-induced\nrepresentations. Given an $N$-class problem, our contribution defines the\n$C(N,2)$ combinations of HP statistics as a sample from a distribution of\nclass-pair separations. This allows us to characterize the distributional\nchange to class separation induced at each layer of the model. Fisher\npermutation tests are used to detect statistically significant changes within a\nmodel. By comparing the HP statistic distributions between layers, one can\nstatistically characterize: layer adaptation during training, the contribution\nof each layer to the classification task, and the presence or absence of\nconsistency between training and validation data. This is demonstrated for a\nsimple deep neural network using CIFAR10 with random-labels, CIFAR10, and MNIST\ndatasets.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1907.04223v2",
        "date": "2019-07-09 14:58:59+00:00"
    },
    {
        "title": "Distinction between features extracted using deep belief networks",
        "authors": [
            "Mohammad Pezeshki",
            "Sajjad Gholami",
            "Ahmad Nickabadi"
        ],
        "abstract": "Data representation is an important pre-processing step in many machine\nlearning algorithms. There are a number of methods used for this task such as\nDeep Belief Networks (DBNs) and Discrete Fourier Transforms (DFTs). Since some\nof the features extracted using automated feature extraction methods may not\nalways be related to a specific machine learning task, in this paper we propose\ntwo methods in order to make a distinction between extracted features based on\ntheir relevancy to the task. We applied these two methods to a Deep Belief\nNetwork trained for a face recognition task.",
        "categories": [
            "cs.LG",
            "cs.NE"
        ],
        "link": "http://arxiv.org/pdf/1312.6157v2",
        "date": "2013-12-20 21:52:08+00:00"
    },
    {
        "title": "The (Un)reliability of saliency methods",
        "authors": [
            "Pieter-Jan Kindermans",
            "Sara Hooker",
            "Julius Adebayo",
            "Maximilian Alber",
            "Kristof T. Sch\u00fctt",
            "Sven D\u00e4hne",
            "Dumitru Erhan",
            "Been Kim"
        ],
        "abstract": "Saliency methods aim to explain the predictions of deep neural networks.\nThese methods lack reliability when the explanation is sensitive to factors\nthat do not contribute to the model prediction. We use a simple and common\npre-processing step ---adding a constant shift to the input data--- to show\nthat a transformation with no effect on the model can cause numerous methods to\nincorrectly attribute. In order to guarantee reliability, we posit that methods\nshould fulfill input invariance, the requirement that a saliency method mirror\nthe sensitivity of the model with respect to transformations of the input. We\nshow, through several examples, that saliency methods that do not satisfy input\ninvariance result in misleading attribution.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1711.00867v1",
        "date": "2017-11-02 18:01:30+00:00"
    },
    {
        "title": "Compositionally-Warped Gaussian Processes",
        "authors": [
            "Gonzalo Rios",
            "Felipe Tobar"
        ],
        "abstract": "The Gaussian process (GP) is a nonparametric prior distribution over\nfunctions indexed by time, space, or other high-dimensional index set. The GP\nis a flexible model yet its limitation is given by its very nature: it can only\nmodel Gaussian marginal distributions. To model non-Gaussian data, a GP can be\nwarped by a nonlinear transformation (or warping) as performed by warped GPs\n(WGPs) and more computationally-demanding alternatives such as Bayesian WGPs\nand deep GPs. However, the WGP requires a numerical approximation of the\ninverse warping for prediction, which increases the computational complexity in\npractice. To sidestep this issue, we construct a novel class of warpings\nconsisting of compositions of multiple elementary functions, for which the\ninverse is known explicitly. We then propose the compositionally-warped GP\n(CWGP), a non-Gaussian generative model whose expressiveness follows from its\ndeep compositional architecture, and its computational efficiency is guaranteed\nby the analytical inverse warping. Experimental validation using synthetic and\nreal-world datasets confirms that the proposed CWGP is robust to the choice of\nwarpings and provides more accurate point predictions, better trained models\nand shorter computation times than WGP.",
        "categories": [
            "stat.ML",
            "cs.LG",
            "eess.SP"
        ],
        "link": "http://arxiv.org/pdf/1906.09665v2",
        "date": "2019-06-23 22:41:32+00:00"
    },
    {
        "title": "Interpretable Latent Variables in Deep State Space Models",
        "authors": [
            "Haoxuan Wu",
            "David S. Matteson",
            "Martin T. Wells"
        ],
        "abstract": "We introduce a new version of deep state-space models (DSSMs) that combines a\nrecurrent neural network with a state-space framework to forecast time series\ndata. The model estimates the observed series as functions of latent variables\nthat evolve non-linearly through time. Due to the complexity and non-linearity\ninherent in DSSMs, previous works on DSSMs typically produced latent variables\nthat are very difficult to interpret. Our paper focus on producing\ninterpretable latent parameters with two key modifications. First, we simplify\nthe predictive decoder by restricting the response variables to be a linear\ntransformation of the latent variables plus some noise. Second, we utilize\nshrinkage priors on the latent variables to reduce redundancy and improve\nrobustness. These changes make the latent variables much easier to understand\nand allow us to interpret the resulting latent variables as random effects in a\nlinear mixed model. We show through two public benchmark datasets the resulting\nmodel improves forecasting performances.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2203.02057v2",
        "date": "2022-03-03 23:10:58+00:00"
    },
    {
        "title": "Transformers over Directed Acyclic Graphs",
        "authors": [
            "Yuankai Luo",
            "Veronika Thost",
            "Yicheng Pan",
            "Lei Shi"
        ],
        "abstract": "Transformer models have recently gained popularity in graph representation\nlearning as they have the potential to learn complex relationships beyond the\nones captured by regular graph neural networks. The main research question is\nhow to inject the structural bias of graphs into the transformer architecture,\nand several proposals have been made for undirected molecular graphs and,\nrecently, also for larger network graphs. In this paper, we study transformers\nover directed acyclic graphs (DAGs) and propose architecture adaptations\ntailored to DAGs: (1) An attention mechanism that is more efficient than the\nregular quadratic complexity of transformers and at the same time faithfully\ncaptures the DAG structure, and (2) a positional encoding of the DAG's partial\norder, complementing the former. We rigorously evaluate our framework in\nablation studies and show that it is effective in improving different kinds of\nbaseline transformers over various types of data, in experiments ranging from\nclassifying source code graphs to nodes in self-citation networks. In\nparticular, our proposal makes (graph) transformers competitive to or\noutperform graph neural networks tailored to DAGs.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2210.13148v4",
        "date": "2022-10-24 12:04:52+00:00"
    },
    {
        "title": "Universal Approximation Under Constraints is Possible with Transformers",
        "authors": [
            "Anastasis Kratsios",
            "Behnoosh Zamanlooy",
            "Tianlin Liu",
            "Ivan Dokmani\u0107"
        ],
        "abstract": "Many practical problems need the output of a machine learning model to\nsatisfy a set of constraints, $K$. Nevertheless, there is no known guarantee\nthat classical neural network architectures can exactly encode constraints\nwhile simultaneously achieving universality. We provide a quantitative\nconstrained universal approximation theorem which guarantees that for any\nnon-convex compact set $K$ and any continuous function\n$f:\\mathbb{R}^n\\rightarrow K$, there is a probabilistic transformer $\\hat{F}$\nwhose randomized outputs all lie in $K$ and whose expected output uniformly\napproximates $f$. Our second main result is a \"deep neural version\" of Berge's\nMaximum Theorem (1963). The result guarantees that given an objective function\n$L$, a constraint set $K$, and a family of soft constraint sets, there is a\nprobabilistic transformer $\\hat{F}$ that approximately minimizes $L$ and whose\noutputs belong to $K$; moreover, $\\hat{F}$ approximately satisfies the soft\nconstraints. Our results imply the first universal approximation theorem for\nclassical transformers with exact convex constraint satisfaction. They also\nyield that a chart-free universal approximation theorem for Riemannian\nmanifold-valued functions subject to suitable geodesically convex constraints.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.NE",
            "math.FA",
            "math.MG",
            "68T07, 41A65, 41A29, 51F99"
        ],
        "link": "http://arxiv.org/pdf/2110.03303v2",
        "date": "2021-10-07 09:43:01+00:00"
    },
    {
        "title": "Generative x-vectors for text-independent speaker verification",
        "authors": [
            "Longting Xu",
            "Rohan Kumar Das",
            "Emre Y\u0131lmaz",
            "Jichen Yang",
            "Haizhou Li"
        ],
        "abstract": "Speaker verification (SV) systems using deep neural network embeddings,\nso-called the x-vector systems, are becoming popular due to its good\nperformance superior to the i-vector systems. The fusion of these systems\nprovides improved performance benefiting both from the discriminatively trained\nx-vectors and generative i-vectors capturing distinct speaker characteristics.\nIn this paper, we propose a novel method to include the complementary\ninformation of i-vector and x-vector, that is called generative x-vector. The\ngenerative x-vector utilizes a transformation model learned from the i-vector\nand x-vector representations of the background data. Canonical correlation\nanalysis is applied to derive this transformation model, which is later used to\ntransform the standard x-vectors of the enrollment and test segments to the\ncorresponding generative x-vectors. The SV experiments performed on the NIST\nSRE 2010 dataset demonstrate that the system using generative x-vectors\nprovides considerably better performance than the baseline i-vector and\nx-vector systems. Furthermore, the generative x-vectors outperform the fusion\nof i-vector and x-vector systems for long-duration utterances, while yielding\ncomparable results for short-duration utterances.",
        "categories": [
            "eess.AS",
            "cs.LG",
            "cs.SD",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1809.06798v1",
        "date": "2018-09-17 06:04:54+00:00"
    },
    {
        "title": "GDP: Generalized Device Placement for Dataflow Graphs",
        "authors": [
            "Yanqi Zhou",
            "Sudip Roy",
            "Amirali Abdolrashidi",
            "Daniel Wong",
            "Peter C. Ma",
            "Qiumin Xu",
            "Ming Zhong",
            "Hanxiao Liu",
            "Anna Goldie",
            "Azalia Mirhoseini",
            "James Laudon"
        ],
        "abstract": "Runtime and scalability of large neural networks can be significantly\naffected by the placement of operations in their dataflow graphs on suitable\ndevices. With increasingly complex neural network architectures and\nheterogeneous device characteristics, finding a reasonable placement is\nextremely challenging even for domain experts. Most existing automated device\nplacement approaches are impractical due to the significant amount of compute\nrequired and their inability to generalize to new, previously held-out graphs.\nTo address both limitations, we propose an efficient end-to-end method based on\na scalable sequential attention mechanism over a graph neural network that is\ntransferable to new graphs. On a diverse set of representative deep learning\nmodels, including Inception-v3, AmoebaNet, Transformer-XL, and WaveNet, our\nmethod on average achieves 16% improvement over human experts and 9.2%\nimprovement over the prior art with 15 times faster convergence. To further\nreduce the computation cost, we pre-train the policy network on a set of\ndataflow graphs and use a superposition network to fine-tune it on each\nindividual graph, achieving state-of-the-art performance on large hold-out\ngraphs with over 50k nodes, such as an 8-layer GNMT.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1910.01578v1",
        "date": "2019-09-28 04:13:57+00:00"
    },
    {
        "title": "A Novel Deep Learning Architecture for Decoding Imagined Speech from EEG",
        "authors": [
            "Jerrin Thomas Panachakel",
            "A. G. Ramakrishnan",
            "T. V. Ananthapadmanabha"
        ],
        "abstract": "The recent advances in the field of deep learning have not been fully\nutilised for decoding imagined speech primarily because of the unavailability\nof sufficient training samples to train a deep network. In this paper, we\npresent a novel architecture that employs deep neural network (DNN) for\nclassifying the words \"in\" and \"cooperate\" from the corresponding EEG signals\nin the ASU imagined speech dataset. Nine EEG channels, which best capture the\nunderlying cortical activity, are chosen using common spatial pattern (CSP) and\nare treated as independent data vectors. Discrete wavelet transform (DWT) is\nused for feature extraction. To the best of our knowledge, so far DNN has not\nbeen employed as a classifier in decoding imagined speech. Treating the\nselected EEG channels corresponding to each imagined word as independent data\nvectors helps in providing sufficient number of samples to train a DNN. For\neach test trial, the final class label is obtained by applying a majority\nvoting on the classification results of the individual channels considered in\nthe trial. We have achieved accuracies comparable to the state-of-the-art\nresults. The results can be further improved by using a higher-density EEG\nacquisition system in conjunction with other deep learning techniques such as\nlong short-term memory.",
        "categories": [
            "eess.SP",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2003.09374v1",
        "date": "2020-03-19 00:57:40+00:00"
    },
    {
        "title": "Nonlinear Multiview Analysis: Identifiability and Neural Network-assisted Implementation",
        "authors": [
            "Qi Lyu",
            "Xiao Fu"
        ],
        "abstract": "Multiview analysis aims at extracting shared latent components from data\nsamples that are acquired in different domains, e.g., image, text, and audio.\nClassic multiview analysis, e.g., canonical correlation analysis (CCA), tackles\nthis problem via matching the linearly transformed views in a certain latent\ndomain. More recently, powerful nonlinear learning tools such as kernel methods\nand neural networks are utilized for enhancing the classic CCA. However, unlike\nlinear CCA whose theoretical aspects are clearly understood, nonlinear CCA\napproaches are largely intuition-driven. In particular, it is unclear under\nwhat conditions the shared latent components across the views can be\nidentified---while identifiability plays an essential role in many\napplications. In this work, we revisit nonlinear multiview analysis and address\nboth the theoretical and computational aspects. Our work leverages a useful\nnonlinear model, namely, the post-nonlinear model, from the nonlinear mixture\nseparation literature. Combining with multiview data, we take a nonlinear\nmultiview mixture learning viewpoint, which is a natural extension of the\nclassic generative models for linear CCA. From there, we derive a learning\ncriterion. We show that minimizing this criterion leads to identification of\nthe latent shared components up to certain ambiguities, under reasonable\nconditions. Our derivation and formulation also offer new insights and\ninterpretations to existing deep neural network-based CCA formulations. On the\ncomputation side, we propose an effective algorithm with simple and scalable\nupdate rules. A series of simulations and real-data experiments corroborate our\ntheoretical analysis.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1909.09177v2",
        "date": "2019-09-19 18:08:41+00:00"
    },
    {
        "title": "Generalization Error of Invariant Classifiers",
        "authors": [
            "Jure Sokolic",
            "Raja Giryes",
            "Guillermo Sapiro",
            "Miguel R. D. Rodrigues"
        ],
        "abstract": "This paper studies the generalization error of invariant classifiers. In\nparticular, we consider the common scenario where the classification task is\ninvariant to certain transformations of the input, and that the classifier is\nconstructed (or learned) to be invariant to these transformations. Our approach\nrelies on factoring the input space into a product of a base space and a set of\ntransformations. We show that whereas the generalization error of a\nnon-invariant classifier is proportional to the complexity of the input space,\nthe generalization error of an invariant classifier is proportional to the\ncomplexity of the base space. We also derive a set of sufficient conditions on\nthe geometry of the base space and the set of transformations that ensure that\nthe complexity of the base space is much smaller than the complexity of the\ninput space. Our analysis applies to general classifiers such as convolutional\nneural networks. We demonstrate the implications of the developed theory for\nsuch classifiers with experiments on the MNIST and CIFAR-10 datasets.",
        "categories": [
            "stat.ML",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1610.04574v3",
        "date": "2016-10-14 18:40:52+00:00"
    },
    {
        "title": "A Recurrent Probabilistic Neural Network with Dimensionality Reduction Based on Time-series Discriminant Component Analysis",
        "authors": [
            "Hideaki Hayashi",
            "Taro Shibanoki",
            "Keisuke Shima",
            "Yuichi Kurita",
            "Toshio Tsuji"
        ],
        "abstract": "This paper proposes a probabilistic neural network developed on the basis of\ntime-series discriminant component analysis (TSDCA) that can be used to\nclassify high-dimensional time-series patterns. TSDCA involves the compression\nof high-dimensional time series into a lower-dimensional space using a set of\northogonal transformations and the calculation of posterior probabilities based\non a continuous-density hidden Markov model with a Gaussian mixture model\nexpressed in the reduced-dimensional space. The analysis can be incorporated\ninto a neural network, which is named a time-series discriminant component\nnetwork (TSDCN), so that parameters of dimensionality reduction and\nclassification can be obtained simultaneously as network coefficients according\nto a backpropagation through time-based learning algorithm with the Lagrange\nmultiplier method. The TSDCN is considered to enable high-accuracy\nclassification of high-dimensional time-series patterns and to reduce the\ncomputation time taken for network training. The validity of the TSDCN is\ndemonstrated for high-dimensional artificial data and EEG signals in the\nexperiments conducted during the study.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1911.06009v1",
        "date": "2019-11-14 09:48:41+00:00"
    },
    {
        "title": "FiLM: Frequency improved Legendre Memory Model for Long-term Time Series Forecasting",
        "authors": [
            "Tian Zhou",
            "Ziqing Ma",
            "Xue wang",
            "Qingsong Wen",
            "Liang Sun",
            "Tao Yao",
            "Wotao Yin",
            "Rong Jin"
        ],
        "abstract": "Recent studies have shown that deep learning models such as RNNs and\nTransformers have brought significant performance gains for long-term\nforecasting of time series because they effectively utilize historical\ninformation. We found, however, that there is still great room for improvement\nin how to preserve historical information in neural networks while avoiding\noverfitting to noise presented in the history. Addressing this allows better\nutilization of the capabilities of deep learning models. To this end, we design\na \\textbf{F}requency \\textbf{i}mproved \\textbf{L}egendre \\textbf{M}emory model,\nor {\\bf FiLM}: it applies Legendre Polynomials projections to approximate\nhistorical information, uses Fourier projection to remove noise, and adds a\nlow-rank approximation to speed up computation. Our empirical studies show that\nthe proposed FiLM significantly improves the accuracy of state-of-the-art\nmodels in multivariate and univariate long-term forecasting by\n(\\textbf{20.3\\%}, \\textbf{22.6\\%}), respectively. We also demonstrate that the\nrepresentation module developed in this work can be used as a general plug-in\nto improve the long-term prediction performance of other deep learning modules.\nCode is available at https://github.com/tianzhou2011/FiLM/",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2205.08897v4",
        "date": "2022-05-18 12:37:54+00:00"
    },
    {
        "title": "Finding Archetypal Spaces Using Neural Networks",
        "authors": [
            "David van Dijk",
            "Daniel Burkhardt",
            "Matthew Amodio",
            "Alex Tong",
            "Guy Wolf",
            "Smita Krishnaswamy"
        ],
        "abstract": "Archetypal analysis is a data decomposition method that describes each\nobservation in a dataset as a convex combination of \"pure types\" or archetypes.\nThese archetypes represent extrema of a data space in which there is a\ntrade-off between features, such as in biology where different combinations of\ntraits provide optimal fitness for different environments. Existing methods for\narchetypal analysis work well when a linear relationship exists between the\nfeature space and the archetypal space. However, such methods are not\napplicable to systems where the feature space is generated non-linearly from\nthe combination of archetypes, such as in biological systems or image\ntransformations. Here, we propose a reformulation of the problem such that the\ngoal is to learn a non-linear transformation of the data into a latent\narchetypal space. To solve this problem, we introduce Archetypal Analysis\nnetwork (AAnet), which is a deep neural network framework for learning and\ngenerating from a latent archetypal representation of data. We demonstrate\nstate-of-the-art recovery of ground-truth archetypes in non-linear data\ndomains, show AAnet can generate from data geometry rather than from data\ndensity, and use AAnet to identify biologically meaningful archetypes in\nsingle-cell gene expression data.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1901.09078v2",
        "date": "2019-01-25 20:44:25+00:00"
    },
    {
        "title": "Understanding Deep Neural Function Approximation in Reinforcement Learning via $\u03b5$-Greedy Exploration",
        "authors": [
            "Fanghui Liu",
            "Luca Viano",
            "Volkan Cevher"
        ],
        "abstract": "This paper provides a theoretical study of deep neural function approximation\nin reinforcement learning (RL) with the $\\epsilon$-greedy exploration under the\nonline setting. This problem setting is motivated by the successful deep\nQ-networks (DQN) framework that falls in this regime. In this work, we provide\nan initial attempt on theoretical understanding deep RL from the perspective of\nfunction class and neural networks architectures (e.g., width and depth) beyond\nthe ``linear'' regime. To be specific, we focus on the value based algorithm\nwith the $\\epsilon$-greedy exploration via deep (and two-layer) neural networks\nendowed by Besov (and Barron) function spaces, respectively, which aims at\napproximating an $\\alpha$-smooth Q-function in a $d$-dimensional feature space.\nWe prove that, with $T$ episodes, scaling the width $m =\n\\widetilde{\\mathcal{O}}(T^{\\frac{d}{2\\alpha + d}})$ and the depth\n$L=\\mathcal{O}(\\log T)$ of the neural network for deep RL is sufficient for\nlearning with sublinear regret in Besov spaces. Moreover, for a two layer\nneural network endowed by the Barron space, scaling the width\n$\\Omega(\\sqrt{T})$ is sufficient. To achieve this, the key issue in our\nanalysis is how to estimate the temporal difference error under deep neural\nfunction approximation as the $\\epsilon$-greedy exploration is not enough to\nensure ``optimism''. Our analysis reformulates the temporal difference error in\nan $L^2(\\mathrm{d}\\mu)$-integrable space over a certain averaged measure $\\mu$,\nand transforms it to a generalization problem under the non-iid setting. This\nmight have its own interest in RL theory for better understanding\n$\\epsilon$-greedy exploration in deep RL.",
        "categories": [
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2209.07376v2",
        "date": "2022-09-15 15:42:47+00:00"
    },
    {
        "title": "Evaluation of Deep Convolutional Generative Adversarial Networks for data augmentation of chest X-ray images",
        "authors": [
            "Sagar Kora Venu"
        ],
        "abstract": "Medical image datasets are usually imbalanced, due to the high costs of\nobtaining the data and time-consuming annotations. Training deep neural network\nmodels on such datasets to accurately classify the medical condition does not\nyield desired results and often over-fits the data on majority class samples.\nIn order to address this issue, data augmentation is often performed on\ntraining data by position augmentation techniques such as scaling, cropping,\nflipping, padding, rotation, translation, affine transformation, and color\naugmentation techniques such as brightness, contrast, saturation, and hue to\nincrease the dataset sizes. These augmentation techniques are not guaranteed to\nbe advantageous in domains with limited data, especially medical image data,\nand could lead to further overfitting. In this work, we performed data\naugmentation on the Chest X-rays dataset through generative modeling (deep\nconvolutional generative adversarial network) which creates artificial\ninstances retaining similar characteristics to the original data and evaluation\nof the model resulted in Fr\\'echet Distance of Inception (FID) score of 1.289.",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2009.01181v1",
        "date": "2020-09-02 16:43:55+00:00"
    },
    {
        "title": "End-to-End Auditory Object Recognition via Inception Nucleus",
        "authors": [
            "Mohammad K. Ebrahimpour",
            "Timothy Shea",
            "Andreea Danielescu",
            "David C. Noelle",
            "Christopher T. Kello"
        ],
        "abstract": "Machine learning approaches to auditory object recognition are traditionally\nbased on engineered features such as those derived from the spectrum or\ncepstrum. More recently, end-to-end classification systems in image and\nauditory recognition systems have been developed to learn features jointly with\nclassification and result in improved classification accuracy. In this paper,\nwe propose a novel end-to-end deep neural network to map the raw waveform\ninputs to sound class labels. Our network includes an \"inception nucleus\" that\noptimizes the size of convolutional filters on the fly that results in reducing\nengineering efforts dramatically. Classification results compared favorably\nagainst current state-of-the-art approaches, besting them by 10.4 percentage\npoints on the Urbansound8k dataset. Analyses of learned representations\nrevealed that filters in the earlier hidden layers learned wavelet-like\ntransforms to extract features that were informative for classification.",
        "categories": [
            "cs.SD",
            "cs.LG",
            "eess.AS",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2005.12195v1",
        "date": "2020-05-25 16:08:41+00:00"
    },
    {
        "title": "Equivariant neural networks for recovery of Hadamard matrices",
        "authors": [
            "Augusto Peres",
            "Eduardo Dias",
            "Lu\u00eds Sarmento",
            "Hugo Penedones"
        ],
        "abstract": "We propose a message passing neural network architecture designed to be\nequivariant to column and row permutations of a matrix. We illustrate its\nadvantages over traditional architectures like multi-layer perceptrons (MLPs),\nconvolutional neural networks (CNNs) and even Transformers, on the\ncombinatorial optimization task of recovering a set of deleted entries of a\nHadamard matrix. We argue that this is a powerful application of the principles\nof Geometric Deep Learning to fundamental mathematics, and a potential stepping\nstone toward more insights on the Hadamard conjecture using Machine Learning\ntechniques.",
        "categories": [
            "cs.LG",
            "cs.DM"
        ],
        "link": "http://arxiv.org/pdf/2201.13157v1",
        "date": "2022-01-31 12:07:07+00:00"
    },
    {
        "title": "Deep Multi-Representation Model for Click-Through Rate Prediction",
        "authors": [
            "Shereen Elsayed",
            "Lars Schmidt-Thieme"
        ],
        "abstract": "Click-Through Rate prediction (CTR) is a crucial task in recommender systems,\nand it gained considerable attention in the past few years. The primary purpose\nof recent research emphasizes obtaining meaningful and powerful representations\nthrough mining low and high feature interactions using various components such\nas Deep Neural Networks (DNN), CrossNets, or transformer blocks. In this work,\nwe propose the Deep Multi-Representation model (DeepMR) that jointly trains a\nmixture of two powerful feature representation learning components, namely DNNs\nand multi-head self-attentions. Furthermore, DeepMR integrates the novel\nresidual with zero initialization (ReZero) connections to the DNN and the\nmulti-head self-attention components for learning superior input\nrepresentations. Experiments on three real-world datasets show that the\nproposed model significantly outperforms all state-of-the-art models in the\ntask of click-through rate prediction.",
        "categories": [
            "cs.IR",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2210.10664v2",
        "date": "2022-10-18 09:37:11+00:00"
    },
    {
        "title": "A Deep Bag-of-Features Model for Music Auto-Tagging",
        "authors": [
            "Juhan Nam",
            "Jorge Herrera",
            "Kyogu Lee"
        ],
        "abstract": "Feature learning and deep learning have drawn great attention in recent years\nas a way of transforming input data into more effective representations using\nlearning algorithms. Such interest has grown in the area of music information\nretrieval (MIR) as well, particularly in music audio classification tasks such\nas auto-tagging. In this paper, we present a two-stage learning model to\neffectively predict multiple labels from music audio. The first stage learns to\nproject local spectral patterns of an audio track onto a high-dimensional\nsparse space in an unsupervised manner and summarizes the audio track as a\nbag-of-features. The second stage successively performs the unsupervised\nlearning on the bag-of-features in a layer-by-layer manner to initialize a deep\nneural network and finally fine-tunes it with the tag labels. Through the\nexperiment, we rigorously examine training choices and tuning parameters, and\nshow that the model achieves high performance on Magnatagatune, a popularly\nused dataset in music auto-tagging.",
        "categories": [
            "cs.LG",
            "cs.SD",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1508.04999v3",
        "date": "2015-08-20 14:38:56+00:00"
    },
    {
        "title": "Multi-Band Multi-Resolution Fully Convolutional Neural Networks for Singing Voice Separation",
        "authors": [
            "Emad M. Grais",
            "Fei Zhao",
            "Mark D. Plumbley"
        ],
        "abstract": "Deep neural networks with convolutional layers usually process the entire\nspectrogram of an audio signal with the same time-frequency resolutions, number\nof filters, and dimensionality reduction scale. According to the constant-Q\ntransform, good features can be extracted from audio signals if the low\nfrequency bands are processed with high frequency resolution filters and the\nhigh frequency bands with high time resolution filters. In the spectrogram of a\nmixture of singing voices and music signals, there is usually more information\nabout the voice in the low frequency bands than the high frequency bands. These\nraise the need for processing each part of the spectrogram differently. In this\npaper, we propose a multi-band multi-resolution fully convolutional neural\nnetwork (MBR-FCN) for singing voice separation. The MBR-FCN processes the\nfrequency bands that have more information about the target signals with more\nfilters and smaller dimentionality reduction scale than the bands with less\ninformation. Furthermore, the MBR-FCN processes the low frequency bands with\nhigh frequency resolution filters and the high frequency bands with high time\nresolution filters. Our experimental results show that the proposed MBR-FCN\nwith very few parameters achieves better singing voice separation performance\nthan other deep neural networks.",
        "categories": [
            "cs.SD",
            "cs.LG",
            "eess.AS",
            "eess.SP",
            "stat.ML",
            "68T01, 68T10, 68T45, 62H25",
            "H.5.5; I.5; I.2.6; I.4.3; I.4; I.2"
        ],
        "link": "http://arxiv.org/pdf/1910.09266v1",
        "date": "2019-10-21 11:29:29+00:00"
    },
    {
        "title": "Deep Sequential Neural Network",
        "authors": [
            "Ludovic Denoyer",
            "Patrick Gallinari"
        ],
        "abstract": "Neural Networks sequentially build high-level features through their\nsuccessive layers. We propose here a new neural network model where each layer\nis associated with a set of candidate mappings. When an input is processed, at\neach layer, one mapping among these candidates is selected according to a\nsequential decision process. The resulting model is structured according to a\nDAG like architecture, so that a path from the root to a leaf node defines a\nsequence of transformations. Instead of considering global transformations,\nlike in classical multilayer networks, this model allows us for learning a set\nof local transformations. It is thus able to process data with different\ncharacteristics through specific sequences of such local transformations,\nincreasing the expression power of this model w.r.t a classical multilayered\nnetwork. The learning algorithm is inspired from policy gradient techniques\ncoming from the reinforcement learning domain and is used here instead of the\nclassical back-propagation based gradient descent techniques. Experiments on\ndifferent datasets show the relevance of this approach.",
        "categories": [
            "cs.LG",
            "cs.NE"
        ],
        "link": "http://arxiv.org/pdf/1410.0510v1",
        "date": "2014-10-02 10:58:17+00:00"
    },
    {
        "title": "Persistence Initialization: A novel adaptation of the Transformer architecture for Time Series Forecasting",
        "authors": [
            "Espen Haugsdal",
            "Erlend Aune",
            "Massimiliano Ruocco"
        ],
        "abstract": "Time series forecasting is an important problem, with many real world\napplications. Ensembles of deep neural networks have recently achieved\nimpressive forecasting accuracy, but such large ensembles are impractical in\nmany real world settings. Transformer models been successfully applied to a\ndiverse set of challenging problems. We propose a novel adaptation of the\noriginal Transformer architecture focusing on the task of time series\nforecasting, called Persistence Initialization. The model is initialized as a\nnaive persistence model by using a multiplicative gating mechanism combined\nwith a residual skip connection. We use a decoder Transformer with ReZero\nnormalization and Rotary positional encodings, but the adaptation is applicable\nto any auto-regressive neural network model. We evaluate our proposed\narchitecture on the challenging M4 dataset, achieving competitive performance\ncompared to ensemble based methods. We also compare against existing recently\nproposed Transformer models for time series forecasting, showing superior\nperformance on the M4 dataset. Extensive ablation studies show that Persistence\nInitialization leads to better performance and faster convergence. As the size\nof the model increases, only the models with our proposed adaptation gain in\nperformance. We also perform an additional ablation study to determine the\nimportance of the choice of normalization and positional encoding, and find\nboth the use of Rotary encodings and ReZero normalization to be essential for\ngood forecasting performance.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2208.14236v1",
        "date": "2022-08-30 13:04:48+00:00"
    },
    {
        "title": "Learning to Augment Synthetic Images for Sim2Real Policy Transfer",
        "authors": [
            "Alexander Pashevich",
            "Robin Strudel",
            "Igor Kalevatykh",
            "Ivan Laptev",
            "Cordelia Schmid"
        ],
        "abstract": "Vision and learning have made significant progress that could improve\nrobotics policies for complex tasks and environments. Learning deep neural\nnetworks for image understanding, however, requires large amounts of\ndomain-specific visual data. While collecting such data from real robots is\npossible, such an approach limits the scalability as learning policies\ntypically requires thousands of trials. In this work we attempt to learn\nmanipulation policies in simulated environments. Simulators enable scalability\nand provide access to the underlying world state during training. Policies\nlearned in simulators, however, do not transfer well to real scenes given the\ndomain gap between real and synthetic data. We follow recent work on domain\nrandomization and augment synthetic images with sequences of random\ntransformations. Our main contribution is to optimize the augmentation strategy\nfor sim2real transfer and to enable domain-independent policy learning. We\ndesign an efficient search for depth image augmentations using object\nlocalization as a proxy task. Given the resulting sequence of random\ntransformations, we use it to augment synthetic depth images during policy\nlearning. Our augmentation strategy is policy-independent and enables policy\nlearning with no real images. We demonstrate our approach to significantly\nimprove accuracy on three manipulation tasks evaluated on a real robot.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1903.07740v2",
        "date": "2019-03-18 22:01:57+00:00"
    },
    {
        "title": "Structured Semantic Model supported Deep Neural Network for Click-Through Rate Prediction",
        "authors": [
            "Chenglei Niu",
            "Guojing Zhong",
            "Ying Liu",
            "Yandong Zhang",
            "Yongsheng Sun",
            "Ailong He",
            "Zhaoji Chen"
        ],
        "abstract": "With the rapid development of online advertising and recommendation systems,\nclick-through rate prediction is expected to play an increasingly important\nrole.Recently many DNN-based models which follow a similar Embedding&MLP\nparadigm have been proposed, and have achieved good result in image/voice and\nnlp fields. In these methods the Wide&Deep model announced by Google plays a\nkey role.Most models first map large scale sparse input features into\nlow-dimensional vectors which are transformed to fixed-length vectors, then\nconcatenated together before being fed into a multilayer perceptron (MLP) to\nlearn non-linear relations among input features. The number of trainable\nvariables normally grow dramatically the number of feature fields and the\nembedding dimension grow. It is a big challenge to get state-of-the-art result\nthrough training deep neural network and embedding together, which falls into\nlocal optimal or overfitting easily. In this paper, we propose an Structured\nSemantic Model (SSM) to tackles this challenge by designing a orthogonal base\nconvolution and pooling model which adaptively learn the multi-scale base\nsemantic representation between features supervised by the click label.The\noutput of SSM are then used in the Wide&Deep for CTR prediction.Experiments on\ntwo public datasets as well as real Weibo production dataset with over 1\nbillion samples have demonstrated the effectiveness of our proposed approach\nwith superior performance comparing to state-of-the-art methods.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1812.01353v5",
        "date": "2018-12-04 12:03:52+00:00"
    },
    {
        "title": "Reliability of CKA as a Similarity Measure in Deep Learning",
        "authors": [
            "MohammadReza Davari",
            "Stefan Horoi",
            "Amine Natik",
            "Guillaume Lajoie",
            "Guy Wolf",
            "Eugene Belilovsky"
        ],
        "abstract": "Comparing learned neural representations in neural networks is a challenging\nbut important problem, which has been approached in different ways. The\nCentered Kernel Alignment (CKA) similarity metric, particularly its linear\nvariant, has recently become a popular approach and has been widely used to\ncompare representations of a network's different layers, of architecturally\nsimilar networks trained differently, or of models with different architectures\ntrained on the same data. A wide variety of conclusions about similarity and\ndissimilarity of these various representations have been made using CKA. In\nthis work we present analysis that formally characterizes CKA sensitivity to a\nlarge class of simple transformations, which can naturally occur in the context\nof modern machine learning. This provides a concrete explanation of CKA\nsensitivity to outliers, which has been observed in past works, and to\ntransformations that preserve the linear separability of the data, an important\ngeneralization attribute. We empirically investigate several weaknesses of the\nCKA similarity metric, demonstrating situations in which it gives unexpected or\ncounter-intuitive results. Finally we study approaches for modifying\nrepresentations to maintain functional behaviour while changing the CKA value.\nOur results illustrate that, in many cases, the CKA value can be easily\nmanipulated without substantial changes to the functional behaviour of the\nmodels, and call for caution when leveraging activation alignment metrics.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "link": "http://arxiv.org/pdf/2210.16156v2",
        "date": "2022-10-28 14:32:52+00:00"
    },
    {
        "title": "Learning Single/Multi-Attribute of Object with Symmetry and Group",
        "authors": [
            "Yong-Lu Li",
            "Yue Xu",
            "Xinyu Xu",
            "Xiaohan Mao",
            "Cewu Lu"
        ],
        "abstract": "Attributes and objects can compose diverse compositions. To model the\ncompositional nature of these concepts, it is a good choice to learn them as\ntransformations, e.g., coupling and decoupling. However, complex\ntransformations need to satisfy specific principles to guarantee rationality.\nHere, we first propose a previously ignored principle of attribute-object\ntransformation: Symmetry. For example, coupling peeled-apple with attribute\npeeled should result in peeled-apple, and decoupling peeled from apple should\nstill output apple. Incorporating the symmetry, we propose a transformation\nframework inspired by group theory, i.e., SymNet. It consists of two modules:\nCoupling Network and Decoupling Network. We adopt deep neural networks to\nimplement SymNet and train it in an end-to-end paradigm with the group axioms\nand symmetry as objectives. Then, we propose a Relative Moving Distance (RMD)\nbased method to utilize the attribute change instead of the attribute pattern\nitself to classify attributes. Besides the compositions of single-attribute and\nobject, our RMD is also suitable for complex compositions of multiple\nattributes and objects when incorporating attribute correlations. SymNet can be\nutilized for attribute learning, compositional zero-shot learning and\noutperforms the state-of-the-art on four widely-used benchmarks. Code is at\nhttps://github.com/DirtyHarryLYL/SymNet.",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2110.04603v1",
        "date": "2021-10-09 15:57:17+00:00"
    },
    {
        "title": "Neural Mixture Models with Expectation-Maximization for End-to-end Deep Clustering",
        "authors": [
            "Dumindu Tissera",
            "Kasun Vithanage",
            "Rukshan Wijesinghe",
            "Alex Xavier",
            "Sanath Jayasena",
            "Subha Fernando",
            "Ranga Rodrigo"
        ],
        "abstract": "Any clustering algorithm must synchronously learn to model the clusters and\nallocate data to those clusters in the absence of labels. Mixture model-based\nmethods model clusters with pre-defined statistical distributions and allocate\ndata to those clusters based on the cluster likelihoods. They iteratively\nrefine those distribution parameters and member assignments following the\nExpectation-Maximization (EM) algorithm. However, the cluster representability\nof such hand-designed distributions that employ a limited amount of parameters\nis not adequate for most real-world clustering tasks. In this paper, we realize\nmixture model-based clustering with a neural network where the final layer\nneurons, with the aid of an additional transformation, approximate cluster\ndistribution outputs. The network parameters pose as the parameters of those\ndistributions. The result is an elegant, much-generalized representation of\nclusters than a restricted mixture of hand-designed distributions. We train the\nnetwork end-to-end via batch-wise EM iterations where the forward pass acts as\nthe E-step and the backward pass acts as the M-step. In image clustering, the\nmixture-based EM objective can be used as the clustering objective along with\nexisting representation learning methods. In particular, we show that when\nmixture-EM optimization is fused with consistency optimization, it improves the\nsole consistency optimization performance in clustering. Our trained networks\noutperform single-stage deep clustering methods that still depend on k-means,\nwith unsupervised classification accuracy of 63.8% in STL10, 58% in CIFAR10,\n25.9% in CIFAR100, and 98.9% in MNIST.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "68T10, 62H30",
            "I.2; I.4; I.5"
        ],
        "link": "http://arxiv.org/pdf/2107.02453v2",
        "date": "2021-07-06 08:00:58+00:00"
    },
    {
        "title": "Two Steps Forward and One Behind: Rethinking Time Series Forecasting with Deep Learning",
        "authors": [
            "Riccardo Ughi",
            "Eugenio Lomurno",
            "Matteo Matteucci"
        ],
        "abstract": "The Transformer is a highly successful deep learning model that has\nrevolutionised the world of artificial neural networks, first in natural\nlanguage processing and later in computer vision. This model is based on the\nattention mechanism and is able to capture complex semantic relationships\nbetween a variety of patterns present in the input data. Precisely because of\nthese characteristics, the Transformer has recently been exploited for time\nseries forecasting problems, assuming its natural adaptability to the domain of\ncontinuous numerical series. Despite the acclaimed results in the literature,\nsome works have raised doubts about the robustness of this approach. In this\npaper, we further investigate the effectiveness of Transformer-based models\napplied to the domain of time series forecasting, demonstrate their\nlimitations, and propose a set of alternative models that are better performing\nand significantly less complex. In particular, we empirically show how\nsimplifying this forecasting model almost always leads to an improvement,\nreaching the state of the art among Transformer-based architectures. We also\npropose shallow models without the attention mechanism, which compete with the\noverall state of the art in long time series forecasting, and demonstrate their\nability to accurately predict extremely long windows. We show how it is always\nnecessary to use a simple baseline to verify the effectiveness of one's models,\nand finally we conclude the paper with a reflection on recent research paths\nand the desire to follow trends and apply the latest model even where it may\nnot be necessary.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2304.04553v1",
        "date": "2023-04-10 12:47:42+00:00"
    },
    {
        "title": "Scalable First-Order Bayesian Optimization via Structured Automatic Differentiation",
        "authors": [
            "Sebastian Ament",
            "Carla Gomes"
        ],
        "abstract": "Bayesian Optimization (BO) has shown great promise for the global\noptimization of functions that are expensive to evaluate, but despite many\nsuccesses, standard approaches can struggle in high dimensions. To improve the\nperformance of BO, prior work suggested incorporating gradient information into\na Gaussian process surrogate of the objective, giving rise to kernel matrices\nof size $nd \\times nd$ for $n$ observations in $d$ dimensions. Na\\\"ively\nmultiplying with (resp. inverting) these matrices requires\n$\\mathcal{O}(n^2d^2)$ (resp. $\\mathcal{O}(n^3d^3$)) operations, which becomes\ninfeasible for moderate dimensions and sample sizes. Here, we observe that a\nwide range of kernels gives rise to structured matrices, enabling an exact\n$\\mathcal{O}(n^2d)$ matrix-vector multiply for gradient observations and\n$\\mathcal{O}(n^2d^2)$ for Hessian observations. Beyond canonical kernel\nclasses, we derive a programmatic approach to leveraging this type of structure\nfor transformations and combinations of the discussed kernel classes, which\nconstitutes a structure-aware automatic differentiation algorithm. Our methods\napply to virtually all canonical kernels and automatically extend to complex\nkernels, like the neural network, radial basis function network, and spectral\nmixture kernels without any additional derivations, enabling flexible,\nproblem-dependent modeling while scaling first-order BO to high $d$.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.MS",
            "math.OC",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2206.08366v1",
        "date": "2022-06-16 17:59:48+00:00"
    },
    {
        "title": "Deep Learning and Symbolic Regression for Discovering Parametric Equations",
        "authors": [
            "Michael Zhang",
            "Samuel Kim",
            "Peter Y. Lu",
            "Marin Solja\u010di\u0107"
        ],
        "abstract": "Symbolic regression is a machine learning technique that can learn the\ngoverning formulas of data and thus has the potential to transform scientific\ndiscovery. However, symbolic regression is still limited in the complexity and\ndimensionality of the systems that it can analyze. Deep learning on the other\nhand has transformed machine learning in its ability to analyze extremely\ncomplex and high-dimensional datasets. We propose a neural network architecture\nto extend symbolic regression to parametric systems where some coefficient may\nvary but the structure of the underlying governing equation remains constant.\nWe demonstrate our method on various analytic expressions, ODEs, and PDEs with\nvarying coefficients and show that it extrapolates well outside of the training\ndomain. The neural network-based architecture can also integrate with other\ndeep learning architectures so that it can analyze high-dimensional data while\nbeing trained end-to-end. To this end we integrate our architecture with\nconvolutional neural networks to analyze 1D images of varying spring systems.",
        "categories": [
            "cs.LG",
            "cs.SC",
            "physics.comp-ph",
            "physics.data-an"
        ],
        "link": "http://arxiv.org/pdf/2207.00529v1",
        "date": "2022-07-01 16:25:59+00:00"
    },
    {
        "title": "Machine Learning for Dental Image Analysis",
        "authors": [
            "Young-jun Yu"
        ],
        "abstract": "In order to study the application of artificial intelligence (AI) to dental\nimaging, we applied AI technology to classify a set of panoramic radiographs\nusing (a) a convolutional neural network (CNN) which is a form of an artificial\nneural network (ANN), (b) representative image cognition algorithms that\nimplement scale-invariant feature transform (SIFT), and (c) histogram of\noriented gradients (HOG).",
        "categories": [
            "stat.ML",
            "cs.CV",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1611.09958v2",
        "date": "2016-11-30 01:17:34+00:00"
    },
    {
        "title": "Convolutional Signature for Sequential Data",
        "authors": [
            "Ming Min",
            "Tomoyuki Ichiba"
        ],
        "abstract": "Signature is an infinite graded sequence of statistics known to characterize\ngeometric rough paths, which includes the paths with bounded variation. This\nobject has been studied successfully for machine learning with mostly\napplications in low dimensional cases. In the high dimensional case, it suffers\nfrom exponential growth in the number of features in truncated signature\ntransform. We propose a novel neural network based model which borrows the idea\nfrom Convolutional Neural Network to address this problem. Our model reduces\nthe number of features efficiently in a data dependent way. Some empirical\nexperiments are provided to support our model.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2009.06719v2",
        "date": "2020-09-14 20:01:56+00:00"
    },
    {
        "title": "Combinatorial Convolutional Neural Networks for Words",
        "authors": [
            "Karen Sargsyan"
        ],
        "abstract": "The paper discusses the limitations of deep learning models in identifying\nand utilizing features that remain invariant under a bijective transformation\non the data entries, which we refer to as combinatorial patterns. We argue that\nthe identification of such patterns may be important for certain applications\nand suggest providing neural networks with information that fully describes the\ncombinatorial patterns of input entries and allows the network to determine\nwhat is relevant for prediction. To demonstrate the feasibility of this\napproach, we present a combinatorial convolutional neural network for word\nclassification.",
        "categories": [
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2303.16211v1",
        "date": "2023-03-28 07:49:06+00:00"
    },
    {
        "title": "Analyzing and Improving Neural Networks by Generating Semantic Counterexamples through Differentiable Rendering",
        "authors": [
            "Lakshya Jain",
            "Varun Chandrasekaran",
            "Uyeong Jang",
            "Wilson Wu",
            "Andrew Lee",
            "Andy Yan",
            "Steven Chen",
            "Somesh Jha",
            "Sanjit A. Seshia"
        ],
        "abstract": "Even as deep neural networks (DNNs) have achieved remarkable success on\nvision-related tasks, their performance is brittle to transformations in the\ninput. Of particular interest are semantic transformations that model changes\nthat have a basis in the physical world, such as rotations, translations,\nchanges in lighting or camera pose. In this paper, we show how differentiable\nrendering can be utilized to generate images that are informative, yet\nrealistic, and which can be used to analyze DNN performance and improve its\nrobustness through data augmentation. Given a differentiable renderer and a\nDNN, we show how to use off-the-shelf attacks from adversarial machine learning\nto generate semantic counterexamples -- images where semantic features are\nchanged as to produce misclassifications or misdetections. We validate our\napproach on DNNs for image classification and object detection. For\nclassification, we show that semantic counterexamples, when used to augment the\ndataset, (i) improve generalization performance (ii) enhance robustness to\nsemantic transformations, and (iii) transfer between models. Additionally, in\ncomparison to sampling-based semantic augmentation, our technique generates\nmore informative data in a sample efficient manner.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1910.00727v2",
        "date": "2019-10-02 00:47:57+00:00"
    },
    {
        "title": "An End-to-End HW/SW Co-Design Methodology to Design Efficient Deep Neural Network Systems using Virtual Models",
        "authors": [
            "Michael J. Klaiber",
            "Sebastian Vogel",
            "Axel Acosta",
            "Robert Korn",
            "Leonardo Ecco",
            "Kristine Back",
            "Andre Guntoro",
            "Ingo Feldner"
        ],
        "abstract": "End-to-end performance estimation and measurement of deep neural network\n(DNN) systems become more important with increasing complexity of DNN systems\nconsisting of hardware and software components. The methodology proposed in\nthis paper aims at a reduced turn-around time for evaluating different design\nchoices of hardware and software components of DNN systems. This reduction is\nachieved by moving the performance estimation from the implementation phase to\nthe concept phase by employing virtual hardware models instead of gathering\nmeasurement results from physical prototypes. Deep learning compilers introduce\nhardware-specific transformations and are, therefore, considered a part of the\ndesign flow of virtual system models to extract end-to-end performance\nestimations. To validate the run-time accuracy of the proposed methodology, a\nsystem processing the DilatedVGG DNN is realized both as virtual system model\nand as hardware implementation. The results show that up to 92 % accuracy can\nbe reached in predicting the processing time of the DNN inference.",
        "categories": [
            "cs.LG",
            "cs.DC",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1910.11632v2",
        "date": "2019-10-25 11:42:00+00:00"
    },
    {
        "title": "Rational Neural Networks for Approximating Jump Discontinuities of Graph Convolution Operator",
        "authors": [
            "Zhiqian Chen",
            "Feng Chen",
            "Rongjie Lai",
            "Xuchao Zhang",
            "Chang-Tien Lu"
        ],
        "abstract": "For node level graph encoding, a recent important state-of-art method is the\ngraph convolutional networks (GCN), which nicely integrate local vertex\nfeatures and graph topology in the spectral domain. However, current studies\nsuffer from several drawbacks: (1) graph CNNs relies on Chebyshev polynomial\napproximation which results in oscillatory approximation at jump\ndiscontinuities; (2) Increasing the order of Chebyshev polynomial can reduce\nthe oscillations issue, but also incurs unaffordable computational cost; (3)\nChebyshev polynomials require degree $\\Omega$(poly(1/$\\epsilon$)) to\napproximate a jump signal such as $|x|$, while rational function only needs\n$\\mathcal{O}$(poly log(1/$\\epsilon$))\\cite{liang2016deep,telgarsky2017neural}.\nHowever, it's non-trivial to apply rational approximation without increasing\ncomputational complexity due to the denominator. In this paper, the superiority\nof rational approximation is exploited for graph signal recovering. RatioanlNet\nis proposed to integrate rational function and neural networks. We show that\nrational function of eigenvalues can be rewritten as a function of graph\nLaplacian, which can avoid multiplication by the eigenvector matrix. Focusing\non the analysis of approximation on graph convolution operation, a graph signal\nregression task is formulated. Under graph signal regression task, its time\ncomplexity can be significantly reduced by graph Fourier transform. To overcome\nthe local minimum problem of neural networks model, a relaxed Remez algorithm\nis utilized to initialize the weight parameters. Convergence rate of\nRatioanlNet and polynomial based methods on jump signal is analyzed for a\ntheoretical guarantee. The extensive experimental results demonstrated that our\napproach could effectively characterize the jump discontinuities, outperforming\ncompeting methods by a substantial margin on both synthetic and real-world\ngraphs.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1808.10073v1",
        "date": "2018-08-30 00:33:28+00:00"
    },
    {
        "title": "The global optimum of shallow neural network is attained by ridgelet transform",
        "authors": [
            "Sho Sonoda",
            "Isao Ishikawa",
            "Masahiro Ikeda",
            "Kei Hagihara",
            "Yoshihiro Sawano",
            "Takuo Matsubara",
            "Noboru Murata"
        ],
        "abstract": "We prove that the global minimum of the backpropagation (BP) training problem\nof neural networks with an arbitrary nonlinear activation is given by the\nridgelet transform. A series of computational experiments show that there\nexists an interesting similarity between the scatter plot of hidden parameters\nin a shallow neural network after the BP training and the spectrum of the\nridgelet transform. By introducing a continuous model of neural networks, we\nreduce the training problem to a convex optimization in an infinite dimensional\nHilbert space, and obtain the explicit expression of the global optimizer via\nthe ridgelet transform.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1805.07517v3",
        "date": "2018-05-19 05:28:50+00:00"
    },
    {
        "title": "Stitchable Neural Networks",
        "authors": [
            "Zizheng Pan",
            "Jianfei Cai",
            "Bohan Zhuang"
        ],
        "abstract": "The public model zoo containing enormous powerful pretrained model families\n(e.g., ResNet/DeiT) has reached an unprecedented scope than ever, which\nsignificantly contributes to the success of deep learning. As each model family\nconsists of pretrained models with diverse scales (e.g., DeiT-Ti/S/B), it\nnaturally arises a fundamental question of how to efficiently assemble these\nreadily available models in a family for dynamic accuracy-efficiency trade-offs\nat runtime. To this end, we present Stitchable Neural Networks (SN-Net), a\nnovel scalable and efficient framework for model deployment. It cheaply\nproduces numerous networks with different complexity and performance trade-offs\ngiven a family of pretrained neural networks, which we call anchors.\nSpecifically, SN-Net splits the anchors across the blocks/layers and then\nstitches them together with simple stitching layers to map the activations from\none anchor to another. With only a few epochs of training, SN-Net effectively\ninterpolates between the performance of anchors with varying scales. At\nruntime, SN-Net can instantly adapt to dynamic resource constraints by\nswitching the stitching positions. Extensive experiments on ImageNet\nclassification demonstrate that SN-Net can obtain on-par or even better\nperformance than many individually trained networks while supporting diverse\ndeployment scenarios. For example, by stitching Swin Transformers, we challenge\nhundreds of models in Timm model zoo with a single network. We believe this new\nelastic model framework can serve as a strong baseline for further research in\nwider communities.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "link": "http://arxiv.org/pdf/2302.06586v3",
        "date": "2023-02-13 18:37:37+00:00"
    },
    {
        "title": "Label Consistent Transform Learning for Hyperspectral Image Classification",
        "authors": [
            "Jyoti Maggu",
            "Hemant K. Aggarwal",
            "Angshul Majumdar"
        ],
        "abstract": "This work proposes a new image analysis tool called Label Consistent\nTransform Learning (LCTL). Transform learning is a recent unsupervised\nrepresentation learning approach; we add supervision by incorporating a label\nconsistency constraint. The proposed technique is especially suited for\nhyper-spectral image classification problems owing to its ability to learn from\nfewer samples. We have compared our proposed method on state-of-the-art\ntechniques like label consistent KSVD, Stacked Autoencoder, Deep Belief Network\nand Convolutional Neural Network. Our method yields considerably better results\n(more than 0.1 improvement in Kappa coefficient) than all the aforesaid\ntechniques.",
        "categories": [
            "eess.IV",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1912.11405v1",
        "date": "2019-12-11 09:55:54+00:00"
    },
    {
        "title": "Non-Gaussian information from weak lensing data via deep learning",
        "authors": [
            "Arushi Gupta",
            "Jos\u00e9 Manuel Zorrilla Matilla",
            "Daniel Hsu",
            "Zolt\u00e1n Haiman"
        ],
        "abstract": "Weak lensing maps contain information beyond two-point statistics on small\nscales. Much recent work has tried to extract this information through a range\nof different observables or via nonlinear transformations of the lensing field.\nHere we train and apply a 2D convolutional neural network to simulated\nnoiseless lensing maps covering 96 different cosmological models over a range\nof {$\\Omega_m,\\sigma_8$}. Using the area of the confidence contour in the\n{$\\Omega_m,\\sigma_8$} plane as a figure-of-merit, derived from simulated\nconvergence maps smoothed on a scale of 1.0 arcmin, we show that the neural\nnetwork yields $\\approx 5 \\times$ tighter constraints than the power spectrum,\nand $\\approx 4 \\times$ tighter than the lensing peaks. Such gains illustrate\nthe extent to which weak lensing data encode cosmological information not\naccessible to the power spectrum or even other, non-Gaussian statistics such as\nlensing peaks.",
        "categories": [
            "astro-ph.CO",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1802.01212v3",
        "date": "2018-02-04 22:40:17+00:00"
    },
    {
        "title": "Tabular Transformers for Modeling Multivariate Time Series",
        "authors": [
            "Inkit Padhi",
            "Yair Schiff",
            "Igor Melnyk",
            "Mattia Rigotti",
            "Youssef Mroueh",
            "Pierre Dognin",
            "Jerret Ross",
            "Ravi Nair",
            "Erik Altman"
        ],
        "abstract": "Tabular datasets are ubiquitous in data science applications. Given their\nimportance, it seems natural to apply state-of-the-art deep learning algorithms\nin order to fully unlock their potential. Here we propose neural network models\nthat represent tabular time series that can optionally leverage their\nhierarchical structure. This results in two architectures for tabular time\nseries: one for learning representations that is analogous to BERT and can be\npre-trained end-to-end and used in downstream tasks, and one that is akin to\nGPT and can be used for generation of realistic synthetic tabular sequences. We\ndemonstrate our models on two datasets: a synthetic credit card transaction\ndataset, where the learned representations are used for fraud detection and\nsynthetic data generation, and on a real pollution dataset, where the learned\nencodings are used to predict atmospheric pollutant concentrations. Code and\ndata are available at https://github.com/IBM/TabFormer.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2011.01843v2",
        "date": "2020-11-03 16:58:08+00:00"
    },
    {
        "title": "Learning Symmetries of Classical Integrable Systems",
        "authors": [
            "Roberto Bondesan",
            "Austen Lamacraft"
        ],
        "abstract": "The solution of problems in physics is often facilitated by a change of\nvariables. In this work we present neural transformations to learn symmetries\nof Hamiltonian mechanical systems. Maintaining the Hamiltonian structure\nrequires novel network architectures that parametrize symplectic\ntransformations. We demonstrate the utility of these architectures by learning\nthe structure of integrable models. Our work exemplifies the adaptation of\nneural transformations to a family constrained by more than the condition of\ninvertibility, which we expect to be a common feature of applications of these\nmethods.",
        "categories": [
            "physics.comp-ph",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1906.04645v1",
        "date": "2019-06-11 15:11:37+00:00"
    },
    {
        "title": "Nemesyst: A Hybrid Parallelism Deep Learning-Based Framework Applied for Internet of Things Enabled Food Retailing Refrigeration Systems",
        "authors": [
            "George Onoufriou",
            "Ronald Bickerton",
            "Simon Pearson",
            "Georgios Leontidis"
        ],
        "abstract": "Deep Learning has attracted considerable attention across multiple\napplication domains, including computer vision, signal processing and natural\nlanguage processing. Although quite a few single node deep learning frameworks\nexist, such as tensorflow, pytorch and keras, we still lack a complete\nprocessing structure that can accommodate large scale data processing, version\ncontrol, and deployment, all while staying agnostic of any specific single node\nframework. To bridge this gap, this paper proposes a new, higher level\nframework, i.e. Nemesyst, which uses databases along with model\nsequentialisation to allow processes to be fed unique and transformed data at\nthe point of need. This facilitates near real-time application and makes models\navailable for further training or use at any node that has access to the\ndatabase simultaneously. Nemesyst is well suited as an application framework\nfor internet of things aggregated control systems, deploying deep learning\ntechniques to optimise individual machines in massive networks. To demonstrate\nthis framework, we adopted a case study in a novel domain; deploying deep\nlearning to optimise the high speed control of electrical power consumed by a\nmassive internet of things network of retail refrigeration systems in\nproportion to load available on the UK National Grid (a demand side response).\nThe case study demonstrated for the first time in such a setting how deep\nlearning models, such as Recurrent Neural Networks (vanilla and Long-Short-Term\nMemory) and Generative Adversarial Networks paired with Nemesyst, achieve\ncompelling performance, whilst still being malleable to future adjustments as\nboth the data and requirements inevitably change over time.",
        "categories": [
            "cs.LG",
            "cs.DC",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1906.01600v2",
        "date": "2019-06-04 17:23:09+00:00"
    },
    {
        "title": "Self-Supervised and Interpretable Anomaly Detection using Network Transformers",
        "authors": [
            "Daniel L. Marino",
            "Chathurika S. Wickramasinghe",
            "Craig Rieger",
            "Milos Manic"
        ],
        "abstract": "Monitoring traffic in computer networks is one of the core approaches for\ndefending critical infrastructure against cyber attacks. Machine Learning (ML)\nand Deep Neural Networks (DNNs) have been proposed in the past as a tool to\nidentify anomalies in computer networks. Although detecting these anomalies\nprovides an indication of an attack, just detecting an anomaly is not enough\ninformation for a user to understand the anomaly. The black-box nature of\noff-the-shelf ML models prevents extracting important information that is\nfundamental to isolate the source of the fault/attack and take corrective\nmeasures. In this paper, we introduce the Network Transformer (NeT), a DNN\nmodel for anomaly detection that incorporates the graph structure of the\ncommunication network in order to improve interpretability. The presented\napproach has the following advantages: 1) enhanced interpretability by\nincorporating the graph structure of computer networks; 2) provides a\nhierarchical set of features that enables analysis at different levels of\ngranularity; 3) self-supervised training that does not require labeled data.\nThe presented approach was tested by evaluating the successful detection of\nanomalies in an Industrial Control System (ICS). The presented approach\nsuccessfully identified anomalies, the devices affected, and the specific\nconnections causing the anomalies, providing a data-driven hierarchical\napproach to analyze the behavior of a cyber network.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CR"
        ],
        "link": "http://arxiv.org/pdf/2202.12997v1",
        "date": "2022-02-25 22:05:59+00:00"
    },
    {
        "title": "STFL: A Temporal-Spatial Federated Learning Framework for Graph Neural Networks",
        "authors": [
            "Guannan Lou",
            "Yuze Liu",
            "Tiehua Zhang",
            "Xi Zheng"
        ],
        "abstract": "We present a spatial-temporal federated learning framework for graph neural\nnetworks, namely STFL. The framework explores the underlying correlation of the\ninput spatial-temporal data and transform it to both node features and\nadjacency matrix. The federated learning setting in the framework ensures data\nprivacy while achieving a good model generalization. Experiments results on the\nsleep stage dataset, ISRUC_S3, illustrate the effectiveness of STFL on graph\nprediction tasks.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2111.06750v2",
        "date": "2021-11-12 14:55:57+00:00"
    },
    {
        "title": "Not All Attention Is Needed: Gated Attention Network for Sequence Data",
        "authors": [
            "Lanqing Xue",
            "Xiaopeng Li",
            "Nevin L. Zhang"
        ],
        "abstract": "Although deep neural networks generally have fixed network structures, the\nconcept of dynamic mechanism has drawn more and more attention in recent years.\nAttention mechanisms compute input-dependent dynamic attention weights for\naggregating a sequence of hidden states. Dynamic network configuration in\nconvolutional neural networks (CNNs) selectively activates only part of the\nnetwork at a time for different inputs. In this paper, we combine the two\ndynamic mechanisms for text classification tasks. Traditional attention\nmechanisms attend to the whole sequence of hidden states for an input sentence,\nwhile in most cases not all attention is needed especially for long sequences.\nWe propose a novel method called Gated Attention Network (GA-Net) to\ndynamically select a subset of elements to attend to using an auxiliary\nnetwork, and compute attention weights to aggregate the selected elements. It\navoids a significant amount of unnecessary computation on unattended elements,\nand allows the model to pay attention to important parts of the sequence.\nExperiments in various datasets show that the proposed method achieves better\nperformance compared with all baseline models with global or local attention\nwhile requiring less computation and achieving better interpretability. It is\nalso promising to extend the idea to more complex attention-based models, such\nas transformers and seq-to-seq models.",
        "categories": [
            "cs.LG",
            "cs.CL",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1912.00349v1",
        "date": "2019-12-01 07:57:41+00:00"
    },
    {
        "title": "Unconstrained Monotonic Neural Networks",
        "authors": [
            "Antoine Wehenkel",
            "Gilles Louppe"
        ],
        "abstract": "Monotonic neural networks have recently been proposed as a way to define\ninvertible transformations. These transformations can be combined into powerful\nautoregressive flows that have been shown to be universal approximators of\ncontinuous probability distributions. Architectures that ensure monotonicity\ntypically enforce constraints on weights and activation functions, which\nenables invertibility but leads to a cap on the expressiveness of the resulting\ntransformations. In this work, we propose the Unconstrained Monotonic Neural\nNetwork (UMNN) architecture based on the insight that a function is monotonic\nas long as its derivative is strictly positive. In particular, this latter\ncondition can be enforced with a free-form neural network whose only constraint\nis the positiveness of its output. We evaluate our new invertible building\nblock within a new autoregressive flow (UMNN-MAF) and demonstrate its\neffectiveness on density estimation experiments. We also illustrate the ability\nof UMNNs to improve variational inference.",
        "categories": [
            "cs.LG",
            "cs.NE",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1908.05164v3",
        "date": "2019-08-14 15:11:31+00:00"
    },
    {
        "title": "Randomized Deep Structured Prediction for Discourse-Level Processing",
        "authors": [
            "Manuel Widmoser",
            "Maria Leonor Pacheco",
            "Jean Honorio",
            "Dan Goldwasser"
        ],
        "abstract": "Expressive text encoders such as RNNs and Transformer Networks have been at\nthe center of NLP models in recent work. Most of the effort has focused on\nsentence-level tasks, capturing the dependencies between words in a single\nsentence, or pairs of sentences. However, certain tasks, such as argumentation\nmining, require accounting for longer texts and complicated structural\ndependencies between them. Deep structured prediction is a general framework to\ncombine the complementary strengths of expressive neural encoders and\nstructured inference for highly structured domains. Nevertheless, when the need\narises to go beyond sentences, most work relies on combining the output scores\nof independently trained classifiers. One of the main reasons for this is that\nconstrained inference comes at a high computational cost. In this paper, we\nexplore the use of randomized inference to alleviate this concern and show that\nwe can efficiently leverage deep structured prediction and expressive neural\nencoders for a set of tasks involving complicated argumentative structures.",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2101.10435v1",
        "date": "2021-01-25 21:49:32+00:00"
    },
    {
        "title": "Geometric Multimodal Deep Learning with Multi-Scaled Graph Wavelet Convolutional Network",
        "authors": [
            "Maysam Behmanesh",
            "Peyman Adibi",
            "Mohammad Saeed Ehsani",
            "Jocelyn Chanussot"
        ],
        "abstract": "Multimodal data provide complementary information of a natural phenomenon by\nintegrating data from various domains with very different statistical\nproperties. Capturing the intra-modality and cross-modality information of\nmultimodal data is the essential capability of multimodal learning methods. The\ngeometry-aware data analysis approaches provide these capabilities by\nimplicitly representing data in various modalities based on their geometric\nunderlying structures. Also, in many applications, data are explicitly defined\non an intrinsic geometric structure. Generalizing deep learning methods to the\nnon-Euclidean domains is an emerging research field, which has recently been\ninvestigated in many studies. Most of those popular methods are developed for\nunimodal data. In this paper, a multimodal multi-scaled graph wavelet\nconvolutional network (M-GWCN) is proposed as an end-to-end network. M-GWCN\nsimultaneously finds intra-modality representation by applying the multiscale\ngraph wavelet transform to provide helpful localization properties in the graph\ndomain of each modality, and cross-modality representation by learning\npermutations that encode correlations among various modalities. M-GWCN is not\nlimited to either the homogeneous modalities with the same number of data, or\nany prior knowledge indicating correspondences between modalities. Several\nsemi-supervised node classification experiments have been conducted on three\npopular unimodal explicit graph-based datasets and five multimodal implicit\nones. The experimental results indicate the superiority and effectiveness of\nthe proposed methods compared with both spectral graph domain convolutional\nneural networks and state-of-the-art multimodal methods.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2111.13361v1",
        "date": "2021-11-26 08:41:51+00:00"
    },
    {
        "title": "Neural Random Forest Imitation",
        "authors": [
            "Christoph Reinders",
            "Bodo Rosenhahn"
        ],
        "abstract": "We present Neural Random Forest Imitation - a novel approach for transforming\nrandom forests into neural networks. Existing methods produce very inefficient\narchitectures and do not scale. In this paper, we introduce a new method for\ngenerating data from a random forest and learning a neural network that\nimitates it. Without any additional training data, this transformation creates\nvery efficient neural networks that learn the decision boundaries of a random\nforest. The generated model is fully differentiable and can be combined with\nthe feature extraction in a single pipeline enabling further end-to-end\nprocessing. Experiments on several real-world benchmark datasets demonstrate\noutstanding performance in terms of scalability, accuracy, and learning with\nvery few training examples. Compared to state-of-the-art mappings, we\nsignificantly reduce the network size while achieving the same or even improved\naccuracy due to better generalization.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1911.10829v1",
        "date": "2019-11-25 11:04:30+00:00"
    },
    {
        "title": "Geometric robustness of deep networks: analysis and improvement",
        "authors": [
            "Can Kanbak",
            "Seyed-Mohsen Moosavi-Dezfooli",
            "Pascal Frossard"
        ],
        "abstract": "Deep convolutional neural networks have been shown to be vulnerable to\narbitrary geometric transformations. However, there is no systematic method to\nmeasure the invariance properties of deep networks to such transformations. We\npropose ManiFool as a simple yet scalable algorithm to measure the invariance\nof deep networks. In particular, our algorithm measures the robustness of deep\nnetworks to geometric transformations in a worst-case regime as they can be\nproblematic for sensitive applications. Our extensive experimental results show\nthat ManiFool can be used to measure the invariance of fairly complex networks\non high dimensional datasets and these values can be used for analyzing the\nreasons for it. Furthermore, we build on Manifool to propose a new adversarial\ntraining scheme and we show its effectiveness on improving the invariance\nproperties of deep neural networks.",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1711.09115v1",
        "date": "2017-11-24 19:32:57+00:00"
    },
    {
        "title": "Deep composition of tensor-trains using squared inverse Rosenblatt transports",
        "authors": [
            "Tiangang Cui",
            "Sergey Dolgov"
        ],
        "abstract": "Characterising intractable high-dimensional random variables is one of the\nfundamental challenges in stochastic computation. The recent surge of transport\nmaps offers a mathematical foundation and new insights for tackling this\nchallenge by coupling intractable random variables with tractable reference\nrandom variables. This paper generalises the functional tensor-train\napproximation of the inverse Rosenblatt transport recently developed by Dolgov\net al. (Stat Comput 30:603--625, 2020) to a wide class of high-dimensional\nnon-negative functions, such as unnormalised probability density functions.\nFirst, we extend the inverse Rosenblatt transform to enable the transport to\ngeneral reference measures other than the uniform measure. We develop an\nefficient procedure to compute this transport from a squared tensor-train\ndecomposition which preserves the monotonicity. More crucially, we integrate\nthe proposed order-preserving functional tensor-train transport into a nested\nvariable transformation framework inspired by the layered structure of deep\nneural networks. The resulting deep inverse Rosenblatt transport significantly\nexpands the capability of tensor approximations and transport maps to random\nvariables with complicated nonlinear interactions and concentrated density\nfunctions. We demonstrate the efficiency of the proposed approach on a range of\napplications in statistical learning and uncertainty quantification, including\nparameter estimation for dynamical systems and inverse problems constrained by\npartial differential equations.",
        "categories": [
            "stat.ML",
            "cs.LG",
            "cs.NA",
            "math.NA",
            "stat.CO"
        ],
        "link": "http://arxiv.org/pdf/2007.06968v3",
        "date": "2020-07-14 11:04:18+00:00"
    },
    {
        "title": "One Man's Trash is Another Man's Treasure: Resisting Adversarial Examples by Adversarial Examples",
        "authors": [
            "Chang Xiao",
            "Changxi Zheng"
        ],
        "abstract": "Modern image classification systems are often built on deep neural networks,\nwhich suffer from adversarial examples--images with deliberately crafted,\nimperceptible noise to mislead the network's classification. To defend against\nadversarial examples, a plausible idea is to obfuscate the network's gradient\nwith respect to the input image. This general idea has inspired a long line of\ndefense methods. Yet, almost all of them have proven vulnerable. We revisit\nthis seemingly flawed idea from a radically different perspective. We embrace\nthe omnipresence of adversarial examples and the numerical procedure of\ncrafting them, and turn this harmful attacking process into a useful defense\nmechanism. Our defense method is conceptually simple: before feeding an input\nimage for classification, transform it by finding an adversarial example on a\npre-trained external model. We evaluate our method against a wide range of\npossible attacks. On both CIFAR-10 and Tiny ImageNet datasets, our method is\nsignificantly more robust than state-of-the-art methods. Particularly, in\ncomparison to adversarial training, our method offers lower training cost as\nwell as stronger robustness.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "eess.IV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1911.11219v2",
        "date": "2019-11-25 20:33:59+00:00"
    },
    {
        "title": "Quantaized Winograd/Toom-Cook Convolution for DNNs: Beyond Canonical Polynomials Base",
        "authors": [
            "Barbara Barabasz"
        ],
        "abstract": "The problem how to speed up the convolution computations in Deep Neural\nNetworks is widely investigated in recent years. The Winograd convolution\nalgorithm is a common used method that significantly reduces time consumption.\nHowever, it suffers from a problem with numerical accuracy particularly for\nlower precisions. In this paper we present the application of base change\ntechnique for quantized Winograd-aware training model. We show that we can\ntrain the $8$ bit quantized network to nearly the same accuracy (up to 0.5%\nloss) for tested network (Resnet18) and dataset (CIFAR10) as for quantized\ndirect convolution with few additional operations in pre/post transformations.\nKeeping Hadamard product on $9$ bits allow us to obtain the same accuracy as\nfor direct convolution.",
        "categories": [
            "cs.LG",
            "cs.NA",
            "math.NA",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2004.11077v1",
        "date": "2020-04-23 11:15:27+00:00"
    },
    {
        "title": "GCWSNet: Generalized Consistent Weighted Sampling for Scalable and Accurate Training of Neural Networks",
        "authors": [
            "Ping Li",
            "Weijie Zhao"
        ],
        "abstract": "We develop the \"generalized consistent weighted sampling\" (GCWS) for hashing\nthe \"powered-GMM\" (pGMM) kernel (with a tuning parameter $p$). It turns out\nthat GCWS provides a numerically stable scheme for applying power\ntransformation on the original data, regardless of the magnitude of $p$ and the\ndata. The power transformation is often effective for boosting the performance,\nin many cases considerably so. We feed the hashed data to neural networks on a\nvariety of public classification datasets and name our method ``GCWSNet''. Our\nextensive experiments show that GCWSNet often improves the classification\naccuracy. Furthermore, it is evident from the experiments that GCWSNet\nconverges substantially faster. In fact, GCWS often reaches a reasonable\naccuracy with merely (less than) one epoch of the training process. This\nproperty is much desired because many applications, such as advertisement\nclick-through rate (CTR) prediction models, or data streams (i.e., data seen\nonly once), often train just one epoch. Another beneficial side effect is that\nthe computations of the first layer of the neural networks become additions\ninstead of multiplications because the input data become binary (and highly\nsparse).\n  Empirical comparisons with (normalized) random Fourier features (NRFF) are\nprovided. We also propose to reduce the model size of GCWSNet by count-sketch\nand develop the theory for analyzing the impact of using count-sketch on the\naccuracy of GCWS. Our analysis shows that an ``8-bit'' strategy should work\nwell in that we can always apply an 8-bit count-sketch hashing on the output of\nGCWS hashing without hurting the accuracy much. There are many other ways to\ntake advantage of GCWS when training deep neural networks. For example, one can\napply GCWS on the outputs of the last layer to boost the accuracy of trained\ndeep neural networks.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2201.02283v1",
        "date": "2022-01-07 01:28:27+00:00"
    },
    {
        "title": "Learning Longer-term Dependencies in RNNs with Auxiliary Losses",
        "authors": [
            "Trieu H. Trinh",
            "Andrew M. Dai",
            "Minh-Thang Luong",
            "Quoc V. Le"
        ],
        "abstract": "Despite recent advances in training recurrent neural networks (RNNs),\ncapturing long-term dependencies in sequences remains a fundamental challenge.\nMost approaches use backpropagation through time (BPTT), which is difficult to\nscale to very long sequences. This paper proposes a simple method that improves\nthe ability to capture long term dependencies in RNNs by adding an unsupervised\nauxiliary loss to the original objective. This auxiliary loss forces RNNs to\neither reconstruct previous events or predict next events in a sequence, making\ntruncated backpropagation feasible for long sequences and also improving full\nBPTT. We evaluate our method on a variety of settings, including pixel-by-pixel\nimage classification with sequence lengths up to 16\\,000, and a real document\nclassification benchmark. Our results highlight good performance and resource\nefficiency of this approach over competitive baselines, including other\nrecurrent models and a comparable sized Transformer. Further analyses reveal\nbeneficial effects of the auxiliary loss on optimization and regularization, as\nwell as extreme cases where there is little to no backpropagation.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1803.00144v3",
        "date": "2018-03-01 00:28:07+00:00"
    },
    {
        "title": "Characterizing Audio Adversarial Examples Using Temporal Dependency",
        "authors": [
            "Zhuolin Yang",
            "Bo Li",
            "Pin-Yu Chen",
            "Dawn Song"
        ],
        "abstract": "Recent studies have highlighted adversarial examples as a ubiquitous threat\nto different neural network models and many downstream applications.\nNonetheless, as unique data properties have inspired distinct and powerful\nlearning principles, this paper aims to explore their potentials towards\nmitigating adversarial inputs. In particular, our results reveal the importance\nof using the temporal dependency in audio data to gain discriminate power\nagainst adversarial examples. Tested on the automatic speech recognition (ASR)\ntasks and three recent audio adversarial attacks, we find that (i) input\ntransformation developed from image adversarial defense provides limited\nrobustness improvement and is subtle to advanced attacks; (ii) temporal\ndependency can be exploited to gain discriminative power against audio\nadversarial examples and is resistant to adaptive attacks considered in our\nexperiments. Our results not only show promising means of improving the\nrobustness of ASR systems, but also offer novel insights in exploiting\ndomain-specific data properties to mitigate negative effects of adversarial\nexamples.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CR",
            "cs.SD",
            "eess.AS",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1809.10875v2",
        "date": "2018-09-28 06:39:42+00:00"
    },
    {
        "title": "Non-normal Recurrent Neural Network (nnRNN): learning long time dependencies while improving expressivity with transient dynamics",
        "authors": [
            "Giancarlo Kerg",
            "Kyle Goyette",
            "Maximilian Puelma Touzel",
            "Gauthier Gidel",
            "Eugene Vorontsov",
            "Yoshua Bengio",
            "Guillaume Lajoie"
        ],
        "abstract": "A recent strategy to circumvent the exploding and vanishing gradient problem\nin RNNs, and to allow the stable propagation of signals over long time scales,\nis to constrain recurrent connectivity matrices to be orthogonal or unitary.\nThis ensures eigenvalues with unit norm and thus stable dynamics and training.\nHowever this comes at the cost of reduced expressivity due to the limited\nvariety of orthogonal transformations. We propose a novel connectivity\nstructure based on the Schur decomposition and a splitting of the Schur form\ninto normal and non-normal parts. This allows to parametrize matrices with\nunit-norm eigenspectra without orthogonality constraints on eigenbases. The\nresulting architecture ensures access to a larger space of spectrally\nconstrained matrices, of which orthogonal matrices are a subset. This crucial\ndifference retains the stability advantages and training speed of orthogonal\nRNNs while enhancing expressivity, especially on tasks that require\ncomputations over ongoing input sequences.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1905.12080v2",
        "date": "2019-05-28 20:41:27+00:00"
    },
    {
        "title": "Adafactor: Adaptive Learning Rates with Sublinear Memory Cost",
        "authors": [
            "Noam Shazeer",
            "Mitchell Stern"
        ],
        "abstract": "In several recently proposed stochastic optimization methods (e.g. RMSProp,\nAdam, Adadelta), parameter updates are scaled by the inverse square roots of\nexponential moving averages of squared past gradients. Maintaining these\nper-parameter second-moment estimators requires memory equal to the number of\nparameters. For the case of neural network weight matrices, we propose\nmaintaining only the per-row and per-column sums of these moving averages, and\nestimating the per-parameter second moments based on these sums. We demonstrate\nempirically that this method produces similar results to the baseline.\nSecondly, we show that adaptive methods can produce larger-than-desired updates\nwhen the decay rate of the second moment accumulator is too slow. We propose\nupdate clipping and a gradually increasing decay rate scheme as remedies.\nCombining these methods and dropping momentum, we achieve comparable results to\nthe published Adam regime in training the Transformer model on the WMT 2014\nEnglish-German machine translation task, while using very little auxiliary\nstorage in the optimizer. Finally, we propose scaling the parameter updates\nbased on the scale of the parameters themselves.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1804.04235v1",
        "date": "2018-04-11 21:42:32+00:00"
    },
    {
        "title": "Structured Dropout Variational Inference for Bayesian Neural Networks",
        "authors": [
            "Son Nguyen",
            "Duong Nguyen",
            "Khai Nguyen",
            "Khoat Than",
            "Hung Bui",
            "Nhat Ho"
        ],
        "abstract": "Approximate inference in Bayesian deep networks exhibits a dilemma of how to\nyield high fidelity posterior approximations while maintaining computational\nefficiency and scalability. We tackle this challenge by introducing a novel\nvariational structured approximation inspired by the Bayesian interpretation of\nDropout regularization. Concretely, we focus on the inflexibility of the\nfactorized structure in Dropout posterior and then propose an improved method\ncalled Variational Structured Dropout (VSD). VSD employs an orthogonal\ntransformation to learn a structured representation on the variational Gaussian\nnoise with plausible complexity, and consequently induces statistical\ndependencies in the approximate posterior. Theoretically, VSD successfully\naddresses the pathologies of previous Variational Dropout methods and thus\noffers a standard Bayesian justification. We further show that VSD induces an\nadaptive regularization term with several desirable properties which contribute\nto better generalization. Finally, we conduct extensive experiments on standard\nbenchmarks to demonstrate the effectiveness of VSD over state-of-the-art\nvariational methods on predictive accuracy, uncertainty estimation, and\nout-of-distribution detection.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2102.07927v4",
        "date": "2021-02-16 02:33:43+00:00"
    },
    {
        "title": "Cross-Lingual Relation Extraction with Transformers",
        "authors": [
            "Jian Ni",
            "Taesun Moon",
            "Parul Awasthy",
            "Radu Florian"
        ],
        "abstract": "Relation extraction (RE) is one of the most important tasks in information\nextraction, as it provides essential information for many NLP applications. In\nthis paper, we propose a cross-lingual RE approach that does not require any\nhuman annotation in a target language or any cross-lingual resources. Building\nupon unsupervised cross-lingual representation learning frameworks, we develop\nseveral deep Transformer based RE models with a novel encoding scheme that can\neffectively encode both entity location and entity type information. Our RE\nmodels, when trained with English data, outperform several deep neural network\nbased English RE models. More importantly, our models can be applied to perform\nzero-shot cross-lingual RE, achieving the state-of-the-art cross-lingual RE\nperformance on two datasets (68-89% of the accuracy of the supervised\ntarget-language RE model). The high cross-lingual transfer efficiency without\nrequiring additional training data or cross-lingual resources shows that our RE\nmodels are especially useful for low-resource languages.",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.IR",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2010.08652v1",
        "date": "2020-10-16 22:23:37+00:00"
    },
    {
        "title": "Deep Transform: Time-Domain Audio Error Correction via Probabilistic Re-Synthesis",
        "authors": [
            "Andrew J. R. Simpson"
        ],
        "abstract": "In the process of recording, storage and transmission of time-domain audio\nsignals, errors may be introduced that are difficult to correct in an\nunsupervised way. Here, we train a convolutional deep neural network to\nre-synthesize input time-domain speech signals at its output layer. We then use\nthis abstract transformation, which we call a deep transform (DT), to perform\nprobabilistic re-synthesis on further speech (of the same speaker) which has\nbeen degraded. Using the convolutive DT, we demonstrate the recovery of speech\naudio that has been subject to extreme degradation. This approach may be useful\nfor correction of errors in communications devices.",
        "categories": [
            "cs.SD",
            "cs.LG",
            "cs.NE",
            "68Txx"
        ],
        "link": "http://arxiv.org/pdf/1503.05849v1",
        "date": "2015-03-19 17:24:16+00:00"
    },
    {
        "title": "Integral Transforms in a Physics-Informed (Quantum) Neural Network setting: Applications & Use-Cases",
        "authors": [
            "Niraj Kumar",
            "Evan Philip",
            "Vincent E. Elfving"
        ],
        "abstract": "In many computational problems in engineering and science, function or model\ndifferentiation is essential, but also integration is needed. An important\nclass of computational problems include so-called integro-differential\nequations which include both integrals and derivatives of a function. In\nanother example, stochastic differential equations can be written in terms of a\npartial differential equation of a probability density function of the\nstochastic variable. To learn characteristics of the stochastic variable based\non the density function, specific integral transforms, namely moments, of the\ndensity function need to be calculated. Recently, the machine learning paradigm\nof Physics-Informed Neural Networks emerged with increasing popularity as a\nmethod to solve differential equations by leveraging automatic differentiation.\nIn this work, we propose to augment the paradigm of Physics-Informed Neural\nNetworks with automatic integration in order to compute complex integral\ntransforms on trained solutions, and to solve integro-differential equations\nwhere integrals are computed on-the-fly during training. Furthermore, we\nshowcase the techniques in various application settings, numerically simulating\nquantum computer-based neural networks as well as classical neural networks.",
        "categories": [
            "quant-ph",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2206.14184v1",
        "date": "2022-06-28 17:51:32+00:00"
    },
    {
        "title": "Origami in N dimensions: How feed-forward networks manufacture linear separability",
        "authors": [
            "Christian Keup",
            "Moritz Helias"
        ],
        "abstract": "Neural networks can implement arbitrary functions. But, mechanistically, what\nare the tools at their disposal to construct the target? For classification\ntasks, the network must transform the data classes into a linearly separable\nrepresentation in the final hidden layer. We show that a feed-forward\narchitecture has one primary tool at hand to achieve this separability:\nprogressive folding of the data manifold in unoccupied higher dimensions. The\noperation of folding provides a useful intuition in low-dimensions that\ngeneralizes to high ones. We argue that an alternative method based on shear,\nrequiring very deep architectures, plays only a small role in real-world\nnetworks. The folding operation, however, is powerful as long as layers are\nwider than the data dimensionality, allowing efficient solutions by providing\naccess to arbitrary regions in the distribution, such as data points of one\nclass forming islands within the other classes. We argue that a link exists\nbetween the universal approximation property in ReLU networks and the\nfold-and-cut theorem (Demaine et al., 1998) dealing with physical paper\nfolding. Based on the mechanistic insight, we predict that the progressive\ngeneration of separability is necessarily accompanied by neurons showing mixed\nselectivity and bimodal tuning curves. This is validated in a network trained\non the poker hand task, showing the emergence of bimodal tuning curves during\ntraining. We hope that our intuitive picture of the data transformation in deep\nnetworks can help to provide interpretability, and discuss possible\napplications to the theory of convolutional networks, loss landscapes, and\ngeneralization.\n  TL;DR: Shows that the internal processing of deep networks can be thought of\nas literal folding operations on the data distribution in the N-dimensional\nactivation space. A link to a well-known theorem in origami theory is provided.",
        "categories": [
            "cs.LG",
            "cond-mat.dis-nn",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2203.11355v1",
        "date": "2022-03-21 21:33:55+00:00"
    },
    {
        "title": "PathSAGE: Spatial Graph Attention Neural Networks With Random Path Sampling",
        "authors": [
            "Junhua Ma",
            "Jiajun Li",
            "Xueming Li",
            "Xu Li"
        ],
        "abstract": "Graph Convolutional Networks (GCNs) achieve great success in non-Euclidean\nstructure data processing recently. In existing studies, deeper layers are used\nin CCNs to extract deeper features of Euclidean structure data. However, for\nnon-Euclidean structure data, too deep GCNs will confront with problems like\n\"neighbor explosion\" and \"over-smoothing\", it also cannot be applied to large\ndatasets. To address these problems, we propose a model called PathSAGE, which\ncan learn high-order topological information and improve the model's\nperformance by expanding the receptive field. The model randomly samples paths\nstarting from the central node and aggregates them by Transformer encoder.\nPathSAGE has only one layer of structure to aggregate nodes which avoid those\nproblems above. The results of evaluation shows that our model achieves\ncomparable performance with the state-of-the-art models in inductive learning\ntasks.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2203.05793v1",
        "date": "2022-03-11 08:23:03+00:00"
    },
    {
        "title": "Transformed Low-Rank Parameterization Can Help Robust Generalization for Tensor Neural Networks",
        "authors": [
            "Andong Wang",
            "Chao Li",
            "Mingyuan Bai",
            "Zhong Jin",
            "Guoxu Zhou",
            "Qibin Zhao"
        ],
        "abstract": "Achieving efficient and robust multi-channel data learning is a challenging\ntask in data science. By exploiting low-rankness in the transformed domain,\ni.e., transformed low-rankness, tensor Singular Value Decomposition (t-SVD) has\nachieved extensive success in multi-channel data representation and has\nrecently been extended to function representation such as Neural Networks with\nt-product layers (t-NNs). However, it still remains unclear how t-SVD\ntheoretically affects the learning behavior of t-NNs. This paper is the first\nto answer this question by deriving the upper bounds of the generalization\nerror of both standard and adversarially trained t-NNs. It reveals that the\nt-NNs compressed by exact transformed low-rank parameterization can achieve a\nsharper adversarial generalization bound. In practice, although t-NNs rarely\nhave exactly transformed low-rank weights, our analysis further shows that by\nadversarial training with gradient flow (GF), the over-parameterized t-NNs with\nReLU activations are trained with implicit regularization towards transformed\nlow-rank parameterization under certain conditions. We also establish\nadversarial generalization bounds for t-NNs with approximately transformed\nlow-rank weights. Our analysis indicates that the transformed low-rank\nparameterization can promisingly enhance robust generalization for t-NNs.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2303.00196v1",
        "date": "2023-03-01 03:05:40+00:00"
    },
    {
        "title": "Active Image Indexing",
        "authors": [
            "Pierre Fernandez",
            "Matthijs Douze",
            "Herv\u00e9 J\u00e9gou",
            "Teddy Furon"
        ],
        "abstract": "Image copy detection and retrieval from large databases leverage two\ncomponents. First, a neural network maps an image to a vector representation,\nthat is relatively robust to various transformations of the image. Second, an\nefficient but approximate similarity search algorithm trades scalability (size\nand speed) against quality of the search, thereby introducing a source of\nerror. This paper improves the robustness of image copy detection with active\nindexing, that optimizes the interplay of these two components. We reduce the\nquantization loss of a given image representation by making imperceptible\nchanges to the image before its release. The loss is back-propagated through\nthe deep neural network back to the image, under perceptual constraints. These\nmodifications make the image more retrievable. Our experiments show that the\nretrieval and copy detection of activated images is significantly improved. For\ninstance, activation improves by $+40\\%$ the Recall1@1 on various image\ntransformations, and for several popular indexing structures based on product\nquantization and locality sensitivity hashing.",
        "categories": [
            "cs.IR",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2210.10620v1",
        "date": "2022-10-05 17:55:15+00:00"
    },
    {
        "title": "Persistent Hidden States and Nonlinear Transformation for Long Short-Term Memory",
        "authors": [
            "Heeyoul Choi"
        ],
        "abstract": "Recurrent neural networks (RNNs) have been drawing much attention with great\nsuccess in many applications like speech recognition and neural machine\ntranslation. Long short-term memory (LSTM) is one of the most popular RNN units\nin deep learning applications. LSTM transforms the input and the previous\nhidden states to the next states with the affine transformation, multiplication\noperations and a nonlinear activation function, which makes a good data\nrepresentation for a given task. The affine transformation includes rotation\nand reflection, which change the semantic or syntactic information of\ndimensions in the hidden states. However, considering that a model interprets\nthe output sequence of LSTM over the whole input sequence, the dimensions of\nthe states need to keep the same type of semantic or syntactic information\nregardless of the location in the sequence. In this paper, we propose a simple\nvariant of the LSTM unit, persistent recurrent unit (PRU), where each dimension\nof hidden states keeps persistent information across time, so that the space\nkeeps the same meaning over the whole sequence. In addition, to improve the\nnonlinear transformation power, we add a feedforward layer in the PRU\nstructure. In the experiment, we evaluate our proposed methods with three\ndifferent tasks, and the results confirm that our methods have better\nperformance than the conventional LSTM.",
        "categories": [
            "cs.CL",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1806.08748v2",
        "date": "2018-06-22 16:19:46+00:00"
    },
    {
        "title": "Bayesian Uncertainty Matching for Unsupervised Domain Adaptation",
        "authors": [
            "Jun Wen",
            "Nenggan Zheng",
            "Junsong Yuan",
            "Zhefeng Gong",
            "Changyou Chen"
        ],
        "abstract": "Domain adaptation is an important technique to alleviate performance\ndegradation caused by domain shift, e.g., when training and test data come from\ndifferent domains. Most existing deep adaptation methods focus on reducing\ndomain shift by matching marginal feature distributions through deep\ntransformations on the input features, due to the unavailability of target\ndomain labels. We show that domain shift may still exist via label distribution\nshift at the classifier, thus deteriorating model performances. To alleviate\nthis issue, we propose an approximate joint distribution matching scheme by\nexploiting prediction uncertainty. Specifically, we use a Bayesian neural\nnetwork to quantify prediction uncertainty of a classifier. By imposing\ndistribution matching on both features and labels (via uncertainty), label\ndistribution mismatching in source and target data is effectively alleviated,\nencouraging the classifier to produce consistent predictions across domains. We\nalso propose a few techniques to improve our method by adaptively reweighting\ndomain adaptation loss to achieve nontrivial distribution matching and stable\ntraining. Comparisons with state of the art unsupervised domain adaptation\nmethods on three popular benchmark datasets demonstrate the superiority of our\napproach, especially on the effectiveness of alleviating negative transfer.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1906.09693v1",
        "date": "2019-06-24 02:57:22+00:00"
    },
    {
        "title": "Deep Transform: Error Correction via Probabilistic Re-Synthesis",
        "authors": [
            "Andrew J. R. Simpson"
        ],
        "abstract": "Errors in data are usually unwelcome and so some means to correct them is\nuseful. However, it is difficult to define, detect or correct errors in an\nunsupervised way. Here, we train a deep neural network to re-synthesize its\ninputs at its output layer for a given class of data. We then exploit the fact\nthat this abstract transformation, which we call a deep transform (DT),\ninherently rejects information (errors) existing outside of the abstract\nfeature space. Using the DT to perform probabilistic re-synthesis, we\ndemonstrate the recovery of data that has been subject to extreme degradation.",
        "categories": [
            "cs.LG",
            "68Txx"
        ],
        "link": "http://arxiv.org/pdf/1502.04617v1",
        "date": "2015-02-16 16:41:26+00:00"
    },
    {
        "title": "The Effectiveness of Discretization in Forecasting: An Empirical Study on Neural Time Series Models",
        "authors": [
            "Stephan Rabanser",
            "Tim Januschowski",
            "Valentin Flunkert",
            "David Salinas",
            "Jan Gasthaus"
        ],
        "abstract": "Time series modeling techniques based on deep learning have seen many\nadvancements in recent years, especially in data-abundant settings and with the\ncentral aim of learning global models that can extract patterns across multiple\ntime series. While the crucial importance of appropriate data pre-processing\nand scaling has often been noted in prior work, most studies focus on improving\nmodel architectures. In this paper we empirically investigate the effect of\ndata input and output transformations on the predictive performance of several\nneural forecasting architectures. In particular, we investigate the\neffectiveness of several forms of data binning, i.e. converting real-valued\ntime series into categorical ones, when combined with feed-forward, recurrent\nneural networks, and convolution-based sequence models. In many non-forecasting\napplications where these models have been very successful, the model inputs and\noutputs are categorical (e.g. words from a fixed vocabulary in natural language\nprocessing applications or quantized pixel color intensities in computer\nvision). For forecasting applications, where the time series are typically\nreal-valued, various ad-hoc data transformations have been proposed, but have\nnot been systematically compared. To remedy this, we evaluate the forecasting\naccuracy of instances of the aforementioned model classes when combined with\ndifferent types of data scaling and binning. We find that binning almost always\nimproves performance (compared to using normalized real-valued inputs), but\nthat the particular type of binning chosen is of lesser importance.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2005.10111v1",
        "date": "2020-05-20 15:09:28+00:00"
    },
    {
        "title": "A Theoretical View on Sparsely Activated Networks",
        "authors": [
            "Cenk Baykal",
            "Nishanth Dikkala",
            "Rina Panigrahy",
            "Cyrus Rashtchian",
            "Xin Wang"
        ],
        "abstract": "Deep and wide neural networks successfully fit very complex functions today,\nbut dense models are starting to be prohibitively expensive for inference. To\nmitigate this, one promising direction is networks that activate a sparse\nsubgraph of the network. The subgraph is chosen by a data-dependent routing\nfunction, enforcing a fixed mapping of inputs to subnetworks (e.g., the Mixture\nof Experts (MoE) paradigm in Switch Transformers). However, prior work is\nlargely empirical, and while existing routing functions work well in practice,\nthey do not lead to theoretical guarantees on approximation ability. We aim to\nprovide a theoretical explanation for the power of sparse networks. As our\nfirst contribution, we present a formal model of data-dependent sparse networks\nthat captures salient aspects of popular architectures. We then introduce a\nrouting function based on locality sensitive hashing (LSH) that enables us to\nreason about how well sparse networks approximate target functions. After\nrepresenting LSH-based sparse networks with our model, we prove that sparse\nnetworks can match the approximation power of dense networks on Lipschitz\nfunctions. Applying LSH on the input vectors means that the experts interpolate\nthe target function in different subregions of the input space. To support our\ntheory, we define various datasets based on Lipschitz target functions, and we\nshow that sparse networks give a favorable trade-off between number of active\nunits and approximation quality.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2208.04461v1",
        "date": "2022-08-08 23:14:48+00:00"
    },
    {
        "title": "SSDNet: State Space Decomposition Neural Network for Time Series Forecasting",
        "authors": [
            "Yang Lin",
            "Irena Koprinska",
            "Mashud Rana"
        ],
        "abstract": "In this paper, we present SSDNet, a novel deep learning approach for time\nseries forecasting. SSDNet combines the Transformer architecture with state\nspace models to provide probabilistic and interpretable forecasts, including\ntrend and seasonality components and previous time steps important for the\nprediction. The Transformer architecture is used to learn the temporal patterns\nand estimate the parameters of the state space model directly and efficiently,\nwithout the need for Kalman filters. We comprehensively evaluate the\nperformance of SSDNet on five data sets, showing that SSDNet is an effective\nmethod in terms of accuracy and speed, outperforming state-of-the-art deep\nlearning and statistical methods, and able to provide meaningful trend and\nseasonality components.",
        "categories": [
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2112.10251v1",
        "date": "2021-12-19 20:35:16+00:00"
    },
    {
        "title": "Tensor Programs II: Neural Tangent Kernel for Any Architecture",
        "authors": [
            "Greg Yang"
        ],
        "abstract": "We prove that a randomly initialized neural network of *any architecture* has\nits Tangent Kernel (NTK) converge to a deterministic limit, as the network\nwidths tend to infinity. We demonstrate how to calculate this limit. In prior\nliterature, the heuristic study of neural network gradients often assumes every\nweight matrix used in forward propagation is independent from its transpose\nused in backpropagation (Schoenholz et al. 2017). This is known as the\n*gradient independence assumption (GIA)*. We identify a commonly satisfied\ncondition, which we call *Simple GIA Check*, such that the NTK limit\ncalculation based on GIA is correct. Conversely, when Simple GIA Check fails,\nwe show GIA can result in wrong answers. Our material here presents the NTK\nresults of Yang (2019a) in a friendly manner and showcases the *tensor\nprograms* technique for understanding wide neural networks. We provide\nreference implementations of infinite-width NTKs of recurrent neural network,\ntransformer, and batch normalization at https://github.com/thegregyang/NTK4A.",
        "categories": [
            "stat.ML",
            "cond-mat.dis-nn",
            "cs.LG",
            "cs.NE"
        ],
        "link": "http://arxiv.org/pdf/2006.14548v4",
        "date": "2020-06-25 16:45:23+00:00"
    },
    {
        "title": "Attention-based Conditioning Methods for External Knowledge Integration",
        "authors": [
            "Katerina Margatina",
            "Christos Baziotis",
            "Alexandros Potamianos"
        ],
        "abstract": "In this paper, we present a novel approach for incorporating external\nknowledge in Recurrent Neural Networks (RNNs). We propose the integration of\nlexicon features into the self-attention mechanism of RNN-based architectures.\nThis form of conditioning on the attention distribution, enforces the\ncontribution of the most salient words for the task at hand. We introduce three\nmethods, namely attentional concatenation, feature-based gating and affine\ntransformation. Experiments on six benchmark datasets show the effectiveness of\nour methods. Attentional feature-based gating yields consistent performance\nimprovement across tasks. Our approach is implemented as a simple add-on module\nfor RNN-based models with minimal computational overhead and can be adapted to\nany deep neural architecture.",
        "categories": [
            "cs.LG",
            "cs.CL",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1906.03674v1",
        "date": "2019-06-09 17:06:28+00:00"
    },
    {
        "title": "Learning from Few Samples: Transformation-Invariant SVMs with Composition and Locality at Multiple Scales",
        "authors": [
            "Tao Liu",
            "P. R. Kumar",
            "Ruida Zhou",
            "Xi Liu"
        ],
        "abstract": "Motivated by the problem of learning with small sample sizes, this paper\nshows how to incorporate into support-vector machines (SVMs) those properties\nthat have made convolutional neural networks (CNNs) successful. Particularly\nimportant is the ability to incorporate domain knowledge of invariances, e.g.,\ntranslational invariance of images. Kernels based on the \\textit{maximum}\nsimilarity over a group of transformations are not generally positive definite.\nPerhaps it is for this reason that they have not been studied theoretically. We\naddress this lacuna and show that positive definiteness indeed holds\n\\textit{with high probability} for kernels based on the maximum similarity in\nthe small training sample set regime of interest, and that they do yield the\nbest results in that regime. We also show how additional properties such as\ntheir ability to incorporate local features at multiple spatial scales, e.g.,\nas done in CNNs through max pooling, and to provide the benefits of composition\nthrough the architecture of multiple layers, can also be embedded into SVMs. We\nverify through experiments on widely available image sets that the resulting\nSVMs do provide superior accuracy in comparison to well-established deep neural\nnetwork benchmarks for small sample sizes.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2109.12784v6",
        "date": "2021-09-27 04:02:43+00:00"
    },
    {
        "title": "Fast computation of permutation equivariant layers with the partition algebra",
        "authors": [
            "Charles Godfrey",
            "Michael G. Rawson",
            "Davis Brown",
            "Henry Kvinge"
        ],
        "abstract": "Linear neural network layers that are either equivariant or invariant to\npermutations of their inputs form core building blocks of modern deep learning\narchitectures. Examples include the layers of DeepSets, as well as linear\nlayers occurring in attention blocks of transformers and some graph neural\nnetworks. The space of permutation equivariant linear layers can be identified\nas the invariant subspace of a certain symmetric group representation, and\nrecent work parameterized this space by exhibiting a basis whose vectors are\nsums over orbits of standard basis elements with respect to the symmetric group\naction. A parameterization opens up the possibility of learning the weights of\npermutation equivariant linear layers via gradient descent. The space of\npermutation equivariant linear layers is a generalization of the partition\nalgebra, an object first discovered in statistical physics with deep\nconnections to the representation theory of the symmetric group, and the basis\ndescribed above generalizes the so-called orbit basis of the partition algebra.\nWe exhibit an alternative basis, generalizing the diagram basis of the\npartition algebra, with computational benefits stemming from the fact that the\ntensors making up the basis are low rank in the sense that they naturally\nfactorize into Kronecker products. Just as multiplication by a rank one matrix\nis far less expensive than multiplication by an arbitrary matrix,\nmultiplication with these low rank tensors is far less expensive than\nmultiplication with elements of the orbit basis. Finally, we describe an\nalgorithm implementing multiplication with these basis elements.",
        "categories": [
            "cs.LG",
            "math.CO",
            "math.RT",
            "stat.ML",
            "68T07 (Primary) 05E10, 20C30 (Secondary)",
            "G.3; I.2; I.5; J.2"
        ],
        "link": "http://arxiv.org/pdf/2303.06208v1",
        "date": "2023-03-10 21:13:12+00:00"
    },
    {
        "title": "On Sampling with Approximate Transport Maps",
        "authors": [
            "Louis Grenioux",
            "Alain Durmus",
            "\u00c9ric Moulines",
            "Marylou Gabri\u00e9"
        ],
        "abstract": "Transport maps can ease the sampling of distributions with non-trivial\ngeometries by transforming them into distributions that are easier to handle.\nThe potential of this approach has risen with the development of Normalizing\nFlows (NF) which are maps parameterized with deep neural networks trained to\npush a reference distribution towards a target. NF-enhanced samplers recently\nproposed blend (Markov chain) Monte Carlo methods with either (i) proposal\ndraws from the flow or (ii) a flow-based reparametrization. In both cases, the\nquality of the learned transport conditions performance. The present work\nclarifies for the first time the relative strengths and weaknesses of these two\napproaches. Our study concludes that multimodal targets can reliability be\nhandled with flow-based proposals up to moderately high dimensions. In\ncontrast, methods relying on reparametrization struggle with multimodality but\nare more robust otherwise in high-dimensional settings and under poor training.\nTo further illustrate the influence of target-proposal adequacy, we also derive\na new quantitative bound for the mixing time of the Independent\nMetropolis-Hastings sampler.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2302.04763v1",
        "date": "2023-02-09 16:52:52+00:00"
    },
    {
        "title": "Securing the Spike: On the Transferability and Security of Spiking Neural Networks to Adversarial Examples",
        "authors": [
            "Nuo Xu",
            "Kaleel Mahmood",
            "Haowen Fang",
            "Ethan Rathbun",
            "Caiwen Ding",
            "Wujie Wen"
        ],
        "abstract": "Spiking neural networks (SNNs) have attracted much attention for their high\nenergy efficiency and for recent advances in their classification performance.\nHowever, unlike traditional deep learning approaches, the analysis and study of\nthe robustness of SNNs to adversarial examples remain relatively\nunderdeveloped. In this work we focus on advancing the adversarial attack side\nof SNNs and make three major contributions. First, we show that successful\nwhite-box adversarial attacks on SNNs are highly dependent on the underlying\nsurrogate gradient technique. Second, using the best surrogate gradient\ntechnique, we analyze the transferability of adversarial attacks on SNNs and\nother state-of-the-art architectures like Vision Transformers (ViTs) and Big\nTransfer Convolutional Neural Networks (CNNs). We demonstrate that SNNs are not\noften deceived by adversarial examples generated by Vision Transformers and\ncertain types of CNNs. Third, due to the lack of an ubiquitous white-box attack\nthat is effective across both the SNN and CNN/ViT domains, we develop a new\nwhite-box attack, the Auto Self-Attention Gradient Attack (Auto SAGA). Our\nnovel attack generates adversarial examples capable of fooling both SNN models\nand non-SNN models simultaneously. Auto SAGA is as much as $87.9\\%$ more\neffective on SNN/ViT model ensembles than conventional white-box attacks like\nPGD. Our experiments and analyses are broad and rigorous covering three\ndatasets (CIFAR-10, CIFAR-100 and ImageNet), five different white-box attacks\nand nineteen different classifier models (seven for each CIFAR dataset and five\ndifferent models for ImageNet).",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.CR",
            "cs.CV",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2209.03358v2",
        "date": "2022-09-07 17:05:48+00:00"
    },
    {
        "title": "Parameterized Temperature Scaling for Boosting the Expressive Power in Post-Hoc Uncertainty Calibration",
        "authors": [
            "Christian Tomani",
            "Daniel Cremers",
            "Florian Buettner"
        ],
        "abstract": "We address the problem of uncertainty calibration and introduce a novel\ncalibration method, Parametrized Temperature Scaling (PTS). Standard deep\nneural networks typically yield uncalibrated predictions, which can be\ntransformed into calibrated confidence scores using post-hoc calibration\nmethods. In this contribution, we demonstrate that the performance of\naccuracy-preserving state-of-the-art post-hoc calibrators is limited by their\nintrinsic expressive power. We generalize temperature scaling by computing\nprediction-specific temperatures, parameterized by a neural network. We show\nwith extensive experiments that our novel accuracy-preserving approach\nconsistently outperforms existing algorithms across a large number of model\narchitectures, datasets and metrics.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2102.12182v2",
        "date": "2021-02-24 10:18:30+00:00"
    },
    {
        "title": "Are Vision Transformers Robust to Spurious Correlations?",
        "authors": [
            "Soumya Suvra Ghosal",
            "Yifei Ming",
            "Yixuan Li"
        ],
        "abstract": "Deep neural networks may be susceptible to learning spurious correlations\nthat hold on average but not in atypical test samples. As with the recent\nemergence of vision transformer (ViT) models, it remains underexplored how\nspurious correlations are manifested in such architectures. In this paper, we\nsystematically investigate the robustness of vision transformers to spurious\ncorrelations on three challenging benchmark datasets and compare their\nperformance with popular CNNs. Our study reveals that when pre-trained on a\nsufficiently large dataset, ViT models are more robust to spurious correlations\nthan CNNs. Key to their success is the ability to generalize better from the\nexamples where spurious correlations do not hold. Further, we perform extensive\nablations and experiments to understand the role of the self-attention\nmechanism in providing robustness under spuriously correlated environments. We\nhope that our work will inspire future research on further understanding the\nrobustness of ViT models.",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2203.09125v1",
        "date": "2022-03-17 07:03:37+00:00"
    },
    {
        "title": "Missing Value Imputation on Multidimensional Time Series",
        "authors": [
            "Parikshit Bansal",
            "Prathamesh Deshpande",
            "Sunita Sarawagi"
        ],
        "abstract": "We present DeepMVI, a deep learning method for missing value imputation in\nmultidimensional time-series datasets. Missing values are commonplace in\ndecision support platforms that aggregate data over long time stretches from\ndisparate sources, and reliable data analytics calls for careful handling of\nmissing data. One strategy is imputing the missing values, and a wide variety\nof algorithms exist spanning simple interpolation, matrix factorization methods\nlike SVD, statistical models like Kalman filters, and recent deep learning\nmethods. We show that often these provide worse results on aggregate analytics\ncompared to just excluding the missing data. DeepMVI uses a neural network to\ncombine fine-grained and coarse-grained patterns along a time series, and\ntrends from related series across categorical dimensions. After failing with\noff-the-shelf neural architectures, we design our own network that includes a\ntemporal transformer with a novel convolutional window feature, and kernel\nregression with learned embeddings. The parameters and their training are\ndesigned carefully to generalize across different placements of missing blocks\nand data characteristics. Experiments across nine real datasets, four different\nmissing scenarios, comparing seven existing methods show that DeepMVI is\nsignificantly more accurate, reducing error by more than 50% in more than half\nthe cases, compared to the best existing method. Although slower than simpler\nmatrix factorization methods, we justify the increased time overheads by\nshowing that DeepMVI is the only option that provided overall more accurate\nanalytics than dropping missing values.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2103.01600v2",
        "date": "2021-03-02 09:55:05+00:00"
    },
    {
        "title": "AutoDropout: Learning Dropout Patterns to Regularize Deep Networks",
        "authors": [
            "Hieu Pham",
            "Quoc V. Le"
        ],
        "abstract": "Neural networks are often over-parameterized and hence benefit from\naggressive regularization. Conventional regularization methods, such as Dropout\nor weight decay, do not leverage the structures of the network's inputs and\nhidden states. As a result, these conventional methods are less effective than\nmethods that leverage the structures, such as SpatialDropout and DropBlock,\nwhich randomly drop the values at certain contiguous areas in the hidden states\nand setting them to zero. Although the locations of dropout areas random, the\npatterns of SpatialDropout and DropBlock are manually designed and fixed. Here\nwe propose to learn the dropout patterns. In our method, a controller learns to\ngenerate a dropout pattern at every channel and layer of a target network, such\nas a ConvNet or a Transformer. The target network is then trained with the\ndropout pattern, and its resulting validation performance is used as a signal\nfor the controller to learn from. We show that this method works well for both\nimage recognition on CIFAR-10 and ImageNet, as well as language modeling on\nPenn Treebank and WikiText-2. The learned dropout patterns also transfers to\ndifferent tasks and datasets, such as from language model on Penn Treebank to\nEngligh-French translation on WMT 2014. Our code will be available.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.CV"
        ],
        "link": "http://arxiv.org/pdf/2101.01761v1",
        "date": "2021-01-05 19:54:22+00:00"
    },
    {
        "title": "The Surprising Simplicity of the Early-Time Learning Dynamics of Neural Networks",
        "authors": [
            "Wei Hu",
            "Lechao Xiao",
            "Ben Adlam",
            "Jeffrey Pennington"
        ],
        "abstract": "Modern neural networks are often regarded as complex black-box functions\nwhose behavior is difficult to understand owing to their nonlinear dependence\non the data and the nonconvexity in their loss landscapes. In this work, we\nshow that these common perceptions can be completely false in the early phase\nof learning. In particular, we formally prove that, for a class of well-behaved\ninput distributions, the early-time learning dynamics of a two-layer\nfully-connected neural network can be mimicked by training a simple linear\nmodel on the inputs. We additionally argue that this surprising simplicity can\npersist in networks with more layers and with convolutional architecture, which\nwe verify empirically. Key to our analysis is to bound the spectral norm of the\ndifference between the Neural Tangent Kernel (NTK) at initialization and an\naffine transform of the data kernel; however, unlike many previous results\nutilizing the NTK, we do not require the network to have disproportionately\nlarge width, and the network is allowed to escape the kernel regime later in\ntraining.",
        "categories": [
            "cs.LG",
            "cs.NE",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2006.14599v1",
        "date": "2020-06-25 17:42:49+00:00"
    },
    {
        "title": "RED: Deep Recurrent Neural Networks for Sleep EEG Event Detection",
        "authors": [
            "Nicol\u00e1s I. Tapia",
            "Pablo A. Est\u00e9vez"
        ],
        "abstract": "The brain electrical activity presents several short events during sleep that\ncan be observed as distinctive micro-structures in the electroencephalogram\n(EEG), such as sleep spindles and K-complexes. These events have been\nassociated with biological processes and neurological disorders, making them a\nresearch topic in sleep medicine. However, manual detection limits their study\nbecause it is time-consuming and affected by significant inter-expert\nvariability, motivating automatic approaches. We propose a deep learning\napproach based on convolutional and recurrent neural networks for sleep EEG\nevent detection called Recurrent Event Detector (RED). RED uses one of two\ninput representations: a) the time-domain EEG signal, or b) a complex\nspectrogram of the signal obtained with the Continuous Wavelet Transform (CWT).\nUnlike previous approaches, a fixed time window is avoided and temporal context\nis integrated to better emulate the visual criteria of experts. When evaluated\non the MASS dataset, our detectors outperform the state of the art in both\nsleep spindle and K-complex detection with a mean F1-score of at least 80.9%\nand 82.6%, respectively. Although the CWT-domain model obtained a similar\nperformance than its time-domain counterpart, the former allows in principle a\nmore interpretable input representation due to the use of a spectrogram. The\nproposed approach is event-agnostic and can be used directly to detect other\ntypes of sleep events.",
        "categories": [
            "eess.SP",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2005.07795v2",
        "date": "2020-05-15 21:48:26+00:00"
    },
    {
        "title": "Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes",
        "authors": [
            "Greg Yang"
        ],
        "abstract": "Wide neural networks with random weights and biases are Gaussian processes,\nas originally observed by Neal (1995) and more recently by Lee et al. (2018)\nand Matthews et al. (2018) for deep fully-connected networks, as well as by\nNovak et al. (2019) and Garriga-Alonso et al. (2019) for deep convolutional\nnetworks. We show that this Neural Network-Gaussian Process correspondence\nsurprisingly extends to all modern feedforward or recurrent neural networks\ncomposed of multilayer perceptron, RNNs (e.g. LSTMs, GRUs), (nD or graph)\nconvolution, pooling, skip connection, attention, batch normalization, and/or\nlayer normalization. More generally, we introduce a language for expressing\nneural network computations, and our result encompasses all such expressible\nneural networks. This work serves as a tutorial on the *tensor programs*\ntechnique formulated in Yang (2019) and elucidates the Gaussian Process results\nobtained there. We provide open-source implementations of the Gaussian Process\nkernels of simple RNN, GRU, transformer, and batchnorm+ReLU network at\ngithub.com/thegregyang/GP4A.",
        "categories": [
            "cs.NE",
            "cond-mat.dis-nn",
            "cs.LG",
            "math-ph",
            "math.MP"
        ],
        "link": "http://arxiv.org/pdf/1910.12478v3",
        "date": "2019-10-28 07:31:59+00:00"
    },
    {
        "title": "Single Model Uncertainty Estimation via Stochastic Data Centering",
        "authors": [
            "Jayaraman J. Thiagarajan",
            "Rushil Anirudh",
            "Vivek Narayanaswamy",
            "Peer-Timo Bremer"
        ],
        "abstract": "We are interested in estimating the uncertainties of deep neural networks,\nwhich play an important role in many scientific and engineering problems. In\nthis paper, we present a striking new finding that an ensemble of neural\nnetworks with the same weight initialization, trained on datasets that are\nshifted by a constant bias gives rise to slightly inconsistent trained models,\nwhere the differences in predictions are a strong indicator of epistemic\nuncertainties. Using the neural tangent kernel (NTK), we demonstrate that this\nphenomena occurs in part because the NTK is not shift-invariant. Since this is\nachieved via a trivial input transformation, we show that this behavior can\ntherefore be approximated by training a single neural network -- using a\ntechnique that we call $\\Delta-$UQ -- that estimates uncertainty around\nprediction by marginalizing out the effect of the biases during inference. We\nshow that $\\Delta-$UQ's uncertainty estimates are superior to many of the\ncurrent methods on a variety of benchmarks -- outlier rejection, calibration\nunder distribution shift, and sequential design optimization of black box\nfunctions. Code for $\\Delta-$UQ can be accessed at\nhttps://github.com/LLNL/DeltaUQ",
        "categories": [
            "cs.LG",
            "cs.CV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2207.07235v2",
        "date": "2022-07-14 23:54:54+00:00"
    },
    {
        "title": "Optimization Algorithm Inspired Deep Neural Network Structure Design",
        "authors": [
            "Huan Li",
            "Yibo Yang",
            "Dongmin Chen",
            "Zhouchen Lin"
        ],
        "abstract": "Deep neural networks have been one of the dominant machine learning\napproaches in recent years. Several new network structures are proposed and\nhave better performance than the traditional feedforward neural network\nstructure. Representative ones include the skip connection structure in ResNet\nand the dense connection structure in DenseNet. However, it still lacks a\nunified guidance for the neural network structure design. In this paper, we\npropose the hypothesis that the neural network structure design can be inspired\nby optimization algorithms and a faster optimization algorithm may lead to a\nbetter neural network structure. Specifically, we prove that the propagation in\nthe feedforward neural network with the same linear transformation in different\nlayers is equivalent to minimizing some function using the gradient descent\nalgorithm. Based on this observation, we replace the gradient descent algorithm\nwith the heavy ball algorithm and Nesterov's accelerated gradient descent\nalgorithm, which are faster and inspire us to design new and better network\nstructures. ResNet and DenseNet can be considered as two special cases of our\nframework. Numerical experiments on CIFAR-10, CIFAR-100 and ImageNet verify the\nadvantage of our optimization algorithm inspired structures over ResNet and\nDenseNet.",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1810.01638v1",
        "date": "2018-10-03 08:59:41+00:00"
    },
    {
        "title": "Edge Contraction Pooling for Graph Neural Networks",
        "authors": [
            "Frederik Diehl"
        ],
        "abstract": "Graph Neural Network (GNN) research has concentrated on improving\nconvolutional layers, with little attention paid to developing graph pooling\nlayers. Yet pooling layers can enable GNNs to reason over abstracted groups of\nnodes instead of single nodes. To close this gap, we propose a graph pooling\nlayer relying on the notion of edge contraction: EdgePool learns a localized\nand sparse hard pooling transform. We show that EdgePool outperforms\nalternative pooling methods, can be easily integrated into most GNN models, and\nimproves performance on both node and graph classification.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1905.10990v1",
        "date": "2019-05-27 06:18:24+00:00"
    },
    {
        "title": "Graph Representation Learning via Graphical Mutual Information Maximization",
        "authors": [
            "Zhen Peng",
            "Wenbing Huang",
            "Minnan Luo",
            "Qinghua Zheng",
            "Yu Rong",
            "Tingyang Xu",
            "Junzhou Huang"
        ],
        "abstract": "The richness in the content of various information networks such as social\nnetworks and communication networks provides the unprecedented potential for\nlearning high-quality expressive representations without external supervision.\nThis paper investigates how to preserve and extract the abundant information\nfrom graph-structured data into embedding space in an unsupervised manner. To\nthis end, we propose a novel concept, Graphical Mutual Information (GMI), to\nmeasure the correlation between input graphs and high-level hidden\nrepresentations. GMI generalizes the idea of conventional mutual information\ncomputations from vector space to the graph domain where measuring mutual\ninformation from two aspects of node features and topological structure is\nindispensable. GMI exhibits several benefits: First, it is invariant to the\nisomorphic transformation of input graphs---an inevitable constraint in many\nexisting graph representation learning algorithms; Besides, it can be\nefficiently estimated and maximized by current mutual information estimation\nmethods such as MINE; Finally, our theoretical analysis confirms its\ncorrectness and rationality. With the aid of GMI, we develop an unsupervised\nlearning model trained by maximizing GMI between the input and output of a\ngraph neural encoder. Considerable experiments on transductive as well as\ninductive node classification and link prediction demonstrate that our method\noutperforms state-of-the-art unsupervised counterparts, and even sometimes\nexceeds the performance of supervised ones.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2002.01169v1",
        "date": "2020-02-04 08:33:49+00:00"
    },
    {
        "title": "Formal Algorithms for Transformers",
        "authors": [
            "Mary Phuong",
            "Marcus Hutter"
        ],
        "abstract": "This document aims to be a self-contained, mathematically precise overview of\ntransformer architectures and algorithms (*not* results). It covers what\ntransformers are, how they are trained, what they are used for, their key\narchitectural components, and a preview of the most prominent models. The\nreader is assumed to be familiar with basic ML terminology and simpler neural\nnetwork architectures such as MLPs.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.NE"
        ],
        "link": "http://arxiv.org/pdf/2207.09238v1",
        "date": "2022-07-19 12:49:02+00:00"
    },
    {
        "title": "Scattering GCN: Overcoming Oversmoothness in Graph Convolutional Networks",
        "authors": [
            "Yimeng Min",
            "Frederik Wenkel",
            "Guy Wolf"
        ],
        "abstract": "Graph convolutional networks (GCNs) have shown promising results in\nprocessing graph data by extracting structure-aware features. This gave rise to\nextensive work in geometric deep learning, focusing on designing network\narchitectures that ensure neuron activations conform to regularity patterns\nwithin the input graph. However, in most cases the graph structure is only\naccounted for by considering the similarity of activations between adjacent\nnodes, which limits the capabilities of such methods to discriminate between\nnodes in a graph. Here, we propose to augment conventional GCNs with geometric\nscattering transforms and residual convolutions. The former enables band-pass\nfiltering of graph signals, thus alleviating the so-called oversmoothing often\nencountered in GCNs, while the latter is introduced to clear the resulting\nfeatures of high-frequency noise. We establish the advantages of the presented\nScattering GCN with both theoretical results establishing the complementary\nbenefits of scattering and GCN features, as well as experimental results\nshowing the benefits of our method compared to leading graph neural networks\nfor semi-supervised node classification, including the recently proposed GAT\nnetwork that typically alleviates oversmoothing using graph attention\nmechanisms.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2003.08414v4",
        "date": "2020-03-18 18:03:08+00:00"
    },
    {
        "title": "Certified Robustness to Programmable Transformations in LSTMs",
        "authors": [
            "Yuhao Zhang",
            "Aws Albarghouthi",
            "Loris D'Antoni"
        ],
        "abstract": "Deep neural networks for natural language processing are fragile in the face\nof adversarial examples -- small input perturbations, like synonym substitution\nor word duplication, which cause a neural network to change its prediction. We\npresent an approach to certifying the robustness of LSTMs (and extensions of\nLSTMs) and training models that can be efficiently certified. Our approach can\ncertify robustness to intractably large perturbation spaces defined\nprogrammatically in a language of string transformations. Our evaluation shows\nthat (1) our approach can train models that are more robust to combinations of\nstring transformations than those produced using existing techniques; (2) our\napproach can show high certification accuracy of the resulting models.",
        "categories": [
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2102.07818v2",
        "date": "2021-02-15 19:54:59+00:00"
    },
    {
        "title": "Iterative temporal differencing with random synaptic feedback weights support error backpropagation for deep learning",
        "authors": [
            "Aras R. Dargazany"
        ],
        "abstract": "This work shows that a differentiable activation function is not necessary\nany more for error backpropagation. The derivative of the activation function\ncan be replaced by an iterative temporal differencing using fixed random\nfeedback alignment. Using fixed random synaptic feedback alignment with an\niterative temporal differencing is transforming the traditional error\nbackpropagation into a more biologically plausible approach for learning deep\nneural network architectures. This can be a big step toward the integration of\nSTDP-based error backpropagation in deep learning.",
        "categories": [
            "cs.NE",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1907.07255v1",
        "date": "2019-07-15 06:00:41+00:00"
    },
    {
        "title": "Deep neural networks for the evaluation and design of photonic devices",
        "authors": [
            "Jiaqi Jiang",
            "Mingkun Chen",
            "Jonathan A. Fan"
        ],
        "abstract": "The data sciences revolution is poised to transform the way photonic systems\nare simulated and designed. Photonics are in many ways an ideal substrate for\nmachine learning: the objective of much of computational electromagnetics is\nthe capture of non-linear relationships in high dimensional spaces, which is\nthe core strength of neural networks. Additionally, the mainstream availability\nof Maxwell solvers makes the training and evaluation of neural networks broadly\naccessible and tailorable to specific problems. In this Review, we will show\nhow deep neural networks, configured as discriminative networks, can learn from\ntraining sets and operate as high-speed surrogate electromagnetic solvers. We\nwill also examine how deep generative networks can learn geometric features in\ndevice distributions and even be configured to serve as robust global\noptimizers. Fundamental data sciences concepts framed within the context of\nphotonics will also be discussed, including the network training process,\ndelineation of different network classes and architectures, and dimensionality\nreduction.",
        "categories": [
            "eess.IV",
            "cs.LG",
            "physics.app-ph",
            "physics.optics"
        ],
        "link": "http://arxiv.org/pdf/2007.00084v1",
        "date": "2020-06-30 19:52:54+00:00"
    },
    {
        "title": "Neural Networks for Learning Counterfactual G-Invariances from Single Environments",
        "authors": [
            "S Chandra Mouli",
            "Bruno Ribeiro"
        ],
        "abstract": "Despite -- or maybe because of -- their astonishing capacity to fit data,\nneural networks are believed to have difficulties extrapolating beyond training\ndata distribution. This work shows that, for extrapolations based on finite\ntransformation groups, a model's inability to extrapolate is unrelated to its\ncapacity. Rather, the shortcoming is inherited from a learning hypothesis:\nExamples not explicitly observed with infinitely many training examples have\nunderspecified outcomes in the learner's model. In order to endow neural\nnetworks with the ability to extrapolate over group transformations, we\nintroduce a learning framework counterfactually-guided by the learning\nhypothesis that any group invariance to (known) transformation groups is\nmandatory even without evidence, unless the learner deems it inconsistent with\nthe training data. Unlike existing invariance-driven methods for\n(counterfactual) extrapolations, this framework allows extrapolations from a\nsingle environment. Finally, we introduce sequence and image extrapolation\ntasks that validate our framework and showcase the shortcomings of traditional\napproaches.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2104.10105v1",
        "date": "2021-04-20 16:35:35+00:00"
    },
    {
        "title": "Ultra-low Precision Multiplication-free Training for Deep Neural Networks",
        "authors": [
            "Chang Liu",
            "Rui Zhang",
            "Xishan Zhang",
            "Yifan Hao",
            "Zidong Du",
            "Xing Hu",
            "Ling Li",
            "Qi Guo"
        ],
        "abstract": "The training for deep neural networks (DNNs) demands immense energy\nconsumption, which restricts the development of deep learning as well as\nincreases carbon emissions. Thus, the study of energy-efficient training for\nDNNs is essential. In training, the linear layers consume the most energy\nbecause of the intense use of energy-consuming full-precision (FP32)\nmultiplication in multiply-accumulate (MAC). The energy-efficient works try to\ndecrease the precision of multiplication or replace the multiplication with\nenergy-efficient operations such as addition or bitwise shift, to reduce the\nenergy consumption of FP32 multiplications. However, the existing\nenergy-efficient works cannot replace all of the FP32 multiplications during\nboth forward and backward propagation with low-precision energy-efficient\noperations. In this work, we propose an Adaptive Layer-wise Scaling PoT\nQuantization (ALS-POTQ) method and a Multiplication-Free MAC (MF-MAC) to\nreplace all of the FP32 multiplications with the INT4 additions and 1-bit XOR\noperations. In addition, we propose Weight Bias Correction and Parameterized\nRatio Clipping techniques for stable training and improving accuracy. In our\ntraining scheme, all of the above methods do not introduce extra\nmultiplications, so we reduce up to 95.8% of the energy consumption in linear\nlayers during training. Experimentally, we achieve an accuracy degradation of\nless than 1% for CNN models on ImageNet and Transformer model on the WMT En-De\ntask. In summary, we significantly outperform the existing methods for both\nenergy efficiency and accuracy.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2302.14458v1",
        "date": "2023-02-28 10:05:45+00:00"
    },
    {
        "title": "Neural Architecture Search as Program Transformation Exploration",
        "authors": [
            "Jack Turner",
            "Elliot J. Crowley",
            "Michael O'Boyle"
        ],
        "abstract": "Improving the performance of deep neural networks (DNNs) is important to both\nthe compiler and neural architecture search (NAS) communities. Compilers apply\nprogram transformations in order to exploit hardware parallelism and memory\nhierarchy. However, legality concerns mean they fail to exploit the natural\nrobustness of neural networks. In contrast, NAS techniques mutate networks by\noperations such as the grouping or bottlenecking of convolutions, exploiting\nthe resilience of DNNs. In this work, we express such neural architecture\noperations as program transformations whose legality depends on a notion of\nrepresentational capacity. This allows them to be combined with existing\ntransformations into a unified optimization framework. This unification allows\nus to express existing NAS operations as combinations of simpler\ntransformations. Crucially, it allows us to generate and explore new tensor\nconvolutions. We prototyped the combined framework in TVM and were able to find\noptimizations across different DNNs, that significantly reduce inference time -\nover 3$\\times$ in the majority of cases.\n  Furthermore, our scheme dramatically reduces NAS search time. Code is\navailable\nat~\\href{https://github.com/jack-willturner/nas-as-program-transformation-exploration}{this\nhttps url}.",
        "categories": [
            "cs.LG",
            "cs.PL"
        ],
        "link": "http://arxiv.org/pdf/2102.06599v1",
        "date": "2021-02-12 16:11:05+00:00"
    },
    {
        "title": "Anonymizing Sensor Data on the Edge: A Representation Learning and Transformation Approach",
        "authors": [
            "Omid Hajihassani",
            "Omid Ardakanian",
            "Hamzeh Khazaei"
        ],
        "abstract": "The abundance of data collected by sensors in Internet of Things (IoT)\ndevices, and the success of deep neural networks in uncovering hidden patterns\nin time series data have led to mounting privacy concerns. This is because\nprivate and sensitive information can be potentially learned from sensor data\nby applications that have access to this data. In this paper, we aim to examine\nthe tradeoff between utility and privacy loss by learning low-dimensional\nrepresentations that are useful for data obfuscation. We propose deterministic\nand probabilistic transformations in the latent space of a variational\nautoencoder to synthesize time series data such that intrusive inferences are\nprevented while desired inferences can still be made with sufficient accuracy.\nIn the deterministic case, we use a linear transformation to move the\nrepresentation of input data in the latent space such that the reconstructed\ndata is likely to have the same public attribute but a different private\nattribute than the original input data. In the probabilistic case, we apply the\nlinear transformation to the latent representation of input data with some\nprobability. We compare our technique with autoencoder-based anonymization\ntechniques and additionally show that it can anonymize data in real time on\nresource-constrained edge devices.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CR"
        ],
        "link": "http://arxiv.org/pdf/2011.08315v3",
        "date": "2020-11-16 22:32:30+00:00"
    },
    {
        "title": "Theory and Implementation of Complex-Valued Neural Networks",
        "authors": [
            "Jose Agustin Barrachina",
            "Chengfang Ren",
            "Gilles Vieillard",
            "Christele Morisseau",
            "Jean-Philippe Ovarlez"
        ],
        "abstract": "This work explains in detail the theory behind Complex-Valued Neural Network\n(CVNN), including Wirtinger calculus, complex backpropagation, and basic\nmodules such as complex layers, complex activation functions, or complex weight\ninitialization. We also show the impact of not adapting the weight\ninitialization correctly to the complex domain. This work presents a strong\nfocus on the implementation of such modules on Python using cvnn toolbox. We\nalso perform simulations on real-valued data, casting to the complex domain by\nmeans of the Hilbert Transform, and verifying the potential interest of CVNN\neven for non-complex data.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2302.08286v1",
        "date": "2023-02-16 13:31:10+00:00"
    },
    {
        "title": "TTS-GAN: A Transformer-based Time-Series Generative Adversarial Network",
        "authors": [
            "Xiaomin Li",
            "Vangelis Metsis",
            "Huangyingrui Wang",
            "Anne Hee Hiong Ngu"
        ],
        "abstract": "Signal measurements appearing in the form of time series are one of the most\ncommon types of data used in medical machine learning applications. However,\nsuch datasets are often small, making the training of deep neural network\narchitectures ineffective. For time-series, the suite of data augmentation\ntricks we can use to expand the size of the dataset is limited by the need to\nmaintain the basic properties of the signal. Data generated by a Generative\nAdversarial Network (GAN) can be utilized as another data augmentation tool.\nRNN-based GANs suffer from the fact that they cannot effectively model long\nsequences of data points with irregular temporal relations. To tackle these\nproblems, we introduce TTS-GAN, a transformer-based GAN which can successfully\ngenerate realistic synthetic time-series data sequences of arbitrary length,\nsimilar to the real ones. Both the generator and discriminator networks of the\nGAN model are built using a pure transformer encoder architecture. We use\nvisualizations and dimensionality reduction techniques to demonstrate the\nsimilarity of real and generated time-series data. We also compare the quality\nof our generated data with the best existing alternative, which is an RNN-based\ntime-series GAN.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2202.02691v2",
        "date": "2022-02-06 03:05:47+00:00"
    },
    {
        "title": "Transformers Can Be Expressed In First-Order Logic with Majority",
        "authors": [
            "William Merrill",
            "Ashish Sabharwal"
        ],
        "abstract": "Characterizing the implicit structure of the computation within neural\nnetworks is a foundational problem in the area of deep learning\ninterpretability. Can the inner decision process of neural networks be captured\nsymbolically in some familiar logic? We show that any fixed-precision\ntransformer neural network can be translated into an equivalent fixed-size\n$\\mathsf{FO}(\\mathsf{M})$ formula, i.e., a first-order logic formula that, in\naddition to standard universal and existential quantifiers, may also contain\nmajority-vote quantifiers. The proof idea is to design highly uniform boolean\nthreshold circuits that can simulate transformers, and then leverage known\ntheoretical connections between circuits and logic. Our results reveal a\nsurprisingly simple formalism for capturing the behavior of transformers, show\nthat simple problems like integer division are \"transformer-hard\", and provide\nvaluable insights for comparing transformers to other models like RNNs. Our\nresults suggest that first-order logic with majority may be a useful language\nfor expressing programs extracted from transformers.",
        "categories": [
            "cs.LG",
            "cs.CC"
        ],
        "link": "http://arxiv.org/pdf/2210.02671v3",
        "date": "2022-10-06 04:18:09+00:00"
    },
    {
        "title": "Low-Memory Neural Network Training: A Technical Report",
        "authors": [
            "Nimit S. Sohoni",
            "Christopher R. Aberger",
            "Megan Leszczynski",
            "Jian Zhang",
            "Christopher R\u00e9"
        ],
        "abstract": "Memory is increasingly often the bottleneck when training neural network\nmodels. Despite this, techniques to lower the overall memory requirements of\ntraining have been less widely studied compared to the extensive literature on\nreducing the memory requirements of inference. In this paper we study a\nfundamental question: How much memory is actually needed to train a neural\nnetwork? To answer this question, we profile the overall memory usage of\ntraining on two representative deep learning benchmarks -- the WideResNet model\nfor image classification and the DynamicConv Transformer model for machine\ntranslation -- and comprehensively evaluate four standard techniques for\nreducing the training memory requirements: (1) imposing sparsity on the model,\n(2) using low precision, (3) microbatching, and (4) gradient checkpointing. We\nexplore how each of these techniques in isolation affects both the peak memory\nusage of training and the quality of the end model, and explore the memory,\naccuracy, and computation tradeoffs incurred when combining these techniques.\nUsing appropriate combinations of these techniques, we show that it is possible\nto the reduce the memory required to train a WideResNet-28-2 on CIFAR-10 by up\nto 60.7x with a 0.4% loss in accuracy, and reduce the memory required to train\na DynamicConv model on IWSLT'14 German to English translation by up to 8.7x\nwith a BLEU score drop of 0.15.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1904.10631v2",
        "date": "2019-04-24 03:44:58+00:00"
    },
    {
        "title": "Convolutional Neural Networks for User Identificationbased on Motion Sensors Represented as Image",
        "authors": [
            "Cezara Benegui",
            "Radu Tudor Ionescu"
        ],
        "abstract": "In this paper, we propose a deep learning approach for smartphone user\nidentification based on analyzing motion signals recorded by the accelerometer\nand the gyroscope, during a single tap gesture performed by the user on the\nscreen. We transform the discrete 3-axis signals from the motion sensors into a\ngray-scale image representation which is provided as input to a convolutional\nneural network (CNN) that is pre-trained for multi-class user classification.\nIn the pre-training stage, we benefit from different users and multiple samples\nper user. After pre-training, we use our CNN as feature extractor, generating\nan embedding associated to each single tap on the screen. The resulting\nembeddings are used to train a Support Vector Machines (SVM) model in a\nfew-shot user identification setting, i.e. requiring only 20 taps on the screen\nduring the registration phase. We compare our identification system based on\nCNN features with two baseline systems, one that employs handcrafted features\nand another that employs recurrent neural network (RNN) features. All systems\nare based on the same classifier, namely SVM. To pre-train the CNN and the RNN\nmodels for multi-class user classification, we use a different set of users\nthan the set used for few-shot user identification, ensuring a realistic\nscenario. The empirical results demonstrate that our CNN model yields a top\naccuracy of 89.75% in multi-class user classification and a top accuracy of\n96.72% in few-shot user identification. In conclusion, we believe that our\nsystem is ready for practical use, having a better generalization capacity than\nboth baselines.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1912.03760v2",
        "date": "2019-12-08 21:04:43+00:00"
    },
    {
        "title": "Monitoring Object Detection Abnormalities via Data-Label and Post-Algorithm Abstractions",
        "authors": [
            "Yuhang Chen",
            "Chih-Hong Cheng",
            "Jun Yan",
            "Rongjie Yan"
        ],
        "abstract": "While object detection modules are essential functionalities for any\nautonomous vehicle, the performance of such modules that are implemented using\ndeep neural networks can be, in many cases, unreliable. In this paper, we\ndevelop abstraction-based monitoring as a logical framework for filtering\npotentially erroneous detection results. Concretely, we consider two types of\nabstraction, namely data-label abstraction and post-algorithm abstraction.\nOperated on the training dataset, the construction of data-label abstraction\niterates each input, aggregates region-wise information over its associated\nlabels, and stores the vector under a finite history length. Post-algorithm\nabstraction builds an abstract transformer for the tracking algorithm. Elements\nbeing associated together by the abstract transformer can be checked against\nconsistency over their original values. We have implemented the overall\nframework to a research prototype and validated it using publicly available\nobject detection datasets.",
        "categories": [
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2103.15456v1",
        "date": "2021-03-29 09:40:37+00:00"
    },
    {
        "title": "Compressing deep neural networks by matrix product operators",
        "authors": [
            "Ze-Feng Gao",
            "Song Cheng",
            "Rong-Qiang He",
            "Z. Y. Xie",
            "Hui-Hai Zhao",
            "Zhong-Yi Lu",
            "Tao Xiang"
        ],
        "abstract": "A deep neural network is a parametrization of a multilayer mapping of signals\nin terms of many alternatively arranged linear and nonlinear transformations.\nThe linear transformations, which are generally used in the fully connected as\nwell as convolutional layers, contain most of the variational parameters that\nare trained and stored. Compressing a deep neural network to reduce its number\nof variational parameters but not its prediction power is an important but\nchallenging problem toward the establishment of an optimized scheme in training\nefficiently these parameters and in lowering the risk of overfitting. Here we\nshow that this problem can be effectively solved by representing linear\ntransformations with matrix product operators (MPOs), which is a tensor network\noriginally proposed in physics to characterize the short-range entanglement in\none-dimensional quantum states. We have tested this approach in five typical\nneural networks, including FC2, LeNet-5, VGG, ResNet, and DenseNet on two\nwidely used data sets, namely, MNIST and CIFAR-10, and found that this MPO\nrepresentation indeed sets up a faithful and efficient mapping between input\nand output signals, which can keep or even improve the prediction accuracy with\na dramatically reduced number of parameters. Our method greatly simplifies the\nrepresentations in deep learning, and opens a possible route toward\nestablishing a framework of modern neural networks which might be simpler and\ncheaper, but more efficient.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "cs.NE",
            "physics.comp-ph",
            "quant-ph"
        ],
        "link": "http://arxiv.org/pdf/1904.06194v2",
        "date": "2019-04-11 17:59:00+00:00"
    },
    {
        "title": "Predicting Movie Genres Based on Plot Summaries",
        "authors": [
            "Quan Hoang"
        ],
        "abstract": "This project explores several Machine Learning methods to predict movie\ngenres based on plot summaries. Naive Bayes, Word2Vec+XGBoost and Recurrent\nNeural Networks are used for text classification, while K-binary\ntransformation, rank method and probabilistic classification with learned\nprobability threshold are employed for the multi-label problem involved in the\ngenre tagging task.Experiments with more than 250,000 movies show that\nemploying the Gated Recurrent Units (GRU) neural networks for the probabilistic\nclassification with learned probability threshold approach achieves the best\nresult on the test set. The model attains a Jaccard Index of 50.0%, a F-score\nof 0.56, and a hit rate of 80.5%.",
        "categories": [
            "cs.CL",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1801.04813v1",
        "date": "2018-01-15 14:11:57+00:00"
    },
    {
        "title": "Discrete-Valued Neural Communication",
        "authors": [
            "Dianbo Liu",
            "Alex Lamb",
            "Kenji Kawaguchi",
            "Anirudh Goyal",
            "Chen Sun",
            "Michael Curtis Mozer",
            "Yoshua Bengio"
        ],
        "abstract": "Deep learning has advanced from fully connected architectures to structured\nmodels organized into components, e.g., the transformer composed of positional\nelements, modular architectures divided into slots, and graph neural nets made\nup of nodes. In structured models, an interesting question is how to conduct\ndynamic and possibly sparse communication among the separate components. Here,\nwe explore the hypothesis that restricting the transmitted information among\ncomponents to discrete representations is a beneficial bottleneck. The\nmotivating intuition is human language in which communication occurs through\ndiscrete symbols. Even though individuals have different understandings of what\na \"cat\" is based on their specific experiences, the shared discrete token makes\nit possible for communication among individuals to be unimpeded by individual\ndifferences in internal representation. To discretize the values of concepts\ndynamically communicated among specialist components, we extend the\nquantization mechanism from the Vector-Quantized Variational Autoencoder to\nmulti-headed discretization with shared codebooks and use it for\ndiscrete-valued neural communication (DVNC). Our experiments show that DVNC\nsubstantially improves systematic generalization in a variety of architectures\n-- transformers, modular architectures, and graph neural networks. We also show\nthat the DVNC is robust to the choice of hyperparameters, making the method\nvery useful in practice. Moreover, we establish a theoretical justification of\nour discretization process, proving that it has the ability to increase noise\nrobustness and reduce the underlying dimensionality of the model.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2107.02367v3",
        "date": "2021-07-06 03:09:25+00:00"
    },
    {
        "title": "Neural network compression via learnable wavelet transforms",
        "authors": [
            "Moritz Wolter",
            "Shaohui Lin",
            "Angela Yao"
        ],
        "abstract": "Wavelets are well known for data compression, yet have rarely been applied to\nthe compression of neural networks. This paper shows how the fast wavelet\ntransform can be used to compress linear layers in neural networks. Linear\nlayers still occupy a significant portion of the parameters in recurrent neural\nnetworks (RNNs). Through our method, we can learn both the wavelet bases and\ncorresponding coefficients to efficiently represent the linear layers of RNNs.\nOur wavelet compressed RNNs have significantly fewer parameters yet still\nperform competitively with the state-of-the-art on synthetic and real-world RNN\nbenchmarks. Wavelet optimization adds basis flexibility, without large numbers\nof extra weights. Source code is available at\nhttps://github.com/v0lta/Wavelet-network-compression.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2004.09569v3",
        "date": "2020-04-20 18:52:05+00:00"
    },
    {
        "title": "Boosting Deep Neural Networks with Geometrical Prior Knowledge: A Survey",
        "authors": [
            "Matthias Rath",
            "Alexandru Paul Condurache"
        ],
        "abstract": "Deep Neural Networks achieve state-of-the-art results in many different\nproblem settings by exploiting vast amounts of training data. However,\ncollecting, storing and - in the case of supervised learning - labelling the\ndata is expensive and time-consuming. Additionally, assessing the networks'\ngeneralization abilities or predicting how the inferred output changes under\ninput transformations is complicated since the networks are usually treated as\na black box. Both of these problems can be mitigated by incorporating prior\nknowledge into the neural network. One promising approach, inspired by the\nsuccess of convolutional neural networks in computer vision tasks, is to\nincorporate knowledge about symmetric geometrical transformations of the\nproblem to solve that affect the output in a predictable way. This promises an\nincreased data efficiency and more interpretable network outputs. In this\nsurvey, we try to give a concise overview about different approaches that\nincorporate geometrical prior knowledge into neural networks. Additionally, we\nconnect those methods to 3D object detection for autonomous driving, where we\nexpect promising results when applying those methods.",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2006.16867v2",
        "date": "2020-06-30 14:56:05+00:00"
    },
    {
        "title": "Product-based Neural Networks for User Response Prediction over Multi-field Categorical Data",
        "authors": [
            "Yanru Qu",
            "Bohui Fang",
            "Weinan Zhang",
            "Ruiming Tang",
            "Minzhe Niu",
            "Huifeng Guo",
            "Yong Yu",
            "Xiuqiang He"
        ],
        "abstract": "User response prediction is a crucial component for personalized information\nretrieval and filtering scenarios, such as recommender system and web search.\nThe data in user response prediction is mostly in a multi-field categorical\nformat and transformed into sparse representations via one-hot encoding. Due to\nthe sparsity problems in representation and optimization, most research focuses\non feature engineering and shallow modeling. Recently, deep neural networks\nhave attracted research attention on such a problem for their high capacity and\nend-to-end training scheme. In this paper, we study user response prediction in\nthe scenario of click prediction. We first analyze a coupled gradient issue in\nlatent vector-based models and propose kernel product to learn field-aware\nfeature interactions. Then we discuss an insensitive gradient issue in\nDNN-based models and propose Product-based Neural Network (PNN) which adopts a\nfeature extractor to explore feature interactions. Generalizing the kernel\nproduct to a net-in-net architecture, we further propose Product-network In\nNetwork (PIN) which can generalize previous models. Extensive experiments on 4\nindustrial datasets and 1 contest dataset demonstrate that our models\nconsistently outperform 8 baselines on both AUC and log loss. Besides, PIN\nmakes great CTR improvement (relatively 34.67%) in online A/B test.",
        "categories": [
            "cs.IR",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1807.00311v1",
        "date": "2018-07-01 11:02:50+00:00"
    },
    {
        "title": "Attention Enables Zero Approximation Error",
        "authors": [
            "Zhiying Fang",
            "Yidong Ouyang",
            "Ding-Xuan Zhou",
            "Guang Cheng"
        ],
        "abstract": "Deep learning models have been widely applied in various aspects of daily\nlife. Many variant models based on deep learning structures have achieved even\nbetter performances. Attention-based architectures have become almost\nubiquitous in deep learning structures. Especially, the transformer model has\nnow defeated the convolutional neural network in image classification tasks to\nbecome the most widely used tool. However, the theoretical properties of\nattention-based models are seldom considered. In this work, we show that with\nsuitable adaptations, the single-head self-attention transformer with a fixed\nnumber of transformer encoder blocks and free parameters is able to generate\nany desired polynomial of the input with no error. The number of transformer\nencoder blocks is the same as the degree of the target polynomial. Even more\nexciting, we find that these transformer encoder blocks in this model do not\nneed to be trained. As a direct consequence, we show that the single-head\nself-attention transformer with increasing numbers of free parameters is\nuniversal. These surprising theoretical results clearly explain the outstanding\nperformances of the transformer model and may shed light on future\nmodifications in real applications. We also provide some experiments to verify\nour theoretical result.",
        "categories": [
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2202.12166v1",
        "date": "2022-02-24 16:06:01+00:00"
    },
    {
        "title": "Opening Up the Neural Network Classifier for Shap Score Computation",
        "authors": [
            "Leopoldo Bertossi",
            "Jorge E. Leon"
        ],
        "abstract": "We address the problem of efficiently computing Shap explanation scores for\nclassifications with machine learning models. With this goal, we show the\ntransformation of binary neural networks (BNNs) for classification into\ndeterministic and decomposable Boolean circuits, for which knowledge\ncompilation techniques are used. The resulting circuit is treated as an\nopen-box model, to compute Shap scores by means of a recent efficient algorithm\nfor this class of circuits. Detailed experiments show a considerable gain in\nperformance in comparison with computing Shap directly on the BNN treated as a\nblack-box model.",
        "categories": [
            "cs.AI",
            "cs.DB",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2303.06516v1",
        "date": "2023-03-11 23:33:43+00:00"
    },
    {
        "title": "Pooling homogeneous ensembles to build heterogeneous ones",
        "authors": [
            "Maryam Sabzevari",
            "Gonzalo Mart\u00ednez-Mu\u00f1oz",
            "Alberto Su\u00e1rez"
        ],
        "abstract": "In ensemble methods, the outputs of a collection of diverse classifiers are\ncombined in the expectation that the global prediction be more accurate than\nthe individual ones. Heterogeneous ensembles consist of predictors of different\ntypes, which are likely to have different biases. If these biases are\ncomplementary, the combination of their decisions is beneficial. In this work,\na family of heterogeneous ensembles is built by pooling classifiers from M\nhomogeneous ensembles of different types of size T. Depending on the fraction\nof base classifiers of each type, a particular heterogeneous combination in\nthis family is represented by a point in a regular simplex in M dimensions. The\nM vertices of this simplex represent the different homogeneous ensembles. A\ndisplacement away from one of these vertices effects a smooth transformation of\nthe corresponding homogeneous ensemble into a heterogeneous one. The optimal\ncomposition of such heterogeneous ensemble can be determined using\ncross-validation or, if bootstrap samples are used to build the individual\nclassifiers, out-of-bag data. An empirical analysis of such combinations of\nbootstraped ensembles composed of neural networks, SVMs, and random trees (i.e.\nfrom a standard random forest) illustrates the gains that can be achieved by\nthis heterogeneous ensemble creation method.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1802.07877v2",
        "date": "2018-02-21 13:17:42+00:00"
    },
    {
        "title": "Memory-Efficient Backpropagation through Large Linear Layers",
        "authors": [
            "Daniel Bershatsky",
            "Aleksandr Mikhalev",
            "Alexandr Katrutsa",
            "Julia Gusak",
            "Daniil Merkulov",
            "Ivan Oseledets"
        ],
        "abstract": "In modern neural networks like Transformers, linear layers require\nsignificant memory to store activations during backward pass. This study\nproposes a memory reduction approach to perform backpropagation through linear\nlayers. Since the gradients of linear layers are computed by matrix\nmultiplications, we consider methods for randomized matrix multiplications and\ndemonstrate that they require less memory with a moderate decrease of the test\naccuracy. Also, we investigate the variance of the gradient estimate induced by\nthe randomized matrix multiplication. We compare this variance with the\nvariance coming from gradient estimation based on the batch of samples. We\ndemonstrate the benefits of the proposed method on the fine-tuning of the\npre-trained RoBERTa model on GLUE tasks.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2201.13195v3",
        "date": "2022-01-31 13:02:41+00:00"
    },
    {
        "title": "Attending to Emotional Narratives",
        "authors": [
            "Zhengxuan Wu",
            "Xiyu Zhang",
            "Tan Zhi-Xuan",
            "Jamil Zaki",
            "Desmond C. Ong"
        ],
        "abstract": "Attention mechanisms in deep neural networks have achieved excellent\nperformance on sequence-prediction tasks. Here, we show that these\nrecently-proposed attention-based mechanisms---in particular, the Transformer\nwith its parallelizable self-attention layers, and the Memory Fusion Network\nwith attention across modalities and time---also generalize well to multimodal\ntime-series emotion recognition. Using a recently-introduced dataset of\nemotional autobiographical narratives, we adapt and apply these two attention\nmechanisms to predict emotional valence over time. Our models perform extremely\nwell, in some cases reaching a performance comparable with human raters. We end\nwith a discussion of the implications of attention mechanisms to affective\ncomputing.",
        "categories": [
            "cs.LG",
            "cs.CL",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1907.04197v1",
        "date": "2019-07-08 03:50:43+00:00"
    },
    {
        "title": "Provably Robust Adversarial Examples",
        "authors": [
            "Dimitar I. Dimitrov",
            "Gagandeep Singh",
            "Timon Gehr",
            "Martin Vechev"
        ],
        "abstract": "We introduce the concept of provably robust adversarial examples for deep\nneural networks - connected input regions constructed from standard adversarial\nexamples which are guaranteed to be robust to a set of real-world perturbations\n(such as changes in pixel intensity and geometric transformations). We present\na novel method called PARADE for generating these regions in a scalable manner\nwhich works by iteratively refining the region initially obtained via sampling\nuntil a refined region is certified to be adversarial with existing\nstate-of-the-art verifiers. At each step, a novel optimization procedure is\napplied to maximize the region's volume under the constraint that the convex\nrelaxation of the network behavior with respect to the region implies a chosen\nbound on the certification objective. Our experimental evaluation shows the\neffectiveness of PARADE: it successfully finds large provably robust regions\nincluding ones containing $\\approx 10^{573}$ adversarial examples for pixel\nintensity and $\\approx 10^{599}$ for geometric perturbations. The provability\nenables our robust examples to be significantly more effective against\nstate-of-the-art defenses based on randomized smoothing than the individual\nattacks used to construct the regions.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2007.12133v3",
        "date": "2020-07-23 17:03:56+00:00"
    },
    {
        "title": "Towards Deep Conversational Recommendations",
        "authors": [
            "Raymond Li",
            "Samira Kahou",
            "Hannes Schulz",
            "Vincent Michalski",
            "Laurent Charlin",
            "Chris Pal"
        ],
        "abstract": "There has been growing interest in using neural networks and deep learning\ntechniques to create dialogue systems. Conversational recommendation is an\ninteresting setting for the scientific exploration of dialogue with natural\nlanguage as the associated discourse involves goal-driven dialogue that often\ntransforms naturally into more free-form chat. This paper provides two\ncontributions. First, until now there has been no publicly available\nlarge-scale dataset consisting of real-world dialogues centered around\nrecommendations. To address this issue and to facilitate our exploration here,\nwe have collected ReDial, a dataset consisting of over 10,000 conversations\ncentered around the theme of providing movie recommendations. We make this data\navailable to the community for further research. Second, we use this dataset to\nexplore multiple facets of conversational recommendations. In particular we\nexplore new neural architectures, mechanisms, and methods suitable for\ncomposing conversational recommendation systems. Our dataset allows us to\nsystematically probe model sub-components addressing different parts of the\noverall problem domain ranging from: sentiment analysis and cold-start\nrecommendation generation to detailed aspects of how natural language is used\nin this setting in the real world. We combine such sub-components into a\nfull-blown dialogue system and examine its behavior.",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.IR",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1812.07617v2",
        "date": "2018-12-18 19:34:32+00:00"
    },
    {
        "title": "A Deep Graph Wavelet Convolutional Neural Network for Semi-supervised Node Classification",
        "authors": [
            "Jingyi Wang",
            "Zhidong Deng"
        ],
        "abstract": "Graph convolutional neural network provides good solutions for node\nclassification and other tasks with non-Euclidean data. There are several graph\nconvolutional models that attempt to develop deep networks but do not cause\nserious over-smoothing at the same time. Considering that the wavelet transform\ngenerally has a stronger ability to extract useful information than the Fourier\ntransform, we propose a new deep graph wavelet convolutional network (DeepGWC)\nfor semi-supervised node classification tasks. Based on the optimized static\nfiltering matrix parameters of vanilla graph wavelet neural networks and the\ncombination of Fourier bases and wavelet ones, DeepGWC is constructed together\nwith the reuse of residual connection and identity mappings in network\narchitectures. Extensive experiments on three benchmark datasets including\nCora, Citeseer, and Pubmed are conducted. The experimental results demonstrate\nthat our DeepGWC outperforms existing graph deep models with the help of\nadditional wavelet bases and achieves new state-of-the-art performances\neventually.",
        "categories": [
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2102.09780v1",
        "date": "2021-02-19 07:57:28+00:00"
    },
    {
        "title": "Meta-learning Pseudo-differential Operators with Deep Neural Networks",
        "authors": [
            "Jordi Feliu-Faba",
            "Yuwei Fan",
            "Lexing Ying"
        ],
        "abstract": "This paper introduces a meta-learning approach for parameterized\npseudo-differential operators with deep neural networks. With the help of the\nnonstandard wavelet form, the pseudo-differential operators can be approximated\nin a compressed form with a collection of vectors. The nonlinear map from the\nparameter to this collection of vectors and the wavelet transform are learned\ntogether from a small number of matrix-vector multiplications of the\npseudo-differential operator. Numerical results for Green's functions of\nelliptic partial differential equations and the radiative transfer equations\ndemonstrate the efficiency and accuracy of the proposed approach.",
        "categories": [
            "math.NA",
            "cs.LG",
            "cs.NA"
        ],
        "link": "http://arxiv.org/pdf/1906.06782v2",
        "date": "2019-06-16 22:00:12+00:00"
    },
    {
        "title": "Towards Differentiable Resampling",
        "authors": [
            "Michael Zhu",
            "Kevin Murphy",
            "Rico Jonschkowski"
        ],
        "abstract": "Resampling is a key component of sample-based recursive state estimation in\nparticle filters. Recent work explores differentiable particle filters for\nend-to-end learning. However, resampling remains a challenge in these works, as\nit is inherently non-differentiable. We address this challenge by replacing\ntraditional resampling with a learned neural network resampler. We present a\nnovel network architecture, the particle transformer, and train it for particle\nresampling using a likelihood-based loss function over sets of particles.\nIncorporated into a differentiable particle filter, our model can be end-to-end\noptimized jointly with the other particle filter components via gradient\ndescent. Our results show that our learned resampler outperforms traditional\nresampling techniques on synthetic data and in a simulated robot localization\ntask.",
        "categories": [
            "cs.LG",
            "cs.RO",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2004.11938v1",
        "date": "2020-04-24 18:37:17+00:00"
    },
    {
        "title": "Linear Memory Networks",
        "authors": [
            "Davide Bacciu",
            "Antonio Carta",
            "Alessandro Sperduti"
        ],
        "abstract": "Recurrent neural networks can learn complex transduction problems that\nrequire maintaining and actively exploiting a memory of their inputs. Such\nmodels traditionally consider memory and input-output functionalities\nindissolubly entangled. We introduce a novel recurrent architecture based on\nthe conceptual separation between the functional input-output transformation\nand the memory mechanism, showing how they can be implemented through different\nneural components. By building on such conceptualization, we introduce the\nLinear Memory Network, a recurrent model comprising a feedforward neural\nnetwork, realizing the non-linear functional transformation, and a linear\nautoencoder for sequences, implementing the memory component. The resulting\narchitecture can be efficiently trained by building on closed-form solutions to\nlinear optimization problems. Further, by exploiting equivalence results\nbetween feedforward and recurrent neural networks we devise a pretraining\nschema for the proposed architecture. Experiments on polyphonic music datasets\nshow competitive results against gated recurrent networks and other state of\nthe art models.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1811.03356v1",
        "date": "2018-11-08 11:08:04+00:00"
    },
    {
        "title": "Signal Transformer: Complex-valued Attention and Meta-Learning for Signal Recognition",
        "authors": [
            "Yihong Dong",
            "Ying Peng",
            "Muqiao Yang",
            "Songtao Lu",
            "Qingjiang Shi"
        ],
        "abstract": "Deep neural networks have been shown as a class of useful tools for\naddressing signal recognition issues in recent years, especially for\nidentifying the nonlinear feature structures of signals. However, this power of\nmost deep learning techniques heavily relies on an abundant amount of training\ndata, so the performance of classic neural nets decreases sharply when the\nnumber of training data samples is small or unseen data are presented in the\ntesting phase. This calls for an advanced strategy, i.e., model-agnostic\nmeta-learning (MAML), which is able to capture the invariant representation of\nthe data samples or signals. In this paper, inspired by the special structure\nof the signal, i.e., real and imaginary parts consisted in practical\ntime-series signals, we propose a Complex-valued Attentional MEta Learner\n(CAMEL) for the problem of few-shot signal recognition by leveraging attention\nand meta-learning in the complex domain. To the best of our knowledge, this is\nalso the first complex-valued MAML that can find the first-order stationary\npoints of general nonconvex problems with theoretical convergence guarantees.\nExtensive experiments results showcase the superiority of the proposed CAMEL\ncompared with the state-of-the-art methods.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "eess.SP"
        ],
        "link": "http://arxiv.org/pdf/2106.04392v2",
        "date": "2021-06-05 03:57:41+00:00"
    },
    {
        "title": "How Does Momentum Benefit Deep Neural Networks Architecture Design? A Few Case Studies",
        "authors": [
            "Bao Wang",
            "Hedi Xia",
            "Tan Nguyen",
            "Stanley Osher"
        ],
        "abstract": "We present and review an algorithmic and theoretical framework for improving\nneural network architecture design via momentum. As case studies, we consider\nhow momentum can improve the architecture design for recurrent neural networks\n(RNNs), neural ordinary differential equations (ODEs), and transformers. We\nshow that integrating momentum into neural network architectures has several\nremarkable theoretical and empirical benefits, including 1) integrating\nmomentum into RNNs and neural ODEs can overcome the vanishing gradient issues\nin training RNNs and neural ODEs, resulting in effective learning long-term\ndependencies. 2) momentum in neural ODEs can reduce the stiffness of the ODE\ndynamics, which significantly enhances the computational efficiency in training\nand testing. 3) momentum can improve the efficiency and accuracy of\ntransformers.",
        "categories": [
            "cs.LG",
            "cs.NA",
            "math.DS",
            "math.NA",
            "68Txx"
        ],
        "link": "http://arxiv.org/pdf/2110.07034v2",
        "date": "2021-10-13 21:11:04+00:00"
    },
    {
        "title": "ReLU Neural Networks of Polynomial Size for Exact Maximum Flow Computation",
        "authors": [
            "Christoph Hertrich",
            "Leon Sering"
        ],
        "abstract": "This paper studies the expressive power of artificial neural networks with\nrectified linear units. In order to study them as a model of real-valued\ncomputation, we introduce the concept of Max-Affine Arithmetic Programs and\nshow equivalence between them and neural networks concerning natural complexity\nmeasures. We then use this result to show that two fundamental combinatorial\noptimization problems can be solved with polynomial-size neural networks.\nFirst, we show that for any undirected graph with $n$ nodes, there is a neural\nnetwork (with fixed weights and biases) of size $\\mathcal{O}(n^3)$ that takes\nthe edge weights as input and computes the value of a minimum spanning tree of\nthe graph. Second, we show that for any directed graph with $n$ nodes and $m$\narcs, there is a neural network of size $\\mathcal{O}(m^2n^2)$ that takes the\narc capacities as input and computes a maximum flow. Our results imply that\nthese two problems can be solved with strongly polynomial time algorithms that\nsolely uses affine transformations and maxima computations, but no\ncomparison-based branchings.",
        "categories": [
            "cs.LG",
            "cs.CC",
            "cs.DS",
            "cs.NE",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2102.06635v4",
        "date": "2021-02-12 17:23:34+00:00"
    },
    {
        "title": "ProcessTransformer: Predictive Business Process Monitoring with Transformer Network",
        "authors": [
            "Zaharah A. Bukhsh",
            "Aaqib Saeed",
            "Remco M. Dijkman"
        ],
        "abstract": "Predictive business process monitoring focuses on predicting future\ncharacteristics of a running process using event logs. The foresight into\nprocess execution promises great potentials for efficient operations, better\nresource management, and effective customer services. Deep learning-based\napproaches have been widely adopted in process mining to address the\nlimitations of classical algorithms for solving multiple problems, especially\nthe next event and remaining-time prediction tasks. Nevertheless, designing a\ndeep neural architecture that performs competitively across various tasks is\nchallenging as existing methods fail to capture long-range dependencies in the\ninput sequences and perform poorly for lengthy process traces. In this paper,\nwe propose ProcessTransformer, an approach for learning high-level\nrepresentations from event logs with an attention-based network. Our model\nincorporates long-range memory and relies on a self-attention mechanism to\nestablish dependencies between a multitude of event sequences and corresponding\noutputs. We evaluate the applicability of our technique on nine real event\nlogs. We demonstrate that the transformer-based model outperforms several\nbaselines of prior techniques by obtaining on average above 80% accuracy for\nthe task of predicting the next activity. Our method also perform\ncompetitively, compared to baselines, for the tasks of predicting event time\nand remaining time of a running case",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2104.00721v1",
        "date": "2021-04-01 18:58:46+00:00"
    },
    {
        "title": "Supervised Contrastive Prototype Learning: Augmentation Free Robust Neural Network",
        "authors": [
            "Iordanis Fostiropoulos",
            "Laurent Itti"
        ],
        "abstract": "Transformations in the input space of Deep Neural Networks (DNN) lead to\nunintended changes in the feature space. Almost perceptually identical inputs,\nsuch as adversarial examples, can have significantly distant feature\nrepresentations. On the contrary, Out-of-Distribution (OOD) samples can have\nhighly similar feature representations to training set samples. Our theoretical\nanalysis for DNNs trained with a categorical classification head suggests that\nthe inflexible logit space restricted by the classification problem size is one\nof the root causes for the lack of $\\textit{robustness}$. Our second\nobservation is that DNNs over-fit to the training augmentation technique and do\nnot learn $\\textit{nuance invariant}$ representations. Inspired by the recent\nsuccess of prototypical and contrastive learning frameworks for both improving\nrobustness and learning nuance invariant representations, we propose a training\nframework, $\\textbf{Supervised Contrastive Prototype Learning}$ (SCPL). We use\nN-pair contrastive loss with prototypes of the same and opposite classes and\nreplace a categorical classification head with a $\\textbf{Prototype\nClassification Head}$ (PCH). Our approach is $\\textit{sample efficient}$, does\nnot require $\\textit{sample mining}$, can be implemented on any existing DNN\nwithout modification to their architecture, and combined with other training\naugmentation techniques. We empirically evaluate the $\\textbf{clean}$\nrobustness of our method on out-of-distribution and adversarial samples. Our\nframework outperforms other state-of-the-art contrastive and prototype learning\napproaches in $\\textit{robustness}$.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2211.14424v1",
        "date": "2022-11-26 01:17:15+00:00"
    },
    {
        "title": "Convolutional Graph-Tensor Net for Graph Data Completion",
        "authors": [
            "Xiao-Yang Liu",
            "Ming Zhu"
        ],
        "abstract": "Graph data completion is a fundamentally important issue as data generally\nhas a graph structure, e.g., social networks, recommendation systems, and the\nInternet of Things. We consider a graph where each node has a data matrix,\nrepresented as a \\textit{graph-tensor} by stacking the data matrices in the\nthird dimension. In this paper, we propose a \\textit{Convolutional Graph-Tensor\nNet} (\\textit{Conv GT-Net}) for the graph data completion problem, which uses\ndeep neural networks to learn the general transform of graph-tensors. The\nexperimental results on the ego-Facebook data sets show that the proposed\n\\textit{Conv GT-Net} achieves significant improvements on both completion\naccuracy (50\\% higher) and completion speed (3.6x $\\sim$ 8.1x faster) over the\nexisting algorithms.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2103.04485v2",
        "date": "2021-03-07 23:33:38+00:00"
    },
    {
        "title": "Approaching sales forecasting using recurrent neural networks and transformers",
        "authors": [
            "Iv\u00e1n Vall\u00e9s-P\u00e9rez",
            "Emilio Soria-Olivas",
            "Marcelino Mart\u00ednez-Sober",
            "Antonio J. Serrano-L\u00f3pez",
            "Juan G\u00f3mez-Sanch\u00eds",
            "Fernando Mateo"
        ],
        "abstract": "Accurate and fast demand forecast is one of the hot topics in supply chain\nfor enabling the precise execution of the corresponding downstream processes\n(inbound and outbound planning, inventory placement, network planning, etc). We\ndevelop three alternatives to tackle the problem of forecasting the customer\nsales at day/store/item level using deep learning techniques and the\nCorporaci\\'on Favorita data set, published as part of a Kaggle competition. Our\nempirical results show how good performance can be achieved by using a simple\nsequence to sequence architecture with minimal data preprocessing effort.\nAdditionally, we describe a training trick for making the model more time\nindependent and hence improving generalization over time. The proposed solution\nachieves a RMSLE of around 0.54, which is competitive with other more specific\nsolutions to the problem proposed in the Kaggle competition.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "stat.AP"
        ],
        "link": "http://arxiv.org/pdf/2204.07786v1",
        "date": "2022-04-16 12:03:52+00:00"
    },
    {
        "title": "Modelling graph dynamics in fraud detection with \"Attention\"",
        "authors": [
            "Susie Xi Rao",
            "Cl\u00e9mence Lanfranchi",
            "Shuai Zhang",
            "Zhichao Han",
            "Zitao Zhang",
            "Wei Min",
            "Mo Cheng",
            "Yinan Shan",
            "Yang Zhao",
            "Ce Zhang"
        ],
        "abstract": "At online retail platforms, detecting fraudulent accounts and transactions is\ncrucial to improve customer experience, minimize loss, and avoid unauthorized\ntransactions. Despite the variety of different models for deep learning on\ngraphs, few approaches have been proposed for dealing with graphs that are both\nheterogeneous and dynamic. In this paper, we propose DyHGN (Dynamic\nHeterogeneous Graph Neural Network) and its variants to capture both temporal\nand heterogeneous information. We first construct dynamic heterogeneous graphs\nfrom registration and transaction data from eBay. Then, we build models with\ndiachronic entity embedding and heterogeneous graph transformer. We also use\nmodel explainability techniques to understand the behaviors of DyHGN-* models.\nOur findings reveal that modelling graph dynamics with heterogeneous inputs\nneed to be conducted with \"attention\" depending on the data structure,\ndistribution, and computation cost.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.SI"
        ],
        "link": "http://arxiv.org/pdf/2204.10614v1",
        "date": "2022-04-22 10:17:21+00:00"
    },
    {
        "title": "Densely Connected $G$-invariant Deep Neural Networks with Signed Permutation Representations",
        "authors": [
            "Devanshu Agrawal",
            "James Ostrowski"
        ],
        "abstract": "We introduce and investigate, for finite groups $G$, $G$-invariant deep\nneural network ($G$-DNN) architectures with ReLU activation that are densely\nconnected -- i.e., include all possible skip connections. In contrast to other\n$G$-invariant architectures in the literature, the preactivations of\nthe$G$-DNNs presented here are able to transform by \\emph{signed} permutation\nrepresentations (signed perm-reps) of $G$. Moreover, the individual layers of\nthe $G$-DNNs are not required to be $G$-equivariant; instead, the\npreactivations are constrained to be $G$-equivariant functions of the network\ninput in a way that couples weights across all layers. The result is a richer\nfamily of $G$-invariant architectures never seen previously. We derive an\nefficient implementation of $G$-DNNs after a reparameterization of weights, as\nwell as necessary and sufficient conditions for an architecture to be\n\"admissible\" -- i.e., nondegenerate and inequivalent to smaller architectures.\nWe include code that allows a user to build a $G$-DNN interactively\nlayer-by-layer, with the final architecture guaranteed to be admissible.\nFinally, we apply $G$-DNNs to two example problems -- (1) multiplication in\n$\\{-1, 1\\}$ (with theoretical guarantees) and (2) 3D object classification --\nfinding that the inclusion of signed perm-reps significantly boosts predictive\nperformance compared to baselines with only ordinary (i.e., unsigned)\nperm-reps.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2303.04614v1",
        "date": "2023-03-08 14:35:03+00:00"
    },
    {
        "title": "Test-Time Adaptation to Distribution Shift by Confidence Maximization and Input Transformation",
        "authors": [
            "Chaithanya Kumar Mummadi",
            "Robin Hutmacher",
            "Kilian Rambach",
            "Evgeny Levinkov",
            "Thomas Brox",
            "Jan Hendrik Metzen"
        ],
        "abstract": "Deep neural networks often exhibit poor performance on data that is unlikely\nunder the train-time data distribution, for instance data affected by\ncorruptions. Previous works demonstrate that test-time adaptation to data\nshift, for instance using entropy minimization, effectively improves\nperformance on such shifted distributions. This paper focuses on the fully\ntest-time adaptation setting, where only unlabeled data from the target\ndistribution is required. This allows adapting arbitrary pretrained networks.\nSpecifically, we propose a novel loss that improves test-time adaptation by\naddressing both premature convergence and instability of entropy minimization.\nThis is achieved by replacing the entropy by a non-saturating surrogate and\nadding a diversity regularizer based on batch-wise entropy maximization that\nprevents convergence to trivial collapsed solutions. Moreover, we propose to\nprepend an input transformation module to the network that can partially undo\ntest-time distribution shifts. Surprisingly, this preprocessing can be learned\nsolely using the fully test-time adaptation loss in an end-to-end fashion\nwithout any target domain labels or source domain data. We show that our\napproach outperforms previous work in improving the robustness of publicly\navailable pretrained image classifiers to common corruptions on such\nchallenging benchmarks as ImageNet-C.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2106.14999v1",
        "date": "2021-06-28 22:06:10+00:00"
    },
    {
        "title": "Can recurrent neural networks warp time?",
        "authors": [
            "Corentin Tallec",
            "Yann Ollivier"
        ],
        "abstract": "Successful recurrent models such as long short-term memories (LSTMs) and\ngated recurrent units (GRUs) use ad hoc gating mechanisms. Empirically these\nmodels have been found to improve the learning of medium to long term temporal\ndependencies and to help with vanishing gradient issues. We prove that\nlearnable gates in a recurrent model formally provide quasi- invariance to\ngeneral time transformations in the input data. We recover part of the LSTM\narchitecture from a simple axiomatic approach. This result leads to a new way\nof initializing gate biases in LSTMs and GRUs. Ex- perimentally, this new\nchrono initialization is shown to greatly improve learning of long term\ndependencies, with minimal implementation effort.",
        "categories": [
            "cs.LG",
            "cs.NE",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1804.11188v1",
        "date": "2018-03-23 09:17:35+00:00"
    },
    {
        "title": "A Survey on Face Data Augmentation",
        "authors": [
            "Xiang Wang",
            "Kai Wang",
            "Shiguo Lian"
        ],
        "abstract": "The quality and size of training set have great impact on the results of deep\nlearning-based face related tasks. However, collecting and labeling adequate\nsamples with high quality and balanced distributions still remains a laborious\nand expensive work, and various data augmentation techniques have thus been\nwidely used to enrich the training dataset. In this paper, we systematically\nreview the existing works of face data augmentation from the perspectives of\nthe transformation types and methods, with the state-of-the-art approaches\ninvolved. Among all these approaches, we put the emphasis on the deep\nlearning-based works, especially the generative adversarial networks which have\nbeen recognized as more powerful and effective tools in recent years. We\npresent their principles, discuss the results and show their applications as\nwell as limitations. Different evaluation metrics for evaluating these\napproaches are also introduced. We point out the challenges and opportunities\nin the field of face data augmentation, and provide brief yet insightful\ndiscussions.",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "I.4.7; I.4.10; I.5.4"
        ],
        "link": "http://arxiv.org/pdf/1904.11685v1",
        "date": "2019-04-26 06:23:35+00:00"
    },
    {
        "title": "Feature extraction using Latent Dirichlet Allocation and Neural Networks: A case study on movie synopses",
        "authors": [
            "Despoina Christou"
        ],
        "abstract": "Feature extraction has gained increasing attention in the field of machine\nlearning, as in order to detect patterns, extract information, or predict\nfuture observations from big data, the urge of informative features is crucial.\nThe process of extracting features is highly linked to dimensionality reduction\nas it implies the transformation of the data from a sparse high-dimensional\nspace, to higher level meaningful abstractions. This dissertation employs\nNeural Networks for distributed paragraph representations, and Latent Dirichlet\nAllocation to capture higher level features of paragraph vectors. Although\nNeural Networks for distributed paragraph representations are considered the\nstate of the art for extracting paragraph vectors, we show that a quick topic\nanalysis model such as Latent Dirichlet Allocation can provide meaningful\nfeatures too. We evaluate the two methods on the CMU Movie Summary Corpus, a\ncollection of 25,203 movie plot summaries extracted from Wikipedia. Finally,\nfor both approaches, we use K-Nearest Neighbors to discover similar movies, and\nplot the projected representations using T-Distributed Stochastic Neighbor\nEmbedding to depict the context similarities. These similarities, expressed as\nmovie distances, can be used for movies recommendation. The recommended movies\nof this approach are compared with the recommended movies from IMDB, which use\na collaborative filtering recommendation approach, to show that our two models\ncould constitute either an alternative or a supplementary recommendation\napproach.",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.IR",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1604.01272v1",
        "date": "2016-04-05 14:32:48+00:00"
    },
    {
        "title": "General Invertible Transformations for Flow-based Generative Modeling",
        "authors": [
            "Jakub M. Tomczak"
        ],
        "abstract": "In this paper, we present a new class of invertible transformations with an\napplication to flow-based generative models. We indicate that many well-known\ninvertible transformations in reversible logic and reversible neural networks\ncould be derived from our proposition. Next, we propose two new coupling layers\nthat are important building blocks of flow-based generative models. In the\nexperiments on digit data, we present how these new coupling layers could be\nused in Integer Discrete Flows (IDF), and that they achieve better results than\nstandard coupling layers used in IDF and RealNVP.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2011.15056v2",
        "date": "2020-11-30 17:54:43+00:00"
    },
    {
        "title": "Distribution-Based Invariant Deep Networks for Learning Meta-Features",
        "authors": [
            "Gwendoline De Bie",
            "Herilalaina Rakotoarison",
            "Gabriel Peyr\u00e9",
            "Mich\u00e8le Sebag"
        ],
        "abstract": "Recent advances in deep learning from probability distributions successfully\nachieve classification or regression from distribution samples, thus invariant\nunder permutation of the samples. The first contribution of the paper is to\nextend these neural architectures to achieve invariance under permutation of\nthe features, too. The proposed architecture, called Dida, inherits the NN\nproperties of universal approximation, and its robustness w.r.t.\nLipschitz-bounded transformations of the input distribution is established. The\nsecond contribution is to empirically and comparatively demonstrate the merits\nof the approach on two tasks defined at the dataset level. On both tasks, Dida\nlearns meta-features supporting the characterization of a (labelled) dataset.\nThe first task consists of predicting whether two dataset patches are extracted\nfrom the same initial dataset. The second task consists of predicting whether\nthe learning performance achieved by a hyper-parameter configuration under a\nfixed algorithm (ranging in k-NN, SVM, logistic regression and linear\nclassifier with SGD) dominates that of another configuration, for a dataset\nextracted from the OpenML benchmarking suite. On both tasks, Dida outperforms\nthe state of the art: DSS (Maron et al., 2020) and Dataset2Vec (Jomaa et al.,\n2019) architectures, as well as the models based on the hand-crafted\nmeta-features of the literature.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2006.13708v2",
        "date": "2020-06-24 13:15:50+00:00"
    },
    {
        "title": "An Iterative Closest Points Approach to Neural Generative Models",
        "authors": [
            "Joose Rajam\u00e4ki",
            "Perttu H\u00e4m\u00e4l\u00e4inen"
        ],
        "abstract": "We present a simple way to learn a transformation that maps samples of one\ndistribution to the samples of another distribution. Our algorithm comprises an\niteration of 1) drawing samples from some simple distribution and transforming\nthem using a neural network, 2) determining pairwise correspondences between\nthe transformed samples and training data (or a minibatch), and 3) optimizing\nthe weights of the neural network being trained to minimize the distances\nbetween the corresponding vectors. This can be considered as a variant of the\nIterative Closest Points (ICP) algorithm, common in geometric computer vision,\nalthough ICP typically operates on sensor point clouds and linear transforms\ninstead of random sample sets and neural nonlinear transforms. We demonstrate\nthe algorithm on simple synthetic data and MNIST data. We furthermore\ndemonstrate that the algorithm is capable of handling distributions with both\ncontinuous and discrete variables.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/1711.06562v4",
        "date": "2017-11-16 08:07:47+00:00"
    },
    {
        "title": "A Transistor Operations Model for Deep Learning Energy Consumption Scaling Law",
        "authors": [
            "Chen Li",
            "Antonios Tsourdos",
            "Weisi Guo"
        ],
        "abstract": "Deep Learning (DL) has transformed the automation of a wide range of\nindustries and finds increasing ubiquity in society. The high complexity of DL\nmodels and its widespread adoption has led to global energy consumption\ndoubling every 3-4 months. Currently, the relationship between the DL model\nconfiguration and energy consumption is not well established. At a general\ncomputational energy model level, there is both strong dependency to both the\nhardware architecture (e.g. generic processors with different configuration of\ninner components- CPU and GPU, programmable integrated circuits - FPGA), as\nwell as different interacting energy consumption aspects (e.g., data movement,\ncalculation, control). At the DL model level, we need to translate non-linear\nactivation functions and its interaction with data into calculation tasks.\nCurrent methods mainly linearize nonlinear DL models to approximate its\ntheoretical FLOPs and MACs as a proxy for energy consumption. Yet, this is\ninaccurate (est. 93\\% accuracy) due to the highly nonlinear nature of many\nconvolutional neural networks (CNNs) for example.\n  In this paper, we develop a bottom-level Transistor Operations (TOs) method\nto expose the role of non-linear activation functions and neural network\nstructure in energy consumption. We translate a range of feedforward and CNN\nmodels into ALU calculation tasks and then TO steps. This is then statistically\nlinked to real energy consumption values via a regression model for different\nhardware configurations and data sets. We show that our proposed TOs method can\nachieve a 93.61% - 99.51% precision in predicting its energy consumption.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.AR"
        ],
        "link": "http://arxiv.org/pdf/2205.15062v2",
        "date": "2022-05-30 12:42:33+00:00"
    },
    {
        "title": "Neural Network with Unbounded Activation Functions is Universal Approximator",
        "authors": [
            "Sho Sonoda",
            "Noboru Murata"
        ],
        "abstract": "This paper presents an investigation of the approximation property of neural\nnetworks with unbounded activation functions, such as the rectified linear unit\n(ReLU), which is the new de-facto standard of deep learning. The ReLU network\ncan be analyzed by the ridgelet transform with respect to Lizorkin\ndistributions. By showing three reconstruction formulas by using the Fourier\nslice theorem, the Radon transform, and Parseval's relation, it is shown that a\nneural network with unbounded activation functions still satisfies the\nuniversal approximation property. As an additional consequence, the ridgelet\ntransform, or the backprojection filter in the Radon domain, is what the\nnetwork learns after backpropagation. Subject to a constructive admissibility\ncondition, the trained network can be obtained by simply discretizing the\nridgelet transform, without backpropagation. Numerical examples not only\nsupport the consistency of the admissibility condition but also imply that some\nnon-admissible cases result in low-pass filtering.",
        "categories": [
            "cs.NE",
            "cs.LG",
            "math.FA"
        ],
        "link": "http://arxiv.org/pdf/1505.03654v2",
        "date": "2015-05-14 09:03:19+00:00"
    },
    {
        "title": "Learning to Exploit Invariances in Clinical Time-Series Data using Sequence Transformer Networks",
        "authors": [
            "Jeeheh Oh",
            "Jiaxuan Wang",
            "Jenna Wiens"
        ],
        "abstract": "Recently, researchers have started applying convolutional neural networks\n(CNNs) with one-dimensional convolutions to clinical tasks involving\ntime-series data. This is due, in part, to their computational efficiency,\nrelative to recurrent neural networks and their ability to efficiently exploit\ncertain temporal invariances, (e.g., phase invariance). However, it is\nwell-established that clinical data may exhibit many other types of invariances\n(e.g., scaling). While preprocessing techniques, (e.g., dynamic time warping)\nmay successfully transform and align inputs, their use often requires one to\nidentify the types of invariances in advance. In contrast, we propose the use\nof Sequence Transformer Networks, an end-to-end trainable architecture that\nlearns to identify and account for invariances in clinical time-series data.\nApplied to the task of predicting in-hospital mortality, our proposed approach\nachieves an improvement in the area under the receiver operating characteristic\ncurve (AUROC) relative to a baseline CNN (AUROC=0.851 vs. AUROC=0.838). Our\nresults suggest that a variety of valuable invariances can be learned directly\nfrom the data.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1808.06725v1",
        "date": "2018-08-21 00:13:12+00:00"
    },
    {
        "title": "Activation Landscapes as a Topological Summary of Neural Network Performance",
        "authors": [
            "Matthew Wheeler",
            "Jose Bouza",
            "Peter Bubenik"
        ],
        "abstract": "We use topological data analysis (TDA) to study how data transforms as it\npasses through successive layers of a deep neural network (DNN). We compute the\npersistent homology of the activation data for each layer of the network and\nsummarize this information using persistence landscapes. The resulting feature\nmap provides both an informative visual- ization of the network and a kernel\nfor statistical analysis and machine learning. We observe that the topological\ncomplexity often increases with training and that the topological complexity\ndoes not decrease with each layer.",
        "categories": [
            "cs.LG",
            "math.AT"
        ],
        "link": "http://arxiv.org/pdf/2110.10136v1",
        "date": "2021-10-19 17:45:36+00:00"
    },
    {
        "title": "Theory and Experiments on Vector Quantized Autoencoders",
        "authors": [
            "Aurko Roy",
            "Ashish Vaswani",
            "Arvind Neelakantan",
            "Niki Parmar"
        ],
        "abstract": "Deep neural networks with discrete latent variables offer the promise of\nbetter symbolic reasoning, and learning abstractions that are more useful to\nnew tasks. There has been a surge in interest in discrete latent variable\nmodels, however, despite several recent improvements, the training of discrete\nlatent variable models has remained challenging and their performance has\nmostly failed to match their continuous counterparts. Recent work on vector\nquantized autoencoders (VQ-VAE) has made substantial progress in this\ndirection, with its perplexity almost matching that of a VAE on datasets such\nas CIFAR-10. In this work, we investigate an alternate training technique for\nVQ-VAE, inspired by its connection to the Expectation Maximization (EM)\nalgorithm. Training the discrete bottleneck with EM helps us achieve better\nimage generation results on CIFAR-10, and together with knowledge distillation,\nallows us to develop a non-autoregressive machine translation model whose\naccuracy almost matches a strong greedy autoregressive baseline Transformer,\nwhile being 3.3 times faster at inference.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1805.11063v2",
        "date": "2018-05-28 17:16:20+00:00"
    },
    {
        "title": "Differentiation of Blackbox Combinatorial Solvers",
        "authors": [
            "Marin Vlastelica",
            "Anselm Paulus",
            "V\u00edt Musil",
            "Georg Martius",
            "Michal Rol\u00ednek"
        ],
        "abstract": "Achieving fusion of deep learning with combinatorial algorithms promises\ntransformative changes to artificial intelligence. One possible approach is to\nintroduce combinatorial building blocks into neural networks. Such end-to-end\narchitectures have the potential to tackle combinatorial problems on raw input\ndata such as ensuring global consistency in multi-object tracking or route\nplanning on maps in robotics. In this work, we present a method that implements\nan efficient backward pass through blackbox implementations of combinatorial\nsolvers with linear objective functions. We provide both theoretical and\nexperimental backing. In particular, we incorporate the Gurobi MIP solver,\nBlossom V algorithm, and Dijkstra's algorithm into architectures that extract\nsuitable features from raw inputs for the traveling salesman problem, the\nmin-cost perfect matching problem and the shortest path problem. The code is\navailable at https://github.com/martius-lab/blackbox-backprop.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1912.02175v2",
        "date": "2019-12-04 18:54:42+00:00"
    },
    {
        "title": "Variational Mixture of Normalizing Flows",
        "authors": [
            "Guilherme G. P. Freitas Pires",
            "M\u00e1rio A. T. Figueiredo"
        ],
        "abstract": "In the past few years, deep generative models, such as generative adversarial\nnetworks \\autocite{GAN}, variational autoencoders \\autocite{vaepaper}, and\ntheir variants, have seen wide adoption for the task of modelling complex data\ndistributions. In spite of the outstanding sample quality achieved by those\nearly methods, they model the target distributions \\emph{implicitly}, in the\nsense that the probability density functions induced by them are not explicitly\naccessible. This fact renders those methods unfit for tasks that require, for\nexample, scoring new instances of data with the learned distributions.\nNormalizing flows have overcome this limitation by leveraging the\nchange-of-variables formula for probability density functions, and by using\ntransformations designed to have tractable and cheaply computable Jacobians.\nAlthough flexible, this framework lacked (until recently\n\\autocites{semisuplearning_nflows, RAD}) a way to introduce discrete structure\n(such as the one found in mixtures) in the models it allows to construct, in an\nunsupervised scenario. The present work overcomes this by using normalizing\nflows as components in a mixture model and devising an end-to-end training\nprocedure for such a model. This procedure is based on variational inference,\nand uses a variational posterior parameterized by a neural network. As will\nbecome clear, this model naturally lends itself to (multimodal) density\nestimation, semi-supervised learning, and clustering. The proposed model is\nillustrated on two synthetic datasets, as well as on a real-world dataset.\nKeywords: Deep generative models, normalizing flows, variational inference,\nprobabilistic modelling, mixture models.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2009.00585v1",
        "date": "2020-09-01 17:20:08+00:00"
    },
    {
        "title": "LOT: Layer-wise Orthogonal Training on Improving $\\ell_2$ Certified Robustness",
        "authors": [
            "Xiaojun Xu",
            "Linyi Li",
            "Bo Li"
        ],
        "abstract": "Recent studies show that training deep neural networks (DNNs) with Lipschitz\nconstraints are able to enhance adversarial robustness and other model\nproperties such as stability. In this paper, we propose a layer-wise orthogonal\ntraining method (LOT) to effectively train 1-Lipschitz convolution layers via\nparametrizing an orthogonal matrix with an unconstrained matrix. We then\nefficiently compute the inverse square root of a convolution kernel by\ntransforming the input domain to the Fourier frequency domain. On the other\nhand, as existing works show that semi-supervised training helps improve\nempirical robustness, we aim to bridge the gap and prove that semi-supervised\nlearning also improves the certified robustness of Lipschitz-bounded models. We\nconduct comprehensive evaluations for LOT under different settings. We show\nthat LOT significantly outperforms baselines regarding deterministic l2\ncertified robustness, and scales to deeper neural networks. Under the\nsupervised scenario, we improve the state-of-the-art certified robustness for\nall architectures (e.g. from 59.04% to 63.50% on CIFAR-10 and from 32.57% to\n34.59% on CIFAR-100 at radius rho = 36/255 for 40-layer networks). With\nsemi-supervised learning over unlabelled data, we are able to improve\nstate-of-the-art certified robustness on CIFAR-10 at rho = 108/255 from 36.04%\nto 42.39%. In addition, LOT consistently outperforms baselines on different\nmodel architectures with only 1/3 evaluation time.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2210.11620v2",
        "date": "2022-10-20 22:31:26+00:00"
    },
    {
        "title": "Metrics for Deep Generative Models",
        "authors": [
            "Nutan Chen",
            "Alexej Klushyn",
            "Richard Kurle",
            "Xueyan Jiang",
            "Justin Bayer",
            "Patrick van der Smagt"
        ],
        "abstract": "Neural samplers such as variational autoencoders (VAEs) or generative\nadversarial networks (GANs) approximate distributions by transforming samples\nfrom a simple random source---the latent space---to samples from a more complex\ndistribution represented by a dataset. While the manifold hypothesis implies\nthat the density induced by a dataset contains large regions of low density,\nthe training criterions of VAEs and GANs will make the latent space densely\ncovered. Consequently points that are separated by low-density regions in\nobservation space will be pushed together in latent space, making stationary\ndistances poor proxies for similarity. We transfer ideas from Riemannian\ngeometry to this setting, letting the distance between two points be the\nshortest path on a Riemannian manifold induced by the transformation. The\nmethod yields a principled distance measure, provides a tool for visual\ninspection of deep generative models, and an alternative to linear\ninterpolation in latent space. In addition, it can be applied for robot\nmovement generalization using previously learned skills. The method is\nevaluated on a synthetic dataset with known ground truth; on a simulated robot\narm dataset; on human motion capture data; and on a generative model of\nhandwritten digits.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1711.01204v2",
        "date": "2017-11-03 15:18:10+00:00"
    },
    {
        "title": "A Hybrid Frequency-domain/Image-domain Deep Network for Magnetic Resonance Image Reconstruction",
        "authors": [
            "Roberto Souza",
            "Richard Frayne"
        ],
        "abstract": "Decreasing magnetic resonance (MR) image acquisition times can potentially\nreduce procedural cost and make MR examinations more accessible. Compressed\nsensing (CS)-based image reconstruction methods, for example, decrease MR\nacquisition time by reconstructing high-quality images from data that were\noriginally sampled at rates inferior to the Nyquist-Shannon sampling theorem.\nIn this work we propose a hybrid architecture that works both in the k-space\n(or frequency-domain) and the image (or spatial) domains. Our network is\ncomposed of a complex-valued residual U-net in the k-space domain, an inverse\nFast Fourier Transform (iFFT) operation, and a real-valued U-net in the image\ndomain. Our experiments demonstrated, using MR raw k-space data, that the\nproposed hybrid approach can potentially improve CS reconstruction compared to\ndeep-learning networks that operate only in the image domain. In this study we\ncompare our method with four previously published deep neural networks and\nexamine their ability to reconstruct images that are subsequently used to\ngenerate regional volume estimates. We evaluated undersampling ratios of 75%\nand 80%. Our technique was ranked second in the quantitative analysis, but\nqualitative analysis indicated that our reconstruction performed the best in\nhard to reconstruct regions, such as the cerebellum. All images reconstructed\nwith our method were successfully post-processed, and showed good volumetry\nagreement compared with the fully sampled reconstruction measures.",
        "categories": [
            "eess.IV",
            "cs.LG",
            "eess.SP",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1810.12473v1",
        "date": "2018-10-30 01:10:19+00:00"
    },
    {
        "title": "Dynamic Routing Networks",
        "authors": [
            "Shaofeng Cai",
            "Yao Shu",
            "Wei Wang",
            "Beng Chin Ooi"
        ],
        "abstract": "The deployment of deep neural networks in real-world applications is mostly\nrestricted by their high inference costs. Extensive efforts have been made to\nimprove the accuracy with expert-designed or algorithm-searched architectures.\nHowever, the incremental improvement is typically achieved with increasingly\nmore expensive models that only a small portion of input instances really need.\nInference with a static architecture that processes all input instances via the\nsame transformation would thus incur unnecessary computational costs.\nTherefore, customizing the model capacity in an instance-aware manner is much\nneeded for higher inference efficiency. In this paper, we propose Dynamic\nRouting Networks (DRNets), which support efficient instance-aware inference by\nrouting the input instance to only necessary transformation branches selected\nfrom a candidate set of branches for each connection between transformation\nnodes. The branch selection is dynamically determined via the corresponding\nbranch importance weights, which are first generated from lightweight\nhypernetworks (RouterNets) and then recalibrated with Gumbel-Softmax before the\nselection. Extensive experiments show that DRNets can reduce a substantial\namount of parameter size and FLOPs during inference with prediction performance\ncomparable to state-of-the-art architectures.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1905.04849v5",
        "date": "2019-05-13 03:45:42+00:00"
    },
    {
        "title": "Deep Learning Assisted End-to-End Synthesis of mm-Wave Passive Networks with 3D EM Structures: A Study on A Transformer-Based Matching Network",
        "authors": [
            "Siawpeng Er",
            "Edward Liu",
            "Minshuo Chen",
            "Yan Li",
            "Yuqi Liu",
            "Tuo Zhao",
            "Hua Wang"
        ],
        "abstract": "This paper presents a deep learning assisted synthesis approach for direct\nend-to-end generation of RF/mm-wave passive matching network with 3D EM\nstructures. Different from prior approaches that synthesize EM structures from\ntarget circuit component values and target topologies, our proposed approach\nachieves the direct synthesis of the passive network given the network topology\nfrom desired performance values as input. We showcase the proposed synthesis\nNeural Network (NN) model on an on-chip 1:1 transformer-based impedance\nmatching network. By leveraging parameter sharing, the synthesis NN model\nsuccessfully extracts relevant features from the input impedance and load\ncapacitors, and predict the transformer 3D EM geometry in a 45nm SOI process\nthat will match the standard 50$\\Omega$ load to the target input impedance\nwhile absorbing the two loading capacitors. As a proof-of-concept, several\nexample transformer geometries were synthesized, and verified in Ansys HFSS to\nprovide the desired input impedance.",
        "categories": [
            "cs.LG",
            "cs.SY",
            "eess.SP",
            "eess.SY"
        ],
        "link": "http://arxiv.org/pdf/2201.02141v1",
        "date": "2022-01-06 16:55:41+00:00"
    },
    {
        "title": "A Tale of Three Probabilistic Families: Discriminative, Descriptive and Generative Models",
        "authors": [
            "Ying Nian Wu",
            "Ruiqi Gao",
            "Tian Han",
            "Song-Chun Zhu"
        ],
        "abstract": "The pattern theory of Grenander is a mathematical framework where patterns\nare represented by probability models on random variables of algebraic\nstructures. In this paper, we review three families of probability models,\nnamely, the discriminative models, the descriptive models, and the generative\nmodels. A discriminative model is in the form of a classifier. It specifies the\nconditional probability of the class label given the input signal. A\ndescriptive model specifies the probability distribution of the signal, based\non an energy function defined on the signal. A generative model assumes that\nthe signal is generated by some latent variables via a transformation. We shall\nreview these models within a common framework and explore their connections. We\nshall also review the recent developments that take advantage of the high\napproximation capacities of deep neural networks.",
        "categories": [
            "stat.ML",
            "cs.CV",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1810.04261v2",
        "date": "2018-10-09 21:54:54+00:00"
    },
    {
        "title": "Semantic Perturbations with Normalizing Flows for Improved Generalization",
        "authors": [
            "Oguz Kaan Yuksel",
            "Sebastian U. Stich",
            "Martin Jaggi",
            "Tatjana Chavdarova"
        ],
        "abstract": "Data augmentation is a widely adopted technique for avoiding overfitting when\ntraining deep neural networks. However, this approach requires domain-specific\nknowledge and is often limited to a fixed set of hard-coded transformations.\nRecently, several works proposed to use generative models for generating\nsemantically meaningful perturbations to train a classifier. However, because\naccurate encoding and decoding are critical, these methods, which use\narchitectures that approximate the latent-variable inference, remained limited\nto pilot studies on small datasets.\n  Exploiting the exactly reversible encoder-decoder structure of normalizing\nflows, we perform on-manifold perturbations in the latent space to define fully\nunsupervised data augmentations. We demonstrate that such perturbations match\nthe performance of advanced data augmentation techniques -- reaching 96.6% test\naccuracy for CIFAR-10 using ResNet-18 and outperform existing methods,\nparticularly in low data regimes -- yielding 10--25% relative improvement of\ntest accuracy from classical training. We find that our latent adversarial\nperturbations adaptive to the classifier throughout its training are most\neffective, yielding the first test accuracy improvement results on real-world\ndatasets -- CIFAR-10/100 -- via latent-space perturbations.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2108.07958v1",
        "date": "2021-08-18 03:20:00+00:00"
    },
    {
        "title": "Heterogeneous Calibration: A post-hoc model-agnostic framework for improved generalization",
        "authors": [
            "David Durfee",
            "Aman Gupta",
            "Kinjal Basu"
        ],
        "abstract": "We introduce the notion of heterogeneous calibration that applies a post-hoc\nmodel-agnostic transformation to model outputs for improving AUC performance on\nbinary classification tasks. We consider overconfident models, whose\nperformance is significantly better on training vs test data and give intuition\nonto why they might under-utilize moderately effective simple patterns in the\ndata. We refer to these simple patterns as heterogeneous partitions of the\nfeature space and show theoretically that perfectly calibrating each partition\nseparately optimizes AUC. This gives a general paradigm of heterogeneous\ncalibration as a post-hoc procedure by which heterogeneous partitions of the\nfeature space are identified through tree-based algorithms and post-hoc\ncalibration techniques are applied to each partition to improve AUC. While the\ntheoretical optimality of this framework holds for any model, we focus on deep\nneural networks (DNNs) and test the simplest instantiation of this paradigm on\na variety of open-source datasets. Experiments demonstrate the effectiveness of\nthis framework and the future potential for applying higher-performing\npartitioning schemes along with more effective calibration techniques.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2202.04837v1",
        "date": "2022-02-10 05:08:50+00:00"
    },
    {
        "title": "Parallelizing Legendre Memory Unit Training",
        "authors": [
            "Narsimha Chilkuri",
            "Chris Eliasmith"
        ],
        "abstract": "Recently, a new recurrent neural network (RNN) named the Legendre Memory Unit\n(LMU) was proposed and shown to achieve state-of-the-art performance on several\nbenchmark datasets. Here we leverage the linear time-invariant (LTI) memory\ncomponent of the LMU to construct a simplified variant that can be parallelized\nduring training (and yet executed as an RNN during inference), thus overcoming\na well known limitation of training RNNs on GPUs. We show that this\nreformulation that aids parallelizing, which can be applied generally to any\ndeep network whose recurrent components are linear, makes training up to 200\ntimes faster. Second, to validate its utility, we compare its performance\nagainst the original LMU and a variety of published LSTM and transformer\nnetworks on seven benchmarks, ranging from psMNIST to sentiment analysis to\nmachine translation. We demonstrate that our models exhibit superior\nperformance on all datasets, often using fewer parameters. For instance, our\nLMU sets a new state-of-the-art result on psMNIST, and uses half the parameters\nwhile outperforming DistilBERT and LSTM models on IMDB sentiment analysis.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2102.11417v2",
        "date": "2021-02-22 23:43:47+00:00"
    },
    {
        "title": "PECAN: A Product-Quantized Content Addressable Memory Network",
        "authors": [
            "Jie Ran",
            "Rui Lin",
            "Jason Chun Lok Li",
            "Jiajun Zhou",
            "Ngai Wong"
        ],
        "abstract": "A novel deep neural network (DNN) architecture is proposed wherein the\nfiltering and linear transform are realized solely with product quantization\n(PQ). This results in a natural implementation via content addressable memory\n(CAM), which transcends regular DNN layer operations and requires only simple\ntable lookup. Two schemes are developed for the end-to-end PQ prototype\ntraining, namely, through angle- and distance-based similarities, which differ\nin their multiplicative and additive natures with different complexity-accuracy\ntradeoffs. Even more, the distance-based scheme constitutes a truly\nmultiplier-free DNN solution. Experiments confirm the feasibility of such\nProduct-Quantized Content Addressable Memory Network (PECAN), which has strong\nimplication on hardware-efficient deployments especially for in-memory\ncomputing.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2208.13571v1",
        "date": "2022-08-13 08:33:56+00:00"
    },
    {
        "title": "Brain2Char: A Deep Architecture for Decoding Text from Brain Recordings",
        "authors": [
            "Pengfei Sun",
            "Gopala K. Anumanchipalli",
            "Edward F. Chang"
        ],
        "abstract": "Decoding language representations directly from the brain can enable new\nBrain-Computer Interfaces (BCI) for high bandwidth human-human and\nhuman-machine communication. Clinically, such technologies can restore\ncommunication in people with neurological conditions affecting their ability to\nspeak. In this study, we propose a novel deep network architecture Brain2Char,\nfor directly decoding text (specifically character sequences) from direct brain\nrecordings (called Electrocorticography, ECoG). Brain2Char framework combines\nstate-of-the-art deep learning modules --- 3D Inception layers for multiband\nspatiotemporal feature extraction from neural data and bidirectional recurrent\nlayers, dilated convolution layers followed by language model weighted beam\nsearch to decode character sequences, optimizing a connectionist temporal\nclassification (CTC) loss. Additionally, given the highly non-linear\ntransformations that underlie the conversion of cortical function to character\nsequences, we perform regularizations on the network's latent representations\nmotivated by insights into cortical encoding of speech production and\nartifactual aspects specific to ECoG data acquisition. To do this, we impose\nauxiliary losses on latent representations for articulatory movements, speech\nacoustics and session specific non-linearities. In 3 participants tested here,\nBrain2Char achieves 10.6\\%, 8.5\\% and 7.0\\% Word Error Rates (WER) respectively\non vocabulary sizes ranging from 1200 to 1900 words. Brain2Char also performs\nwell when 2 participants silently mimed sentences. These results set a new\nstate-of-the-art on decoding text from brain and demonstrate the potential of\nBrain2Char as a high-performance communication BCI.",
        "categories": [
            "cs.LG",
            "cs.CL",
            "q-bio.NC",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1909.01401v1",
        "date": "2019-09-03 18:54:43+00:00"
    },
    {
        "title": "CC-Cert: A Probabilistic Approach to Certify General Robustness of Neural Networks",
        "authors": [
            "Mikhail Pautov",
            "Nurislam Tursynbek",
            "Marina Munkhoeva",
            "Nikita Muravev",
            "Aleksandr Petiushko",
            "Ivan Oseledets"
        ],
        "abstract": "In safety-critical machine learning applications, it is crucial to defend\nmodels against adversarial attacks -- small modifications of the input that\nchange the predictions. Besides rigorously studied $\\ell_p$-bounded additive\nperturbations, recently proposed semantic perturbations (e.g. rotation,\ntranslation) raise a serious concern on deploying ML systems in real-world.\nTherefore, it is important to provide provable guarantees for deep learning\nmodels against semantically meaningful input transformations. In this paper, we\npropose a new universal probabilistic certification approach based on\nChernoff-Cramer bounds that can be used in general attack settings. We estimate\nthe probability of a model to fail if the attack is sampled from a certain\ndistribution. Our theoretical findings are supported by experimental results on\ndifferent datasets.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2109.10696v2",
        "date": "2021-09-22 12:46:04+00:00"
    },
    {
        "title": "Learning Spatiotemporal Features of Ride-sourcing Services with Fusion Convolutional Network",
        "authors": [
            "Feng Xiao",
            "Dapeng Zhang",
            "Gang Kou",
            "Lu Li"
        ],
        "abstract": "To collectively forecast the demand for ride-sourcing services in all regions\nof a city, the deep learning approaches have been applied with commendable\nresults. However, the local statistical differences throughout the geographical\nlayout of the city make the spatial stationarity assumption of the convolution\ninvalid, which limits the performance of CNNs on the demand forecasting task.\nIn this paper, we propose a novel deep learning framework called LC-ST-FCN\n(locally connected spatiotemporal fully-convolutional neural network) to\naddress the unique challenges of the region-level demand forecasting problem\nwithin one end-to-end architecture (E2E). We first employ the 3D convolutional\nlayers to fuse the spatial and temporal information existed in the input and\nthen feed the spatiotemporal features extracted by the 3D convolutional layers\nto the subsequent 2D convolutional layers. Afterward, the prediction value of\neach region is obtained by the locally connected convolutional layers which\nrelax the parameter sharing scheme. We evaluate the proposed model on a real\ndataset from a ride-sourcing service platform (DiDiChuxing) and observe\nsignificant improvements compared with a bunch of baseline models. Besides, we\nalso illustrate the effectiveness of our proposed model by visualizing how\ndifferent types of convolutional layers transform their input and capture\nuseful features. The visualization results show that fully convolutional\narchitecture enables the model to better localize the related regions. And the\nlocally connected layers play an important role in dealing with the local\nstatistical differences and activating useful regions.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1904.06823v2",
        "date": "2019-04-15 03:10:45+00:00"
    },
    {
        "title": "DeConFuse : A Deep Convolutional Transform based Unsupervised Fusion Framework",
        "authors": [
            "Pooja Gupta",
            "Jyoti Maggu",
            "Angshul Majumdar",
            "Emilie Chouzenoux",
            "Giovanni Chierchia"
        ],
        "abstract": "This work proposes an unsupervised fusion framework based on deep\nconvolutional transform learning. The great learning ability of convolutional\nfilters for data analysis is well acknowledged. The success of convolutive\nfeatures owes to convolutional neural network (CNN). However, CNN cannot\nperform learning tasks in an unsupervised fashion. In a recent work, we show\nthat such shortcoming can be addressed by adopting a convolutional transform\nlearning (CTL) approach, where convolutional filters are learnt in an\nunsupervised fashion. The present paper aims at (i) proposing a deep version of\nCTL; (ii) proposing an unsupervised fusion formulation taking advantage of the\nproposed deep CTL representation; (iii) developing a mathematically sounded\noptimization strategy for performing the learning task. We apply the proposed\ntechnique, named DeConFuse, on the problem of stock forecasting and trading.\nComparison with state-of-the-art methods (based on CNN and long short-term\nmemory network) shows the superiority of our method for performing a reliable\nfeature extraction.",
        "categories": [
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2011.04337v1",
        "date": "2020-11-09 11:04:09+00:00"
    },
    {
        "title": "A Unifying View on Implicit Bias in Training Linear Neural Networks",
        "authors": [
            "Chulhee Yun",
            "Shankar Krishnan",
            "Hossein Mobahi"
        ],
        "abstract": "We study the implicit bias of gradient flow (i.e., gradient descent with\ninfinitesimal step size) on linear neural network training. We propose a tensor\nformulation of neural networks that includes fully-connected, diagonal, and\nconvolutional networks as special cases, and investigate the linear version of\nthe formulation called linear tensor networks. With this formulation, we can\ncharacterize the convergence direction of the network parameters as singular\nvectors of a tensor defined by the network. For $L$-layer linear tensor\nnetworks that are orthogonally decomposable, we show that gradient flow on\nseparable classification finds a stationary point of the $\\ell_{2/L}$\nmax-margin problem in a \"transformed\" input space defined by the network. For\nunderdetermined regression, we prove that gradient flow finds a global minimum\nwhich minimizes a norm-like function that interpolates between weighted\n$\\ell_1$ and $\\ell_2$ norms in the transformed input space. Our theorems\nsubsume existing results in the literature while removing standard convergence\nassumptions. We also provide experiments that corroborate our analysis.",
        "categories": [
            "cs.LG",
            "math.OC",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2010.02501v3",
        "date": "2020-10-06 06:08:35+00:00"
    },
    {
        "title": "Zero-Shot Contrastive Loss for Text-Guided Diffusion Image Style Transfer",
        "authors": [
            "Serin Yang",
            "Hyunmin Hwang",
            "Jong Chul Ye"
        ],
        "abstract": "Diffusion models have shown great promise in text-guided image style\ntransfer, but there is a trade-off between style transformation and content\npreservation due to their stochastic nature. Existing methods require\ncomputationally expensive fine-tuning of diffusion models or additional neural\nnetwork. To address this, here we propose a zero-shot contrastive loss for\ndiffusion models that doesn't require additional fine-tuning or auxiliary\nnetworks. By leveraging patch-wise contrastive loss between generated samples\nand original image embeddings in the pre-trained diffusion model, our method\ncan generate images with the same semantic content as the source image in a\nzero-shot manner. Our approach outperforms existing methods while preserving\ncontent and requiring no additional training, not only for image style transfer\nbut also for image-to-image translation and manipulation. Our experimental\nresults validate the effectiveness of our proposed method.",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2303.08622v2",
        "date": "2023-03-15 13:47:02+00:00"
    },
    {
        "title": "Accuracy-Guaranteed Collaborative DNN Inference in Industrial IoT via Deep Reinforcement Learning",
        "authors": [
            "Wen Wu",
            "Peng Yang",
            "Weiting Zhang",
            "Conghao Zhou",
            "Xuemin",
            "Shen"
        ],
        "abstract": "Collaboration among industrial Internet of Things (IoT) devices and edge\nnetworks is essential to support computation-intensive deep neural network\n(DNN) inference services which require low delay and high accuracy. Sampling\nrate adaption which dynamically configures the sampling rates of industrial IoT\ndevices according to network conditions, is the key in minimizing the service\ndelay. In this paper, we investigate the collaborative DNN inference problem in\nindustrial IoT networks. To capture the channel variation and task arrival\nrandomness, we formulate the problem as a constrained Markov decision process\n(CMDP). Specifically, sampling rate adaption, inference task offloading and\nedge computing resource allocation are jointly considered to minimize the\naverage service delay while guaranteeing the long-term accuracy requirements of\ndifferent inference services. Since CMDP cannot be directly solved by general\nreinforcement learning (RL) algorithms due to the intractable long-term\nconstraints, we first transform the CMDP into an MDP by leveraging the Lyapunov\noptimization technique. Then, a deep RL-based algorithm is proposed to solve\nthe MDP. To expedite the training process, an optimization subroutine is\nembedded in the proposed algorithm to directly obtain the optimal edge\ncomputing resource allocation. Extensive simulation results are provided to\ndemonstrate that the proposed RL-based algorithm can significantly reduce the\naverage service delay while preserving long-term inference accuracy with a high\nprobability.",
        "categories": [
            "eess.SY",
            "cs.AI",
            "cs.LG",
            "cs.SY"
        ],
        "link": "http://arxiv.org/pdf/2301.00130v1",
        "date": "2022-12-31 05:53:17+00:00"
    },
    {
        "title": "Learning Dynamic Generator Model by Alternating Back-Propagation Through Time",
        "authors": [
            "Jianwen Xie",
            "Ruiqi Gao",
            "Zilong Zheng",
            "Song-Chun Zhu",
            "Ying Nian Wu"
        ],
        "abstract": "This paper studies the dynamic generator model for spatial-temporal processes\nsuch as dynamic textures and action sequences in video data. In this model,\neach time frame of the video sequence is generated by a generator model, which\nis a non-linear transformation of a latent state vector, where the non-linear\ntransformation is parametrized by a top-down neural network. The sequence of\nlatent state vectors follows a non-linear auto-regressive model, where the\nstate vector of the next frame is a non-linear transformation of the state\nvector of the current frame as well as an independent noise vector that\nprovides randomness in the transition. The non-linear transformation of this\ntransition model can be parametrized by a feedforward neural network. We show\nthat this model can be learned by an alternating back-propagation through time\nalgorithm that iteratively samples the noise vectors and updates the parameters\nin the transition model and the generator model. We show that our training\nmethod can learn realistic models for dynamic textures and action patterns.",
        "categories": [
            "stat.ML",
            "cs.CV",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1812.10587v1",
        "date": "2018-12-27 01:34:08+00:00"
    },
    {
        "title": "Deep Directed Generative Autoencoders",
        "authors": [
            "Sherjil Ozair",
            "Yoshua Bengio"
        ],
        "abstract": "For discrete data, the likelihood $P(x)$ can be rewritten exactly and\nparametrized into $P(X = x) = P(X = x | H = f(x)) P(H = f(x))$ if $P(X | H)$\nhas enough capacity to put no probability mass on any $x'$ for which $f(x')\\neq\nf(x)$, where $f(\\cdot)$ is a deterministic discrete function. The log of the\nfirst factor gives rise to the log-likelihood reconstruction error of an\nautoencoder with $f(\\cdot)$ as the encoder and $P(X|H)$ as the (probabilistic)\ndecoder. The log of the second term can be seen as a regularizer on the encoded\nactivations $h=f(x)$, e.g., as in sparse autoencoders. Both encoder and decoder\ncan be represented by a deep neural network and trained to maximize the average\nof the optimal log-likelihood $\\log p(x)$. The objective is to learn an encoder\n$f(\\cdot)$ that maps $X$ to $f(X)$ that has a much simpler distribution than\n$X$ itself, estimated by $P(H)$. This \"flattens the manifold\" or concentrates\nprobability mass in a smaller number of (relevant) dimensions over which the\ndistribution factorizes. Generating samples from the model is straightforward\nusing ancestral sampling. One challenge is that regular back-propagation cannot\nbe used to obtain the gradient on the parameters of the encoder, but we find\nthat using the straight-through estimator works well here. We also find that\nalthough optimizing a single level of such architecture may be difficult, much\nbetter results can be obtained by pre-training and stacking them, gradually\ntransforming the data distribution into one that is more easily captured by a\nsimple parametric model.",
        "categories": [
            "stat.ML",
            "cs.LG",
            "cs.NE"
        ],
        "link": "http://arxiv.org/pdf/1410.0630v1",
        "date": "2014-10-02 18:09:42+00:00"
    },
    {
        "title": "Random vector functional link neural network based ensemble deep learning for short-term load forecasting",
        "authors": [
            "Ruobin Gao",
            "Liang Du",
            "P. N. Suganthan",
            "Qin Zhou",
            "Kum Fai Yuen"
        ],
        "abstract": "Electricity load forecasting is crucial for the power systems' planning and\nmaintenance. However, its un-stationary and non-linear characteristics impose\nsignificant difficulties in anticipating future demand. This paper proposes a\nnovel ensemble deep Random Vector Functional Link (edRVFL) network for\nelectricity load forecasting. The weights of hidden layers are randomly\ninitialized and kept fixed during the training process. The hidden layers are\nstacked to enforce deep representation learning. Then, the model generates the\nforecasts by ensembling the outputs of each layer. Moreover, we also propose to\naugment the random enhancement features by empirical wavelet transformation\n(EWT). The raw load data is decomposed by EWT in a walk-forward fashion, not\nintroducing future data leakage problems in the decomposition process. Finally,\nall the sub-series generated by the EWT, including raw data, are fed into the\nedRVFL for forecasting purposes. The proposed model is evaluated on twenty\npublicly available time series from the Australian Energy Market Operator of\nthe year 2020. The simulation results demonstrate the proposed model's superior\nperformance over eleven forecasting methods in three error metrics and\nstatistical tests on electricity load forecasting tasks.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "eess.SP"
        ],
        "link": "http://arxiv.org/pdf/2107.14385v1",
        "date": "2021-07-30 01:20:48+00:00"
    },
    {
        "title": "ClimaX: A foundation model for weather and climate",
        "authors": [
            "Tung Nguyen",
            "Johannes Brandstetter",
            "Ashish Kapoor",
            "Jayesh K. Gupta",
            "Aditya Grover"
        ],
        "abstract": "Most state-of-the-art approaches for weather and climate modeling are based\non physics-informed numerical models of the atmosphere. These approaches aim to\nmodel the non-linear dynamics and complex interactions between multiple\nvariables, which are challenging to approximate. Additionally, many such\nnumerical models are computationally intensive, especially when modeling the\natmospheric phenomenon at a fine-grained spatial and temporal resolution.\nRecent data-driven approaches based on machine learning instead aim to directly\nsolve a downstream forecasting or projection task by learning a data-driven\nfunctional mapping using deep neural networks. However, these networks are\ntrained using curated and homogeneous climate datasets for specific\nspatiotemporal tasks, and thus lack the generality of numerical models. We\ndevelop and demonstrate ClimaX, a flexible and generalizable deep learning\nmodel for weather and climate science that can be trained using heterogeneous\ndatasets spanning different variables, spatio-temporal coverage, and physical\ngroundings. ClimaX extends the Transformer architecture with novel encoding and\naggregation blocks that allow effective use of available compute while\nmaintaining general utility. ClimaX is pre-trained with a self-supervised\nlearning objective on climate datasets derived from CMIP6. The pre-trained\nClimaX can then be fine-tuned to address a breadth of climate and weather\ntasks, including those that involve atmospheric variables and spatio-temporal\nscales unseen during pretraining. Compared to existing data-driven baselines,\nwe show that this generality in ClimaX results in superior performance on\nbenchmarks for weather forecasting and climate projections, even when\npretrained at lower resolutions and compute budgets.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2301.10343v2",
        "date": "2023-01-24 23:19:01+00:00"
    },
    {
        "title": "Digital Signal Processing Using Deep Neural Networks",
        "authors": [
            "Brian Shevitski",
            "Yijing Watkins",
            "Nicole Man",
            "Michael Girard"
        ],
        "abstract": "Currently there is great interest in the utility of deep neural networks\n(DNNs) for the physical layer of radio frequency (RF) communications. In this\nmanuscript, we describe a custom DNN specially designed to solve problems in\nthe RF domain. Our model leverages the mechanisms of feature extraction and\nattention through the combination of an autoencoder convolutional network with\na transformer network, to accomplish several important communications network\nand digital signals processing (DSP) tasks. We also present a new open dataset\nand physical data augmentation model that enables training of DNNs that can\nperform automatic modulation classification, infer and correct transmission\nchannel effects, and directly demodulate baseband RF signals.",
        "categories": [
            "eess.SP",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2109.10404v1",
        "date": "2021-09-21 18:59:32+00:00"
    },
    {
        "title": "Controlling Computation versus Quality for Neural Sequence Models",
        "authors": [
            "Ankur Bapna",
            "Naveen Arivazhagan",
            "Orhan Firat"
        ],
        "abstract": "Most neural networks utilize the same amount of compute for every example\nindependent of the inherent complexity of the input. Further, methods that\nadapt the amount of computation to the example focus on finding a fixed\ninference-time computational graph per example, ignoring any external\ncomputational budgets or varying inference time limitations. In this work, we\nutilize conditional computation to make neural sequence models (Transformer)\nmore efficient and computation-aware during inference. We first modify the\nTransformer architecture, making each set of operations conditionally\nexecutable depending on the output of a learned control network. We then train\nthis model in a multi-task setting, where each task corresponds to a particular\ncomputation budget. This allows us to train a single model that can be\ncontrolled to operate on different points of the computation-quality trade-off\ncurve, depending on the available computation budget at inference time. We\nevaluate our approach on two tasks: (i) WMT English-French Translation and (ii)\nUnsupervised representation learning (BERT). Our experiments demonstrate that\nthe proposed Conditional Computation Transformer (CCT) is competitive with\nvanilla Transformers when allowed to utilize its full computational budget,\nwhile improving significantly over computationally equivalent baselines when\noperating on smaller computational budgets.",
        "categories": [
            "cs.LG",
            "cs.CL",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2002.07106v2",
        "date": "2020-02-17 17:54:27+00:00"
    },
    {
        "title": "Detecting residues of cosmic events using residual neural network",
        "authors": [
            "Hrithika Dodia"
        ],
        "abstract": "The detection of gravitational waves is considered to be one of the most\nmagnificent discoveries of the century. Due to the high computational cost of\nmatched filtering pipeline, there is a hunt for an alternative powerful system.\nI present, for the first time, the use of 1D residual neural network for\ndetection of gravitational waves. Residual networks have transformed many\nfields like image classification, face recognition and object detection with\ntheir robust structure. With increase in sensitivity of LIGO detectors we\nexpect many more sources of gravitational waves in the universe to be detected.\nHowever, deep learning networks are trained only once. When used for\nclassification task, deep neural networks are trained to predict only a fixed\nnumber of classes. Therefore, when a new type of gravitational wave is to be\ndetected, this turns out to be a drawback of deep learning. Shallow neural\nnetworks can be used to learn data with simple patterns but fail to give good\nresults with increase in complexity of data. Remodelling the neural network\nwith detection of each new type of GW is highly infeasible. In this letter, I\nalso discuss ways to reduce the time required to adapt to such changes in\ndetection of gravitational waves for deep learning methods. Primarily, I aim to\ncreate a custom residual neural network for 1-dimensional time series inputs,\nwhich can learn a ton of features from dataset without giving up on increasing\nthe number of classes or increasing the complexity of data. I use the two class\nof binary coalescence signals (Binary Black Hole Merger and Binary Neutron Star\nMerger signals) detected by LIGO to check the performance of residual structure\non gravitational waves detection.",
        "categories": [
            "astro-ph.IM",
            "astro-ph.CO",
            "cs.LG",
            "gr-qc"
        ],
        "link": "http://arxiv.org/pdf/2101.00195v1",
        "date": "2021-01-01 08:44:58+00:00"
    },
    {
        "title": "Neural Importance Sampling",
        "authors": [
            "Thomas M\u00fcller",
            "Brian McWilliams",
            "Fabrice Rousselle",
            "Markus Gross",
            "Jan Nov\u00e1k"
        ],
        "abstract": "We propose to use deep neural networks for generating samples in Monte Carlo\nintegration. Our work is based on non-linear independent components estimation\n(NICE), which we extend in numerous ways to improve performance and enable its\napplication to integration problems. First, we introduce piecewise-polynomial\ncoupling transforms that greatly increase the modeling power of individual\ncoupling layers. Second, we propose to preprocess the inputs of neural networks\nusing one-blob encoding, which stimulates localization of computation and\nimproves inference. Third, we derive a gradient-descent-based optimization for\nthe KL and the $\\chi^2$ divergence for the specific application of Monte Carlo\nintegration with unnormalized stochastic estimates of the target distribution.\nOur approach enables fast and accurate inference and efficient sample\ngeneration independently of the dimensionality of the integration domain. We\nshow its benefits on generating natural images and in two applications to\nlight-transport simulation: first, we demonstrate learning of joint\npath-sampling densities in the primary sample space and importance sampling of\nmulti-dimensional path prefixes thereof. Second, we use our technique to\nextract conditional directional densities driven by the product of incident\nillumination and the BSDF in the rendering equation, and we leverage the\ndensities for path guiding. In all applications, our approach yields on-par or\nhigher performance than competing techniques at equal sample count.",
        "categories": [
            "cs.LG",
            "cs.GR",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1808.03856v5",
        "date": "2018-08-11 20:12:49+00:00"
    },
    {
        "title": "EEG based Continuous Speech Recognition using Transformers",
        "authors": [
            "Gautam Krishna",
            "Co Tran",
            "Mason Carnahan",
            "Ahmed H Tewfik"
        ],
        "abstract": "In this paper we investigate continuous speech recognition using\nelectroencephalography (EEG) features using recently introduced end-to-end\ntransformer based automatic speech recognition (ASR) model. Our results\ndemonstrate that transformer based model demonstrate faster training compared\nto recurrent neural network (RNN) based sequence-to-sequence EEG models and\nbetter performance during inference time for smaller test set vocabulary but as\nwe increase the vocabulary size, the performance of the RNN based models were\nbetter than transformer based model on a limited English vocabulary.",
        "categories": [
            "eess.AS",
            "cs.LG",
            "cs.SD",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2001.00501v3",
        "date": "2019-12-31 08:36:59+00:00"
    },
    {
        "title": "Neural Generalization of Multiple Kernel Learning",
        "authors": [
            "Ahmad Navid Ghanizadeh",
            "Kamaledin Ghiasi-Shirazi",
            "Reza Monsefi",
            "Mohammadreza Qaraei"
        ],
        "abstract": "Multiple Kernel Learning is a conventional way to learn the kernel function\nin kernel-based methods. MKL algorithms enhance the performance of kernel\nmethods. However, these methods have a lower complexity compared to deep\nlearning models and are inferior to these models in terms of recognition\naccuracy. Deep learning models can learn complex functions by applying\nnonlinear transformations to data through several layers. In this paper, we\nshow that a typical MKL algorithm can be interpreted as a one-layer neural\nnetwork with linear activation functions. By this interpretation, we propose a\nNeural Generalization of Multiple Kernel Learning (NGMKL), which extends the\nconventional multiple kernel learning framework to a multi-layer neural network\nwith nonlinear activation functions. Our experiments on several benchmarks show\nthat the proposed method improves the complexity of MKL algorithms and leads to\nhigher recognition accuracy.",
        "categories": [
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2102.13337v1",
        "date": "2021-02-26 07:28:37+00:00"
    },
    {
        "title": "Orthogonal Recurrent Neural Networks with Scaled Cayley Transform",
        "authors": [
            "Kyle Helfrich",
            "Devin Willmott",
            "Qiang Ye"
        ],
        "abstract": "Recurrent Neural Networks (RNNs) are designed to handle sequential data but\nsuffer from vanishing or exploding gradients. Recent work on Unitary Recurrent\nNeural Networks (uRNNs) have been used to address this issue and in some cases,\nexceed the capabilities of Long Short-Term Memory networks (LSTMs). We propose\na simpler and novel update scheme to maintain orthogonal recurrent weight\nmatrices without using complex valued matrices. This is done by parametrizing\nwith a skew-symmetric matrix using the Cayley transform. Such a parametrization\nis unable to represent matrices with negative one eigenvalues, but this\nlimitation is overcome by scaling the recurrent weight matrix by a diagonal\nmatrix consisting of ones and negative ones. The proposed training scheme\ninvolves a straightforward gradient calculation and update step. In several\nexperiments, the proposed scaled Cayley orthogonal recurrent neural network\n(scoRNN) achieves superior results with fewer trainable parameters than other\nunitary RNNs.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1707.09520v3",
        "date": "2017-07-29 14:37:48+00:00"
    },
    {
        "title": "Simultaneous Detection of Multiple Appliances from Smart-meter Measurements via Multi-Label Consistent Deep Dictionary Learning and Deep Transform Learning",
        "authors": [
            "Vanika Singhal",
            "Jyoti Maggu",
            "Angshul Majumdar"
        ],
        "abstract": "Currently there are several well-known approaches to non-intrusive appliance\nload monitoring rule based, stochastic finite state machines, neural networks\nand sparse coding. Recently several studies have proposed a new approach based\non multi label classification. Different appliances are treated as separate\nclasses, and the task is to identify the classes given the aggregate\nsmart-meter reading. Prior studies in this area have used off the shelf\nalgorithms like MLKNN and RAKEL to address this problem. In this work, we\npropose a deep learning based technique. There are hardly any studies in deep\nlearning based multi label classification; two new deep learning techniques to\nsolve the said problem are fundamental contributions of this work. These are\ndeep dictionary learning and deep transform learning. Thorough experimental\nresults on benchmark datasets show marked improvement over existing studies.",
        "categories": [
            "eess.SP",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1912.07568v1",
        "date": "2019-12-11 10:26:16+00:00"
    },
    {
        "title": "FedRel: An Adaptive Federated Relevance Framework for Spatial Temporal Graph Learning",
        "authors": [
            "Tiehua Zhang",
            "Yuze Liu",
            "Zhishu Shen",
            "Rui Xu",
            "Xin Chen",
            "Xiaowei Huang",
            "Xi Zheng"
        ],
        "abstract": "Spatial-temporal data contains rich information and has been widely studied\nin recent years due to the rapid development of relevant applications in many\nfields. For instance, medical institutions often use electrodes attached to\ndifferent parts of a patient to analyse the electorencephal data rich with\nspatial and temporal features for health assessment and disease diagnosis.\nExisting research has mainly used deep learning techniques such as\nconvolutional neural network (CNN) or recurrent neural network (RNN) to extract\nhidden spatial-temporal features. Yet, it is challenging to incorporate both\ninter-dependencies spatial information and dynamic temporal changes\nsimultaneously. In reality, for a model that leverages these spatial-temporal\nfeatures to fulfil complex prediction tasks, it often requires a colossal\namount of training data in order to obtain satisfactory model performance.\nConsidering the above-mentioned challenges, we propose an adaptive federated\nrelevance framework, namely FedRel, for spatial-temporal graph learning in this\npaper. After transforming the raw spatial-temporal data into high quality\nfeatures, the core Dynamic Inter-Intra Graph (DIIG) module in the framework is\nable to use these features to generate the spatial-temporal graphs capable of\ncapturing the hidden topological and long-term temporal correlation information\nin these graphs. To improve the model generalization ability and performance\nwhile preserving the local data privacy, we also design a relevance-driven\nfederated learning module in our framework to leverage diverse data\ndistributions from different participants with attentive aggregations of their\nmodels.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2206.03420v2",
        "date": "2022-06-07 16:12:17+00:00"
    },
    {
        "title": "WaveletNet: Logarithmic Scale Efficient Convolutional Neural Networks for Edge Devices",
        "authors": [
            "Li Jing",
            "Rumen Dangovski",
            "Marin Soljacic"
        ],
        "abstract": "We present a logarithmic-scale efficient convolutional neural network\narchitecture for edge devices, named WaveletNet. Our model is based on the\nwell-known depthwise convolution, and on two new layers, which we introduce in\nthis work: a wavelet convolution and a depthwise fast wavelet transform. By\nbreaking the symmetry in channel dimensions and applying a fast algorithm,\nWaveletNet shrinks the complexity of convolutional blocks by an O(logD/D)\nfactor, where D is the number of channels. Experiments on CIFAR-10 and ImageNet\nclassification show superior and comparable performances of WaveletNet compared\nto state-of-the-art models such as MobileNetV2.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1811.11644v1",
        "date": "2018-11-28 16:04:30+00:00"
    },
    {
        "title": "Nonlinear integro-differential operator regression with neural networks",
        "authors": [
            "Ravi G. Patel",
            "Olivier Desjardins"
        ],
        "abstract": "This note introduces a regression technique for finding a class of nonlinear\nintegro-differential operators from data. The method parametrizes the spatial\noperator with neural networks and Fourier transforms such that it can fit a\nclass of nonlinear operators without needing a library of a priori selected\noperators. We verify that this method can recover the spatial operators in the\nfractional heat equation and the Kuramoto-Sivashinsky equation from numerical\nsolutions of the equations.",
        "categories": [
            "cs.LG",
            "physics.comp-ph",
            "physics.data-an",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1810.08552v1",
        "date": "2018-10-19 15:33:59+00:00"
    },
    {
        "title": "Hartley Spectral Pooling for Deep Learning",
        "authors": [
            "Hao Zhang",
            "Jianwei Ma"
        ],
        "abstract": "In most convolution neural networks (CNNs), downsampling hidden layers is\nadopted for increasing computation efficiency and the receptive field size.\nSuch operation is commonly so-called pooling. Maximation and averaging over\nsliding windows (max/average pooling), and plain downsampling in the form of\nstrided convolution are popular pooling methods. Since the pooling is a lossy\nprocedure, a motivation of our work is to design a new pooling approach for\nless lossy in the dimensionality reduction. Inspired by the Fourier spectral\npooling(FSP) proposed by Rippel et. al. [1], we present the Hartley transform\nbased spectral pooling method in CNNs. Compared with FSP, the proposed spectral\npooling avoids the use of complex arithmetic for frequency representation and\nreduces the computation. Spectral pooling preserves more structure features for\nnetwork's discriminability than max and average pooling. We empirically show\nthat Hartley spectral pooling gives rise to the convergence of training CNNs on\nMNIST and CIFAR-10 datasets.",
        "categories": [
            "cs.CV",
            "cs.LG",
            "eess.SP",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1810.04028v2",
        "date": "2018-10-07 06:57:01+00:00"
    },
    {
        "title": "Hopfield Networks is All You Need",
        "authors": [
            "Hubert Ramsauer",
            "Bernhard Sch\u00e4fl",
            "Johannes Lehner",
            "Philipp Seidl",
            "Michael Widrich",
            "Thomas Adler",
            "Lukas Gruber",
            "Markus Holzleitner",
            "Milena Pavlovi\u0107",
            "Geir Kjetil Sandve",
            "Victor Greiff",
            "David Kreil",
            "Michael Kopp",
            "G\u00fcnter Klambauer",
            "Johannes Brandstetter",
            "Sepp Hochreiter"
        ],
        "abstract": "We introduce a modern Hopfield network with continuous states and a\ncorresponding update rule. The new Hopfield network can store exponentially\n(with the dimension of the associative space) many patterns, retrieves the\npattern with one update, and has exponentially small retrieval errors. It has\nthree types of energy minima (fixed points of the update): (1) global fixed\npoint averaging over all patterns, (2) metastable states averaging over a\nsubset of patterns, and (3) fixed points which store a single pattern. The new\nupdate rule is equivalent to the attention mechanism used in transformers. This\nequivalence enables a characterization of the heads of transformer models.\nThese heads perform in the first layers preferably global averaging and in\nhigher layers partial averaging via metastable states. The new modern Hopfield\nnetwork can be integrated into deep learning architectures as layers to allow\nthe storage of and access to raw input data, intermediate results, or learned\nprototypes. These Hopfield layers enable new ways of deep learning, beyond\nfully-connected, convolutional, or recurrent networks, and provide pooling,\nmemory, association, and attention mechanisms. We demonstrate the broad\napplicability of the Hopfield layers across various domains. Hopfield layers\nimproved state-of-the-art on three out of four considered multiple instance\nlearning problems as well as on immune repertoire classification with several\nhundreds of thousands of instances. On the UCI benchmark collections of small\nclassification tasks, where deep learning methods typically struggle, Hopfield\nlayers yielded a new state-of-the-art when compared to different machine\nlearning methods. Finally, Hopfield layers achieved state-of-the-art on two\ndrug design datasets. The implementation is available at:\nhttps://github.com/ml-jku/hopfield-layers",
        "categories": [
            "cs.NE",
            "cs.CL",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2008.02217v3",
        "date": "2020-07-16 17:52:37+00:00"
    },
    {
        "title": "BATT: Backdoor Attack with Transformation-based Triggers",
        "authors": [
            "Tong Xu",
            "Yiming Li",
            "Yong Jiang",
            "Shu-Tao Xia"
        ],
        "abstract": "Deep neural networks (DNNs) are vulnerable to backdoor attacks. The backdoor\nadversaries intend to maliciously control the predictions of attacked DNNs by\ninjecting hidden backdoors that can be activated by adversary-specified trigger\npatterns during the training process. One recent research revealed that most of\nthe existing attacks failed in the real physical world since the trigger\ncontained in the digitized test samples may be different from that of the one\nused for training. Accordingly, users can adopt spatial transformations as the\nimage pre-processing to deactivate hidden backdoors. In this paper, we explore\nthe previous findings from another side. We exploit classical spatial\ntransformations (i.e. rotation and translation) with the specific parameter as\ntrigger patterns to design a simple yet effective poisoning-based backdoor\nattack. For example, only images rotated to a particular angle can activate the\nembedded backdoor of attacked DNNs. Extensive experiments are conducted,\nverifying the effectiveness of our attack under both digital and physical\nsettings and its resistance to existing backdoor defenses.",
        "categories": [
            "cs.CR",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2211.01806v2",
        "date": "2022-11-02 16:03:43+00:00"
    },
    {
        "title": "GradDiv: Adversarial Robustness of Randomized Neural Networks via Gradient Diversity Regularization",
        "authors": [
            "Sungyoon Lee",
            "Hoki Kim",
            "Jaewook Lee"
        ],
        "abstract": "Deep learning is vulnerable to adversarial examples. Many defenses based on\nrandomized neural networks have been proposed to solve the problem, but fail to\nachieve robustness against attacks using proxy gradients such as the\nExpectation over Transformation (EOT) attack. We investigate the effect of the\nadversarial attacks using proxy gradients on randomized neural networks and\ndemonstrate that it highly relies on the directional distribution of the loss\ngradients of the randomized neural network. We show in particular that proxy\ngradients are less effective when the gradients are more scattered. To this\nend, we propose Gradient Diversity (GradDiv) regularizations that minimize the\nconcentration of the gradients to build a robust randomized neural network. Our\nexperiments on MNIST, CIFAR10, and STL10 show that our proposed GradDiv\nregularizations improve the adversarial robustness of randomized neural\nnetworks against a variety of state-of-the-art attack methods. Moreover, our\nmethod efficiently reduces the transferability among sample models of\nrandomized neural networks.",
        "categories": [
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2107.02425v1",
        "date": "2021-07-06 06:57:40+00:00"
    },
    {
        "title": "Graph Neural Networks With Lifting-based Adaptive Graph Wavelets",
        "authors": [
            "Mingxing Xu",
            "Wenrui Dai",
            "Chenglin Li",
            "Junni Zou",
            "Hongkai Xiong",
            "Pascal Frossard"
        ],
        "abstract": "Spectral-based graph neural networks (SGNNs) have been attracting increasing\nattention in graph representation learning. However, existing SGNNs are limited\nin implementing graph filters with rigid transforms (e.g., graph Fourier or\npredefined graph wavelet transforms) and cannot adapt to signals residing on\ngraphs and tasks at hand. In this paper, we propose a novel class of graph\nneural networks that realizes graph filters with adaptive graph wavelets.\nSpecifically, the adaptive graph wavelets are learned with neural\nnetwork-parameterized lifting structures, where structure-aware attention-based\nlifting operations (i.e., prediction and update operations) are developed to\njointly consider graph structures and node features. We propose to lift based\non diffusion wavelets to alleviate the structural information loss induced by\npartitioning non-bipartite graphs. By design, the locality and sparsity of the\nresulting wavelet transform as well as the scalability of the lifting structure\nare guaranteed. We further derive a soft-thresholding filtering operation by\nlearning sparse graph representations in terms of the learned wavelets,\nyielding a localized, efficient, and scalable wavelet-based graph filters. To\nensure that the learned graph representations are invariant to node\npermutations, a layer is employed at the input of the networks to reorder the\nnodes according to their local topology information. We evaluate the proposed\nnetworks in both node-level and graph-level representation learning tasks on\nbenchmark citation and bioinformatics graph datasets. Extensive experiments\ndemonstrate the superiority of the proposed networks over existing SGNNs in\nterms of accuracy, efficiency, and scalability.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2108.01660v3",
        "date": "2021-08-03 17:57:53+00:00"
    },
    {
        "title": "Risk-Sensitive Deep RL: Variance-Constrained Actor-Critic Provably Finds Globally Optimal Policy",
        "authors": [
            "Han Zhong",
            "Ethan X. Fang",
            "Zhuoran Yang",
            "Zhaoran Wang"
        ],
        "abstract": "While deep reinforcement learning has achieved tremendous successes in\nvarious applications, most existing works only focus on maximizing the expected\nvalue of total return and thus ignore its inherent stochasticity. Such\nstochasticity is also known as the aleatoric uncertainty and is closely related\nto the notion of risk. In this work, we make the first attempt to study\nrisk-sensitive deep reinforcement learning under the average reward setting\nwith the variance risk criteria. In particular, we focus on a\nvariance-constrained policy optimization problem where the goal is to find a\npolicy that maximizes the expected value of the long-run average reward,\nsubject to a constraint that the long-run variance of the average reward is\nupper bounded by a threshold. Utilizing Lagrangian and Fenchel dualities, we\ntransform the original problem into an unconstrained saddle-point policy\noptimization problem, and propose an actor-critic algorithm that iteratively\nand efficiently updates the policy, the Lagrange multiplier, and the Fenchel\ndual variable. When both the value and policy functions are represented by\nmulti-layer overparameterized neural networks, we prove that our actor-critic\nalgorithm generates a sequence of policies that finds a globally optimal policy\nat a sublinear rate.",
        "categories": [
            "cs.LG",
            "math.OC",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2012.14098v1",
        "date": "2020-12-28 05:02:26+00:00"
    },
    {
        "title": "Compositionality decomposed: how do neural networks generalise?",
        "authors": [
            "Dieuwke Hupkes",
            "Verna Dankers",
            "Mathijs Mul",
            "Elia Bruni"
        ],
        "abstract": "Despite a multitude of empirical studies, little consensus exists on whether\nneural networks are able to generalise compositionally, a controversy that, in\npart, stems from a lack of agreement about what it means for a neural model to\nbe compositional. As a response to this controversy, we present a set of tests\nthat provide a bridge between, on the one hand, the vast amount of linguistic\nand philosophical theory about compositionality of language and, on the other,\nthe successful neural models of language. We collect different interpretations\nof compositionality and translate them into five theoretically grounded tests\nfor models that are formulated on a task-independent level. In particular, we\nprovide tests to investigate (i) if models systematically recombine known parts\nand rules (ii) if models can extend their predictions beyond the length they\nhave seen in the training data (iii) if models' composition operations are\nlocal or global (iv) if models' predictions are robust to synonym substitutions\nand (v) if models favour rules or exceptions during training. To demonstrate\nthe usefulness of this evaluation paradigm, we instantiate these five tests on\na highly compositional data set which we dub PCFG SET and apply the resulting\ntests to three popular sequence-to-sequence models: a recurrent, a\nconvolution-based and a transformer model. We provide an in-depth analysis of\nthe results, which uncover the strengths and weaknesses of these three\narchitectures and point to potential areas of improvement.",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1908.08351v2",
        "date": "2019-08-22 13:08:26+00:00"
    },
    {
        "title": "DeepRx MIMO: Convolutional MIMO Detection with Learned Multiplicative Transformations",
        "authors": [
            "Dani Korpi",
            "Mikko Honkala",
            "Janne M. J. Huttunen",
            "Vesa Starck"
        ],
        "abstract": "Recently, deep learning has been proposed as a potential technique for\nimproving the physical layer performance of radio receivers. Despite the large\namount of encouraging results, most works have not considered spatial\nmultiplexing in the context of multiple-input and multiple-output (MIMO)\nreceivers. In this paper, we present a deep learning-based MIMO receiver\narchitecture that consists of a ResNet-based convolutional neural network, also\nknown as DeepRx, combined with a so-called transformation layer, all trained\ntogether. We propose two novel alternatives for the transformation layer: a\nmaximal ratio combining-based transformation, or a fully learned\ntransformation. The former relies more on expert knowledge, while the latter\nutilizes learned multiplicative layers. Both proposed transformation layers are\nshown to clearly outperform the conventional baseline receiver, especially with\nsparse pilot configurations. To the best of our knowledge, these are some of\nthe first results showing such high performance for a fully learned MIMO\nreceiver.",
        "categories": [
            "eess.SP",
            "cs.LG",
            "cs.NI"
        ],
        "link": "http://arxiv.org/pdf/2010.16283v1",
        "date": "2020-10-30 14:11:40+00:00"
    },
    {
        "title": "The Unstoppable Rise of Computational Linguistics in Deep Learning",
        "authors": [
            "James Henderson"
        ],
        "abstract": "In this paper, we trace the history of neural networks applied to natural\nlanguage understanding tasks, and identify key contributions which the nature\nof language has made to the development of neural network architectures. We\nfocus on the importance of variable binding and its instantiation in\nattention-based models, and argue that Transformer is not a sequence model but\nan induced-structure model. This perspective leads to predictions of the\nchallenges facing research in deep learning architectures for natural language\nunderstanding.",
        "categories": [
            "cs.CL",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2005.06420v3",
        "date": "2020-05-13 16:51:02+00:00"
    },
    {
        "title": "Neural Network Approximation of Graph Fourier Transforms for Sparse Sampling of Networked Flow Dynamics",
        "authors": [
            "Alessio Pagani",
            "Zhuangkun Wei",
            "Ricardo Silva",
            "Weisi Guo"
        ],
        "abstract": "Infrastructure monitoring is critical for safe operations and sustainability.\nWater distribution networks (WDNs) are large-scale networked critical systems\nwith complex cascade dynamics which are difficult to predict. Ubiquitous\nmonitoring is expensive and a key challenge is to infer the contaminant\ndynamics from partial sparse monitoring data. Existing approaches use\nmulti-objective optimisation to find the minimum set of essential monitoring\npoints, but lack performance guarantees and a theoretical framework.\n  Here, we first develop Graph Fourier Transform (GFT) operators to compress\nnetworked contamination spreading dynamics to identify the essential principle\ndata collection points with inference performance guarantees. We then build\nautoencoder (AE) inspired neural networks (NN) to generalize the GFT sampling\nprocess and under-sample further from the initial sampling set, allowing a very\nsmall set of data points to largely reconstruct the contamination dynamics over\nreal and artificial WDNs. Various sources of the contamination are tested and\nwe obtain high accuracy reconstruction using around 5-10% of the sample set.\nThis general approach of compression and under-sampled recovery via neural\nnetworks can be applied to a wide range of networked infrastructures to enable\ndigital twins.",
        "categories": [
            "cs.LG",
            "eess.SP",
            "physics.soc-ph",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2002.05508v1",
        "date": "2020-02-11 20:18:37+00:00"
    },
    {
        "title": "A Unified View on Graph Neural Networks as Graph Signal Denoising",
        "authors": [
            "Yao Ma",
            "Xiaorui Liu",
            "Tong Zhao",
            "Yozen Liu",
            "Jiliang Tang",
            "Neil Shah"
        ],
        "abstract": "Graph Neural Networks (GNNs) have risen to prominence in learning\nrepresentations for graph structured data. A single GNN layer typically\nconsists of a feature transformation and a feature aggregation operation. The\nformer normally uses feed-forward networks to transform features, while the\nlatter aggregates the transformed features over the graph. Numerous recent\nworks have proposed GNN models with different designs in the aggregation\noperation. In this work, we establish mathematically that the aggregation\nprocesses in a group of representative GNN models including GCN, GAT, PPNP, and\nAPPNP can be regarded as (approximately) solving a graph denoising problem with\na smoothness assumption. Such a unified view across GNNs not only provides a\nnew perspective to understand a variety of aggregation operations but also\nenables us to develop a unified graph neural network framework UGNN. To\ndemonstrate its promising potential, we instantiate a novel GNN model,\nADA-UGNN, derived from UGNN, to handle graphs with adaptive smoothness across\nnodes. Comprehensive experiments show the effectiveness of ADA-UGNN.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2010.01777v2",
        "date": "2020-10-05 04:57:18+00:00"
    },
    {
        "title": "HeunNet: Extending ResNet using Heun's Methods",
        "authors": [
            "Mehrdad Maleki",
            "Mansura Habiba",
            "Barak A. Pearlmutter"
        ],
        "abstract": "There is an analogy between the ResNet (Residual Network) architecture for\ndeep neural networks and an Euler solver for an ODE. The transformation\nperformed by each layer resembles an Euler step in solving an ODE. We consider\nthe Heun Method, which involves a single predictor-corrector cycle, and\ncomplete the analogy, building a predictor-corrector variant of ResNet, which\nwe call a HeunNet. Just as Heun's method is more accurate than Euler's,\nexperiments show that HeunNet achieves high accuracy with low computational\n(both training and test) time compared to both vanilla recurrent neural\nnetworks and other ResNet variants.",
        "categories": [
            "cs.LG",
            "cs.NE"
        ],
        "link": "http://arxiv.org/pdf/2105.06168v2",
        "date": "2021-05-13 09:55:26+00:00"
    },
    {
        "title": "A Deep Generative Model of Speech Complex Spectrograms",
        "authors": [
            "Aditya Arie Nugraha",
            "Kouhei Sekiguchi",
            "Kazuyoshi Yoshii"
        ],
        "abstract": "This paper proposes an approach to the joint modeling of the short-time\nFourier transform magnitude and phase spectrograms with a deep generative\nmodel. We assume that the magnitude follows a Gaussian distribution and the\nphase follows a von Mises distribution. To improve the consistency of the phase\nvalues in the time-frequency domain, we also apply the von Mises distribution\nto the phase derivatives, i.e., the group delay and the instantaneous\nfrequency. Based on these assumptions, we explore and compare several\ncombinations of loss functions for training our models. Built upon the\nvariational autoencoder framework, our model consists of three convolutional\nneural networks acting as an encoder, a magnitude decoder, and a phase decoder.\nIn addition to the latent variables, we propose to also condition the phase\nestimation on the estimated magnitude. Evaluated for a time-domain speech\nreconstruction task, our models could generate speech with a high perceptual\nquality and a high intelligibility.",
        "categories": [
            "cs.SD",
            "cs.LG",
            "eess.AS",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1903.03269v1",
        "date": "2019-03-08 03:57:30+00:00"
    },
    {
        "title": "Adversarially Training for Audio Classifiers",
        "authors": [
            "Raymel Alfonso Sallo",
            "Mohammad Esmaeilpour",
            "Patrick Cardinal"
        ],
        "abstract": "In this paper, we investigate the potential effect of the adversarially\ntraining on the robustness of six advanced deep neural networks against a\nvariety of targeted and non-targeted adversarial attacks. We firstly show that,\nthe ResNet-56 model trained on the 2D representation of the discrete wavelet\ntransform appended with the tonnetz chromagram outperforms other models in\nterms of recognition accuracy. Then we demonstrate the positive impact of\nadversarially training on this model as well as other deep architectures\nagainst six types of attack algorithms (white and black-box) with the cost of\nthe reduced recognition accuracy and limited adversarial perturbation. We run\nour experiments on two benchmarking environmental sound datasets and show that\nwithout any imposed limitations on the budget allocations for the adversary,\nthe fooling rate of the adversarially trained models can exceed 90\\%. In other\nwords, adversarial attacks exist in any scales, but they might require higher\nadversarial perturbations compared to non-adversarially trained models.",
        "categories": [
            "eess.AS",
            "cs.LG",
            "cs.SD",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2008.11618v2",
        "date": "2020-08-26 15:15:32+00:00"
    },
    {
        "title": "Adversarial Sample Detection for Deep Neural Network through Model Mutation Testing",
        "authors": [
            "Jingyi Wang",
            "Guoliang Dong",
            "Jun Sun",
            "Xinyu Wang",
            "Peixin Zhang"
        ],
        "abstract": "Deep neural networks (DNN) have been shown to be useful in a wide range of\napplications. However, they are also known to be vulnerable to adversarial\nsamples. By transforming a normal sample with some carefully crafted human\nimperceptible perturbations, even highly accurate DNN make wrong decisions.\nMultiple defense mechanisms have been proposed which aim to hinder the\ngeneration of such adversarial samples. However, a recent work show that most\nof them are ineffective. In this work, we propose an alternative approach to\ndetect adversarial samples at runtime. Our main observation is that adversarial\nsamples are much more sensitive than normal samples if we impose random\nmutations on the DNN. We thus first propose a measure of `sensitivity' and show\nempirically that normal samples and adversarial samples have distinguishable\nsensitivity. We then integrate statistical hypothesis testing and model\nmutation testing to check whether an input sample is likely to be normal or\nadversarial at runtime by measuring its sensitivity. We evaluated our approach\non the MNIST and CIFAR10 datasets. The results show that our approach detects\nadversarial samples generated by state-of-the-art attacking methods efficiently\nand accurately.",
        "categories": [
            "cs.LG",
            "cs.SE",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1812.05793v2",
        "date": "2018-12-14 06:04:04+00:00"
    },
    {
        "title": "End-to-End Multi-Task Denoising for joint SDR and PESQ Optimization",
        "authors": [
            "Jaeyoung Kim",
            "Mostafa El-Khamy",
            "Jungwon Lee"
        ],
        "abstract": "Supervised learning based on a deep neural network recently has achieved\nsubstantial improvement on speech enhancement. Denoising networks learn mapping\nfrom noisy speech to clean one directly, or to a spectrum mask which is the\nratio between clean and noisy spectra. In either case, the network is optimized\nby minimizing mean square error (MSE) between ground-truth labels and\ntime-domain or spectrum output. However, existing schemes have either of two\ncritical issues: spectrum and metric mismatches. The spectrum mismatch is a\nwell known issue that any spectrum modification after short-time Fourier\ntransform (STFT), in general, cannot be fully recovered after inverse\nshort-time Fourier transform (ISTFT). The metric mismatch is that a\nconventional MSE metric is sub-optimal to maximize our target metrics,\nsignal-to-distortion ratio (SDR) and perceptual evaluation of speech quality\n(PESQ). This paper presents a new end-to-end denoising framework with the goal\nof joint SDR and PESQ optimization. First, the network optimization is\nperformed on the time-domain signals after ISTFT to avoid spectrum mismatch.\nSecond, two loss functions which have improved correlations with SDR and PESQ\nmetrics are proposed to minimize metric mismatch. The experimental result\nshowed that the proposed denoising scheme significantly improved both SDR and\nPESQ performance over the existing methods.",
        "categories": [
            "cs.SD",
            "cs.LG",
            "eess.AS",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1901.09146v4",
        "date": "2019-01-26 02:48:08+00:00"
    },
    {
        "title": "Deep convolutional encoder-decoder networks for uncertainty quantification of dynamic multiphase flow in heterogeneous media",
        "authors": [
            "Shaoxing Mo",
            "Yinhao Zhu",
            "Nicholas Zabaras",
            "Xiaoqing Shi",
            "Jichun Wu"
        ],
        "abstract": "Surrogate strategies are used widely for uncertainty quantification of\ngroundwater models in order to improve computational efficiency. However, their\napplication to dynamic multiphase flow problems is hindered by the curse of\ndimensionality, the saturation discontinuity due to capillarity effects, and\nthe time-dependence of the multi-output responses. In this paper, we propose a\ndeep convolutional encoder-decoder neural network methodology to tackle these\nissues. The surrogate modeling task is transformed to an image-to-image\nregression strategy. This approach extracts high-level coarse features from the\nhigh-dimensional input permeability images using an encoder, and then refines\nthe coarse features to provide the output pressure/saturation images through a\ndecoder. A training strategy combining a regression loss and a segmentation\nloss is proposed in order to better approximate the discontinuous saturation\nfield. To characterize the high-dimensional time-dependent outputs of the\ndynamic system, time is treated as an additional input to the network that is\ntrained using pairs of input realizations and of the corresponding system\noutputs at a limited number of time instances. The proposed method is evaluated\nusing a geological carbon storage process-based multiphase flow model with a\n2500-dimensional stochastic permeability field. With a relatively small number\nof training data, the surrogate model is capable of accurately characterizing\nthe spatio-temporal evolution of the pressure and discontinuous CO2 saturation\nfields and can be used efficiently to compute the statistics of the system\nresponses.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1807.00882v1",
        "date": "2018-07-02 20:41:03+00:00"
    },
    {
        "title": "Transforming Musical Signals through a Genre Classifying Convolutional Neural Network",
        "authors": [
            "S. Geng",
            "G. Ren",
            "M. Ogihara"
        ],
        "abstract": "Convolutional neural networks (CNNs) have been successfully applied on both\ndiscriminative and generative modeling for music-related tasks. For a\nparticular task, the trained CNN contains information representing the decision\nmaking or the abstracting process. One can hope to manipulate existing music\nbased on this 'informed' network and create music with new features\ncorresponding to the knowledge obtained by the network. In this paper, we\npropose a method to utilize the stored information from a CNN trained on\nmusical genre classification task. The network was composed of three\nconvolutional layers, and was trained to classify five-second song clips into\nfive different genres. After training, randomly selected clips were modified by\nmaximizing the sum of outputs from the network layers. In addition to the\npotential of such CNNs to produce interesting audio transformation, more\ninformation about the network and the original music could be obtained from the\nanalysis of the generated features since these features indicate how the\nnetwork 'understands' the music.",
        "categories": [
            "cs.SD",
            "cs.LG",
            "cs.MM",
            "cs.NE",
            "68Txx",
            "C.1.3; H.5.1"
        ],
        "link": "http://arxiv.org/pdf/1706.09553v1",
        "date": "2017-06-29 02:39:00+00:00"
    },
    {
        "title": "Benchmarking Deep Learning Interpretability in Time Series Predictions",
        "authors": [
            "Aya Abdelsalam Ismail",
            "Mohamed Gunady",
            "H\u00e9ctor Corrada Bravo",
            "Soheil Feizi"
        ],
        "abstract": "Saliency methods are used extensively to highlight the importance of input\nfeatures in model predictions. These methods are mostly used in vision and\nlanguage tasks, and their applications to time series data is relatively\nunexplored. In this paper, we set out to extensively compare the performance of\nvarious saliency-based interpretability methods across diverse neural\narchitectures, including Recurrent Neural Network, Temporal Convolutional\nNetworks, and Transformers in a new benchmark of synthetic time series data. We\npropose and report multiple metrics to empirically evaluate the performance of\nsaliency methods for detecting feature importance over time using both\nprecision (i.e., whether identified features contain meaningful signals) and\nrecall (i.e., the number of features with signal identified as important).\nThrough several experiments, we show that (i) in general, network architectures\nand saliency methods fail to reliably and accurately identify feature\nimportance over time in time series data, (ii) this failure is mainly due to\nthe conflation of time and feature domains, and (iii) the quality of saliency\nmaps can be improved substantially by using our proposed two-step temporal\nsaliency rescaling (TSR) approach that first calculates the importance of each\ntime step before calculating the importance of each feature at a time step.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2010.13924v1",
        "date": "2020-10-26 22:07:53+00:00"
    },
    {
        "title": "G2SAT: Learning to Generate SAT Formulas",
        "authors": [
            "Jiaxuan You",
            "Haoze Wu",
            "Clark Barrett",
            "Raghuram Ramanujan",
            "Jure Leskovec"
        ],
        "abstract": "The Boolean Satisfiability (SAT) problem is the canonical NP-complete problem\nand is fundamental to computer science, with a wide array of applications in\nplanning, verification, and theorem proving. Developing and evaluating\npractical SAT solvers relies on extensive empirical testing on a set of\nreal-world benchmark formulas. However, the availability of such real-world SAT\nformulas is limited. While these benchmark formulas can be augmented with\nsynthetically generated ones, existing approaches for doing so are heavily\nhand-crafted and fail to simultaneously capture a wide range of characteristics\nexhibited by real-world SAT instances. In this work, we present G2SAT, the\nfirst deep generative framework that learns to generate SAT formulas from a\ngiven set of input formulas. Our key insight is that SAT formulas can be\ntransformed into latent bipartite graph representations which we model using a\nspecialized deep generative neural network. We show that G2SAT can generate SAT\nformulas that closely resemble given real-world SAT instances, as measured by\nboth graph metrics and SAT solver behavior. Further, we show that our synthetic\nSAT formulas could be used to improve SAT solver performance on real-world\nbenchmarks, which opens up new opportunities for the continued development of\nSAT solvers and a deeper understanding of their performance.",
        "categories": [
            "cs.LG",
            "cs.LO",
            "cs.SI",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1910.13445v1",
        "date": "2019-10-29 07:48:50+00:00"
    },
    {
        "title": "Finding Differences Between Transformers and ConvNets Using Counterfactual Simulation Testing",
        "authors": [
            "Nataniel Ruiz",
            "Sarah Adel Bargal",
            "Cihang Xie",
            "Kate Saenko",
            "Stan Sclaroff"
        ],
        "abstract": "Modern deep neural networks tend to be evaluated on static test sets. One\nshortcoming of this is the fact that these deep neural networks cannot be\neasily evaluated for robustness issues with respect to specific scene\nvariations. For example, it is hard to study the robustness of these networks\nto variations of object scale, object pose, scene lighting and 3D occlusions.\nThe main reason is that collecting real datasets with fine-grained naturalistic\nvariations of sufficient scale can be extremely time-consuming and expensive.\nIn this work, we present Counterfactual Simulation Testing, a counterfactual\nframework that allows us to study the robustness of neural networks with\nrespect to some of these naturalistic variations by building realistic\nsynthetic scenes that allow us to ask counterfactual questions to the models,\nultimately providing answers to questions such as \"Would your classification\nstill be correct if the object were viewed from the top?\" or \"Would your\nclassification still be correct if the object were partially occluded by\nanother object?\". Our method allows for a fair comparison of the robustness of\nrecently released, state-of-the-art Convolutional Neural Networks and Vision\nTransformers, with respect to these naturalistic variations. We find evidence\nthat ConvNext is more robust to pose and scale variations than Swin, that\nConvNext generalizes better to our simulated domain and that Swin handles\npartial occlusion better than ConvNext. We also find that robustness for all\nnetworks improves with network scale and with data scale and variety. We\nrelease the Naturalistic Variation Object Dataset (NVD), a large simulated\ndataset of 272k images of everyday objects with naturalistic variations such as\nobject pose, scale, viewpoint, lighting and occlusions. Project page:\nhttps://counterfactualsimulation.github.io",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2211.16499v1",
        "date": "2022-11-29 18:59:23+00:00"
    },
    {
        "title": "Out-of-distribution Detection and Generation using Soft Brownian Offset Sampling and Autoencoders",
        "authors": [
            "Felix M\u00f6ller",
            "Diego Botache",
            "Denis Huseljic",
            "Florian Heidecker",
            "Maarten Bieshaar",
            "Bernhard Sick"
        ],
        "abstract": "Deep neural networks often suffer from overconfidence which can be partly\nremedied by improved out-of-distribution detection. For this purpose, we\npropose a novel approach that allows for the generation of out-of-distribution\ndatasets based on a given in-distribution dataset. This new dataset can then be\nused to improve out-of-distribution detection for the given dataset and machine\nlearning task at hand. The samples in this dataset are with respect to the\nfeature space close to the in-distribution dataset and therefore realistic and\nplausible. Hence, this dataset can also be used to safeguard neural networks,\ni.e., to validate the generalization performance. Our approach first generates\nsuitable representations of an in-distribution dataset using an autoencoder and\nthen transforms them using our novel proposed Soft Brownian Offset method.\nAfter transformation, the decoder part of the autoencoder allows for the\ngeneration of these implicit out-of-distribution samples. This newly generated\ndataset then allows for mixing with other datasets and thus improved training\nof an out-of-distribution classifier, increasing its performance.\nExperimentally, we show that our approach is promising for time series using\nsynthetic data. Using our new method, we also show in a quantitative case study\nthat we can improve the out-of-distribution detection for the MNIST dataset.\nFinally, we provide another case study on the synthetic generation of\nout-of-distribution trajectories, which can be used to validate trajectory\nprediction algorithms for automated driving.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2105.02965v1",
        "date": "2021-05-04 06:59:24+00:00"
    },
    {
        "title": "Mesh-TensorFlow: Deep Learning for Supercomputers",
        "authors": [
            "Noam Shazeer",
            "Youlong Cheng",
            "Niki Parmar",
            "Dustin Tran",
            "Ashish Vaswani",
            "Penporn Koanantakool",
            "Peter Hawkins",
            "HyoukJoong Lee",
            "Mingsheng Hong",
            "Cliff Young",
            "Ryan Sepassi",
            "Blake Hechtman"
        ],
        "abstract": "Batch-splitting (data-parallelism) is the dominant distributed Deep Neural\nNetwork (DNN) training strategy, due to its universal applicability and its\namenability to Single-Program-Multiple-Data (SPMD) programming. However,\nbatch-splitting suffers from problems including the inability to train very\nlarge models (due to memory constraints), high latency, and inefficiency at\nsmall batch sizes. All of these can be solved by more general distribution\nstrategies (model-parallelism). Unfortunately, efficient model-parallel\nalgorithms tend to be complicated to discover, describe, and to implement,\nparticularly on large clusters. We introduce Mesh-TensorFlow, a language for\nspecifying a general class of distributed tensor computations. Where\ndata-parallelism can be viewed as splitting tensors and operations along the\n\"batch\" dimension, in Mesh-TensorFlow, the user can specify any\ntensor-dimensions to be split across any dimensions of a multi-dimensional mesh\nof processors. A Mesh-TensorFlow graph compiles into a SPMD program consisting\nof parallel operations coupled with collective communication primitives such as\nAllreduce. We use Mesh-TensorFlow to implement an efficient data-parallel,\nmodel-parallel version of the Transformer sequence-to-sequence model. Using TPU\nmeshes of up to 512 cores, we train Transformer models with up to 5 billion\nparameters, surpassing state of the art results on WMT'14 English-to-French\ntranslation task and the one-billion-word language modeling benchmark.\nMesh-Tensorflow is available at https://github.com/tensorflow/mesh .",
        "categories": [
            "cs.LG",
            "cs.DC",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1811.02084v1",
        "date": "2018-11-05 23:25:02+00:00"
    },
    {
        "title": "Robustmix: Improving Robustness by Regularizing the Frequency Bias of Deep Nets",
        "authors": [
            "Jonas Ngnawe",
            "Marianne ABEMGNIGNI NJIFON",
            "Jonathan Heek",
            "Yann Dauphin"
        ],
        "abstract": "Deep networks have achieved impressive results on a range of well-curated\nbenchmark datasets. Surprisingly, their performance remains sensitive to\nperturbations that have little effect on human performance. In this work, we\npropose a novel extension of Mixup called Robustmix that regularizes networks\nto classify based on lower-frequency spatial features. We show that this type\nof regularization improves robustness on a range of benchmarks such as\nImagenet-C and Stylized Imagenet. It adds little computational overhead and,\nfurthermore, does not require a priori knowledge of a large set of image\ntransformations. We find that this approach further complements recent advances\nin model architecture and data augmentation, attaining a state-of-the-art mCE\nof 44.8 with an EfficientNet-B8 model and RandAugment, which is a reduction of\n16 mCE compared to the baseline.",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2304.02847v1",
        "date": "2023-04-06 03:24:00+00:00"
    },
    {
        "title": "Lyapunov-based uncertainty-aware safe reinforcement learning",
        "authors": [
            "Ashkan B. Jeddi",
            "Nariman L. Dehghani",
            "Abdollah Shafieezadeh"
        ],
        "abstract": "Reinforcement learning (RL) has shown a promising performance in learning\noptimal policies for a variety of sequential decision-making tasks. However, in\nmany real-world RL problems, besides optimizing the main objectives, the agent\nis expected to satisfy a certain level of safety (e.g., avoiding collisions in\nautonomous driving). While RL problems are commonly formalized as Markov\ndecision processes (MDPs), safety constraints are incorporated via constrained\nMarkov decision processes (CMDPs). Although recent advances in safe RL have\nenabled learning safe policies in CMDPs, these safety requirements should be\nsatisfied during both training and in the deployment process. Furthermore, it\nis shown that in memory-based and partially observable environments, these\nmethods fail to maintain safety over unseen out-of-distribution observations.\nTo address these limitations, we propose a Lyapunov-based uncertainty-aware\nsafe RL model. The introduced model adopts a Lyapunov function that converts\ntrajectory-based constraints to a set of local linear constraints. Furthermore,\nto ensure the safety of the agent in highly uncertain environments, an\nuncertainty quantification method is developed that enables identifying\nrisk-averse actions through estimating the probability of constraint\nviolations. Moreover, a Transformers model is integrated to provide the agent\nwith memory to process long time horizons of information via the self-attention\nmechanism. The proposed model is evaluated in grid-world navigation tasks where\nsafety is defined as avoiding static and dynamic obstacles in fully and\npartially observable environments. The results of these experiments show a\nsignificant improvement in the performance of the agent both in achieving\noptimality and satisfying safety constraints.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2107.13944v1",
        "date": "2021-07-29 13:08:15+00:00"
    },
    {
        "title": "Generative Adversarial Perturbations",
        "authors": [
            "Omid Poursaeed",
            "Isay Katsman",
            "Bicheng Gao",
            "Serge Belongie"
        ],
        "abstract": "In this paper, we propose novel generative models for creating adversarial\nexamples, slightly perturbed images resembling natural images but maliciously\ncrafted to fool pre-trained models. We present trainable deep neural networks\nfor transforming images to adversarial perturbations. Our proposed models can\nproduce image-agnostic and image-dependent perturbations for both targeted and\nnon-targeted attacks. We also demonstrate that similar architectures can\nachieve impressive results in fooling classification and semantic segmentation\nmodels, obviating the need for hand-crafting attack methods for each task.\nUsing extensive experiments on challenging high-resolution datasets such as\nImageNet and Cityscapes, we show that our perturbations achieve high fooling\nrates with small perturbation norms. Moreover, our attacks are considerably\nfaster than current iterative methods at inference time.",
        "categories": [
            "cs.CV",
            "cs.CR",
            "cs.LG",
            "cs.NE",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1712.02328v3",
        "date": "2017-12-06 18:52:12+00:00"
    },
    {
        "title": "Neural Network Renormalization Group",
        "authors": [
            "Shuo-Hui Li",
            "Lei Wang"
        ],
        "abstract": "We present a variational renormalization group (RG) approach using a deep\ngenerative model based on normalizing flows. The model performs hierarchical\nchange-of-variables transformations from the physical space to a latent space\nwith reduced mutual information. Conversely, the neural net directly maps\nindependent Gaussian noises to physical configurations following the inverse RG\nflow. The model has an exact and tractable likelihood, which allows unbiased\ntraining and direct access to the renormalized energy function of the latent\nvariables. To train the model, we employ probability density distillation for\nthe bare energy function of the physical problem, in which the training loss\nprovides a variational upper bound of the physical free energy. We demonstrate\npractical usage of the approach by identifying mutually independent collective\nvariables of the Ising model and performing accelerated hybrid Monte Carlo\nsampling in the latent space. Lastly, we comment on the connection of the\npresent approach to the wavelet formulation of RG and the modern pursuit of\ninformation preserving RG.",
        "categories": [
            "cond-mat.stat-mech",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1802.02840v4",
        "date": "2018-02-08 13:16:01+00:00"
    },
    {
        "title": "FNetAR: Mixing Tokens with Autoregressive Fourier Transforms",
        "authors": [
            "Tim Lou",
            "Michael Park",
            "Mohammad Ramezanali",
            "Vincent Tang"
        ],
        "abstract": "In this note we examine the autoregressive generalization of the FNet\nalgorithm, in which self-attention layers from the standard Transformer\narchitecture are substituted with a trivial sparse-uniformsampling procedure\nbased on Fourier transforms. Using the Wikitext-103 benchmark, we\ndemonstratethat FNetAR retains state-of-the-art performance (25.8 ppl) on the\ntask of causal language modelingcompared to a Transformer-XL baseline (24.2\nppl) with only half the number self-attention layers,thus providing further\nevidence for the superfluity of deep neural networks with heavily\ncompoundedattention mechanisms. The autoregressive Fourier transform could\nlikely be used for parameterreduction on most Transformer-based time-series\nprediction models.",
        "categories": [
            "cs.CL",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2107.10932v1",
        "date": "2021-07-22 21:24:02+00:00"
    },
    {
        "title": "Explain and Conquer: Personalised Text-based Reviews to Achieve Transparency",
        "authors": [
            "I\u00f1igo L\u00f3pez-Riob\u00f3o Botana",
            "Ver\u00f3nica Bol\u00f3n-Canedo",
            "Bertha Guijarro-Berdi\u00f1as",
            "Amparo Alonso-Betanzos"
        ],
        "abstract": "There are many contexts in which dyadic data are present. Social networks are\na well-known example. In these contexts, pairs of elements are linked building\na network that reflects interactions. Explaining why these relationships are\nestablished is essential to obtain transparency, an increasingly important\nnotion. These explanations are often presented using text, thanks to the spread\nof the natural language understanding tasks. Our aim is to represent and\nexplain pairs established by any agent (e.g., a recommender system or a paid\npromotion mechanism), so that text-based personalisation is taken into account.\nWe have focused on the TripAdvisor platform, considering the applicability to\nother dyadic data contexts. The items are a subset of users and restaurants and\nthe interactions the reviews posted by these users. We propose the PTER\n(Personalised TExt-based Reviews) model. We predict, from the available reviews\nfor a given restaurant, those that fit to the specific user interactions. PTER\nleverages the BERT (Bidirectional Encoders Representations from Transformers)\ntransformer-encoder model. We customised a deep neural network following the\nfeature-based approach, presenting a LTR (Learning To Rank) downstream task. We\ncarried out several comparisons of our proposal with a random baseline and\nother models of the state of the art, following the EXTRA (EXplanaTion RAnking)\nbenchmark. Our method outperforms other collaborative filtering proposals.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.IR",
            "cs.SI",
            "I.2.7; I.5.1; I.5.2; I.5.3; I.5.4"
        ],
        "link": "http://arxiv.org/pdf/2205.01759v2",
        "date": "2022-05-03 20:04:32+00:00"
    },
    {
        "title": "BCD-Net for Low-dose CT Reconstruction: Acceleration, Convergence, and Generalization",
        "authors": [
            "Il Yong Chun",
            "Xuehang Zheng",
            "Yong Long",
            "Jeffrey A. Fessler"
        ],
        "abstract": "Obtaining accurate and reliable images from low-dose computed tomography (CT)\nis challenging. Regression convolutional neural network (CNN) models that are\nlearned from training data are increasingly gaining attention in low-dose CT\nreconstruction. This paper modifies the architecture of an iterative regression\nCNN, BCD-Net, for fast, stable, and accurate low-dose CT reconstruction, and\npresents the convergence property of the modified BCD-Net. Numerical results\nwith phantom data show that applying faster numerical solvers to model-based\nimage reconstruction (MBIR) modules of BCD-Net leads to faster and more\naccurate BCD-Net; BCD-Net significantly improves the reconstruction accuracy,\ncompared to the state-of-the-art MBIR method using learned transforms; BCD-Net\nachieves better image quality, compared to a state-of-the-art iterative NN\narchitecture, ADMM-Net. Numerical results with clinical data show that BCD-Net\ngeneralizes significantly better than a state-of-the-art deep (non-iterative)\nregression NN, FBPConvNet, that lacks MBIR modules.",
        "categories": [
            "eess.IV",
            "cs.CV",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1908.01287v1",
        "date": "2019-08-04 07:10:24+00:00"
    },
    {
        "title": "Operationalizing Convolutional Neural Network Architectures for Prohibited Object Detection in X-Ray Imagery",
        "authors": [
            "Thomas W. Webb",
            "Neelanjan Bhowmik",
            "Yona Falinie A. Gaus",
            "Toby P. Breckon"
        ],
        "abstract": "The recent advancement in deep Convolutional Neural Network (CNN) has brought\ninsight into the automation of X-ray security screening for aviation security\nand beyond. Here, we explore the viability of two recent end-to-end object\ndetection CNN architectures, Cascade R-CNN and FreeAnchor, for prohibited item\ndetection by balancing processing time and the impact of image data compression\nfrom an operational viewpoint. Overall, we achieve maximal detection\nperformance using a FreeAnchor architecture with a ResNet50 backbone, obtaining\nmean Average Precision (mAP) of 87.7 and 85.8 for using the OPIXray and SIXray\nbenchmark datasets, showing superior performance over prior work on both. With\nfewer parameters and less training time, FreeAnchor achieves the highest\ndetection inference speed of ~13 fps (3.9 ms per image). Furthermore, we\nevaluate the impact of lossy image compression upon detector performance. The\nCNN models display substantial resilience to the lossy compression, resulting\nin only a 1.1% decrease in mAP at the JPEG compression level of 50.\nAdditionally, a thorough evaluation of data augmentation techniques is\nprovided, including adaptions of MixUp and CutMix strategy as well as other\nstandard transformations, further improving the detection accuracy.",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2110.04906v1",
        "date": "2021-10-10 21:20:04+00:00"
    },
    {
        "title": "Temporal-Clustering Invariance in Irregular Healthcare Time Series",
        "authors": [
            "Mohammad Taha Bahadori",
            "Zachary Chase Lipton"
        ],
        "abstract": "Electronic records contain sequences of events, some of which take place all\nat once in a single visit, and others that are dispersed over multiple visits,\neach with a different timestamp. We postulate that fine temporal detail, e.g.,\nwhether a series of blood tests are completed at once or in rapid succession\nshould not alter predictions based on this data. Motivated by this intuition,\nwe propose models for analyzing sequences of multivariate clinical time series\ndata that are invariant to this temporal clustering. We propose an efficient\ndata augmentation technique that exploits the postulated temporal-clustering\ninvariance to regularize deep neural networks optimized for several clinical\nprediction tasks. We introduce two techniques to temporally coarsen\n(downsample) irregular time series: (i) grouping the data points based on\nregularly-spaced timestamps; and (ii) clustering them, yielding\nirregularly-paced timestamps. Moreover, we propose a MultiResolution Ensemble\n(MRE) model, improving predictive accuracy by ensembling predictions based on\ninputs sequences transformed by different coarsening operators. Our experiments\nshow that MRE improves the mAP on the benchmark mortality prediction task from\n51.53% to 53.92%.",
        "categories": [
            "cs.LG",
            "q-bio.QM",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1904.12206v1",
        "date": "2019-04-27 20:30:26+00:00"
    },
    {
        "title": "Consistency Regularization for Certified Robustness of Smoothed Classifiers",
        "authors": [
            "Jongheon Jeong",
            "Jinwoo Shin"
        ],
        "abstract": "A recent technique of randomized smoothing has shown that the worst-case\n(adversarial) $\\ell_2$-robustness can be transformed into the average-case\nGaussian-robustness by \"smoothing\" a classifier, i.e., by considering the\naveraged prediction over Gaussian noise. In this paradigm, one should rethink\nthe notion of adversarial robustness in terms of generalization ability of a\nclassifier under noisy observations. We found that the trade-off between\naccuracy and certified robustness of smoothed classifiers can be greatly\ncontrolled by simply regularizing the prediction consistency over noise. This\nrelationship allows us to design a robust training objective without\napproximating a non-existing smoothed classifier, e.g., via soft smoothing. Our\nexperiments under various deep neural network architectures and datasets show\nthat the \"certified\" $\\ell_2$-robustness can be dramatically improved with the\nproposed regularization, even achieving better or comparable results to the\nstate-of-the-art approaches with significantly less training costs and\nhyperparameters.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2006.04062v4",
        "date": "2020-06-07 06:57:43+00:00"
    },
    {
        "title": "A Factored Generalized Additive Model for Clinical Decision Support in the Operating Room",
        "authors": [
            "Zhicheng Cui",
            "Bradley A Fritz",
            "Christopher R King",
            "Michael S Avidan",
            "Yixin Chen"
        ],
        "abstract": "Logistic regression (LR) is widely used in clinical prediction because it is\nsimple to deploy and easy to interpret. Nevertheless, being a linear model, LR\nhas limited expressive capability and often has unsatisfactory performance.\nGeneralized additive models (GAMs) extend the linear model with transformations\nof input features, though feature interaction is not allowed for all GAM\nvariants. In this paper, we propose a factored generalized additive model\n(F-GAM) to preserve the model interpretability for targeted features while\nallowing a rich model for interaction with features fixed within the\nindividual. We evaluate F-GAM on prediction of two targets, postoperative acute\nkidney injury and acute respiratory failure, from a single-center database. We\nfind superior model performance of F-GAM in terms of AUPRC and AUROC compared\nto several other GAM implementations, random forests, support vector machine,\nand a deep neural network. We find that the model interpretability is good with\nresults with high face validity.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1907.12596v1",
        "date": "2019-07-29 18:39:50+00:00"
    },
    {
        "title": "Dimension Estimation Using Autoencoders",
        "authors": [
            "Nitish Bahadur",
            "Randy Paffenroth"
        ],
        "abstract": "Dimension Estimation (DE) and Dimension Reduction (DR) are two closely\nrelated topics, but with quite different goals. In DE, one attempts to estimate\nthe intrinsic dimensionality or number of latent variables in a set of\nmeasurements of a random vector. However, in DR, one attempts to project a\nrandom vector, either linearly or non-linearly, to a lower dimensional space\nthat preserves the information contained in the original higher dimensional\nspace. Of course, these two ideas are quite closely linked since, for example,\ndoing DR to a dimension smaller than suggested by DE will likely lead to\ninformation loss. Accordingly, in this paper we will focus on a particular\nclass of deep neural networks called autoencoders which are used extensively\nfor DR but are less well studied for DE. We show that several important\nquestions arise when using autoencoders for DE, above and beyond those that\narise for more classic DR/DE techniques such as Principal Component Analysis.\nWe address autoencoder architectural choices and regularization techniques that\nallow one to transform autoencoder latent layer representations into estimates\nof intrinsic dimension.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1909.10702v1",
        "date": "2019-09-24 04:09:48+00:00"
    },
    {
        "title": "Generalized Gumbel-Softmax Gradient Estimator for Generic Discrete Random Variables",
        "authors": [
            "Weonyoung Joo",
            "Dongjun Kim",
            "Seungjae Shin",
            "Il-Chul Moon"
        ],
        "abstract": "Estimating the gradients of stochastic nodes in stochastic computational\ngraphs is one of the crucial research questions in the deep generative modeling\ncommunity, which enables the gradient descent optimization on neural network\nparameters. Stochastic gradient estimators of discrete random variables are\nwidely explored, for example, Gumbel-Softmax reparameterization trick for\nBernoulli and categorical distributions. Meanwhile, other discrete distribution\ncases such as the Poisson, geometric, binomial, multinomial, negative binomial,\netc. have not been explored. This paper proposes a generalized version of the\nGumbel-Softmax estimator, which is able to reparameterize generic discrete\ndistributions, not restricted to the Bernoulli and the categorical. The\nproposed estimator utilizes the truncation of discrete random variables, the\nGumbel-Softmax trick, and a special form of linear transformation. Our\nexperiments consist of (1) synthetic examples and applications on VAE, which\nshow the efficacy of our methods; and (2) topic models, which demonstrate the\nvalue of the proposed estimation in practice.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2003.01847v4",
        "date": "2020-03-04 01:13:15+00:00"
    },
    {
        "title": "Attending to Graph Transformers",
        "authors": [
            "Luis M\u00fcller",
            "Mikhail Galkin",
            "Christopher Morris",
            "Ladislav Ramp\u00e1\u0161ek"
        ],
        "abstract": "Recently, transformer architectures for graphs emerged as an alternative to\nestablished techniques for machine learning with graphs, such as graph neural\nnetworks. So far, they have shown promising empirical results, e.g., on\nmolecular prediction datasets, often attributed to their ability to circumvent\ngraph neural networks' shortcomings, such as over-smoothing and over-squashing.\nHere, we derive a taxonomy of graph transformer architectures, bringing some\norder to this emerging field. We overview their theoretical properties, survey\nstructural and positional encodings, and discuss extensions for important graph\nclasses, e.g., 3D molecular graphs. Empirically, we probe how well graph\ntransformers can recover various graph properties, how well they can deal with\nheterophilic graphs, and to what extent they prevent over-squashing. Further,\nwe outline open challenges and research direction to stimulate future work. Our\ncode is available at\nhttps://github.com/luis-mueller/probing-graph-transformers.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.NE"
        ],
        "link": "http://arxiv.org/pdf/2302.04181v1",
        "date": "2023-02-08 16:40:11+00:00"
    },
    {
        "title": "Efficiently Training Vision Transformers on Structural MRI Scans for Alzheimer's Disease Detection",
        "authors": [
            "Nikhil J. Dhinagar",
            "Sophia I. Thomopoulos",
            "Emily Laltoo",
            "Paul M. Thompson"
        ],
        "abstract": "Neuroimaging of large populations is valuable to identify factors that\npromote or resist brain disease, and to assist diagnosis, subtyping, and\nprognosis. Data-driven models such as convolutional neural networks (CNNs) have\nincreasingly been applied to brain images to perform diagnostic and prognostic\ntasks by learning robust features. Vision transformers (ViT) - a new class of\ndeep learning architectures - have emerged in recent years as an alternative to\nCNNs for several computer vision applications. Here we tested variants of the\nViT architecture for a range of desired neuroimaging downstream tasks based on\ndifficulty, in this case for sex and Alzheimer's disease (AD) classification\nbased on 3D brain MRI. In our experiments, two vision transformer architecture\nvariants achieved an AUC of 0.987 for sex and 0.892 for AD classification,\nrespectively. We independently evaluated our models on data from two benchmark\nAD datasets. We achieved a performance boost of 5% and 9-10% upon fine-tuning\nvision transformer models pre-trained on synthetic (generated by a latent\ndiffusion model) and real MRI scans, respectively. Our main contributions\ninclude testing the effects of different ViT training strategies including\npre-training, data augmentation and learning rate warm-ups followed by\nannealing, as pertaining to the neuroimaging domain. These techniques are\nessential for training ViT-like models for neuroimaging applications where\ntraining data is usually limited. We also analyzed the effect of the amount of\ntraining data utilized on the test-time performance of the ViT via data-model\nscaling curves.",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV",
            "cs.LG",
            "q-bio.QM"
        ],
        "link": "http://arxiv.org/pdf/2303.08216v1",
        "date": "2023-03-14 20:18:12+00:00"
    },
    {
        "title": "EvoGrad: Efficient Gradient-Based Meta-Learning and Hyperparameter Optimization",
        "authors": [
            "Ondrej Bohdal",
            "Yongxin Yang",
            "Timothy Hospedales"
        ],
        "abstract": "Gradient-based meta-learning and hyperparameter optimization have seen\nsignificant progress recently, enabling practical end-to-end training of neural\nnetworks together with many hyperparameters. Nevertheless, existing approaches\nare relatively expensive as they need to compute second-order derivatives and\nstore a longer computational graph. This cost prevents scaling them to larger\nnetwork architectures. We present EvoGrad, a new approach to meta-learning that\ndraws upon evolutionary techniques to more efficiently compute hypergradients.\nEvoGrad estimates hypergradient with respect to hyperparameters without\ncalculating second-order gradients, or storing a longer computational graph,\nleading to significant improvements in efficiency. We evaluate EvoGrad on three\nsubstantial recent meta-learning applications, namely cross-domain few-shot\nlearning with feature-wise transformations, noisy label learning with\nMeta-Weight-Net and low-resource cross-lingual learning with meta\nrepresentation transformation. The results show that EvoGrad significantly\nimproves efficiency and enables scaling meta-learning to bigger architectures\nsuch as from ResNet10 to ResNet34.",
        "categories": [
            "cs.LG",
            "cs.NE",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2106.10575v2",
        "date": "2021-06-19 21:51:39+00:00"
    },
    {
        "title": "A Simple yet Effective Method for Graph Classification",
        "authors": [
            "Junran Wu",
            "Shangzhe Li",
            "Jianhao Li",
            "Yicheng Pan",
            "Ke Xu"
        ],
        "abstract": "In deep neural networks, better results can often be obtained by increasing\nthe complexity of previously developed basic models. However, it is unclear\nwhether there is a way to boost performance by decreasing the complexity of\nsuch models. Intuitively, given a problem, a simpler data structure comes with\na simpler algorithm. Here, we investigate the feasibility of improving graph\nclassification performance while simplifying the learning process. Inspired by\nstructural entropy on graphs, we transform the data sample from graphs to\ncoding trees, which is a simpler but essential structure for graph data.\nFurthermore, we propose a novel message passing scheme, termed hierarchical\nreporting, in which features are transferred from leaf nodes to root nodes by\nfollowing the hierarchical structure of coding trees. We then present a tree\nkernel and a convolutional network to implement our scheme for graph\nclassification. With the designed message passing scheme, the tree kernel and\nconvolutional network have a lower runtime complexity of $O(n)$ than\nWeisfeiler-Lehman subtree kernel and other graph neural networks of at least\n$O(hm)$. We empirically validate our methods with several graph classification\nbenchmarks and demonstrate that they achieve better performance and lower\ncomputational consumption than competing approaches.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2206.02404v1",
        "date": "2022-06-06 07:24:44+00:00"
    },
    {
        "title": "Mnemosyne: Learning to Train Transformers with Transformers",
        "authors": [
            "Deepali Jain",
            "Krzysztof Marcin Choromanski",
            "Sumeet Singh",
            "Vikas Sindhwani",
            "Tingnan Zhang",
            "Jie Tan",
            "Avinava Dubey"
        ],
        "abstract": "Training complex machine learning (ML) architectures requires a compute and\ntime consuming process of selecting the right optimizer and tuning its\nhyper-parameters. A new paradigm of learning optimizers from data has emerged\nas a better alternative to hand-designed ML optimizers. We propose Mnemosyne\noptimizer, that uses Performers: implicit low-rank attention Transformers. It\ncan learn to train entire neural network architectures including other\nTransformers without any task-specific optimizer tuning. We show that\nMnemosyne: (a) generalizes better than popular LSTM optimizer, (b) in\nparticular can successfully train Vision Transformers (ViTs) while\nmeta--trained on standard MLPs and (c) can initialize optimizers for faster\nconvergence in Robotics applications. We believe that these results open the\npossibility of using Transformers to build foundational optimization models\nthat can address the challenges of regular Transformer training. We complement\nour results with an extensive theoretical analysis of the compact associative\nmemory used by Mnemosyne.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2302.01128v1",
        "date": "2023-02-02 14:40:28+00:00"
    },
    {
        "title": "Application of Deep Neural Network in Estimation of the Weld Bead Parameters",
        "authors": [
            "Soheil Keshmiri",
            "Xin Zheng",
            "Chee Meng Chew",
            "Chee Khiang Pang"
        ],
        "abstract": "We present a deep learning approach to estimation of the bead parameters in\nwelding tasks. Our model is based on a four-hidden-layer neural network\narchitecture. More specifically, the first three hidden layers of this\narchitecture utilize Sigmoid function to produce their respective intermediate\noutputs. On the other hand, the last hidden layer uses a linear transformation\nto generate the final output of this architecture. This transforms our deep\nnetwork architecture from a classifier to a non-linear regression model. We\ncompare the performance of our deep network with a selected number of results\nin the literature to show a considerable improvement in reducing the errors in\nestimation of these values. Furthermore, we show its scalability on estimating\nthe weld bead parameters with same level of accuracy on combination of datasets\nthat pertain to different welding techniques. This is a nontrivial result that\nis counter-intuitive to the general belief in this field of research.",
        "categories": [
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1502.04187v2",
        "date": "2015-02-14 10:58:53+00:00"
    },
    {
        "title": "Fully-Connected Network on Noncompact Symmetric Space and Ridgelet Transform based on Helgason-Fourier Analysis",
        "authors": [
            "Sho Sonoda",
            "Isao Ishikawa",
            "Masahiro Ikeda"
        ],
        "abstract": "Neural network on Riemannian symmetric space such as hyperbolic space and the\nmanifold of symmetric positive definite (SPD) matrices is an emerging subject\nof research in geometric deep learning. Based on the well-established framework\nof the Helgason-Fourier transform on the noncompact symmetric space, we present\na fully-connected network and its associated ridgelet transform on the\nnoncompact symmetric space, covering the hyperbolic neural network (HNN) and\nthe SPDNet as special cases. The ridgelet transform is an analysis operator of\na depth-2 continuous network spanned by neurons, namely, it maps an arbitrary\ngiven function to the weights of a network. Thanks to the coordinate-free\nreformulation, the role of nonlinear activation functions is revealed to be a\nwavelet function, and the reconstruction formula directly yields the\nuniversality of the proposed networks.",
        "categories": [
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2203.01631v2",
        "date": "2022-03-03 10:45:53+00:00"
    },
    {
        "title": "Meta Input: How to Leverage Off-the-Shelf Deep Neural Networks",
        "authors": [
            "Minsu Kim",
            "Youngjoon Yu",
            "Sungjune Park",
            "Yong Man Ro"
        ],
        "abstract": "These days, although deep neural networks (DNNs) have achieved a noticeable\nprogress in a wide range of research area, it lacks the adaptability to be\nemployed in the real-world applications because of the environment discrepancy\nproblem. Such a problem originates from the difference between training and\ntesting environments, and it is widely known that it causes serious performance\ndegradation, when a pretrained DNN model is applied to a new testing\nenvironment. Therefore, in this paper, we introduce a novel approach that\nallows end-users to exploit pretrained DNN models in their own testing\nenvironment without modifying the models. To this end, we present a\n\\textit{meta input} which is an additional input transforming the distribution\nof testing data to be aligned with that of training data. The proposed meta\ninput can be optimized with a small number of testing data only by considering\nthe relation between testing input data and its output prediction. Also, it\ndoes not require any knowledge of the network's internal architecture and\nmodification of its weight parameters. Then, the obtained meta input is added\nto testing data in order to shift the distribution of testing data to that of\noriginally used training data. As a result, end-users can exploit well-trained\nmodels in their own testing environment which can differ from the training\nenvironment. We validate the effectiveness and versatility of the proposed meta\ninput by showing the robustness against the environment discrepancy through the\ncomprehensive experiments with various tasks.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "link": "http://arxiv.org/pdf/2210.13186v1",
        "date": "2022-10-21 02:11:38+00:00"
    },
    {
        "title": "A Systematic Review of Machine Learning Techniques for Cattle Identification: Datasets, Methods and Future Directions",
        "authors": [
            "Md Ekramul Hossain",
            "Muhammad Ashad Kabir",
            "Lihong Zheng",
            "Dave L. Swain",
            "Shawn McGrath",
            "Jonathan Medway"
        ],
        "abstract": "Increased biosecurity and food safety requirements may increase demand for\nefficient traceability and identification systems of livestock in the supply\nchain. The advanced technologies of machine learning and computer vision have\nbeen applied in precision livestock management, including critical disease\ndetection, vaccination, production management, tracking, and health monitoring.\nThis paper offers a systematic literature review (SLR) of vision-based cattle\nidentification. More specifically, this SLR is to identify and analyse the\nresearch related to cattle identification using Machine Learning (ML) and Deep\nLearning (DL). For the two main applications of cattle detection and cattle\nidentification, all the ML based papers only solve cattle identification\nproblems. However, both detection and identification problems were studied in\nthe DL based papers. Based on our survey report, the most used ML models for\ncattle identification were support vector machine (SVM), k-nearest neighbour\n(KNN), and artificial neural network (ANN). Convolutional neural network (CNN),\nresidual network (ResNet), Inception, You Only Look Once (YOLO), and Faster\nR-CNN were popular DL models in the selected papers. Among these papers, the\nmost distinguishing features were the muzzle prints and coat patterns of\ncattle. Local binary pattern (LBP), speeded up robust features (SURF),\nscale-invariant feature transform (SIFT), and Inception or CNN were identified\nas the most used feature extraction methods.",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2210.09215v1",
        "date": "2022-10-13 14:10:12+00:00"
    },
    {
        "title": "BYOL-S: Learning Self-supervised Speech Representations by Bootstrapping",
        "authors": [
            "Gasser Elbanna",
            "Neil Scheidwasser-Clow",
            "Mikolaj Kegler",
            "Pierre Beckmann",
            "Karl El Hajal",
            "Milos Cernak"
        ],
        "abstract": "Methods for extracting audio and speech features have been studied since\npioneering work on spectrum analysis decades ago. Recent efforts are guided by\nthe ambition to develop general-purpose audio representations. For example,\ndeep neural networks can extract optimal embeddings if they are trained on\nlarge audio datasets. This work extends existing methods based on\nself-supervised learning by bootstrapping, proposes various encoder\narchitectures, and explores the effects of using different pre-training\ndatasets. Lastly, we present a novel training framework to come up with a\nhybrid audio representation, which combines handcrafted and data-driven learned\naudio features. All the proposed representations were evaluated within the HEAR\nNeurIPS 2021 challenge for auditory scene classification and timestamp\ndetection tasks. Our results indicate that the hybrid model with a\nconvolutional transformer as the encoder yields superior performance in most\nHEAR challenge tasks.",
        "categories": [
            "cs.SD",
            "cs.AI",
            "cs.LG",
            "eess.AS"
        ],
        "link": "http://arxiv.org/pdf/2206.12038v4",
        "date": "2022-06-24 02:26:40+00:00"
    },
    {
        "title": "Optimizing Learning Rate Schedules for Iterative Pruning of Deep Neural Networks",
        "authors": [
            "Shiyu Liu",
            "Rohan Ghosh",
            "John Tan Chong Min",
            "Mehul Motani"
        ],
        "abstract": "The importance of learning rate (LR) schedules on network pruning has been\nobserved in a few recent works. As an example, Frankle and Carbin (2019)\nhighlighted that winning tickets (i.e., accuracy preserving subnetworks) can\nnot be found without applying a LR warmup schedule and Renda, Frankle and\nCarbin (2020) demonstrated that rewinding the LR to its initial state at the\nend of each pruning cycle improves performance. In this paper, we go one step\nfurther by first providing a theoretical justification for the surprising\neffect of LR schedules. Next, we propose a LR schedule for network pruning\ncalled SILO, which stands for S-shaped Improved Learning rate Optimization. The\nadvantages of SILO over existing state-of-the-art (SOTA) LR schedules are\ntwo-fold: (i) SILO has a strong theoretical motivation and dynamically adjusts\nthe LR during pruning to improve generalization. Specifically, SILO increases\nthe LR upper bound (max_lr) in an S-shape. This leads to an improvement of 2% -\n4% in extensive experiments with various types of networks (e.g., Vision\nTransformers, ResNet) on popular datasets such as ImageNet, CIFAR-10/100. (ii)\nIn addition to the strong theoretical motivation, SILO is empirically optimal\nin the sense of matching an Oracle, which exhaustively searches for the optimal\nvalue of max_lr via grid search. We find that SILO is able to precisely adjust\nthe value of max_lr to be within the Oracle optimized interval, resulting in\nperformance competitive with the Oracle with significantly lower complexity.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2212.06144v2",
        "date": "2022-12-09 14:39:50+00:00"
    },
    {
        "title": "Modern Hopfield Networks and Attention for Immune Repertoire Classification",
        "authors": [
            "Michael Widrich",
            "Bernhard Sch\u00e4fl",
            "Hubert Ramsauer",
            "Milena Pavlovi\u0107",
            "Lukas Gruber",
            "Markus Holzleitner",
            "Johannes Brandstetter",
            "Geir Kjetil Sandve",
            "Victor Greiff",
            "Sepp Hochreiter",
            "G\u00fcnter Klambauer"
        ],
        "abstract": "A central mechanism in machine learning is to identify, store, and recognize\npatterns. How to learn, access, and retrieve such patterns is crucial in\nHopfield networks and the more recent transformer architectures. We show that\nthe attention mechanism of transformer architectures is actually the update\nrule of modern Hopfield networks that can store exponentially many patterns. We\nexploit this high storage capacity of modern Hopfield networks to solve a\nchallenging multiple instance learning (MIL) problem in computational biology:\nimmune repertoire classification. Accurate and interpretable machine learning\nmethods solving this problem could pave the way towards new vaccines and\ntherapies, which is currently a very relevant research topic intensified by the\nCOVID-19 crisis. Immune repertoire classification based on the vast number of\nimmunosequences of an individual is a MIL problem with an unprecedentedly\nmassive number of instances, two orders of magnitude larger than currently\nconsidered problems, and with an extremely low witness rate. In this work, we\npresent our novel method DeepRC that integrates transformer-like attention, or\nequivalently modern Hopfield networks, into deep learning architectures for\nmassive MIL such as immune repertoire classification. We demonstrate that\nDeepRC outperforms all other methods with respect to predictive performance on\nlarge-scale experiments, including simulated and real-world virus infection\ndata, and enables the extraction of sequence motifs that are connected to a\ngiven disease class. Source code and datasets: https://github.com/ml-jku/DeepRC",
        "categories": [
            "cs.LG",
            "q-bio.BM",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2007.13505v1",
        "date": "2020-07-16 20:35:46+00:00"
    },
    {
        "title": "Ensemble Augmentation for Deep Neural Networks Using 1-D Time Series Vibration Data",
        "authors": [
            "Atik Faysal",
            "Ngui Wai Keng",
            "M. H. Lim"
        ],
        "abstract": "Time-series data are one of the fundamental types of raw data representation\nused in data-driven techniques. In machine condition monitoring, time-series\nvibration data are overly used in data mining for deep neural networks.\nTypically, vibration data is converted into images for classification using\nDeep Neural Networks (DNNs), and scalograms are the most effective form of\nimage representation. However, the DNN classifiers require huge labeled\ntraining samples to reach their optimum performance. So, many forms of data\naugmentation techniques are applied to the classifiers to compensate for the\nlack of training samples. However, the scalograms are graphical representations\nwhere the existing augmentation techniques suffer because they either change\nthe graphical meaning or have too much noise in the samples that change the\nphysical meaning. In this study, a data augmentation technique named ensemble\naugmentation is proposed to overcome this limitation. This augmentation method\nuses the power of white noise added in ensembles to the original samples to\ngenerate real-like samples. After averaging the signal with ensembles, a new\nsignal is obtained that contains the characteristics of the original signal.\nThe parameters for the ensemble augmentation are validated using a simulated\nsignal. The proposed method is evaluated using 10 class bearing vibration data\nusing three state-of-the-art Transfer Learning (TL) models, namely,\nInception-V3, MobileNet-V2, and ResNet50. Augmented samples are generated in\ntwo increments: the first increment generates the same number of fake samples\nas the training samples, and in the second increment, the number of samples is\nincreased gradually. The outputs from the proposed method are compared with no\naugmentation, augmentations using deep convolution generative adversarial\nnetwork (DCGAN), and several geometric transformation-based augmentations...",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2108.03288v2",
        "date": "2021-08-06 20:04:29+00:00"
    },
    {
        "title": "Towards Structured Deep Neural Network for Automatic Speech Recognition",
        "authors": [
            "Yi-Hsiu Liao",
            "Hung-Yi Lee",
            "Lin-shan Lee"
        ],
        "abstract": "In this paper we propose the Structured Deep Neural Network (Structured DNN)\nas a structured and deep learning algorithm, learning to find the best\nstructured object (such as a label sequence) given a structured input (such as\na vector sequence) by globally considering the mapping relationships between\nthe structure rather than item by item.\n  When automatic speech recognition is viewed as a special case of such a\nstructured learning problem, where we have the acoustic vector sequence as the\ninput and the phoneme label sequence as the output, it becomes possible to\ncomprehensively learned utterance by utterance as a whole, rather than frame by\nframe.\n  Structured Support Vector Machine (structured SVM) was proposed to perform\nASR with structured learning previously, but limited by the linear nature of\nSVM. Here we propose structured DNN to use nonlinear transformations in\nmulti-layers as a structured and deep learning algorithm. It was shown to beat\nstructured SVM in preliminary experiments on TIMIT.",
        "categories": [
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1506.01163v1",
        "date": "2015-06-03 08:41:05+00:00"
    },
    {
        "title": "Koopman-Based Bound for Generalization: New Aspect of Neural Networks Regarding Nonlinear Noise Filtering",
        "authors": [
            "Yuka Hashimoto",
            "Sho Sonoda",
            "Isao Ishikawa",
            "Atsushi Nitanda",
            "Taiji Suzuki"
        ],
        "abstract": "We propose a new bound for generalization of neural networks using Koopman\noperators. Unlike most of the existing works, we focus on the role of the final\nnonlinear transformation of the networks. Our bound is described by the\nreciprocal of the determinant of the weight matrices and is tighter than\nexisting norm-based bounds when the weight matrices do not have small singular\nvalues. According to existing theories about the low-rankness of the weight\nmatrices, it may be counter-intuitive that we focus on the case where singular\nvalues of weight matrices are not small. However, motivated by the final\nnonlinear transformation, we can see that our result sheds light on a new\nperspective regarding a noise filtering property of neural networks. Since our\nbound comes from Koopman operators, this work also provides a connection\nbetween operator-theoretic analysis and generalization of neural networks.\nNumerical results support the validity of our theoretical results.",
        "categories": [
            "cs.LG",
            "math.FA",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2302.05825v1",
        "date": "2023-02-12 00:39:25+00:00"
    },
    {
        "title": "Topology-based Representative Datasets to Reduce Neural Network Training Resources",
        "authors": [
            "Rocio Gonzalez-Diaz",
            "Miguel A. Guti\u00e9rrez-Naranjo",
            "Eduardo Paluzo-Hidalgo"
        ],
        "abstract": "One of the main drawbacks of the practical use of neural networks is the long\ntime required in the training process. Such a training process consists of an\niterative change of parameters trying to minimize a loss function. These\nchanges are driven by a dataset, which can be seen as a set of labelled points\nin an n-dimensional space. In this paper, we explore the concept of are\nrepresentative dataset which is a dataset smaller than the original one,\nsatisfying a nearness condition independent of isometric transformations.\nRepresentativeness is measured using persistence diagrams (a computational\ntopology tool) due to its computational efficiency. We prove that the accuracy\nof the learning process of a neural network on a representative dataset is\n\"similar\" to the accuracy on the original dataset when the neural network\narchitecture is a perceptron and the loss function is the mean squared error.\nThese theoretical results accompanied by experimentation open a door to\nreducing the size of the dataset to gain time in the training process of any\nneural network.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1903.08519v3",
        "date": "2019-03-20 14:33:20+00:00"
    },
    {
        "title": "3D-Rotation-Equivariant Quaternion Neural Networks",
        "authors": [
            "Wen Shen",
            "Binbin Zhang",
            "Shikun Huang",
            "Zhihua Wei",
            "Quanshi Zhang"
        ],
        "abstract": "This paper proposes a set of rules to revise various neural networks for 3D\npoint cloud processing to rotation-equivariant quaternion neural networks\n(REQNNs). We find that when a neural network uses quaternion features under\ncertain conditions, the network feature naturally has the rotation-equivariance\nproperty. Rotation equivariance means that applying a specific rotation\ntransformation to the input point cloud is equivalent to applying the same\nrotation transformation to all intermediate-layer quaternion features. Besides,\nthe REQNN also ensures that the intermediate-layer features are invariant to\nthe permutation of input points. Compared with the original neural network, the\nREQNN exhibits higher rotation robustness.",
        "categories": [
            "cs.CV",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1911.09040v2",
        "date": "2019-11-20 17:10:52+00:00"
    },
    {
        "title": "Neural Clamping: Joint Input Perturbation and Temperature Scaling for Neural Network Calibration",
        "authors": [
            "Yung-Chen Tang",
            "Pin-Yu Chen",
            "Tsung-Yi Ho"
        ],
        "abstract": "Neural network calibration is an essential task in deep learning to ensure\nconsistency between the confidence of model prediction and the true correctness\nlikelihood. In this paper, we propose a new post-processing calibration method\ncalled Neural Clamping, which employs a simple joint input-output\ntransformation on a pre-trained classifier via a learnable universal input\nperturbation and an output temperature scaling parameter. Moreover, we provide\ntheoretical explanations on why Neural Clamping is provably better than\ntemperature scaling. Evaluated on CIFAR-100 and ImageNet image recognition\ndatasets and a variety of deep neural network models, our empirical results\nshow that Neural Clamping significantly outperforms state-of-the-art\npost-processing calibration methods.",
        "categories": [
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2209.11604v1",
        "date": "2022-09-23 14:18:39+00:00"
    },
    {
        "title": "Emotional Video to Audio Transformation Using Deep Recurrent Neural Networks and a Neuro-Fuzzy System",
        "authors": [
            "Gwenaelle Cunha Sergio",
            "Minho Lee"
        ],
        "abstract": "Generating music with emotion similar to that of an input video is a very\nrelevant issue nowadays. Video content creators and automatic movie directors\nbenefit from maintaining their viewers engaged, which can be facilitated by\nproducing novel material eliciting stronger emotions in them. Moreover, there's\ncurrently a demand for more empathetic computers to aid humans in applications\nsuch as augmenting the perception ability of visually and/or hearing impaired\npeople. Current approaches overlook the video's emotional characteristics in\nthe music generation step, only consider static images instead of videos, are\nunable to generate novel music, and require a high level of human effort and\nskills. In this study, we propose a novel hybrid deep neural network that uses\nan Adaptive Neuro-Fuzzy Inference System to predict a video's emotion from its\nvisual features and a deep Long Short-Term Memory Recurrent Neural Network to\ngenerate its corresponding audio signals with similar emotional inkling. The\nformer is able to appropriately model emotions due to its fuzzy properties, and\nthe latter is able to model data with dynamic time properties well due to the\navailability of the previous hidden state information. The novelty of our\nproposed method lies in the extraction of visual emotional features in order to\ntransform them into audio signals with corresponding emotional aspects for\nusers. Quantitative experiments show low mean absolute errors of 0.217 and\n0.255 in the Lindsey and DEAP datasets respectively, and similar global\nfeatures in the spectrograms. This indicates that our model is able to\nappropriately perform domain transformation between visual and audio features.\nBased on experimental results, our model can effectively generate audio that\nmatches the scene eliciting a similar emotion from the viewer in both datasets,\nand music generated by our model is also chosen more often.",
        "categories": [
            "cs.SD",
            "cs.CV",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2004.02113v1",
        "date": "2020-04-05 07:18:28+00:00"
    },
    {
        "title": "Enhancing Explainability of Neural Networks through Architecture Constraints",
        "authors": [
            "Zebin Yang",
            "Aijun Zhang",
            "Agus Sudjianto"
        ],
        "abstract": "Prediction accuracy and model explainability are the two most important\nobjectives when developing machine learning algorithms to solve real-world\nproblems. The neural networks are known to possess good prediction performance,\nbut lack of sufficient model interpretability. In this paper, we propose to\nenhance the explainability of neural networks through the following\narchitecture constraints: a) sparse additive subnetworks; b) projection pursuit\nwith orthogonality constraint; and c) smooth function approximation. It leads\nto an explainable neural network (xNN) with the superior balance between\nprediction performance and model interpretability. We derive the necessary and\nsufficient identifiability conditions for the proposed xNN model. The multiple\nparameters are simultaneously estimated by a modified mini-batch gradient\ndescent method based on the backpropagation algorithm for calculating the\nderivatives and the Cayley transform for preserving the projection\northogonality. Through simulation study under six different scenarios, we\ncompare the proposed method to several benchmarks including least absolute\nshrinkage and selection operator, support vector machine, random forest,\nextreme learning machine, and multi-layer perceptron. It is shown that the\nproposed xNN model keeps the flexibility of pursuing high prediction accuracy\nwhile attaining improved interpretability. Finally, a real data example is\nemployed as a showcase application.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1901.03838v2",
        "date": "2019-01-12 10:17:36+00:00"
    },
    {
        "title": "Recurrent Transform Learning",
        "authors": [
            "Megha Gupta",
            "Angshul Majumdar"
        ],
        "abstract": "The objective of this work is to improve the accuracy of building demand\nforecasting. This is a more challenging task than grid level forecasting. For\nthe said purpose, we develop a new technique called recurrent transform\nlearning (RTL). Two versions are proposed. The first one (RTL) is unsupervised;\nthis is used as a feature extraction tool that is further fed into a regression\nmodel. The second formulation embeds regression into the RTL framework leading\nto regressing recurrent transform learning (R2TL). Forecasting experiments have\nbeen carried out on three popular publicly available datasets. Both of our\nproposed techniques yield results superior to the state-of-the-art like long\nshort term memory network, echo state network and sparse coding regression.",
        "categories": [
            "cs.LG",
            "cs.NE",
            "eess.SP",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1912.05198v1",
        "date": "2019-12-11 09:29:57+00:00"
    },
    {
        "title": "Tensor Decomposition for Model Reduction in Neural Networks: A Review",
        "authors": [
            "Xingyi Liu",
            "Keshab K. Parhi"
        ],
        "abstract": "Modern neural networks have revolutionized the fields of computer vision (CV)\nand Natural Language Processing (NLP). They are widely used for solving complex\nCV tasks and NLP tasks such as image classification, image generation, and\nmachine translation. Most state-of-the-art neural networks are\nover-parameterized and require a high computational cost. One straightforward\nsolution is to replace the layers of the networks with their low-rank tensor\napproximations using different tensor decomposition methods. This paper reviews\nsix tensor decomposition methods and illustrates their ability to compress\nmodel parameters of convolutional neural networks (CNNs), recurrent neural\nnetworks (RNNs) and Transformers. The accuracy of some compressed models can be\nhigher than the original versions. Evaluations indicate that tensor\ndecompositions can achieve significant reductions in model size, run-time and\nenergy consumption, and are well suited for implementing neural networks on\nedge devices.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2304.13539v1",
        "date": "2023-04-26 13:12:00+00:00"
    },
    {
        "title": "DDSP: Differentiable Digital Signal Processing",
        "authors": [
            "Jesse Engel",
            "Lamtharn Hantrakul",
            "Chenjie Gu",
            "Adam Roberts"
        ],
        "abstract": "Most generative models of audio directly generate samples in one of two\ndomains: time or frequency. While sufficient to express any signal, these\nrepresentations are inefficient, as they do not utilize existing knowledge of\nhow sound is generated and perceived. A third approach (vocoders/synthesizers)\nsuccessfully incorporates strong domain knowledge of signal processing and\nperception, but has been less actively researched due to limited expressivity\nand difficulty integrating with modern auto-differentiation-based machine\nlearning methods. In this paper, we introduce the Differentiable Digital Signal\nProcessing (DDSP) library, which enables direct integration of classic signal\nprocessing elements with deep learning methods. Focusing on audio synthesis, we\nachieve high-fidelity generation without the need for large autoregressive\nmodels or adversarial losses, demonstrating that DDSP enables utilizing strong\ninductive biases without losing the expressive power of neural networks.\nFurther, we show that combining interpretable modules permits manipulation of\neach separate model component, with applications such as independent control of\npitch and loudness, realistic extrapolation to pitches not seen during\ntraining, blind dereverberation of room acoustics, transfer of extracted room\nacoustics to new environments, and transformation of timbre between disparate\nsources. In short, DDSP enables an interpretable and modular approach to\ngenerative modeling, without sacrificing the benefits of deep learning. The\nlibrary is publicly available at https://github.com/magenta/ddsp and we welcome\nfurther contributions from the community and domain experts.",
        "categories": [
            "cs.LG",
            "cs.SD",
            "eess.AS",
            "eess.SP",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2001.04643v1",
        "date": "2020-01-14 06:49:37+00:00"
    },
    {
        "title": "Orthogonal Graph Neural Networks",
        "authors": [
            "Kai Guo",
            "Kaixiong Zhou",
            "Xia Hu",
            "Yu Li",
            "Yi Chang",
            "Xin Wang"
        ],
        "abstract": "Graph neural networks (GNNs) have received tremendous attention due to their\nsuperiority in learning node representations. These models rely on message\npassing and feature transformation functions to encode the structural and\nfeature information from neighbors. However, stacking more convolutional layers\nsignificantly decreases the performance of GNNs. Most recent studies attribute\nthis limitation to the over-smoothing issue, where node embeddings converge to\nindistinguishable vectors. Through a number of experimental observations, we\nargue that the main factor degrading the performance is the unstable forward\nnormalization and backward gradient resulted from the improper design of the\nfeature transformation, especially for shallow GNNs where the over-smoothing\nhas not happened. Therefore, we propose a novel orthogonal feature\ntransformation, named Ortho-GConv, which could generally augment the existing\nGNN backbones to stabilize the model training and improve the model's\ngeneralization performance. Specifically, we maintain the orthogonality of the\nfeature transformation comprehensively from three perspectives, namely hybrid\nweight initialization, orthogonal transformation, and orthogonal\nregularization. By equipping the existing GNNs (e.g. GCN, JKNet, GCNII) with\nOrtho-GConv, we demonstrate the generality of the orthogonal feature\ntransformation to enable stable training, and show its effectiveness for node\nand graph classification tasks.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2109.11338v2",
        "date": "2021-09-23 12:39:01+00:00"
    },
    {
        "title": "Beef Cattle Instance Segmentation Using Fully Convolutional Neural Network",
        "authors": [
            "Aram Ter-Sarkisov",
            "Robert Ross",
            "John Kelleher",
            "Bernadette Earley",
            "Michael Keane"
        ],
        "abstract": "We present an instance segmentation algorithm trained and applied to a CCTV\nrecording of beef cattle during a winter finishing period. A fully\nconvolutional network was transformed into an instance segmentation network\nthat learns to label each instance of an animal separately. We introduce a\nconceptually simple framework that the network uses to output a single\nprediction for every animal. These results are a contribution towards behaviour\nanalysis in winter finishing beef cattle for early detection of animal\nwelfare-related problems.",
        "categories": [
            "cs.CV",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1807.01972v2",
        "date": "2018-07-05 12:52:07+00:00"
    },
    {
        "title": "Perspective Transformation Layer",
        "authors": [
            "Nishan Khatri",
            "Agnibh Dasgupta",
            "Yucong Shen",
            "Xin Zhong",
            "Frank Y. Shih"
        ],
        "abstract": "Incorporating geometric transformations that reflect the relative position\nchanges between an observer and an object into computer vision and deep\nlearning models has attracted much attention in recent years. However, the\nexisting proposals mainly focus on the affine transformation that is\ninsufficient to reflect such geometric position changes. Furthermore, current\nsolutions often apply a neural network module to learn a single transformation\nmatrix, which not only ignores the importance of multi-view analysis but also\nincludes extra training parameters from the module apart from the\ntransformation matrix parameters that increase the model complexity. In this\npaper, a perspective transformation layer is proposed in the context of deep\nlearning. The proposed layer can learn homography, therefore reflecting the\ngeometric positions between observers and objects. In addition, by directly\ntraining its transformation matrices, a single proposed layer can learn an\nadjustable number of multiple viewpoints without considering module parameters.\nThe experiments and evaluations confirm the superiority of the proposed layer.",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2201.05706v2",
        "date": "2022-01-14 23:09:26+00:00"
    },
    {
        "title": "Hierarchical Mixtures of Generators for Adversarial Learning",
        "authors": [
            "Alper Ahmeto\u011flu",
            "Ethem Alpayd\u0131n"
        ],
        "abstract": "Generative adversarial networks (GANs) are deep neural networks that allow us\nto sample from an arbitrary probability distribution without explicitly\nestimating the distribution. There is a generator that takes a latent vector as\ninput and transforms it into a valid sample from the distribution. There is\nalso a discriminator that is trained to discriminate such fake samples from\ntrue samples of the distribution; at the same time, the generator is trained to\ngenerate fakes that the discriminator cannot tell apart from the true samples.\nInstead of learning a global generator, a recent approach involves training\nmultiple generators each responsible from one part of the distribution. In this\nwork, we review such approaches and propose the hierarchical mixture of\ngenerators, inspired from the hierarchical mixture of experts model, that\nlearns a tree structure implementing a hierarchical clustering with soft splits\nin the decision nodes and local generators in the leaves. Since the generators\nare combined softly, the whole model is continuous and can be trained using\ngradient-based optimization, just like the original GAN model. Our experiments\non five image data sets, namely, MNIST, FashionMNIST, UTZap50K, Oxford Flowers,\nand CelebA, show that our proposed model generates samples of high quality and\ndiversity in terms of popular GAN evaluation metrics. The learned hierarchical\nstructure also leads to knowledge extraction.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1911.02069v1",
        "date": "2019-11-05 20:13:31+00:00"
    },
    {
        "title": "Mathematical Reasoning in Latent Space",
        "authors": [
            "Dennis Lee",
            "Christian Szegedy",
            "Markus N. Rabe",
            "Sarah M. Loos",
            "Kshitij Bansal"
        ],
        "abstract": "We design and conduct a simple experiment to study whether neural networks\ncan perform several steps of approximate reasoning in a fixed dimensional\nlatent space. The set of rewrites (i.e. transformations) that can be\nsuccessfully performed on a statement represents essential semantic features of\nthe statement. We can compress this information by embedding the formula in a\nvector space, such that the vector associated with a statement can be used to\npredict whether a statement can be rewritten by other theorems. Predicting the\nembedding of a formula generated by some rewrite rule is naturally viewed as\napproximate reasoning in the latent space. In order to measure the\neffectiveness of this reasoning, we perform approximate deduction sequences in\nthe latent space and use the resulting embedding to inform the semantic\nfeatures of the corresponding formal statement (which is obtained by performing\nthe corresponding rewrite sequence using real formulas). Our experiments show\nthat graph neural networks can make non-trivial predictions about the\nrewrite-success of statements, even when they propagate predicted latent\nrepresentations for several steps. Since our corpus of mathematical formulas\nincludes a wide variety of mathematical disciplines, this experiment is a\nstrong indicator for the feasibility of deduction in latent space in general.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1909.11851v1",
        "date": "2019-09-26 02:33:07+00:00"
    },
    {
        "title": "New pointwise convolution in Deep Neural Networks through Extremely Fast and Non Parametric Transforms",
        "authors": [
            "Joonhyun Jeong",
            "Sung-Ho Bae"
        ],
        "abstract": "Some conventional transforms such as Discrete Walsh-Hadamard Transform (DWHT)\nand Discrete Cosine Transform (DCT) have been widely used as feature extractors\nin image processing but rarely applied in neural networks. However, we found\nthat these conventional transforms have the ability to capture the\ncross-channel correlations without any learnable parameters in DNNs. This paper\nfirstly proposes to apply conventional transforms to pointwise convolution,\nshowing that such transforms significantly reduce the computational complexity\nof neural networks without accuracy performance degradation. Especially for\nDWHT, it requires no floating point multiplications but only additions and\nsubtractions, which can considerably reduce computation overheads. In addition,\nits fast algorithm further reduces complexity of floating point addition from\n$\\mathcal{O}(n^2)$ to $\\mathcal{O}(n\\log n)$. These nice properties construct\nextremely efficient networks in the number parameters and operations, enjoying\naccuracy gain. Our proposed DWHT-based model gained 1.49\\% accuracy increase\nwith 79.1\\% reduced parameters and 48.4\\% reduced FLOPs compared with its\nbaseline model (MoblieNet-V1) on the CIFAR 100 dataset.",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1906.12172v1",
        "date": "2019-06-25 10:47:08+00:00"
    },
    {
        "title": "Gravitational-wave parameter estimation with autoregressive neural network flows",
        "authors": [
            "Stephen R. Green",
            "Christine Simpson",
            "Jonathan Gair"
        ],
        "abstract": "We introduce the use of autoregressive normalizing flows for rapid\nlikelihood-free inference of binary black hole system parameters from\ngravitational-wave data with deep neural networks. A normalizing flow is an\ninvertible mapping on a sample space that can be used to induce a\ntransformation from a simple probability distribution to a more complex one: if\nthe simple distribution can be rapidly sampled and its density evaluated, then\nso can the complex distribution. Our first application to gravitational waves\nuses an autoregressive flow, conditioned on detector strain data, to map a\nmultivariate standard normal distribution into the posterior distribution over\nsystem parameters. We train the model on artificial strain data consisting of\nIMRPhenomPv2 waveforms drawn from a five-parameter $(m_1, m_2, \\phi_0, t_c,\nd_L)$ prior and stationary Gaussian noise realizations with a fixed power\nspectral density. This gives performance comparable to current best\ndeep-learning approaches to gravitational-wave parameter estimation. We then\nbuild a more powerful latent variable model by incorporating autoregressive\nflows within the variational autoencoder framework. This model has performance\ncomparable to Markov chain Monte Carlo and, in particular, successfully models\nthe multimodal $\\phi_0$ posterior. Finally, we train the autoregressive latent\nvariable model on an expanded parameter space, including also aligned spins\n$(\\chi_{1z}, \\chi_{2z})$ and binary inclination $\\theta_{JN}$, and show that\nall parameters and degeneracies are well-recovered. In all cases, sampling is\nextremely fast, requiring less than two seconds to draw $10^4$ posterior\nsamples.",
        "categories": [
            "astro-ph.IM",
            "cs.LG",
            "gr-qc",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2002.07656v1",
        "date": "2020-02-18 15:44:04+00:00"
    },
    {
        "title": "Edge-Cloud Polarization and Collaboration: A Comprehensive Survey for AI",
        "authors": [
            "Jiangchao Yao",
            "Shengyu Zhang",
            "Yang Yao",
            "Feng Wang",
            "Jianxin Ma",
            "Jianwei Zhang",
            "Yunfei Chu",
            "Luo Ji",
            "Kunyang Jia",
            "Tao Shen",
            "Anpeng Wu",
            "Fengda Zhang",
            "Ziqi Tan",
            "Kun Kuang",
            "Chao Wu",
            "Fei Wu",
            "Jingren Zhou",
            "Hongxia Yang"
        ],
        "abstract": "Influenced by the great success of deep learning via cloud computing and the\nrapid development of edge chips, research in artificial intelligence (AI) has\nshifted to both of the computing paradigms, i.e., cloud computing and edge\ncomputing. In recent years, we have witnessed significant progress in\ndeveloping more advanced AI models on cloud servers that surpass traditional\ndeep learning models owing to model innovations (e.g., Transformers, Pretrained\nfamilies), explosion of training data and soaring computing capabilities.\nHowever, edge computing, especially edge and cloud collaborative computing, are\nstill in its infancy to announce their success due to the resource-constrained\nIoT scenarios with very limited algorithms deployed. In this survey, we conduct\na systematic review for both cloud and edge AI. Specifically, we are the first\nto set up the collaborative learning mechanism for cloud and edge modeling with\na thorough review of the architectures that enable such mechanism. We also\ndiscuss potentials and practical experiences of some on-going advanced edge AI\ntopics including pretraining models, graph neural networks and reinforcement\nlearning. Finally, we discuss the promising directions and challenges in this\nfield.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2111.06061v3",
        "date": "2021-11-11 05:58:23+00:00"
    },
    {
        "title": "Towards Structured Deep Neural Network for Automatic Speech Recognition",
        "authors": [
            "Yi-Hsiu Liao",
            "Hung-yi Lee",
            "Lin-shan Lee"
        ],
        "abstract": "In this paper we propose the Structured Deep Neural Network (structured DNN)\nas a structured and deep learning framework. This approach can learn to find\nthe best structured object (such as a label sequence) given a structured input\n(such as a vector sequence) by globally considering the mapping relationships\nbetween the structures rather than item by item.\n  When automatic speech recognition is viewed as a special case of such a\nstructured learning problem, where we have the acoustic vector sequence as the\ninput and the phoneme label sequence as the output, it becomes possible to\ncomprehensively learn utterance by utterance as a whole, rather than frame by\nframe.\n  Structured Support Vector Machine (structured SVM) was proposed to perform\nASR with structured learning previously, but limited by the linear nature of\nSVM. Here we propose structured DNN to use nonlinear transformations in\nmulti-layers as a structured and deep learning approach. This approach was\nshown to beat structured SVM in preliminary experiments on TIMIT.",
        "categories": [
            "cs.CL",
            "cs.LG",
            "cs.NE"
        ],
        "link": "http://arxiv.org/pdf/1511.02506v1",
        "date": "2015-11-08 17:08:54+00:00"
    },
    {
        "title": "Is Robustness To Transformations Driven by Invariant Neural Representations?",
        "authors": [
            "Syed Suleman Abbas Zaidi",
            "Xavier Boix",
            "Neeraj Prasad",
            "Sharon Gilad-Gutnick",
            "Shlomit Ben-Ami",
            "Pawan Sinha"
        ],
        "abstract": "Deep Convolutional Neural Networks (DCNNs) have demonstrated impressive\nrobustness to recognize objects under transformations (e.g. blur or noise) when\nthese transformations are included in the training set. A hypothesis to explain\nsuch robustness is that DCNNs develop invariant neural representations that\nremain unaltered when the image is transformed. Yet, to what extent this\nhypothesis holds true is an outstanding question, as including transformations\nin the training set could lead to properties different from invariance, e.g.\nparts of the network could be specialized to recognize either transformed or\nnon-transformed images. In this paper, we analyze the conditions under which\ninvariance emerges. To do so, we leverage that invariant representations\nfacilitate robustness to transformations for object categories that are not\nseen transformed during training. Our results with state-of-the-art DCNNs\nindicate that invariant representations strengthen as the number of transformed\ncategories in the training set is increased. This is much more prominent with\nlocal transformations such as blurring and high-pass filtering, compared to\ngeometric transformations such as rotation and thinning, that entail changes in\nthe spatial arrangement of the object. Our results contribute to a better\nunderstanding of invariant representations in deep learning, and the conditions\nunder which invariance spontaneously emerges.",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2007.00112v2",
        "date": "2020-06-30 21:18:08+00:00"
    },
    {
        "title": "Increasing Depth Leads to U-Shaped Test Risk in Over-parameterized Convolutional Networks",
        "authors": [
            "Eshaan Nichani",
            "Adityanarayanan Radhakrishnan",
            "Caroline Uhler"
        ],
        "abstract": "Recent works have demonstrated that increasing model capacity through width\nin over-parameterized neural networks leads to a decrease in test risk. For\nneural networks, however, model capacity can also be increased through depth,\nyet understanding the impact of increasing depth on test risk remains an open\nquestion. In this work, we demonstrate that the test risk of over-parameterized\nconvolutional networks is a U-shaped curve (i.e. monotonically decreasing, then\nincreasing) with increasing depth. We first provide empirical evidence for this\nphenomenon via image classification experiments using both ResNets and the\nconvolutional neural tangent kernel (CNTK). We then present a novel linear\nregression framework for characterizing the impact of depth on test risk, and\nshow that increasing depth leads to a U-shaped test risk for the linear CNTK.\nIn particular, we prove that the linear CNTK corresponds to a depth-dependent\nlinear transformation on the original space and characterize properties of this\ntransformation. We then analyze over-parameterized linear regression under\narbitrary linear transformations and, in simplified settings, provably identify\nthe depths which minimize each of the bias and variance terms of the test risk.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2010.09610v2",
        "date": "2020-10-19 15:46:48+00:00"
    },
    {
        "title": "Sharing Generative Models Instead of Private Data: A Simulation Study on Mammography Patch Classification",
        "authors": [
            "Zuzanna Szafranowska",
            "Richard Osuala",
            "Bennet Breier",
            "Kaisar Kushibar",
            "Karim Lekadir",
            "Oliver Diaz"
        ],
        "abstract": "Early detection of breast cancer in mammography screening via deep-learning\nbased computer-aided detection systems shows promising potential in improving\nthe curability and mortality rates of breast cancer. However, many clinical\ncentres are restricted in the amount and heterogeneity of available data to\ntrain such models to (i) achieve promising performance and to (ii) generalise\nwell across acquisition protocols and domains. As sharing data between centres\nis restricted due to patient privacy concerns, we propose a potential solution:\nsharing trained generative models between centres as substitute for real\npatient data. In this work, we use three well known mammography datasets to\nsimulate three different centres, where one centre receives the trained\ngenerator of Generative Adversarial Networks (GANs) from the two remaining\ncentres in order to augment the size and heterogeneity of its training dataset.\nWe evaluate the utility of this approach on mammography patch classification on\nthe test set of the GAN-receiving centre using two different classification\nmodels, (a) a convolutional neural network and (b) a transformer neural\nnetwork. Our experiments demonstrate that shared GANs notably increase the\nperformance of both transformer and convolutional classification models and\nhighlight this approach as a viable alternative to inter-centre data sharing.",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV",
            "cs.LG",
            "I.2.0; I.4.0; I.5.0; J.3"
        ],
        "link": "http://arxiv.org/pdf/2203.04961v2",
        "date": "2022-03-08 19:37:08+00:00"
    },
    {
        "title": "Improving the Trainability of Deep Neural Networks through Layerwise Batch-Entropy Regularization",
        "authors": [
            "David Peer",
            "Bart Keulen",
            "Sebastian Stabinger",
            "Justus Piater",
            "Antonio Rodr\u00edguez-S\u00e1nchez"
        ],
        "abstract": "Training deep neural networks is a very demanding task, especially\nchallenging is how to adapt architectures to improve the performance of trained\nmodels. We can find that sometimes, shallow networks generalize better than\ndeep networks, and the addition of more layers results in higher training and\ntest errors. The deep residual learning framework addresses this degradation\nproblem by adding skip connections to several neural network layers. It would\nat first seem counter-intuitive that such skip connections are needed to train\ndeep networks successfully as the expressivity of a network would grow\nexponentially with depth. In this paper, we first analyze the flow of\ninformation through neural networks. We introduce and evaluate the\nbatch-entropy which quantifies the flow of information through each layer of a\nneural network. We prove empirically and theoretically that a positive\nbatch-entropy is required for gradient descent-based training approaches to\noptimize a given loss function successfully. Based on those insights, we\nintroduce batch-entropy regularization to enable gradient descent-based\ntraining algorithms to optimize the flow of information through each hidden\nlayer individually. With batch-entropy regularization, gradient descent\noptimizers can transform untrainable networks into trainable networks. We show\nempirically that we can therefore train a \"vanilla\" fully connected network and\nconvolutional neural network -- no skip connections, batch normalization,\ndropout, or any other architectural tweak -- with 500 layers by simply adding\nthe batch-entropy regularization term to the loss function. The effect of\nbatch-entropy regularization is not only evaluated on vanilla neural networks,\nbut also on residual networks, autoencoders, and also transformer models over a\nwide range of computer vision as well as natural language processing tasks.",
        "categories": [
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2208.01134v1",
        "date": "2022-08-01 20:31:58+00:00"
    },
    {
        "title": "Architecture Agnostic Federated Learning for Neural Networks",
        "authors": [
            "Disha Makhija",
            "Xing Han",
            "Nhat Ho",
            "Joydeep Ghosh"
        ],
        "abstract": "With growing concerns regarding data privacy and rapid increase in data\nvolume, Federated Learning(FL) has become an important learning paradigm.\nHowever, jointly learning a deep neural network model in a FL setting proves to\nbe a non-trivial task because of the complexities associated with the neural\nnetworks, such as varied architectures across clients, permutation invariance\nof the neurons, and presence of non-linear transformations in each layer. This\nwork introduces a novel Federated Heterogeneous Neural Networks (FedHeNN)\nframework that allows each client to build a personalised model without\nenforcing a common architecture across clients. This allows each client to\noptimize with respect to local data and compute constraints, while still\nbenefiting from the learnings of other (potentially more powerful) clients. The\nkey idea of FedHeNN is to use the instance-level representations obtained from\npeer clients to guide the simultaneous training on each client. The extensive\nexperimental results demonstrate that the FedHeNN framework is capable of\nlearning better performing models on clients in both the settings of\nhomogeneous and heterogeneous architectures across clients.",
        "categories": [
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2202.07757v3",
        "date": "2022-02-15 22:16:06+00:00"
    },
    {
        "title": "Show me your NFT and I tell you how it will perform: Multimodal representation learning for NFT selling price prediction",
        "authors": [
            "Davide Costa",
            "Lucio La Cava",
            "Andrea Tagarelli"
        ],
        "abstract": "Non-Fungible Tokens (NFTs) represent deeds of ownership, based on blockchain\ntechnologies and smart contracts, of unique crypto assets on digital art forms\n(e.g., artworks or collectibles). In the spotlight after skyrocketing in 2021,\nNFTs have attracted the attention of crypto enthusiasts and investors intent on\nplacing promising investments in this profitable market. However, the NFT\nfinancial performance prediction has not been widely explored to date.\n  In this work, we address the above problem based on the hypothesis that NFT\nimages and their textual descriptions are essential proxies to predict the NFT\nselling prices. To this purpose, we propose MERLIN, a novel multimodal deep\nlearning framework designed to train Transformer-based language and visual\nmodels, along with graph neural network models, on collections of NFTs' images\nand texts. A key aspect in MERLIN is its independence on financial features, as\nit exploits only the primary data a user interested in NFT trading would like\nto deal with, i.e., NFT images and textual descriptions. By learning dense\nrepresentations of such data, a price-category classification task is performed\nby MERLIN models, which can also be tuned according to user preferences in the\ninference phase to mimic different risk-return investment profiles.\nExperimental evaluation on a publicly available dataset has shown that MERLIN\nmodels achieve significant performances according to several financial\nassessment criteria, fostering profitable investments, and also beating\nbaseline machine-learning classifiers based on financial features.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.NE"
        ],
        "link": "http://arxiv.org/pdf/2302.01676v2",
        "date": "2023-02-03 11:56:38+00:00"
    },
    {
        "title": "TransDreamer: Reinforcement Learning with Transformer World Models",
        "authors": [
            "Chang Chen",
            "Yi-Fu Wu",
            "Jaesik Yoon",
            "Sungjin Ahn"
        ],
        "abstract": "The Dreamer agent provides various benefits of Model-Based Reinforcement\nLearning (MBRL) such as sample efficiency, reusable knowledge, and safe\nplanning. However, its world model and policy networks inherit the limitations\nof recurrent neural networks and thus an important question is how an MBRL\nframework can benefit from the recent advances of transformers and what the\nchallenges are in doing so. In this paper, we propose a transformer-based MBRL\nagent, called TransDreamer. We first introduce the Transformer State-Space\nModel, a world model that leverages a transformer for dynamics predictions. We\nthen share this world model with a transformer-based policy network and obtain\nstability in training a transformer-based RL agent. In experiments, we apply\nthe proposed model to 2D visual RL and 3D first-person visual RL tasks both\nrequiring long-range memory access for memory-based reasoning. We show that the\nproposed model outperforms Dreamer in these complex tasks.",
        "categories": [
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2202.09481v1",
        "date": "2022-02-19 00:30:52+00:00"
    },
    {
        "title": "Tasks, stability, architecture, and compute: Training more effective learned optimizers, and using them to train themselves",
        "authors": [
            "Luke Metz",
            "Niru Maheswaranathan",
            "C. Daniel Freeman",
            "Ben Poole",
            "Jascha Sohl-Dickstein"
        ],
        "abstract": "Much as replacing hand-designed features with learned functions has\nrevolutionized how we solve perceptual tasks, we believe learned algorithms\nwill transform how we train models. In this work we focus on general-purpose\nlearned optimizers capable of training a wide variety of problems with no\nuser-specified hyperparameters. We introduce a new, neural network\nparameterized, hierarchical optimizer with access to additional features such\nas validation loss to enable automatic regularization. Most learned optimizers\nhave been trained on only a single task, or a small number of tasks. We train\nour optimizers on thousands of tasks, making use of orders of magnitude more\ncompute, resulting in optimizers that generalize better to unseen tasks. The\nlearned optimizers not only perform well, but learn behaviors that are distinct\nfrom existing first order optimizers. For instance, they generate update steps\nthat have implicit regularization and adapt as the problem hyperparameters\n(e.g. batch size) or architecture (e.g. neural network width) change. Finally,\nthese learned optimizers show evidence of being useful for out of distribution\ntasks such as training themselves from scratch.",
        "categories": [
            "cs.LG",
            "cs.NE",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2009.11243v1",
        "date": "2020-09-23 16:35:09+00:00"
    },
    {
        "title": "Raw Produce Quality Detection with Shifted Window Self-Attention",
        "authors": [
            "Oh Joon Kwon",
            "Byungsoo Kim",
            "Youngduck Choi"
        ],
        "abstract": "Global food insecurity is expected to worsen in the coming decades with the\naccelerated rate of climate change and the rapidly increasing population. In\nthis vein, it is important to remove inefficiencies at every level of food\nproduction. The recent advances in deep learning can help reduce such\ninefficiencies, yet their application has not yet become mainstream throughout\nthe industry, inducing economic costs at a massive scale. To this point, modern\ntechniques such as CNNs (Convolutional Neural Networks) have been applied to\nRPQD (Raw Produce Quality Detection) tasks. On the other hand, Transformer's\nsuccessful debut in the vision among other modalities led us to expect a better\nperformance with these Transformer-based models in RPQD. In this work, we\nexclusively investigate the recent state-of-the-art Swin (Shifted Windows)\nTransformer which computes self-attention in both intra- and inter-window\nfashion. We compare Swin Transformer against CNN models on four RPQD image\ndatasets, each containing different kinds of raw produce: fruits and\nvegetables, fish, pork, and beef. We observe that Swin Transformer not only\nachieves better or competitive performance but also is data- and\ncompute-efficient, making it ideal for actual deployment in real-world setting.\nTo the best of our knowledge, this is the first large-scale empirical study on\nRPQD task, which we hope will gain more attention in future works.",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2112.13845v1",
        "date": "2021-12-24 10:16:28+00:00"
    },
    {
        "title": "COSET: A Benchmark for Evaluating Neural Program Embeddings",
        "authors": [
            "Ke Wang",
            "Mihai Christodorescu"
        ],
        "abstract": "Neural program embedding can be helpful in analyzing large software, a task\nthat is challenging for traditional logic-based program analyses due to their\nlimited scalability. A key focus of recent machine-learning advances in this\narea is on modeling program semantics instead of just syntax. Unfortunately\nevaluating such advances is not obvious, as program semantics does not lend\nitself to straightforward metrics. In this paper, we introduce a benchmarking\nframework called COSET for standardizing the evaluation of neural program\nembeddings. COSET consists of a diverse dataset of programs in source-code\nformat, labeled by human experts according to a number of program properties of\ninterest. A point of novelty is a suite of program transformations included in\nCOSET. These transformations when applied to the base dataset can simulate\nnatural changes to program code due to optimization and refactoring and can\nserve as a \"debugging\" tool for classification mistakes. We conducted a pilot\nstudy on four prominent models: TreeLSTM, gated graph neural network (GGNN),\nAST-Path neural network (APNN), and DYPRO. We found that COSET is useful in\nidentifying the strengths and limitations of each model and in pinpointing\nspecific syntactic and semantic characteristics of programs that pose\nchallenges.",
        "categories": [
            "cs.LG",
            "cs.PL",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1905.11445v1",
        "date": "2019-05-27 18:44:54+00:00"
    },
    {
        "title": "Clustering-Based Representation Learning through Output Translation and Its Application to Remote--Sensing Images",
        "authors": [
            "Qinglin Li",
            "Bin Li",
            "Jonathan M Garibaldi",
            "Guoping Qiu"
        ],
        "abstract": "In supervised deep learning, learning good representations for\nremote--sensing images (RSI) relies on manual annotations. However, in the area\nof remote sensing, it is hard to obtain huge amounts of labeled data. Recently,\nself--supervised learning shows its outstanding capability to learn\nrepresentations of images, especially the methods of instance discrimination.\nComparing methods of instance discrimination, clustering--based methods not\nonly view the transformations of the same image as ``positive\" samples but also\nsimilar images. In this paper, we propose a new clustering-based method for\nrepresentation learning. We first introduce a quantity to measure\nrepresentations' discriminativeness and from which we show that even\ndistribution requires the most discriminative representations. This provides a\ntheoretical insight into why evenly distributing the images works well. We\nnotice that only the even distributions that preserve representations'\nneighborhood relations are desirable. Therefore, we develop an algorithm that\ntranslates the outputs of a neural network to achieve the goal of evenly\ndistributing the samples while preserving outputs' neighborhood relations.\nExtensive experiments have demonstrated that our method can learn\nrepresentations that are as good as or better than the state of the art\napproaches, and that our method performs computationally efficiently and\nrobustly on various RSI datasets.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "link": "http://arxiv.org/pdf/2107.05948v4",
        "date": "2021-07-13 09:39:43+00:00"
    },
    {
        "title": "CVNets: High Performance Library for Computer Vision",
        "authors": [
            "Sachin Mehta",
            "Farzad Abdolhosseini",
            "Mohammad Rastegari"
        ],
        "abstract": "We introduce CVNets, a high-performance open-source library for training deep\nneural networks for visual recognition tasks, including classification,\ndetection, and segmentation. CVNets supports image and video understanding\ntools, including data loading, data transformations, novel data sampling\nmethods, and implementations of several standard networks with similar or\nbetter performance than previous studies.\n  Our source code is available at: \\url{https://github.com/apple/ml-cvnets}.",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2206.02002v1",
        "date": "2022-06-04 14:55:24+00:00"
    },
    {
        "title": "A Transfer Principle: Universal Approximators Between Metric Spaces From Euclidean Universal Approximators",
        "authors": [
            "Anastasis Kratsios",
            "Chong Liu",
            "Matti Lassas",
            "Maarten V. de Hoop",
            "Ivan Dokmani\u0107"
        ],
        "abstract": "We build universal approximators of continuous maps between arbitrary Polish\nmetric spaces $\\mathcal{X}$ and $\\mathcal{Y}$ using universal approximators\nbetween Euclidean spaces as building blocks. Earlier results assume that the\noutput space $\\mathcal{Y}$ is a topological vector space. We overcome this\nlimitation by \"randomization\": our approximators output discrete probability\nmeasures over $\\mathcal{Y}$. When $\\mathcal{X}$ and $\\mathcal{Y}$ are Polish\nwithout additional structure, we prove very general qualitative guarantees;\nwhen they have suitable combinatorial structure, we prove quantitative\nguarantees for H\\\"older-like maps, including maps between finite graphs,\nsolution operators to rough differential equations between certain Carnot\ngroups, and continuous non-linear operators between Banach spaces arising in\ninverse problems. In particular, we show that the required number of Dirac\nmeasures is determined by the combinatorial structure of $\\mathcal{X}$ and\n$\\mathcal{Y}$. For barycentric $\\mathcal{Y}$, including Banach spaces,\n$\\mathbb{R}$-trees, Hadamard manifolds, or Wasserstein spaces on Polish metric\nspaces, our approximators reduce to $\\mathcal{Y}$-valued functions. When the\nEuclidean approximators are neural networks, our constructions generalize\ntransformer networks, providing a new probabilistic viewpoint of geometric deep\nlearning.",
        "categories": [
            "cs.LG",
            "cs.NA",
            "cs.NE",
            "math.NA",
            "math.PR",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2304.12231v1",
        "date": "2023-04-24 16:18:22+00:00"
    },
    {
        "title": "Accelerated Linearized Laplace Approximation for Bayesian Deep Learning",
        "authors": [
            "Zhijie Deng",
            "Feng Zhou",
            "Jun Zhu"
        ],
        "abstract": "Laplace approximation (LA) and its linearized variant (LLA) enable effortless\nadaptation of pretrained deep neural networks to Bayesian neural networks. The\ngeneralized Gauss-Newton (GGN) approximation is typically introduced to improve\ntheir tractability. However, LA and LLA are still confronted with non-trivial\ninefficiency issues and should rely on Kronecker-factored, diagonal, or even\nlast-layer approximate GGN matrices in practical use. These approximations are\nlikely to harm the fidelity of learning outcomes. To tackle this issue,\ninspired by the connections between LLA and neural tangent kernels (NTKs), we\ndevelop a Nystrom approximation to NTKs to accelerate LLA. Our method benefits\nfrom the capability of popular deep learning libraries for forward mode\nautomatic differentiation, and enjoys reassuring theoretical guarantees.\nExtensive studies reflect the merits of the proposed method in aspects of both\nscalability and performance. Our method can even scale up to architectures like\nvision transformers. We also offer valuable ablation studies to diagnose our\nmethod. Code is available at \\url{https://github.com/thudzj/ELLA}.",
        "categories": [
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2210.12642v1",
        "date": "2022-10-23 07:49:03+00:00"
    },
    {
        "title": "Deep PDF: Probabilistic Surface Optimization and Density Estimation",
        "authors": [
            "Dmitry Kopitkov",
            "Vadim Indelman"
        ],
        "abstract": "A probability density function (pdf) encodes the entire stochastic knowledge\nabout data distribution, where data may represent stochastic observations in\nrobotics, transition state pairs in reinforcement learning or any other\nempirically acquired modality. Inferring data pdf is of prime importance,\nallowing to analyze various model hypotheses and perform smart decision making.\nHowever, most density estimation techniques are limited in their representation\nexpressiveness to specific kernel type or predetermined distribution family,\nand have other restrictions. For example, kernel density estimation (KDE)\nmethods require meticulous parameter search and are extremely slow at querying\nnew points. In this paper we present a novel non-parametric density estimation\napproach, DeepPDF, that uses a neural network to approximate a target pdf given\nsamples from thereof. Such a representation provides high inference accuracy\nfor a wide range of target pdfs using a relatively simple network structure,\nmaking our method highly statistically robust. This is done via a new\nstochastic optimization algorithm, \\emph{Probabilistic Surface Optimization}\n(PSO), that turns to advantage the stochastic nature of sample points in order\nto force network output to be identical to the output of a target pdf. Once\ntrained, query point evaluation can be efficiently done in DeepPDF by a simple\nnetwork forward pass, with linear complexity in the number of query points.\nMoreover, the PSO algorithm is capable of inferring the frequency of data\nsamples and may also be used in other statistical tasks such as conditional\nestimation and distribution transformation. We compare the derived approach\nwith KDE methods showing its superior performance and accuracy.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1807.10728v2",
        "date": "2018-07-27 16:55:33+00:00"
    },
    {
        "title": "Similarity and Matching of Neural Network Representations",
        "authors": [
            "Adri\u00e1n Csisz\u00e1rik",
            "P\u00e9ter K\u0151r\u00f6si-Szab\u00f3",
            "\u00c1kos K. Matszangosz",
            "Gergely Papp",
            "D\u00e1niel Varga"
        ],
        "abstract": "We employ a toolset -- dubbed Dr. Frankenstein -- to analyse the similarity\nof representations in deep neural networks. With this toolset, we aim to match\nthe activations on given layers of two trained neural networks by joining them\nwith a stitching layer. We demonstrate that the inner representations emerging\nin deep convolutional neural networks with the same architecture but different\ninitializations can be matched with a surprisingly high degree of accuracy even\nwith a single, affine stitching layer. We choose the stitching layer from\nseveral possible classes of linear transformations and investigate their\nperformance and properties. The task of matching representations is closely\nrelated to notions of similarity. Using this toolset, we also provide a novel\nviewpoint on the current line of research regarding similarity indices of\nneural network representations: the perspective of the performance on a task.",
        "categories": [
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2110.14633v1",
        "date": "2021-10-27 17:59:46+00:00"
    },
    {
        "title": "Automatic diagnosis of the 12-lead ECG using a deep neural network",
        "authors": [
            "Ant\u00f4nio H. Ribeiro",
            "Manoel Horta Ribeiro",
            "Gabriela M. M. Paix\u00e3o",
            "Derick M. Oliveira",
            "Paulo R. Gomes",
            "J\u00e9ssica A. Canazart",
            "Milton P. S. Ferreira",
            "Carl R. Andersson",
            "Peter W. Macfarlane",
            "Wagner Meira Jr.",
            "Thomas B. Sch\u00f6n",
            "Antonio Luiz P. Ribeiro"
        ],
        "abstract": "The role of automatic electrocardiogram (ECG) analysis in clinical practice\nis limited by the accuracy of existing models. Deep Neural Networks (DNNs) are\nmodels composed of stacked transformations that learn tasks by examples. This\ntechnology has recently achieved striking success in a variety of task and\nthere are great expectations on how it might improve clinical practice. Here we\npresent a DNN model trained in a dataset with more than 2 million labeled exams\nanalyzed by the Telehealth Network of Minas Gerais and collected under the\nscope of the CODE (Clinical Outcomes in Digital Electrocardiology) study. The\nDNN outperform cardiology resident medical doctors in recognizing 6 types of\nabnormalities in 12-lead ECG recordings, with F1 scores above 80% and\nspecificity over 99%. These results indicate ECG analysis based on DNNs,\npreviously studied in a single-lead setup, generalizes well to 12-lead exams,\ntaking the technology closer to the standard clinical practice.",
        "categories": [
            "cs.LG",
            "eess.SP",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1904.01949v2",
        "date": "2019-04-02 03:20:08+00:00"
    },
    {
        "title": "Latent Transformations for Discrete-Data Normalising Flows",
        "authors": [
            "Rob Hesselink",
            "Wilker Aziz"
        ],
        "abstract": "Normalising flows (NFs) for discrete data are challenging because\nparameterising bijective transformations of discrete variables requires\npredicting discrete/integer parameters. Having a neural network architecture\npredict discrete parameters takes a non-differentiable activation function (eg,\nthe step function) which precludes gradient-based learning. To circumvent this\nnon-differentiability, previous work has employed biased proxy gradients, such\nas the straight-through estimator. We present an unbiased alternative where\nrather than deterministically parameterising one transformation, we predict a\ndistribution over latent transformations. With stochastic transformations, the\nmarginal likelihood of the data is differentiable and gradient-based learning\nis possible via score function estimation. To test the viability of\ndiscrete-data NFs we investigate performance on binary MNIST. We observe great\nchallenges with both deterministic proxy gradients and unbiased score function\nestimation. Whereas the former often fails to learn even a shallow\ntransformation, the variance of the latter could not be sufficiently controlled\nto admit deeper NFs.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2006.06346v1",
        "date": "2020-06-11 11:41:28+00:00"
    },
    {
        "title": "A Perturbation Resistant Transformation and Classification System for Deep Neural Networks",
        "authors": [
            "Nathaniel Dean",
            "Dilip Sarkar"
        ],
        "abstract": "Deep convolutional neural networks accurately classify a diverse range of\nnatural images, but may be easily deceived when designed, imperceptible\nperturbations are embedded in the images. In this paper, we design a\nmulti-pronged training, input transformation, and image ensemble system that is\nattack agnostic and not easily estimated. Our system incorporates two novel\nfeatures. The first is a transformation layer that computes feature level\npolynomial kernels from class-level training data samples and iteratively\nupdates input image copies at inference time based on their feature kernel\ndifferences to create an ensemble of transformed inputs. The second is a\nclassification system that incorporates the prediction of the undefended\nnetwork with a hard vote on the ensemble of filtered images. Our evaluations on\nthe CIFAR10 dataset show our system improves the robustness of an undefended\nnetwork against a variety of bounded and unbounded white-box attacks under\ndifferent distance metrics, while sacrificing little accuracy on clean images.\nAgainst adaptive full-knowledge attackers creating end-to-end attacks, our\nsystem successfully augments the existing robustness of adversarially trained\nnetworks, for which our methods are most effectively applied.",
        "categories": [
            "cs.CV",
            "cs.LG",
            "cs.NE",
            "I.5.1"
        ],
        "link": "http://arxiv.org/pdf/2208.11839v1",
        "date": "2022-08-25 02:58:47+00:00"
    },
    {
        "title": "Comparative Performance Analysis of Neural Networks Architectures on H2O Platform for Various Activation Functions",
        "authors": [
            "Yuriy Kochura",
            "Sergii Stirenko",
            "Yuri Gordienko"
        ],
        "abstract": "Deep learning (deep structured learning, hierarchi- cal learning or deep\nmachine learning) is a branch of machine learning based on a set of algorithms\nthat attempt to model high- level abstractions in data by using multiple\nprocessing layers with complex structures or otherwise composed of multiple\nnon-linear transformations. In this paper, we present the results of testing\nneural networks architectures on H2O platform for various activation functions,\nstopping metrics, and other parameters of machine learning algorithm. It was\ndemonstrated for the use case of MNIST database of handwritten digits in\nsingle-threaded mode that blind selection of these parameters can hugely\nincrease (by 2-3 orders) the runtime without the significant increase of\nprecision. This result can have crucial influence for opitmization of available\nand new machine learning methods, especially for image recognition problems.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "cs.PF"
        ],
        "link": "http://arxiv.org/pdf/1707.04940v1",
        "date": "2017-07-16 19:57:28+00:00"
    },
    {
        "title": "Geometric Scattering on Measure Spaces",
        "authors": [
            "Joyce Chew",
            "Matthew Hirn",
            "Smita Krishnaswamy",
            "Deanna Needell",
            "Michael Perlmutter",
            "Holly Steach",
            "Siddharth Viswanath",
            "Hau-Tieng Wu"
        ],
        "abstract": "The scattering transform is a multilayered, wavelet-based transform initially\nintroduced as a model of convolutional neural networks (CNNs) that has played a\nfoundational role in our understanding of these networks' stability and\ninvariance properties. Subsequently, there has been widespread interest in\nextending the success of CNNs to data sets with non-Euclidean structure, such\nas graphs and manifolds, leading to the emerging field of geometric deep\nlearning. In order to improve our understanding of the architectures used in\nthis new field, several papers have proposed generalizations of the scattering\ntransform for non-Euclidean data structures such as undirected graphs and\ncompact Riemannian manifolds without boundary.\n  In this paper, we introduce a general, unified model for geometric scattering\non measure spaces. Our proposed framework includes previous work on geometric\nscattering as special cases but also applies to more general settings such as\ndirected graphs, signed graphs, and manifolds with boundary. We propose a new\ncriterion that identifies to which groups a useful representation should be\ninvariant and show that this criterion is sufficient to guarantee that the\nscattering transform has desirable stability and invariance properties.\nAdditionally, we consider finite measure spaces that are obtained from randomly\nsampling an unknown manifold. We propose two methods for constructing a\ndata-driven graph on which the associated graph scattering transform\napproximates the scattering transform on the underlying manifold. Moreover, we\nuse a diffusion-maps based approach to prove quantitative estimates on the rate\nof convergence of one of these approximations as the number of sample points\ntends to infinity. Lastly, we showcase the utility of our method on spherical\nimages, directed graphs, and on high-dimensional single-cell data.",
        "categories": [
            "stat.ML",
            "cs.LG",
            "math.SP",
            "68T07"
        ],
        "link": "http://arxiv.org/pdf/2208.08561v2",
        "date": "2022-08-17 22:40:09+00:00"
    },
    {
        "title": "Koopman operator for time-dependent reliability analysis",
        "authors": [
            "Navaneeth N.",
            "Souvik Chakraborty"
        ],
        "abstract": "Time-dependent structural reliability analysis of nonlinear dynamical systems\nis non-trivial; subsequently, scope of most of the structural reliability\nanalysis methods is limited to time-independent reliability analysis only. In\nthis work, we propose a Koopman operator based approach for time-dependent\nreliability analysis of nonlinear dynamical systems. Since the Koopman\nrepresentations can transform any nonlinear dynamical system into a linear\ndynamical system, the time evolution of dynamical systems can be obtained by\nKoopman operators seamlessly regardless of the nonlinear or chaotic behavior.\nDespite the fact that the Koopman theory has been in vogue a long time back,\nidentifying intrinsic coordinates is a challenging task; to address this, we\npropose an end-to-end deep learning architecture that learns the Koopman\nobservables and then use it for time marching the dynamical response. Unlike\npurely data-driven approaches, the proposed approach is robust even in the\npresence of uncertainties; this renders the proposed approach suitable for\ntime-dependent reliability analysis. We propose two architectures; one suitable\nfor time-dependent reliability analysis when the system is subjected to random\ninitial condition and the other suitable when the underlying system have\nuncertainties in system parameters. The proposed approach is robust and\ngeneralizes to unseen environment (out-of-distribution prediction). Efficacy of\nthe proposed approached is illustrated using three numerical examples. Results\nobtained indicate supremacy of the proposed approach as compared to purely\ndata-driven auto-regressive neural network and long-short term memory network.",
        "categories": [
            "stat.ML",
            "cs.LG",
            "cs.SY"
        ],
        "link": "http://arxiv.org/pdf/2203.02658v2",
        "date": "2022-03-05 04:57:20+00:00"
    },
    {
        "title": "Neural Nets via Forward State Transformation and Backward Loss Transformation",
        "authors": [
            "Bart Jacobs",
            "David Sprunger"
        ],
        "abstract": "This article studies (multilayer perceptron) neural networks with an emphasis\non the transformations involved --- both forward and backward --- in order to\ndevelop a semantical/logical perspective that is in line with standard program\nsemantics. The common two-pass neural network training algorithms make this\nviewpoint particularly fitting. In the forward direction, neural networks act\nas state transformers. In the reverse direction, however, neural networks\nchange losses of outputs to losses of inputs, thereby acting like a\n(real-valued) predicate transformer. In this way, backpropagation is functorial\nby construction, as shown earlier in recent other work. We illustrate this\nperspective by training a simple instance of a neural network.",
        "categories": [
            "cs.NE",
            "cs.LG",
            "92B20 (Primary) 18C50 (Secondary)",
            "C.1.3; F.3.2"
        ],
        "link": "http://arxiv.org/pdf/1803.09356v1",
        "date": "2018-03-25 22:01:32+00:00"
    },
    {
        "title": "Unsupervised Learning of Visual Structure using Predictive Generative Networks",
        "authors": [
            "William Lotter",
            "Gabriel Kreiman",
            "David Cox"
        ],
        "abstract": "The ability to predict future states of the environment is a central pillar\nof intelligence. At its core, effective prediction requires an internal model\nof the world and an understanding of the rules by which the world changes.\nHere, we explore the internal models developed by deep neural networks trained\nusing a loss based on predicting future frames in synthetic video sequences,\nusing a CNN-LSTM-deCNN framework. We first show that this architecture can\nachieve excellent performance in visual sequence prediction tasks, including\nstate-of-the-art performance in a standard 'bouncing balls' dataset (Sutskever\net al., 2009). Using a weighted mean-squared error and adversarial loss\n(Goodfellow et al., 2014), the same architecture successfully extrapolates\nout-of-the-plane rotations of computer-generated faces. Furthermore, despite\nbeing trained end-to-end to predict only pixel-level information, our\nPredictive Generative Networks learn a representation of the latent structure\nof the underlying three-dimensional objects themselves. Importantly, we find\nthat this representation is naturally tolerant to object transformations, and\ngeneralizes well to new tasks, such as classification of static images. Similar\nmodels trained solely with a reconstruction loss fail to generalize as\neffectively. We argue that prediction can serve as a powerful unsupervised loss\nfor learning rich internal representations of high-level object features.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "q-bio.NC"
        ],
        "link": "http://arxiv.org/pdf/1511.06380v2",
        "date": "2015-11-19 21:10:17+00:00"
    },
    {
        "title": "Reference Product Search",
        "authors": [
            "Chu Wang",
            "Lei Tang",
            "Shujun Bian",
            "Da Zhang",
            "Zuohua Zhang",
            "Yongning Wu"
        ],
        "abstract": "For a product of interest, we propose a search method to surface a set of\nreference products. The reference products can be used as candidates to support\ndownstream modeling tasks and business applications. The search method consists\nof product representation learning and fingerprint-type vector searching. The\nproduct catalog information is transformed into a high-quality embedding of low\ndimensions via a novel attention auto-encoder neural network, and the embedding\nis further coupled with a binary encoding vector for fast retrieval. We conduct\nextensive experiments to evaluate the proposed method, and compare it with peer\nservices to demonstrate its advantage in terms of search return rate and\nprecision.",
        "categories": [
            "stat.ML",
            "cs.IR",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1904.05985v1",
        "date": "2019-04-11 23:47:01+00:00"
    },
    {
        "title": "TensorProjection Layer: A Tensor-Based Dimensionality Reduction Method in CNN",
        "authors": [
            "Toshinari Morimoto",
            "Su-Yun Huang"
        ],
        "abstract": "In this paper, we propose a dimensionality reduction method applied to\ntensor-structured data as a hidden layer (we call it TensorProjection Layer) in\na convolutional neural network. Our proposed method transforms input tensors\ninto ones with a smaller dimension by projection. The directions of projection\nare viewed as training parameters associated with our proposed layer and\ntrained via a supervised learning criterion such as minimization of the\ncross-entropy loss function. We discuss the gradients of the loss function with\nrespect to the parameters associated with our proposed layer. We also implement\nsimple numerical experiments to evaluate the performance of the\nTensorProjection Layer.",
        "categories": [
            "stat.ML",
            "cs.CV",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2004.04454v1",
        "date": "2020-04-09 09:52:49+00:00"
    },
    {
        "title": "Information Theory Measures via Multidimensional Gaussianization",
        "authors": [
            "Valero Laparra",
            "J. Emmanuel Johnson",
            "Gustau Camps-Valls",
            "Raul Santos-Rodr\u00edguez",
            "Jesus Malo"
        ],
        "abstract": "Information theory is an outstanding framework to measure uncertainty,\ndependence and relevance in data and systems. It has several desirable\nproperties for real world applications: it naturally deals with multivariate\ndata, it can handle heterogeneous data types, and the measures can be\ninterpreted in physical units. However, it has not been adopted by a wider\naudience because obtaining information from multidimensional data is a\nchallenging problem due to the curse of dimensionality. Here we propose an\nindirect way of computing information based on a multivariate Gaussianization\ntransform. Our proposal mitigates the difficulty of multivariate density\nestimation by reducing it to a composition of tractable (marginal) operations\nand simple linear transformations, which can be interpreted as a particular\ndeep neural network. We introduce specific Gaussianization-based methodologies\nto estimate total correlation, entropy, mutual information and Kullback-Leibler\ndivergence. We compare them to recent estimators showing the accuracy on\nsynthetic data generated from different multivariate distributions. We made the\ntools and datasets publicly available to provide a test-bed to analyze future\nmethodologies. Results show that our proposal is superior to previous\nestimators particularly in high-dimensional scenarios; and that it leads to\ninteresting insights in neuroscience, geoscience, computer vision, and machine\nlearning.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2010.03807v2",
        "date": "2020-10-08 07:22:16+00:00"
    },
    {
        "title": "Molecular CT: Unifying Geometry and Representation Learning for Molecules at Different Scales",
        "authors": [
            "Jun Zhang",
            "Yaqiang Zhou",
            "Yao-Kun Lei",
            "Yi Isaac Yang",
            "Yi Qin Gao"
        ],
        "abstract": "Deep learning is changing many areas in molecular physics, and it has shown\ngreat potential to deliver new solutions to challenging molecular modeling\nproblems. Along with this trend arises the increasing demand of expressive and\nversatile neural network architectures which are compatible with molecular\nsystems. A new deep neural network architecture, Molecular Configuration\nTransformer (Molecular CT), is introduced for this purpose. Molecular CT is\ncomposed of a relation-aware encoder module and a computationally universal\ngeometry learning unit, thus able to account for the relational constraints\nbetween particles meanwhile scalable to different particle numbers and\ninvariant w.r.t. the trans-rotational transforms. The computational efficiency\nand universality make Molecular CT versatile for a variety of molecular\nlearning scenarios and especially appealing for transferable representation\nlearning across different molecular systems. As examples, we show that\nMolecular CT enables representational learning for molecular systems at\ndifferent scales, and achieves comparable or improved results on common\nbenchmarks using a more light-weighted structure compared to baseline models.",
        "categories": [
            "cs.LG",
            "cond-mat.soft"
        ],
        "link": "http://arxiv.org/pdf/2012.11816v2",
        "date": "2020-12-22 03:41:16+00:00"
    },
    {
        "title": "Non-technical Loss Detection with Statistical Profile Images Based on Semi-supervised Learning",
        "authors": [
            "Jiangteng Li",
            "Fei Wang"
        ],
        "abstract": "In order to keep track of the operational state of power grid, the world's\nlargest sensor systems, smart grid, was built by deploying hundreds of millions\nof smart meters. Such system makes it possible to discover and make quick\nresponse to any hidden threat to the entire power grid. Non-technical losses\n(NTLs) have always been a major concern for its consequent security risks as\nwell as immeasurable revenue loss. However, various causes of NTL may have\ndifferent characteristics reflected in the data. Accurately capturing these\nanomalies faced with such large scale of collected data records is rather\ntricky as a result. In this paper, we proposed a new methodology of detecting\nabnormal electricity consumptions. We did a transformation of the collected\ntime-series data which turns it into an image representation that could well\nreflect users' relatively long term consumption behaviors. Inspired by the\nexcellent neural network architecture used for objective detection in computer\nvision domain, we designed our deep learning model that takes the transformed\nimages as input and yields joint featured inferred from the multiple aspects\nthe input provides. Considering the limited labeled samples, especially the\nabnormal ones, we used our model in a semi-supervised fashion that is brought\nout in recent years. The model is tested on samples which are verified by\non-field inspections and our method showed significant improvement.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1907.03925v1",
        "date": "2019-07-09 01:06:03+00:00"
    },
    {
        "title": "A Scalable, Interpretable, Verifiable & Differentiable Logic Gate Convolutional Neural Network Architecture From Truth Tables",
        "authors": [
            "Adrien Benamira",
            "Tristan Gu\u00e9rand",
            "Thomas Peyrin",
            "Trevor Yap",
            "Bryan Hooi"
        ],
        "abstract": "We propose $\\mathcal{T}$ruth $\\mathcal{T}$able net ($\\mathcal{TT}$net), a\nnovel Convolutional Neural Network (CNN) architecture that addresses, by\ndesign, the open challenges of interpretability, formal verification, and logic\ngate conversion. $\\mathcal{TT}$net is built using CNNs' filters that are\nequivalent to tractable truth tables and that we call Learning Truth Table\n(LTT) blocks. The dual form of LTT blocks allows the truth tables to be easily\ntrained with gradient descent and makes these CNNs easy to interpret, verify\nand infer. Specifically, $\\mathcal{TT}$net is a deep CNN model that can be\nautomatically represented, after post-training transformation, as a sum of\nBoolean decision trees, or as a sum of Disjunctive/Conjunctive Normal Form\n(DNF/CNF) formulas, or as a compact Boolean logic circuit. We demonstrate the\neffectiveness and scalability of $\\mathcal{TT}$net on multiple datasets,\nshowing comparable interpretability to decision trees, fast complete/sound\nformal verification, and scalable logic gate representation, all compared to\nstate-of-the-art methods. We believe this work represents a step towards making\nCNNs more transparent and trustworthy for real-world critical applications.",
        "categories": [
            "cs.AI",
            "cs.FL",
            "cs.LG",
            "cs.SC"
        ],
        "link": "http://arxiv.org/pdf/2208.08609v3",
        "date": "2022-08-18 03:06:25+00:00"
    },
    {
        "title": "A deep learning-based remaining useful life prediction approach for bearings",
        "authors": [
            "Cheng Cheng",
            "Guijun Ma",
            "Yong Zhang",
            "Mingyang Sun",
            "Fei Teng",
            "Han Ding",
            "Ye Yuan"
        ],
        "abstract": "In industrial applications, nearly half the failures of motors are caused by\nthe degradation of rolling element bearings (REBs). Therefore, accurately\nestimating the remaining useful life (RUL) for REBs are of crucial importance\nto ensure the reliability and safety of mechanical systems. To tackle this\nchallenge, model-based approaches are often limited by the complexity of\nmathematical modeling. Conventional data-driven approaches, on the other hand,\nrequire massive efforts to extract the degradation features and construct\nhealth index. In this paper, a novel online data-driven framework is proposed\nto exploit the adoption of deep convolutional neural networks (CNN) in\npredicting the RUL of bearings. More concretely, the raw vibrations of training\nbearings are first processed using the Hilbert-Huang transform (HHT) and a\nnovel nonlinear degradation indicator is constructed as the label for learning.\nThe CNN is then employed to identify the hidden pattern between the extracted\ndegradation indicator and the vibration of training bearings, which makes it\npossible to estimate the degradation of the test bearings automatically.\nFinally, testing bearings' RULs are predicted by using a $\\epsilon$-support\nvector regression model. The superior performance of the proposed RUL\nestimation framework, compared with the state-of-the-art approaches, is\ndemonstrated through the experimental results. The generality of the proposed\nCNN model is also validated by transferring to bearings undergoing different\noperating conditions.",
        "categories": [
            "cs.LG",
            "eess.SP",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1812.03315v2",
        "date": "2018-12-08 12:56:37+00:00"
    },
    {
        "title": "Human Action Recognition Using Deep Multilevel Multimodal (M2) Fusion of Depth and Inertial Sensors",
        "authors": [
            "Zeeshan Ahmad",
            "Naimul Khan"
        ],
        "abstract": "Multimodal fusion frameworks for Human Action Recognition (HAR) using depth\nand inertial sensor data have been proposed over the years. In most of the\nexisting works, fusion is performed at a single level (feature level or\ndecision level), missing the opportunity to fuse rich mid-level features\nnecessary for better classification. To address this shortcoming, in this\npaper, we propose three novel deep multilevel multimodal fusion frameworks to\ncapitalize on different fusion strategies at various stages and to leverage the\nsuperiority of multilevel fusion. At input, we transform the depth data into\ndepth images called sequential front view images (SFIs) and inertial sensor\ndata into signal images. Each input modality, depth and inertial, is further\nmade multimodal by taking convolution with the Prewitt filter. Creating\n\"modality within modality\" enables further complementary and discriminative\nfeature extraction through Convolutional Neural Networks (CNNs). CNNs are\ntrained on input images of each modality to learn low-level, high-level and\ncomplex features. Learned features are extracted and fused at different stages\nof the proposed frameworks to combine discriminative and complementary\ninformation. These highly informative features are served as input to a\nmulti-class Support Vector Machine (SVM). We evaluate the proposed frameworks\non three publicly available multimodal HAR datasets, namely, UTD Multimodal\nHuman Action Dataset (MHAD), Berkeley MHAD, and UTD-MHAD Kinect V2.\nExperimental results show the supremacy of the proposed fusion frameworks over\nexisting methods.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1910.11482v1",
        "date": "2019-10-25 01:29:58+00:00"
    },
    {
        "title": "Learning Optimal Data Augmentation Policies via Bayesian Optimization for Image Classification Tasks",
        "authors": [
            "Chunxu Zhang",
            "Jiaxu Cui",
            "Bo Yang"
        ],
        "abstract": "In recent years, deep learning has achieved remarkable achievements in many\nfields, including computer vision, natural language processing, speech\nrecognition and others. Adequate training data is the key to ensure the\neffectiveness of the deep models. However, obtaining valid data requires a lot\nof time and labor resources. Data augmentation (DA) is an effective alternative\napproach, which can generate new labeled data based on existing data using\nlabel-preserving transformations. Although we can benefit a lot from DA,\ndesigning appropriate DA policies requires a lot of expert experience and time\nconsumption, and the evaluation of searching the optimal policies is costly. So\nwe raise a new question in this paper: how to achieve automated data\naugmentation at as low cost as possible? We propose a method named BO-Aug for\nautomating the process by finding the optimal DA policies using the Bayesian\noptimization approach. Our method can find the optimal policies at a relatively\nlow search cost, and the searched policies based on a specific dataset are\ntransferable across different neural network architectures or even different\ndatasets. We validate the BO-Aug on three widely used image classification\ndatasets, including CIFAR-10, CIFAR-100 and SVHN. Experimental results show\nthat the proposed method can achieve state-of-the-art or near advanced\nclassification accuracy. Code to reproduce our experiments is available at\nhttps://github.com/zhangxiaozao/BO-Aug.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1905.02610v2",
        "date": "2019-05-06 15:49:02+00:00"
    },
    {
        "title": "DETECT: Deep Trajectory Clustering for Mobility-Behavior Analysis",
        "authors": [
            "Mingxuan Yue",
            "Yaguang Li",
            "Haoze Yang",
            "Ritesh Ahuja",
            "Yao-Yi Chiang",
            "Cyrus Shahabi"
        ],
        "abstract": "Identifying mobility behaviors in rich trajectory data is of great economic\nand social interest to various applications including urban planning, marketing\nand intelligence. Existing work on trajectory clustering often relies on\nsimilarity measurements that utilize raw spatial and/or temporal information of\ntrajectories. These measures are incapable of identifying similar moving\nbehaviors that exhibit varying spatio-temporal scales of movement. In addition,\nthe expense of labeling massive trajectory data is a barrier to supervised\nlearning models. To address these challenges, we propose an unsupervised neural\napproach for mobility behavior clustering, called the Deep Embedded TrajEctory\nClusTering network (DETECT). DETECT operates in three parts: first it\ntransforms the trajectories by summarizing their critical parts and augmenting\nthem with context derived from their geographical locality (e.g., using POIs\nfrom gazetteers). In the second part, it learns a powerful representation of\ntrajectories in the latent space of behaviors, thus enabling a clustering\nfunction (such as $k$-means) to be applied. Finally, a clustering oriented loss\nis directly built on the embedded features to jointly perform feature\nrefinement and cluster assignment, thus improving separability between mobility\nbehaviors. Exhaustive quantitative and qualitative experiments on two\nreal-world datasets demonstrate the effectiveness of our approach for mobility\nbehavior analyses.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2003.01351v1",
        "date": "2020-03-03 06:09:15+00:00"
    },
    {
        "title": "Deep learning to discover and predict dynamics on an inertial manifold",
        "authors": [
            "Alec J. Linot",
            "Michael D. Graham"
        ],
        "abstract": "A data-driven framework is developed to represent chaotic dynamics on an\ninertial manifold (IM), and applied to solutions of the Kuramoto-Sivashinsky\nequation. A hybrid method combining linear and nonlinear (neural-network)\ndimension reduction transforms between coordinates in the full state space and\non the IM. Additional neural networks predict time-evolution on the IM. The\nformalism accounts for translation invariance and energy conservation, and\nsubstantially outperforms linear dimension reduction, reproducing very well key\ndynamic and statistical features of the attractor.",
        "categories": [
            "cs.LG",
            "physics.flu-dyn"
        ],
        "link": "http://arxiv.org/pdf/2001.04263v3",
        "date": "2019-12-20 15:51:04+00:00"
    },
    {
        "title": "Skin feature point tracking using deep feature encodings",
        "authors": [
            "Jose Ramon Chang",
            "Torbj\u00f6rn E. M. Nordling"
        ],
        "abstract": "Facial feature tracking is a key component of imaging ballistocardiography\n(BCG) where accurate quantification of the displacement of facial keypoints is\nneeded for good heart rate estimation. Skin feature tracking enables\nvideo-based quantification of motor degradation in Parkinson's disease.\nTraditional computer vision algorithms include Scale Invariant Feature\nTransform (SIFT), Speeded-Up Robust Features (SURF), and Lucas-Kanade method\n(LK). These have long represented the state-of-the-art in efficiency and\naccuracy but fail when common deformations, like affine local transformations\nor illumination changes, are present.\n  Over the past five years, deep convolutional neural networks have\noutperformed traditional methods for most computer vision tasks. We propose a\npipeline for feature tracking, that applies a convolutional stacked autoencoder\nto identify the most similar crop in an image to a reference crop containing\nthe feature of interest. The autoencoder learns to represent image crops into\ndeep feature encodings specific to the object category it is trained on.\n  We train the autoencoder on facial images and validate its ability to track\nskin features in general using manually labeled face and hand videos. The\ntracking errors of distinctive skin features (moles) are so small that we\ncannot exclude that they stem from the manual labelling based on a\n$\\chi^2$-test. With a mean error of 0.6-4.2 pixels, our method outperformed the\nother methods in all but one scenario. More importantly, our method was the\nonly one to not diverge.\n  We conclude that our method creates better feature descriptors for feature\ntracking, feature matching, and image registration than the traditional\nalgorithms.",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2112.14159v2",
        "date": "2021-12-28 14:29:08+00:00"
    },
    {
        "title": "Deep clustering with fusion autoencoder",
        "authors": [
            "Shuai Chang"
        ],
        "abstract": "Embracing the deep learning techniques for representation learning in\nclustering research has attracted broad attention in recent years, yielding a\nnewly developed clustering paradigm, viz. the deep clustering (DC). Typically,\nthe DC models capitalize on autoencoders to learn the intrinsic features which\nfacilitate the clustering process in consequence. Nowadays, a generative model\nnamed variational autoencoder (VAE) has got wide acceptance in DC studies.\nNevertheless, the plain VAE is insufficient to perceive the comprehensive\nlatent features, leading to the deteriorative clustering performance. In this\npaper, a novel DC method is proposed to address this issue. Specifically, the\ngenerative adversarial network and VAE are coalesced into a new autoencoder\ncalled fusion autoencoder (FAE) for discerning more discriminative\nrepresentation that benefits the downstream clustering task. Besides, the FAE\nis implemented with the deep residual network architecture which further\nenhances the representation learning ability. Finally, the latent space of the\nFAE is transformed to an embedding space shaped by a deep dense neural network\nfor pulling away different clusters from each other and collapsing data points\nwithin individual clusters. Experiment conducted on several image datasets\ndemonstrate the effectiveness of the proposed DC model against the baseline\nmethods.",
        "categories": [
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2201.04727v2",
        "date": "2022-01-11 07:38:03+00:00"
    },
    {
        "title": "An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution",
        "authors": [
            "Rosanne Liu",
            "Joel Lehman",
            "Piero Molino",
            "Felipe Petroski Such",
            "Eric Frank",
            "Alex Sergeev",
            "Jason Yosinski"
        ],
        "abstract": "Few ideas have enjoyed as large an impact on deep learning as convolution.\nFor any problem involving pixels or spatial representations, common intuition\nholds that convolutional neural networks may be appropriate. In this paper we\nshow a striking counterexample to this intuition via the seemingly trivial\ncoordinate transform problem, which simply requires learning a mapping between\ncoordinates in (x,y) Cartesian space and one-hot pixel space. Although\nconvolutional networks would seem appropriate for this task, we show that they\nfail spectacularly. We demonstrate and carefully analyze the failure first on a\ntoy problem, at which point a simple fix becomes obvious. We call this solution\nCoordConv, which works by giving convolution access to its own input\ncoordinates through the use of extra coordinate channels. Without sacrificing\nthe computational and parametric efficiency of ordinary convolution, CoordConv\nallows networks to learn either complete translation invariance or varying\ndegrees of translation dependence, as required by the end task. CoordConv\nsolves the coordinate transform problem with perfect generalization and 150\ntimes faster with 10--100 times fewer parameters than convolution. This stark\ncontrast raises the question: to what extent has this inability of convolution\npersisted insidiously inside other tasks, subtly hampering performance from\nwithin? A complete answer to this question will require further investigation,\nbut we show preliminary evidence that swapping convolution for CoordConv can\nimprove models on a diverse set of tasks. Using CoordConv in a GAN produced\nless mode collapse as the transform between high-level spatial latents and\npixels becomes easier to learn. A Faster R-CNN detection model trained on MNIST\nshowed 24% better IOU when using CoordConv, and in the RL domain agents playing\nAtari games benefit significantly from the use of CoordConv layers.",
        "categories": [
            "cs.CV",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1807.03247v2",
        "date": "2018-07-09 15:48:08+00:00"
    },
    {
        "title": "Pretraining the Vision Transformer using self-supervised methods for vision based Deep Reinforcement Learning",
        "authors": [
            "Manuel Goul\u00e3o",
            "Arlindo L. Oliveira"
        ],
        "abstract": "The Vision Transformer architecture has shown to be competitive in the\ncomputer vision (CV) space where it has dethroned convolution-based networks in\nseveral benchmarks. Nevertheless, Convolutional Neural Networks (CNN) remain\nthe preferential architecture for the representation module in Reinforcement\nLearning. In this work, we study pretraining a Vision Transformer using several\nstate-of-the-art self-supervised methods and assess data-efficiency gains from\nthis training framework. We propose a new self-supervised learning method\ncalled TOV-VICReg that extends VICReg to better capture temporal relations\nbetween observations by adding a temporal order verification task. Furthermore,\nwe evaluate the resultant encoders with Atari games in a sample-efficiency\nregime. Our results show that the vision transformer, when pretrained with\nTOV-VICReg, outperforms the other self-supervised methods but still struggles\nto overcome a CNN. Nevertheless, we were able to outperform a CNN in two of the\nten games where we perform a 100k steps evaluation. Ultimately, we believe that\nsuch approaches in Deep Reinforcement Learning (DRL) might be the key to\nachieving new levels of performance as seen in natural language processing and\ncomputer vision. Source code will be available at:\nhttps://github.com/mgoulao/TOV-VICReg",
        "categories": [
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2209.10901v1",
        "date": "2022-09-22 10:18:59+00:00"
    },
    {
        "title": "Flowsheet synthesis through hierarchical reinforcement learning and graph neural networks",
        "authors": [
            "Laura Stops",
            "Roel Leenhouts",
            "Qinghe Gao",
            "Artur M. Schweidtmann"
        ],
        "abstract": "Process synthesis experiences a disruptive transformation accelerated by\ndigitization and artificial intelligence. We propose a reinforcement learning\nalgorithm for chemical process design based on a state-of-the-art actor-critic\nlogic. Our proposed algorithm represents chemical processes as graphs and uses\ngraph convolutional neural networks to learn from process graphs. In\nparticular, the graph neural networks are implemented within the agent\narchitecture to process the states and make decisions. Moreover, we implement a\nhierarchical and hybrid decision-making process to generate flowsheets, where\nunit operations are placed iteratively as discrete decisions and corresponding\ndesign variables are selected as continuous decisions. We demonstrate the\npotential of our method to design economically viable flowsheets in an\nillustrative case study comprising equilibrium reactions, azeotropic\nseparation, and recycles. The results show quick learning in discrete,\ncontinuous, and hybrid action spaces. Due to the flexible architecture of the\nproposed reinforcement learning agent, the method is predestined to include\nlarge action-state spaces and an interface to process simulators in future\nresearch.",
        "categories": [
            "cs.LG",
            "math.OC",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2207.12051v1",
        "date": "2022-07-25 10:42:15+00:00"
    },
    {
        "title": "Training Spiking Deep Networks for Neuromorphic Hardware",
        "authors": [
            "Eric Hunsberger",
            "Chris Eliasmith"
        ],
        "abstract": "We describe a method to train spiking deep networks that can be run using\nleaky integrate-and-fire (LIF) neurons, achieving state-of-the-art results for\nspiking LIF networks on five datasets, including the large ImageNet ILSVRC-2012\nbenchmark. Our method for transforming deep artificial neural networks into\nspiking networks is scalable and works with a wide range of neural\nnonlinearities. We achieve these results by softening the neural response\nfunction, such that its derivative remains bounded, and by training the network\nwith noise to provide robustness against the variability introduced by spikes.\nOur analysis shows that implementations of these networks on neuromorphic\nhardware will be many times more power-efficient than the equivalent\nnon-spiking networks on traditional hardware.",
        "categories": [
            "cs.NE",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1611.05141v1",
        "date": "2016-11-16 04:32:22+00:00"
    },
    {
        "title": "Gradient Boosted Decision Tree Neural Network",
        "authors": [
            "Mohammad Saberian",
            "Pablo Delgado",
            "Yves Raimond"
        ],
        "abstract": "In this paper we propose a method to build a neural network that is similar\nto an ensemble of decision trees. We first illustrate how to convert a learned\nensemble of decision trees to a single neural network with one hidden layer and\nan input transformation. We then relax some properties of this network such as\nthresholds and activation functions to train an approximately equivalent\ndecision tree ensemble. The final model, Hammock, is surprisingly simple: a\nfully connected two layers neural network where the input is quantized and\none-hot encoded. Experiments on large and small datasets show this simple\nmethod can achieve performance similar to that of Gradient Boosted Decision\nTrees.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1910.09340v2",
        "date": "2019-10-17 18:21:43+00:00"
    },
    {
        "title": "Interpretable Neural Networks based classifiers for categorical inputs",
        "authors": [
            "Stefano Zamuner",
            "Paolo De Los Rios"
        ],
        "abstract": "Because of the pervasive usage of Neural Networks in human sensitive\napplications, their interpretability is becoming an increasingly important\ntopic in machine learning. In this work we introduce a simple way to interpret\nthe output function of a neural network classifier that take as input\ncategorical variables. By exploiting a mapping between a neural network\nclassifier and a physical energy model, we show that in these cases each layer\nof the network, and the logits layer in particular, can be expanded as a sum of\nterms that account for the contribution to the classification of each input\npattern. For instance, at the first order, the expansion considers just the\nlinear relation between input features and output while at the second order\npairwise dependencies between input features are also accounted for. The\nanalysis of the contributions of each pattern, after an appropriate gauge\ntransformation, is presented in two cases where the effectiveness of the method\ncan be appreciated.",
        "categories": [
            "cs.LG",
            "stat.ML",
            "I.5"
        ],
        "link": "http://arxiv.org/pdf/2102.03202v1",
        "date": "2021-02-05 14:38:50+00:00"
    },
    {
        "title": "Tricks and Plugins to GBM on Images and Sequences",
        "authors": [
            "Biyi Fang",
            "Jean Utke",
            "Diego Klabjan"
        ],
        "abstract": "Convolutional neural networks (CNNs) and transformers, which are composed of\nmultiple processing layers and blocks to learn the representations of data with\nmultiple abstract levels, are the most successful machine learning models in\nrecent years. However, millions of parameters and many blocks make them\ndifficult to be trained, and sometimes several days or weeks are required to\nfind an ideal architecture or tune the parameters. Within this paper, we\npropose a new algorithm for boosting Deep Convolutional Neural Networks\n(BoostCNN) to combine the merits of dynamic feature selection and BoostCNN, and\nanother new family of algorithms combining boosting and transformers. To learn\nthese new models, we introduce subgrid selection and importance sampling\nstrategies and propose a set of algorithms to incorporate boosting weights into\na deep learning architecture based on a least squares objective function. These\nalgorithms not only reduce the required manual effort for finding an\nappropriate network architecture but also result in superior performance and\nlower running time. Experiments show that the proposed methods outperform\nbenchmarks on several fine-grained classification tasks.",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "link": "http://arxiv.org/pdf/2203.00761v1",
        "date": "2022-03-01 21:59:00+00:00"
    },
    {
        "title": "Regularize, Expand and Compress: Multi-task based Lifelong Learning via NonExpansive AutoML",
        "authors": [
            "Jie Zhang",
            "Junting Zhang",
            "Shalini Ghosh",
            "Dawei Li",
            "Jingwen Zhu",
            "Heming Zhang",
            "Yalin Wang"
        ],
        "abstract": "Lifelong learning, the problem of continual learning where tasks arrive in\nsequence, has been lately attracting more attention in the computer vision\ncommunity. The aim of lifelong learning is to develop a system that can learn\nnew tasks while maintaining the performance on the previously learned tasks.\nHowever, there are two obstacles for lifelong learning of deep neural networks:\ncatastrophic forgetting and capacity limitation. To solve the above issues,\ninspired by the recent breakthroughs in automatically learning good neural\nnetwork architectures, we develop a Multi-task based lifelong learning via\nnonexpansive AutoML framework termed Regularize, Expand and Compress (REC). REC\nis composed of three stages: 1) continually learns the sequential tasks without\nthe learned tasks' data via a newly proposed multi-task weight consolidation\n(MWC) algorithm; 2) expands the network to help the lifelong learning with\npotentially improved model capability and performance by network-transformation\nbased AutoML; 3) compresses the expanded model after learning every new task to\nmaintain model efficiency and performance. The proposed MWC and REC algorithms\nachieve superior performance over other lifelong learning algorithms on four\ndifferent datasets.",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1903.08362v1",
        "date": "2019-03-20 07:16:58+00:00"
    },
    {
        "title": "Extension of Convolutional Neural Network with General Image Processing Kernels",
        "authors": [
            "Jay Hoon Jung",
            "Yousun Shin",
            "YoungMin Kwon"
        ],
        "abstract": "We applied pre-defined kernels also known as filters or masks developed for\nimage processing to convolution neural network. Instead of letting neural\nnetworks find its own kernels, we used 41 different general-purpose kernels of\nblurring, edge detecting, sharpening, discrete cosine transformation, etc. for\nthe first layer of the convolution neural networks. This architecture, thus\nnamed as general filter convolutional neural network (GFNN), can reduce\ntraining time by 30% with a better accuracy compared to the regular\nconvolutional neural network (CNN). GFNN also can be trained to achieve 90%\naccuracy with only 500 samples. Furthermore, even though these kernels are not\nspecialized for the MNIST dataset, we achieved 99.56% accuracy without ensemble\nnor any other special algorithms.",
        "categories": [
            "cs.CV",
            "cs.LG",
            "eess.IV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1901.07375v1",
        "date": "2019-01-16 07:44:58+00:00"
    },
    {
        "title": "Traversing Latent Space using Decision Ferns",
        "authors": [
            "Yan Zuo",
            "Gil Avraham",
            "Tom Drummond"
        ],
        "abstract": "The practice of transforming raw data to a feature space so that inference\ncan be performed in that space has been popular for many years. Recently, rapid\nprogress in deep neural networks has given both researchers and practitioners\nenhanced methods that increase the richness of feature representations, be it\nfrom images, text or speech. In this work we show how a constructed latent\nspace can be explored in a controlled manner and argue that this complements\nwell founded inference methods. For constructing the latent space a Variational\nAutoencoder is used. We present a novel controller module that allows for\nsmooth traversal in the latent space and construct an end-to-end trainable\nframework. We explore the applicability of our method for performing spatial\ntransformations as well as kinematics for predicting future latent vectors of a\nvideo sequence.",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1812.02636v1",
        "date": "2018-12-06 16:15:24+00:00"
    },
    {
        "title": "Learning Non-Parametric Invariances from Data with Permanent Random Connectomes",
        "authors": [
            "Dipan K. Pal",
            "Akshay Chawla",
            "Marios Savvides"
        ],
        "abstract": "One of the fundamental problems in supervised classification and in machine\nlearning in general, is the modelling of non-parametric invariances that exist\nin data. Most prior art has focused on enforcing priors in the form of\ninvariances to parametric nuisance transformations that are expected to be\npresent in data. Learning non-parametric invariances directly from data remains\nan important open problem. In this paper, we introduce a new architectural\nlayer for convolutional networks which is capable of learning general\ninvariances from data itself. This layer can learn invariance to non-parametric\ntransformations and interestingly, motivates and incorporates permanent random\nconnectomes, thereby being called Permanent Random Connectome Non-Parametric\nTransformation Networks (PRC-NPTN). PRC-NPTN networks are initialized with\nrandom connections (not just weights) which are a small subset of the\nconnections in a fully connected convolution layer. Importantly, these\nconnections in PRC-NPTNs once initialized remain permanent throughout training\nand testing. Permanent random connectomes make these architectures loosely more\nbiologically plausible than many other mainstream network architectures which\nrequire highly ordered structures. We motivate randomly initialized connections\nas a simple method to learn invariance from data itself while invoking\ninvariance towards multiple nuisance transformations simultaneously. We find\nthat these randomly initialized permanent connections have positive effects on\ngeneralization, outperform much larger ConvNet baselines and the recently\nproposed Non-Parametric Transformation Network (NPTN) on benchmarks that\nenforce learning invariances from the data itself.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "cs.NE",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1911.05266v3",
        "date": "2019-11-13 03:03:48+00:00"
    },
    {
        "title": "WDiscOOD: Out-of-Distribution Detection via Whitened Linear Discriminant Analysis",
        "authors": [
            "Yiye Chen",
            "Yunzhi Lin",
            "Ruinian Xu",
            "Patricio A. Vela"
        ],
        "abstract": "Deep neural networks are susceptible to generating overconfident yet\nerroneous predictions when presented with data beyond known concepts. This\nchallenge underscores the importance of detecting out-of-distribution (OOD)\nsamples in the open world. In this work, we propose a novel feature-space OOD\ndetection score that jointly reasons with both class-specific and\nclass-agnostic information. Specifically, our approach utilizes Whitened Linear\nDiscriminant Analysis to project features into two subspaces - the\ndiscriminative and residual subspaces - in which the ID classes are maximally\nseparated and closely clustered, respectively. The OOD score is then determined\nby combining the deviation from the input data to the ID distribution in both\nsubspaces. The efficacy of our method, named WDiscOOD, is verified on the\nlarge-scale ImageNet-1k benchmark, with six OOD datasets that covers a variety\nof distribution shifts. WDiscOOD demonstrates superior performance on deep\nclassifiers with diverse backbone architectures, including CNN and vision\ntransformer. Furthermore, we also show that our method can more effectively\ndetect novel concepts in representation space trained with contrastive\nobjectives, including supervised contrastive loss and multi-modality\ncontrastive loss.",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2303.07543v3",
        "date": "2023-03-14 00:13:57+00:00"
    },
    {
        "title": "Robustness Verification for Transformers",
        "authors": [
            "Zhouxing Shi",
            "Huan Zhang",
            "Kai-Wei Chang",
            "Minlie Huang",
            "Cho-Jui Hsieh"
        ],
        "abstract": "Robustness verification that aims to formally certify the prediction behavior\nof neural networks has become an important tool for understanding model\nbehavior and obtaining safety guarantees. However, previous methods can usually\nonly handle neural networks with relatively simple architectures. In this\npaper, we consider the robustness verification problem for Transformers.\nTransformers have complex self-attention layers that pose many challenges for\nverification, including cross-nonlinearity and cross-position dependency, which\nhave not been discussed in previous works. We resolve these challenges and\ndevelop the first robustness verification algorithm for Transformers. The\ncertified robustness bounds computed by our method are significantly tighter\nthan those by naive Interval Bound Propagation. These bounds also shed light on\ninterpreting Transformers as they consistently reflect the importance of\ndifferent words in sentiment analysis.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2002.06622v2",
        "date": "2020-02-16 17:16:31+00:00"
    },
    {
        "title": "Query Efficient Decision Based Sparse Attacks Against Black-Box Deep Learning Models",
        "authors": [
            "Viet Quoc Vo",
            "Ehsan Abbasnejad",
            "Damith C. Ranasinghe"
        ],
        "abstract": "Despite our best efforts, deep learning models remain highly vulnerable to\neven tiny adversarial perturbations applied to the inputs. The ability to\nextract information from solely the output of a machine learning model to craft\nadversarial perturbations to black-box models is a practical threat against\nreal-world systems, such as autonomous cars or machine learning models exposed\nas a service (MLaaS). Of particular interest are sparse attacks. The\nrealization of sparse attacks in black-box models demonstrates that machine\nlearning models are more vulnerable than we believe. Because these attacks aim\nto minimize the number of perturbed pixels measured by l_0 norm-required to\nmislead a model by solely observing the decision (the predicted label) returned\nto a model query; the so-called decision-based attack setting. But, such an\nattack leads to an NP-hard optimization problem. We develop an evolution-based\nalgorithm-SparseEvo-for the problem and evaluate against both convolutional\ndeep neural networks and vision transformers. Notably, vision transformers are\nyet to be investigated under a decision-based attack setting. SparseEvo\nrequires significantly fewer model queries than the state-of-the-art sparse\nattack Pointwise for both untargeted and targeted attacks. The attack\nalgorithm, although conceptually simple, is also competitive with only a\nlimited query budget against the state-of-the-art gradient-based whitebox\nattacks in standard computer vision tasks such as ImageNet. Importantly, the\nquery efficient SparseEvo, along with decision-based attacks, in general, raise\nnew questions regarding the safety of deployed systems and poses new directions\nto study and understand the robustness of machine learning models.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CR",
            "cs.CV"
        ],
        "link": "http://arxiv.org/pdf/2202.00091v2",
        "date": "2022-01-31 21:10:47+00:00"
    },
    {
        "title": "Efficient 8-Bit Quantization of Transformer Neural Machine Language Translation Model",
        "authors": [
            "Aishwarya Bhandare",
            "Vamsi Sripathi",
            "Deepthi Karkada",
            "Vivek Menon",
            "Sun Choi",
            "Kushal Datta",
            "Vikram Saletore"
        ],
        "abstract": "In this work, we quantize a trained Transformer machine language translation\nmodel leveraging INT8/VNNI instructions in the latest Intel$^\\circledR$\nXeon$^\\circledR$ Cascade Lake processors to improve inference performance while\nmaintaining less than 0.5$\\%$ drop in accuracy. To the best of our knowledge,\nthis is the first attempt in the industry to quantize the Transformer model.\nThis has high impact as it clearly demonstrates the various complexities of\nquantizing the language translation model. We present novel quantization\ntechniques directly in TensorFlow to opportunistically replace 32-bit floating\npoint (FP32) computations with 8-bit integers (INT8) and transform the FP32\ncomputational graph. We also present a bin-packing parallel batching technique\nto maximize CPU utilization. Overall, our optimizations with INT8/VNNI deliver\n1.5X improvement over the best FP32 performance. Furthermore, it reveals the\nopportunities and challenges to boost performance of quantized deep learning\ninference and establishes best practices to run inference with high efficiency\non Intel CPUs.",
        "categories": [
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1906.00532v2",
        "date": "2019-06-03 02:29:22+00:00"
    },
    {
        "title": "Predicting CSI Sequences With Attention-Based Neural Networks",
        "authors": [
            "Valentina Rizzello",
            "Benedikt B\u00f6ck",
            "Michael Joham",
            "Wolfgang Utschick"
        ],
        "abstract": "In this work, we consider the problem of multi-step channel prediction in\nwireless communication systems. In existing works, autoregressive (AR) models\nare either replaced or combined with feed-forward neural networks(NNs) or,\nalternatively, with recurrent neural networks (RNNs). This paper explores the\npossibility of using sequence-to-sequence (Seq2Seq) and transformer neural\nnetwork (TNN) models for channel state information (CSI) prediction. Simulation\nresults show that both, Seq2Seq and TNNs, represent an appealing alternative to\nRNNs and feed-forward NNs in the context of CSI prediction. Additionally, the\nTNN with a few adaptations can extrapolate better than other models to CSI\nsequences that are either shorter or longer than the ones the model saw during\ntraining.",
        "categories": [
            "stat.ML",
            "cs.LG",
            "cs.NI",
            "eess.SP"
        ],
        "link": "http://arxiv.org/pdf/2302.00341v1",
        "date": "2023-02-01 09:53:57+00:00"
    },
    {
        "title": "Sequence Transduction with Recurrent Neural Networks",
        "authors": [
            "Alex Graves"
        ],
        "abstract": "Many machine learning tasks can be expressed as the transformation---or\n\\emph{transduction}---of input sequences into output sequences: speech\nrecognition, machine translation, protein secondary structure prediction and\ntext-to-speech to name but a few. One of the key challenges in sequence\ntransduction is learning to represent both the input and output sequences in a\nway that is invariant to sequential distortions such as shrinking, stretching\nand translating. Recurrent neural networks (RNNs) are a powerful sequence\nlearning architecture that has proven capable of learning such representations.\nHowever RNNs traditionally require a pre-defined alignment between the input\nand output sequences to perform transduction. This is a severe limitation since\n\\emph{finding} the alignment is the most difficult aspect of many sequence\ntransduction problems. Indeed, even determining the length of the output\nsequence is often challenging. This paper introduces an end-to-end,\nprobabilistic sequence transduction system, based entirely on RNNs, that is in\nprinciple able to transform any input sequence into any finite, discrete output\nsequence. Experimental results for phoneme recognition are provided on the\nTIMIT speech corpus.",
        "categories": [
            "cs.NE",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1211.3711v1",
        "date": "2012-11-14 19:25:21+00:00"
    },
    {
        "title": "Vector Quantized Time Series Generation with a Bidirectional Prior Model",
        "authors": [
            "Daesoo Lee",
            "Sara Malacarne",
            "Erlend Aune"
        ],
        "abstract": "Time series generation (TSG) studies have mainly focused on the use of\nGenerative Adversarial Networks (GANs) combined with recurrent neural network\n(RNN) variants. However, the fundamental limitations and challenges of training\nGANs still remain. In addition, the RNN-family typically has difficulties with\ntemporal consistency between distant timesteps. Motivated by the successes in\nthe image generation (IMG) domain, we propose TimeVQVAE, the first work, to our\nknowledge, that uses vector quantization (VQ) techniques to address the TSG\nproblem. Moreover, the priors of the discrete latent spaces are learned with\nbidirectional transformer models that can better capture global temporal\nconsistency. We also propose VQ modeling in a time-frequency domain, separated\ninto low-frequency (LF) and high-frequency (HF). This allows us to retain\nimportant characteristics of the time series and, in turn, generate new\nsynthetic signals that are of better quality, with sharper changes in\nmodularity, than its competing TSG methods. Our experimental evaluation is\nconducted on all datasets from the UCR archive, using well-established metrics\nin the IMG literature, such as Fr\\'echet inception distance and inception\nscores. Our implementation on GitHub:\n\\url{https://github.com/ML4ITS/TimeVQVAE}.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2303.04743v3",
        "date": "2023-03-08 17:27:39+00:00"
    },
    {
        "title": "Graph Convolutional Neural Networks with Node Transition Probability-based Message Passing and DropNode Regularization",
        "authors": [
            "Tien Huu Do",
            "Duc Minh Nguyen",
            "Giannis Bekoulis",
            "Adrian Munteanu",
            "Nikos Deligiannis"
        ],
        "abstract": "Graph convolutional neural networks (GCNNs) have received much attention\nrecently, owing to their capability in handling graph-structured data. Among\nthe existing GCNNs, many methods can be viewed as instances of a neural message\npassing motif; features of nodes are passed around their neighbors, aggregated\nand transformed to produce better nodes' representations. Nevertheless, these\nmethods seldom use node transition probabilities, a measure that has been found\nuseful in exploring graphs. Furthermore, when the transition probabilities are\nused, their transition direction is often improperly considered in the feature\naggregation step, resulting in an inefficient weighting scheme. In addition,\nalthough a great number of GCNN models with increasing level of complexity have\nbeen introduced, the GCNNs often suffer from over-fitting when being trained on\nsmall graphs. Another issue of the GCNNs is over-smoothing, which tends to make\nnodes' representations indistinguishable. This work presents a new method to\nimprove the message passing process based on node transition probabilities by\nproperly considering the transition direction, leading to a better weighting\nscheme in nodes' features aggregation compared to the existing counterpart.\nMoreover, we propose a novel regularization method termed DropNode to address\nthe over-fitting and over-smoothing issues simultaneously. DropNode randomly\ndiscards part of a graph, thus it creates multiple deformed versions of the\ngraph, leading to data augmentation regularization effect. Additionally,\nDropNode lessens the connectivity of the graph, mitigating the effect of\nover-smoothing in deep GCNNs. Extensive experiments on eight benchmark datasets\nfor node and graph classification tasks demonstrate the effectiveness of the\nproposed methods in comparison with the state of the art.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2008.12578v2",
        "date": "2020-08-28 10:51:03+00:00"
    },
    {
        "title": "Relating transformers to models and neural representations of the hippocampal formation",
        "authors": [
            "James C. R. Whittington",
            "Joseph Warren",
            "Timothy E. J. Behrens"
        ],
        "abstract": "Many deep neural network architectures loosely based on brain networks have\nrecently been shown to replicate neural firing patterns observed in the brain.\nOne of the most exciting and promising novel architectures, the Transformer\nneural network, was developed without the brain in mind. In this work, we show\nthat transformers, when equipped with recurrent position encodings, replicate\nthe precisely tuned spatial representations of the hippocampal formation; most\nnotably place and grid cells. Furthermore, we show that this result is no\nsurprise since it is closely related to current hippocampal models from\nneuroscience. We additionally show the transformer version offers dramatic\nperformance gains over the neuroscience version. This work continues to bind\ncomputations of artificial and brain networks, offers a novel understanding of\nthe hippocampal-cortical interaction, and suggests how wider cortical areas may\nperform complex tasks beyond current neuroscience models such as language\ncomprehension.",
        "categories": [
            "cs.NE",
            "cs.LG",
            "q-bio.NC"
        ],
        "link": "http://arxiv.org/pdf/2112.04035v2",
        "date": "2021-12-07 23:14:07+00:00"
    },
    {
        "title": "Deep Neural Networks with Efficient Guaranteed Invariances",
        "authors": [
            "Matthias Rath",
            "Alexandru Paul Condurache"
        ],
        "abstract": "We address the problem of improving the performance and in particular the\nsample complexity of deep neural networks by enforcing and guaranteeing\ninvariances to symmetry transformations rather than learning them from data.\nGroup-equivariant convolutions are a popular approach to obtain equivariant\nrepresentations. The desired corresponding invariance is then imposed using\npooling operations. For rotations, it has been shown that using invariant\nintegration instead of pooling further improves the sample complexity. In this\ncontribution, we first expand invariant integration beyond rotations to flips\nand scale transformations. We then address the problem of incorporating\nmultiple desired invariances into a single network. For this purpose, we\npropose a multi-stream architecture, where each stream is invariant to a\ndifferent transformation such that the network can simultaneously benefit from\nmultiple invariances. We demonstrate our approach with successful experiments\non Scaled-MNIST, SVHN, CIFAR-10 and STL-10.",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "link": "http://arxiv.org/pdf/2303.01567v1",
        "date": "2023-03-02 20:44:45+00:00"
    },
    {
        "title": "Feedback is Good, Active Feedback is Better: Block Attention Active Feedback Codes",
        "authors": [
            "Emre Ozfatura",
            "Yulin Shao",
            "Amin Ghazanfari",
            "Alberto Perotti",
            "Branislav Popovic",
            "Deniz Gunduz"
        ],
        "abstract": "Deep neural network (DNN)-assisted channel coding designs, such as\nlow-complexity neural decoders for existing codes, or end-to-end\nneural-network-based auto-encoder designs are gaining interest recently due to\ntheir improved performance and flexibility; particularly for communication\nscenarios in which high-performing structured code designs do not exist.\nCommunication in the presence of feedback is one such communication scenario,\nand practical code design for feedback channels has remained an open challenge\nin coding theory for many decades. Recently, DNN-based designs have shown\nimpressive results in exploiting feedback. In particular, generalized block\nattention feedback (GBAF) codes, which utilizes the popular transformer\narchitecture, achieved significant improvement in terms of the block error rate\n(BLER) performance. However, previous works have focused mainly on passive\nfeedback, where the transmitter observes a noisy version of the signal at the\nreceiver. In this work, we show that GBAF codes can also be used for channels\nwith active feedback. We implement a pair of transformer architectures, at the\ntransmitter and the receiver, which interact with each other sequentially, and\nachieve a new state-of-the-art BLER performance, especially in the low SNR\nregime.",
        "categories": [
            "cs.IT",
            "cs.AI",
            "cs.LG",
            "eess.SP",
            "math.IT"
        ],
        "link": "http://arxiv.org/pdf/2211.01730v1",
        "date": "2022-11-03 11:44:06+00:00"
    },
    {
        "title": "Fair Classification via Transformer Neural Networks: Case Study of an Educational Domain",
        "authors": [
            "Modar Sulaiman",
            "Kallol Roy"
        ],
        "abstract": "Educational technologies nowadays increasingly use data and Machine Learning\n(ML) models. This gives the students, instructors, and administrators support\nand insights for the optimum policy. However, it is well acknowledged that ML\nmodels are subject to bias, which raises concerns about the fairness, bias, and\ndiscrimination of using these automated ML algorithms in education and its\nunintended and unforeseen negative consequences. The contribution of bias\nduring the decision-making comes from datasets used for training ML models and\nthe model architecture. This paper presents a preliminary investigation of the\nfairness of transformer neural networks on the two tabular datasets: Law School\nand Student-Mathematics. In contrast to classical ML models, the\ntransformer-based models transform these tabular datasets into a richer\nrepresentation while solving the classification task. We use different fairness\nmetrics for evaluation and check the trade-off between fairness and accuracy of\nthe transformer-based models over the tabular datasets. Empirically, our\napproach shows impressive results regarding the trade-off between fairness and\nperformance on the Law School dataset.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2206.01410v2",
        "date": "2022-06-03 06:34:16+00:00"
    },
    {
        "title": "Epileptic Seizure Classification with Symmetric and Hybrid Bilinear Models",
        "authors": [
            "Tennison Liu",
            "Nhan Duy Truong",
            "Armin Nikpour",
            "Luping Zhou",
            "Omid Kavehei"
        ],
        "abstract": "Epilepsy affects nearly 1% of the global population, of which two thirds can\nbe treated by anti-epileptic drugs and a much lower percentage by surgery.\nDiagnostic procedures for epilepsy and monitoring are highly specialized and\nlabour-intensive. The accuracy of the diagnosis is also complicated by\noverlapping medical symptoms, varying levels of experience and inter-observer\nvariability among clinical professions. This paper proposes a novel hybrid\nbilinear deep learning network with an application in the clinical procedures\nof epilepsy classification diagnosis, where the use of surface\nelectroencephalogram (sEEG) and audiovisual monitoring is standard practice.\nHybrid bilinear models based on two types of feature extractors, namely\nConvolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), are\ntrained using Short-Time Fourier Transform (STFT) of one-second sEEG. In the\nproposed hybrid models, CNNs extract spatio-temporal patterns, while RNNs focus\non the characteristics of temporal dynamics in relatively longer intervals\ngiven the same input data. Second-order features, based on interactions between\nthese spatio-temporal features are further explored by bilinear pooling and\nused for epilepsy classification. Our proposed methods obtain an F1-score of\n97.4% on the Temple University Hospital Seizure Corpus and 97.2% on the\nEPILEPSIAE dataset, comparing favourably to existing benchmarks for sEEG-based\nseizure type classification. The open-source implementation of this study is\navailable at https://github.com/NeuroSyd/Epileptic-Seizure-Classification",
        "categories": [
            "eess.SP",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2001.06282v1",
        "date": "2020-01-15 03:22:10+00:00"
    },
    {
        "title": "Neural Networks on Groups",
        "authors": [
            "Stella Rose Biderman"
        ],
        "abstract": "Although neural networks traditionally are typically used to approximate\nfunctions defined over $\\mathbb{R}^n$, the successes of graph neural networks,\npoint-cloud neural networks, and manifold deep learning among other methods\nhave demonstrated the clear value of leveraging neural networks to approximate\nfunctions defined over more general spaces. The theory of neural networks has\nnot kept up however,and the relevant theoretical results (when they exist at\nall) have been proven on a case-by-case basis without a general theory or\nconnection to classical work. The process of deriving new theoretical backing\nfor each new type of network has become a bottleneck to understanding and\nvalidating new approaches.\n  In this paper we extend the definition of neural networks to general\ntopological groups and prove that neural networks with a single hidden layer\nand a bounded non-constant activation function can approximate any\n$\\mathcal{L}^p$ function defined over any locally compact Abelian group. This\nframework and universal approximation theorem encompass all of the\naforementioned contexts. We also derive important corollaries and extensions\nwith minor modification, including the case for approximating continuous\nfunctions on a compact subset, neural networks with ReLU activation functions\non a linearly bi-ordered group, and neural networks with affine transformations\non a vector space. Our work obtains as special cases the recent theorems of Qi\net al. [2017], Sennai et al. [2019], Keriven and Peyre [2019], and Maron et al.\n[2019]",
        "categories": [
            "cs.NE",
            "cs.LG",
            "math.FA"
        ],
        "link": "http://arxiv.org/pdf/1907.03742v2",
        "date": "2019-06-13 01:34:12+00:00"
    },
    {
        "title": "A Computationally Efficient Neural Network Invariant to the Action of Symmetry Subgroups",
        "authors": [
            "Piotr Kicki",
            "Mete Ozay",
            "Piotr Skrzypczy\u0144ski"
        ],
        "abstract": "We introduce a method to design a computationally efficient $G$-invariant\nneural network that approximates functions invariant to the action of a given\npermutation subgroup $G \\leq S_n$ of the symmetric group on input data. The key\nelement of the proposed network architecture is a new $G$-invariant\ntransformation module, which produces a $G$-invariant latent representation of\nthe input data. This latent representation is then processed with a multi-layer\nperceptron in the network. We prove the universality of the proposed\narchitecture, discuss its properties and highlight its computational and memory\nefficiency. Theoretical considerations are supported by numerical experiments\ninvolving different network configurations, which demonstrate the effectiveness\nand strong generalization properties of the proposed method in comparison to\nother $G$-invariant neural networks.",
        "categories": [
            "cs.LG",
            "cs.NE",
            "stat.ML",
            "I.2.6"
        ],
        "link": "http://arxiv.org/pdf/2002.07528v1",
        "date": "2020-02-18 12:50:56+00:00"
    },
    {
        "title": "Benchmarking Robustness of 3D Point Cloud Recognition Against Common Corruptions",
        "authors": [
            "Jiachen Sun",
            "Qingzhao Zhang",
            "Bhavya Kailkhura",
            "Zhiding Yu",
            "Chaowei Xiao",
            "Z. Morley Mao"
        ],
        "abstract": "Deep neural networks on 3D point cloud data have been widely used in the real\nworld, especially in safety-critical applications. However, their robustness\nagainst corruptions is less studied. In this paper, we present ModelNet40-C,\nthe first comprehensive benchmark on 3D point cloud corruption robustness,\nconsisting of 15 common and realistic corruptions. Our evaluation shows a\nsignificant gap between the performances on ModelNet40 and ModelNet40-C for\nstate-of-the-art (SOTA) models. To reduce the gap, we propose a simple but\neffective method by combining PointCutMix-R and TENT after evaluating a wide\nrange of augmentation and test-time adaptation strategies. We identify a number\nof critical insights for future studies on corruption robustness in point cloud\nrecognition. For instance, we unveil that Transformer-based architectures with\nproper training recipes achieve the strongest robustness. We hope our in-depth\nanalysis will motivate the development of robust training strategies or\narchitecture designs in the 3D point cloud domain. Our codebase and dataset are\nincluded in https://github.com/jiachens/ModelNet40-C",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "link": "http://arxiv.org/pdf/2201.12296v1",
        "date": "2022-01-28 18:01:42+00:00"
    },
    {
        "title": "A new activation for neural networks and its approximation",
        "authors": [
            "Jianfei Li",
            "Han Feng",
            "Ding-Xuan Zhou"
        ],
        "abstract": "Deep learning with deep neural networks (DNNs) has attracted tremendous\nattention from various fields of science and technology recently. Activation\nfunctions for a DNN define the output of a neuron given an input or set of\ninputs. They are essential and inevitable in learning non-linear\ntransformations and performing diverse computations among successive neuron\nlayers. Thus, the design of activation functions is still an important topic in\ndeep learning research. Meanwhile, theoretical studies on the approximation\nability of DNNs with activation functions have been investigated within the\nlast few years. In this paper, we propose a new activation function, named as\n\"DLU\", and investigate its approximation ability for functions with various\nsmoothness and structures. Our theoretical results show that DLU networks can\nprocess competitive approximation performance with rational and ReLU networks,\nand have some advantages. Numerical experiments are conducted comparing DLU\nwith the existing activations-ReLU, Leaky ReLU, and ELU, which illustrate the\ngood practical performance of DLU.",
        "categories": [
            "cs.LG",
            "cs.GT",
            "eess.IV",
            "math.FA"
        ],
        "link": "http://arxiv.org/pdf/2210.10264v1",
        "date": "2022-10-19 02:59:31+00:00"
    },
    {
        "title": "Multimodal Data Integration for Oncology in the Era of Deep Neural Networks: A Review",
        "authors": [
            "Asim Waqas",
            "Aakash Tripathi",
            "Ravi P. Ramachandran",
            "Paul Stewart",
            "Ghulam Rasool"
        ],
        "abstract": "Cancer has relational information residing at varying scales, modalities, and\nresolutions of the acquired data, such as radiology, pathology, genomics,\nproteomics, and clinical records. Integrating diverse data types can improve\nthe accuracy and reliability of cancer diagnosis and treatment. There can be\ndisease-related information that is too subtle for humans or existing\ntechnological tools to discern visually. Traditional methods typically focus on\npartial or unimodal information about biological systems at individual scales\nand fail to encapsulate the complete spectrum of the heterogeneous nature of\ndata. Deep neural networks have facilitated the development of sophisticated\nmultimodal data fusion approaches that can extract and integrate relevant\ninformation from multiple sources. Recent deep learning frameworks such as\nGraph Neural Networks (GNNs) and Transformers have shown remarkable success in\nmultimodal learning. This review article provides an in-depth analysis of the\nstate-of-the-art in GNNs and Transformers for multimodal data fusion in\noncology settings, highlighting notable research studies and their findings. We\nalso discuss the foundations of multimodal learning, inherent challenges, and\nopportunities for integrative learning in oncology. By examining the current\nstate and potential future developments of multimodal data integration in\noncology, we aim to demonstrate the promising role that multimodal neural\nnetworks can play in cancer prevention, early detection, and treatment through\ninformed oncology practices in personalized settings.",
        "categories": [
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2303.06471v1",
        "date": "2023-03-11 17:52:03+00:00"
    },
    {
        "title": "On the Equivalence of Decoupled Graph Convolution Network and Label Propagation",
        "authors": [
            "Hande Dong",
            "Jiawei Chen",
            "Fuli Feng",
            "Xiangnan He",
            "Shuxian Bi",
            "Zhaolin Ding",
            "Peng Cui"
        ],
        "abstract": "The original design of Graph Convolution Network (GCN) couples feature\ntransformation and neighborhood aggregation for node representation learning.\nRecently, some work shows that coupling is inferior to decoupling, which\nsupports deep graph propagation better and has become the latest paradigm of\nGCN (e.g., APPNP and SGCN). Despite effectiveness, the working mechanisms of\nthe decoupled GCN are not well understood. In this paper, we explore the\ndecoupled GCN for semi-supervised node classification from a novel and\nfundamental perspective -- label propagation. We conduct thorough theoretical\nanalyses, proving that the decoupled GCN is essentially the same as the\ntwo-step label propagation: first, propagating the known labels along the graph\nto generate pseudo-labels for the unlabeled nodes, and second, training normal\nneural network classifiers on the augmented pseudo-labeled data. More\ninterestingly, we reveal the effectiveness of decoupled GCN: going beyond the\nconventional label propagation, it could automatically assign structure- and\nmodel- aware weights to the pseudo-label data. This explains why the decoupled\nGCN is relatively robust to the structure noise and over-smoothing, but\nsensitive to the label noise and model initialization. Based on this insight,\nwe propose a new label propagation method named Propagation then Training\nAdaptively (PTA), which overcomes the flaws of the decoupled GCN with a dynamic\nand adaptive weighting strategy. Our PTA is simple yet more effective and\nrobust than decoupled GCN. We empirically validate our findings on four\nbenchmark datasets, demonstrating the advantages of our method. The code is\navailable at https://github.com/DongHande/PT_propagation_then_training.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2010.12408v2",
        "date": "2020-10-23 13:57:39+00:00"
    },
    {
        "title": "Neighbor2Seq: Deep Learning on Massive Graphs by Transforming Neighbors to Sequences",
        "authors": [
            "Meng Liu",
            "Shuiwang Ji"
        ],
        "abstract": "Modern graph neural networks (GNNs) use a message passing scheme and have\nachieved great success in many fields. However, this recursive design\ninherently leads to excessive computation and memory requirements, making it\nnot applicable to massive real-world graphs. In this work, we propose the\nNeighbor2Seq to transform the hierarchical neighborhood of each node into a\nsequence. This novel transformation enables the subsequent mini-batch training\nfor general deep learning operations, such as convolution and attention, that\nare designed for grid-like data and are shown to be powerful in various\ndomains. Therefore, our Neighbor2Seq naturally endows GNNs with the efficiency\nand advantages of deep learning operations on grid-like data by precomputing\nthe Neighbor2Seq transformations. We evaluate our method on a massive graph,\nwith more than 111 million nodes and 1.6 billion edges, as well as several\nmedium-scale graphs. Results show that our proposed method is scalable to\nmassive graphs and achieves superior performance across massive and\nmedium-scale graphs. Our code is available at\nhttps://github.com/divelab/Neighbor2Seq.",
        "categories": [
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2202.03341v1",
        "date": "2022-02-07 16:38:36+00:00"
    },
    {
        "title": "Precise Recovery of Latent Vectors from Generative Adversarial Networks",
        "authors": [
            "Zachary C. Lipton",
            "Subarna Tripathi"
        ],
        "abstract": "Generative adversarial networks (GANs) transform latent vectors into visually\nplausible images. It is generally thought that the original GAN formulation\ngives no out-of-the-box method to reverse the mapping, projecting images back\ninto latent space. We introduce a simple, gradient-based technique called\nstochastic clipping. In experiments, for images generated by the GAN, we\nprecisely recover their latent vector pre-images 100% of the time. Additional\nexperiments demonstrate that this method is robust to noise. Finally, we show\nthat even for unseen images, our method appears to recover unique encodings.",
        "categories": [
            "cs.LG",
            "cs.NE",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1702.04782v2",
        "date": "2017-02-15 21:26:21+00:00"
    },
    {
        "title": "Applying Recent Innovations from NLP to MOOC Student Course Trajectory Modeling",
        "authors": [
            "Clarence Chen",
            "Zachary Pardos"
        ],
        "abstract": "This paper presents several strategies that can improve neural network-based\npredictive methods for MOOC student course trajectory modeling, applying\nmultiple ideas previously applied to tackle NLP (Natural Language Processing)\ntasks. In particular, this paper investigates LSTM networks enhanced with two\nforms of regularization, along with the more recently introduced Transformer\narchitecture.",
        "categories": [
            "cs.LG",
            "cs.CY",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2001.08333v2",
        "date": "2020-01-23 01:36:57+00:00"
    },
    {
        "title": "Goldilocks Neural Networks",
        "authors": [
            "Jan Rosenzweig",
            "Zoran Cvetkovic",
            "Ivana Rosenzweig"
        ],
        "abstract": "We introduce the new \"Goldilocks\" class of activation functions, which\nnon-linearly deform the input signal only locally when the input signal is in\nthe appropriate range. The small local deformation of the signal enables better\nunderstanding of how and why the signal is transformed through the layers.\nNumerical results on CIFAR-10 and CIFAR-100 data sets show that Goldilocks\nnetworks perform better than, or comparably to SELU and RELU, while introducing\ntractability of data deformation through the layers.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2002.05059v2",
        "date": "2020-02-11 07:26:30+00:00"
    },
    {
        "title": "Provable Lipschitz Certification for Generative Models",
        "authors": [
            "Matt Jordan",
            "Alexandros G. Dimakis"
        ],
        "abstract": "We present a scalable technique for upper bounding the Lipschitz constant of\ngenerative models. We relate this quantity to the maximal norm over the set of\nattainable vector-Jacobian products of a given generative model. We approximate\nthis set by layerwise convex approximations using zonotopes. Our approach\ngeneralizes and improves upon prior work using zonotope transformers and we\nextend to Lipschitz estimation of neural networks with large output dimension.\nThis provides efficient and tight bounds on small networks and can scale to\ngenerative models on VAE and DCGAN architectures.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2107.02732v1",
        "date": "2021-07-06 17:00:29+00:00"
    },
    {
        "title": "A Deep-Learning-Based Geological Parameterization for History Matching Complex Models",
        "authors": [
            "Yimin Liu",
            "Wenyue Sun",
            "Louis J. Durlofsky"
        ],
        "abstract": "A new low-dimensional parameterization based on principal component analysis\n(PCA) and convolutional neural networks (CNN) is developed to represent complex\ngeological models. The CNN-PCA method is inspired by recent developments in\ncomputer vision using deep learning. CNN-PCA can be viewed as a generalization\nof an existing optimization-based PCA (O-PCA) method. Both CNN-PCA and O-PCA\nentail post-processing a PCA model to better honor complex geological features.\nIn CNN-PCA, rather than use a histogram-based regularization as in O-PCA, a new\nregularization involving a set of metrics for multipoint statistics is\nintroduced. The metrics are based on summary statistics of the nonlinear filter\nresponses of geological models to a pre-trained deep CNN. In addition, in the\nCNN-PCA formulation presented here, a convolutional neural network is trained\nas an explicit transform function that can post-process PCA models quickly.\nCNN-PCA is shown to provide both unconditional and conditional realizations\nthat honor the geological features present in reference SGeMS geostatistical\nrealizations for a binary channelized system. Flow statistics obtained through\nsimulation of random CNN-PCA models closely match results for random SGeMS\nmodels for a demanding case in which O-PCA models lead to significant\ndiscrepancies. Results for history matching are also presented. In this\nassessment CNN-PCA is applied with derivative-free optimization, and a subspace\nrandomized maximum likelihood method is used to provide multiple posterior\nmodels. Data assimilation and significant uncertainty reduction are achieved\nfor existing wells, and physically reasonable predictions are also obtained for\nnew wells. Finally, the CNN-PCA method is extended to a more complex\nnon-stationary bimodal deltaic fan system, and is shown to provide high-quality\nrealizations for this challenging example.",
        "categories": [
            "stat.ML",
            "cs.CV",
            "cs.LG",
            "physics.geo-ph"
        ],
        "link": "http://arxiv.org/pdf/1807.02716v1",
        "date": "2018-07-07 20:34:04+00:00"
    },
    {
        "title": "Being Friends Instead of Adversaries: Deep Networks Learn from Data Simplified by Other Networks",
        "authors": [
            "Simone Marullo",
            "Matteo Tiezzi",
            "Marco Gori",
            "Stefano Melacci"
        ],
        "abstract": "Amongst a variety of approaches aimed at making the learning procedure of\nneural networks more effective, the scientific community developed strategies\nto order the examples according to their estimated complexity, to distil\nknowledge from larger networks, or to exploit the principles behind adversarial\nmachine learning. A different idea has been recently proposed, named Friendly\nTraining, which consists in altering the input data by adding an automatically\nestimated perturbation, with the goal of facilitating the learning process of a\nneural classifier. The transformation progressively fades-out as long as\ntraining proceeds, until it completely vanishes. In this work we revisit and\nextend this idea, introducing a radically different and novel approach inspired\nby the effectiveness of neural generators in the context of Adversarial Machine\nLearning. We propose an auxiliary multi-layer network that is responsible of\naltering the input data to make them easier to be handled by the classifier at\nthe current stage of the training procedure. The auxiliary network is trained\njointly with the neural classifier, thus intrinsically increasing the 'depth'\nof the classifier, and it is expected to spot general regularities in the data\nalteration process. The effect of the auxiliary network is progressively\nreduced up to the end of training, when it is fully dropped and the classifier\nis deployed for applications. We refer to this approach as Neural Friendly\nTraining. An extended experimental procedure involving several datasets and\ndifferent neural architectures shows that Neural Friendly Training overcomes\nthe originally proposed Friendly Training technique, improving the\ngeneralization of the classifier, especially in the case of noisy data.",
        "categories": [
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2112.09968v1",
        "date": "2021-12-18 16:59:35+00:00"
    },
    {
        "title": "A Novel Method Combines Moving Fronts, Data Decomposition and Deep Learning to Forecast Intricate Time Series",
        "authors": [
            "Debdarsan Niyogi"
        ],
        "abstract": "A univariate time series with high variability can pose a challenge even to\nDeep Neural Network (DNN). To overcome this, a univariate time series is\ndecomposed into simpler constituent series, whose sum equals the original\nseries. As demonstrated in this article, the conventional one-time\ndecomposition technique suffers from a leak of information from the future,\nreferred to as a data leak. In this work, a novel Moving Front (MF) method is\nproposed to prevent data leakage, so that the decomposed series can be treated\nlike other time series. Indian Summer Monsoon Rainfall (ISMR) is a very complex\ntime series, which poses a challenge to DNN and is therefore selected as an\nexample. From the many signal processing tools available, Empirical Wavelet\nTransform (EWT) was chosen for decomposing the ISMR into simpler constituent\nseries, as it was found to be more effective than the other popular algorithm,\nComplete Ensemble Empirical Mode Decomposition with Adaptive Noise (CEEMDAN).\nThe proposed MF method was used to generate the constituent leakage-free time\nseries. Predictions and forecasts were made by state-of-the-art Long and\nShort-Term Memory (LSTM) network architecture, especially suitable for making\npredictions of sequential patterns. The constituent MF series has been divided\ninto training, testing, and forecasting. It has been found that the model\n(EWT-MF-LSTM) developed here made exceptionally good train and test\npredictions, as well as Walk-Forward Validation (WFV), forecasts with\nPerformance Parameter ($PP$) values of 0.99, 0.86, and 0.95, respectively,\nwhere $PP$ = 1.0 signifies perfect reproduction of the data.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2303.06394v1",
        "date": "2023-03-11 12:07:26+00:00"
    },
    {
        "title": "Nonlinear Optical Data Transformer for Machine Learning",
        "authors": [
            "Mustafa Yildirim",
            "Ilker Oguz",
            "Fabian Kaufmann",
            "Marc Reig Escale",
            "Rachel Grange",
            "Demetri Psaltis",
            "Christophe Moser"
        ],
        "abstract": "Modern machine learning models use an ever-increasing number of parameters to\ntrain (175 billion parameters for GPT-3) with large datasets to obtain better\nperformance. Bigger is better has been the norm. Optical computing has been\nreawakened as a potential solution to large-scale computing through optical\naccelerators that carry out linear operations while reducing electrical power.\nHowever, to achieve efficient computing with light, creating and controlling\nnonlinearity optically rather than electronically remains a challenge. This\nstudy explores a reservoir computing (RC) approach whereby a 14 mm long\nfew-mode waveguide in LiNbO3 on insulator is used as a complex nonlinear\noptical processor. A dataset is encoded digitally on the spectrum of a\nfemtosecond pulse which is then launched in the waveguide. The output spectrum\ndepends nonlinearly on the input. We experimentally show that a simple digital\nlinear classifier with 784 parameters using the output spectrum from the\nwaveguide as input increased the classification accuracy of several databases\ncompared to non-transformed data, approximately 10$\\%$. In comparison, a deep\ndigital neural network (NN) with 40000 parameters was necessary to achieve the\nsame accuracy. Reducing the number of parameters by a factor of $\\sim$50\nillustrates that a compact optical RC approach can perform on par with a deep\ndigital NN.",
        "categories": [
            "physics.optics",
            "cs.AI",
            "cs.ET",
            "cs.LG",
            "physics.app-ph"
        ],
        "link": "http://arxiv.org/pdf/2208.09398v1",
        "date": "2022-08-19 15:28:48+00:00"
    },
    {
        "title": "EPVT: Environment-aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition",
        "authors": [
            "Siyuan Yan",
            "Chi Liu",
            "Zhen Yu",
            "Lie Ju",
            "Dwarikanath Mahapatrainst",
            "Victoria Mar",
            "Monika Janda",
            "Peter Soyer",
            "Zongyuan Ge"
        ],
        "abstract": "Skin lesion recognition using deep learning has made remarkable progress, and\nthere is an increasing need for deploying these systems in real-world\nscenarios. However, recent research has revealed that deep neural networks for\nskin lesion recognition may overly depend on disease-irrelevant image artifacts\n(i.e. dark corners, dense hairs), leading to poor generalization in unseen\nenvironments. To address this issue, we propose a novel domain generalization\nmethod called EPVT, which involves embedding prompts into the vision\ntransformer to collaboratively learn knowledge from diverse domains.\nConcretely, EPVT leverages a set of domain prompts, each of which plays as a\ndomain expert, to capture domain-specific knowledge; and a shared prompt for\ngeneral knowledge over the entire dataset. To facilitate knowledge sharing and\nthe interaction of different prompts, we introduce a domain prompt generator\nthat enables low-rank multiplicative updates between domain prompts and the\nshared prompt. A domain mixup strategy is additionally devised to reduce the\nco-occurring artifacts in each domain, which allows for more flexible decision\nmargins and mitigates the issue of incorrectly assigned domain labels.\nExperiments on four out-of-distribution datasets and six different biased ISIC\ndatasets demonstrate the superior generalization ability of EPVT in skin lesion\nrecognition across various environments. Our code and dataset will be released\nat https://github.com/SiyuanYan1/EPVT.",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2304.01508v2",
        "date": "2023-04-04 03:36:14+00:00"
    },
    {
        "title": "Ontology Pre-training for Poison Prediction",
        "authors": [
            "Martin Glauer",
            "Fabian Neuhaus",
            "Till Mossakowski",
            "Janna Hastings"
        ],
        "abstract": "Integrating human knowledge into neural networks has the potential to improve\ntheir robustness and interpretability. We have developed a novel approach to\nintegrate knowledge from ontologies into the structure of a Transformer network\nwhich we call ontology pre-training: we train the network to predict membership\nin ontology classes as a way to embed the structure of the ontology into the\nnetwork, and subsequently fine-tune the network for the particular prediction\ntask. We apply this approach to a case study in predicting the potential\ntoxicity of a small molecule based on its molecular structure, a challenging\ntask for machine learning in life sciences chemistry. Our approach improves on\nthe state of the art, and moreover has several additional benefits. First, we\nare able to show that the model learns to focus attention on more meaningful\nchemical groups when making predictions with ontology pre-training than\nwithout, paving a path towards greater robustness and interpretability. Second,\nthe training time is reduced after ontology pre-training, indicating that the\nmodel is better placed to learn what matters for toxicity prediction with the\nontology pre-training than without. This strategy has general applicability as\na neuro-symbolic approach to embed meaningful semantics into neural networks.",
        "categories": [
            "cs.AI",
            "cs.LG",
            "q-bio.QM"
        ],
        "link": "http://arxiv.org/pdf/2301.08577v1",
        "date": "2023-01-20 13:47:11+00:00"
    },
    {
        "title": "Feature learning in feature-sample networks using multi-objective optimization",
        "authors": [
            "Filipe Alves Neto Verri",
            "Renato Tin\u00f3s",
            "Liang Zhao"
        ],
        "abstract": "Data and knowledge representation are fundamental concepts in machine\nlearning. The quality of the representation impacts the performance of the\nlearning model directly. Feature learning transforms or enhances raw data to\nstructures that are effectively exploited by those models. In recent years,\nseveral works have been using complex networks for data representation and\nanalysis. However, no feature learning method has been proposed for such\ncategory of techniques. Here, we present an unsupervised feature learning\nmechanism that works on datasets with binary features. First, the dataset is\nmapped into a feature--sample network. Then, a multi-objective optimization\nprocess selects a set of new vertices to produce an enhanced version of the\nnetwork. The new features depend on a nonlinear function of a combination of\npreexisting features. Effectively, the process projects the input data into a\nhigher-dimensional space. To solve the optimization problem, we design two\nmetaheuristics based on the lexicographic genetic algorithm and the improved\nstrength Pareto evolutionary algorithm (SPEA2). We show that the enhanced\nnetwork contains more information and can be exploited to improve the\nperformance of machine learning methods. The advantages and disadvantages of\neach optimization strategy are discussed.",
        "categories": [
            "cs.AI",
            "cs.LG",
            "cs.NE"
        ],
        "link": "http://arxiv.org/pdf/1710.09300v1",
        "date": "2017-10-25 15:18:27+00:00"
    },
    {
        "title": "Arch-Net: Model Distillation for Architecture Agnostic Model Deployment",
        "authors": [
            "Weixin Xu",
            "Zipeng Feng",
            "Shuangkang Fang",
            "Song Yuan",
            "Yi Yang",
            "Shuchang Zhou"
        ],
        "abstract": "Vast requirement of computation power of Deep Neural Networks is a major\nhurdle to their real world applications. Many recent Application Specific\nIntegrated Circuit (ASIC) chips feature dedicated hardware support for Neural\nNetwork Acceleration. However, as ASICs take multiple years to develop, they\nare inevitably out-paced by the latest development in Neural Architecture\nResearch. For example, Transformer Networks do not have native support on many\npopular chips, and hence are difficult to deploy. In this paper, we propose\nArch-Net, a family of Neural Networks made up of only operators efficiently\nsupported across most architectures of ASICs. When a Arch-Net is produced, less\ncommon network constructs, like Layer Normalization and Embedding Layers, are\neliminated in a progressive manner through label-free Blockwise Model\nDistillation, while performing sub-eight bit quantization at the same time to\nmaximize performance. Empirical results on machine translation and image\nclassification tasks confirm that we can transform latest developed Neural\nArchitectures into fast running and as-accurate Arch-Net, ready for deployment\non multiple mass-produced ASIC chips. The code will be available at\nhttps://github.com/megvii-research/Arch-Net.",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "link": "http://arxiv.org/pdf/2111.01135v2",
        "date": "2021-11-01 15:49:32+00:00"
    },
    {
        "title": "Theoretical Aspects of Group Equivariant Neural Networks",
        "authors": [
            "Carlos Esteves"
        ],
        "abstract": "Group equivariant neural networks have been explored in the past few years\nand are interesting from theoretical and practical standpoints. They leverage\nconcepts from group representation theory, non-commutative harmonic analysis\nand differential geometry that do not often appear in machine learning. In\npractice, they have been shown to reduce sample and model complexity, notably\nin challenging tasks where input transformations such as arbitrary rotations\nare present. We begin this work with an exposition of group representation\ntheory and the machinery necessary to define and evaluate integrals and\nconvolutions on groups. Then, we show applications to recent SO(3) and SE(3)\nequivariant networks, namely the Spherical CNNs, Clebsch-Gordan Networks, and\n3D Steerable CNNs. We proceed to discuss two recent theoretical results. The\nfirst, by Kondor and Trivedi (ICML'18), shows that a neural network is group\nequivariant if and only if it has a convolutional structure. The second, by\nCohen et al. (NeurIPS'19), generalizes the first to a larger class of networks,\nwith feature maps as fields on homogeneous spaces.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2004.05154v2",
        "date": "2020-04-10 17:57:27+00:00"
    },
    {
        "title": "\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier",
        "authors": [
            "Marco Tulio Ribeiro",
            "Sameer Singh",
            "Carlos Guestrin"
        ],
        "abstract": "Despite widespread adoption, machine learning models remain mostly black\nboxes. Understanding the reasons behind predictions is, however, quite\nimportant in assessing trust, which is fundamental if one plans to take action\nbased on a prediction, or when choosing whether to deploy a new model. Such\nunderstanding also provides insights into the model, which can be used to\ntransform an untrustworthy model or prediction into a trustworthy one. In this\nwork, we propose LIME, a novel explanation technique that explains the\npredictions of any classifier in an interpretable and faithful manner, by\nlearning an interpretable model locally around the prediction. We also propose\na method to explain models by presenting representative individual predictions\nand their explanations in a non-redundant way, framing the task as a submodular\noptimization problem. We demonstrate the flexibility of these methods by\nexplaining different models for text (e.g. random forests) and image\nclassification (e.g. neural networks). We show the utility of explanations via\nnovel experiments, both simulated and with human subjects, on various scenarios\nthat require trust: deciding if one should trust a prediction, choosing between\nmodels, improving an untrustworthy classifier, and identifying why a classifier\nshould not be trusted.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1602.04938v3",
        "date": "2016-02-16 08:20:14+00:00"
    },
    {
        "title": "Data Augmentation through Expert-guided Symmetry Detection to Improve Performance in Offline Reinforcement Learning",
        "authors": [
            "Giorgio Angelotti",
            "Nicolas Drougard",
            "Caroline P. C. Chanel"
        ],
        "abstract": "Offline estimation of the dynamical model of a Markov Decision Process (MDP)\nis a non-trivial task that greatly depends on the data available in the\nlearning phase. Sometimes the dynamics of the model is invariant with respect\nto some transformations of the current state and action. Recent works showed\nthat an expert-guided pipeline relying on Density Estimation methods as Deep\nNeural Network based Normalizing Flows effectively detects this structure in\ndeterministic environments, both categorical and continuous-valued. The\nacquired knowledge can be exploited to augment the original data set, leading\neventually to a reduction in the distributional shift between the true and the\nlearned model. Such data augmentation technique can be exploited as a\npreliminary process to be executed before adopting an Offline Reinforcement\nLearning architecture, increasing its performance. In this work we extend the\nparadigm to also tackle non-deterministic MDPs, in particular, 1) we propose a\ndetection threshold in categorical environments based on statistical distances,\nand 2) we show that the former results lead to a performance improvement when\nsolving the learned MDP and then applying the optimized policy in the real\nenvironment.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2112.09943v3",
        "date": "2021-12-18 14:32:32+00:00"
    },
    {
        "title": "Towards Understanding Grokking: An Effective Theory of Representation Learning",
        "authors": [
            "Ziming Liu",
            "Ouail Kitouni",
            "Niklas Nolte",
            "Eric J. Michaud",
            "Max Tegmark",
            "Mike Williams"
        ],
        "abstract": "We aim to understand grokking, a phenomenon where models generalize long\nafter overfitting their training set. We present both a microscopic analysis\nanchored by an effective theory and a macroscopic analysis of phase diagrams\ndescribing learning performance across hyperparameters. We find that\ngeneralization originates from structured representations whose training\ndynamics and dependence on training set size can be predicted by our effective\ntheory in a toy setting. We observe empirically the presence of four learning\nphases: comprehension, grokking, memorization, and confusion. We find\nrepresentation learning to occur only in a \"Goldilocks zone\" (including\ncomprehension and grokking) between memorization and confusion. We find on\ntransformers the grokking phase stays closer to the memorization phase\n(compared to the comprehension phase), leading to delayed generalization. The\nGoldilocks phase is reminiscent of \"intelligence from starvation\" in Darwinian\nevolution, where resource limitations drive discovery of more efficient\nsolutions. This study not only provides intuitive explanations of the origin of\ngrokking, but also highlights the usefulness of physics-inspired tools, e.g.,\neffective theories and phase diagrams, for understanding deep learning.",
        "categories": [
            "cs.LG",
            "cond-mat.dis-nn",
            "cond-mat.stat-mech",
            "cs.AI",
            "physics.class-ph"
        ],
        "link": "http://arxiv.org/pdf/2205.10343v2",
        "date": "2022-05-20 17:56:17+00:00"
    },
    {
        "title": "Git Re-Basin: Merging Models modulo Permutation Symmetries",
        "authors": [
            "Samuel K. Ainsworth",
            "Jonathan Hayase",
            "Siddhartha Srinivasa"
        ],
        "abstract": "The success of deep learning is due in large part to our ability to solve\ncertain massive non-convex optimization problems with relative ease. Though\nnon-convex optimization is NP-hard, simple algorithms -- often variants of\nstochastic gradient descent -- exhibit surprising effectiveness in fitting\nlarge neural networks in practice. We argue that neural network loss landscapes\noften contain (nearly) a single basin after accounting for all possible\npermutation symmetries of hidden units a la Entezari et al. 2021. We introduce\nthree algorithms to permute the units of one model to bring them into alignment\nwith a reference model in order to merge the two models in weight space. This\ntransformation produces a functionally equivalent set of weights that lie in an\napproximately convex basin near the reference model. Experimentally, we\ndemonstrate the single basin phenomenon across a variety of model architectures\nand datasets, including the first (to our knowledge) demonstration of\nzero-barrier linear mode connectivity between independently trained ResNet\nmodels on CIFAR-10. Additionally, we identify intriguing phenomena relating\nmodel width and training time to mode connectivity. Finally, we discuss\nshortcomings of the linear mode connectivity hypothesis, including a\ncounterexample to the single basin theory.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2209.04836v6",
        "date": "2022-09-11 10:44:27+00:00"
    },
    {
        "title": "Music Signal Processing Using Vector Product Neural Networks",
        "authors": [
            "Z. C. Fan",
            "T. S. Chan",
            "Y. H. Yang",
            "J. S. R. Jang"
        ],
        "abstract": "We propose a novel neural network model for music signal processing using\nvector product neurons and dimensionality transformations. Here, the inputs are\nfirst mapped from real values into three-dimensional vectors then fed into a\nthree-dimensional vector product neural network where the inputs, outputs, and\nweights are all three-dimensional values. Next, the final outputs are mapped\nback to the reals. Two methods for dimensionality transformation are proposed,\none via context windows and the other via spectral coloring. Experimental\nresults on the iKala dataset for blind singing voice separation confirm the\nefficacy of our model.",
        "categories": [
            "cs.SD",
            "cs.LG",
            "cs.MM",
            "cs.NE",
            "68Txx",
            "C.1.3; H.5.1"
        ],
        "link": "http://arxiv.org/pdf/1706.09555v1",
        "date": "2017-06-29 02:41:30+00:00"
    },
    {
        "title": "Spatial Mixture Models with Learnable Deep Priors for Perceptual Grouping",
        "authors": [
            "Jinyang Yuan",
            "Bin Li",
            "Xiangyang Xue"
        ],
        "abstract": "Humans perceive the seemingly chaotic world in a structured and compositional\nway with the prerequisite of being able to segregate conceptual entities from\nthe complex visual scenes. The mechanism of grouping basic visual elements of\nscenes into conceptual entities is termed as perceptual grouping. In this work,\nwe propose a new type of spatial mixture models with learnable priors for\nperceptual grouping. Different from existing methods, the proposed method\ndisentangles the attributes of an object into ``shape'' and ``appearance''\nwhich are modeled separately by the mixture weights and the mixture components.\nMore specifically, each object in the visual scene is fully characterized by\none latent representation, which is in turn transformed into parameters of the\nmixture weight and the mixture component by two neural networks. The mixture\nweights focus on modeling spatial dependencies (i.e., shape) and the mixture\ncomponents deal with intra-object variations (i.e., appearance). In addition,\nthe background is separately modeled as a special component complementary to\nthe foreground objects. Our extensive empirical tests on two perceptual\ngrouping datasets demonstrate that the proposed method outperforms the\nstate-of-the-art methods under most experimental configurations. The learned\nconceptual entities are generalizable to novel visual scenes and insensitive to\nthe diversity of objects. Code is available at\nhttps://github.com/jinyangyuan/learnable-deep-priors.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1902.02502v2",
        "date": "2019-02-07 07:33:12+00:00"
    },
    {
        "title": "Efficient Sparsely Activated Transformers",
        "authors": [
            "Salar Latifi",
            "Saurav Muralidharan",
            "Michael Garland"
        ],
        "abstract": "Transformer-based neural networks have achieved state-of-the-art task\nperformance in a number of machine learning domains including natural language\nprocessing and computer vision. To further improve their accuracy, recent work\nhas explored the integration of dynamic behavior into these networks in the\nform of mixture-of-expert (MoE) layers. In this paper, we explore the\nintroduction of MoE layers to optimize a different metric: inference latency.\nWe introduce a novel system named PLANER that takes an existing\nTransformer-based network and a user-defined latency target and produces an\noptimized, sparsely-activated version of the original network that tries to\nmeet the latency target while maintaining baseline accuracy. We evaluate PLANER\non two real-world language modeling tasks using the Transformer-XL network and\nachieve inference latency reductions of over 2x at iso-accuracy.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "link": "http://arxiv.org/pdf/2208.14580v1",
        "date": "2022-08-31 00:44:27+00:00"
    },
    {
        "title": "BOCK : Bayesian Optimization with Cylindrical Kernels",
        "authors": [
            "ChangYong Oh",
            "Efstratios Gavves",
            "Max Welling"
        ],
        "abstract": "A major challenge in Bayesian Optimization is the boundary issue (Swersky,\n2017) where an algorithm spends too many evaluations near the boundary of its\nsearch space. In this paper, we propose BOCK, Bayesian Optimization with\nCylindrical Kernels, whose basic idea is to transform the ball geometry of the\nsearch space using a cylindrical transformation. Because of the transformed\ngeometry, the Gaussian Process-based surrogate model spends less budget\nsearching near the boundary, while concentrating its efforts relatively more\nnear the center of the search region, where we expect the solution to be\nlocated. We evaluate BOCK extensively, showing that it is not only more\naccurate and efficient, but it also scales successfully to problems with a\ndimensionality as high as 500. We show that the better accuracy and scalability\nof BOCK even allows optimizing modestly sized neural network layers, as well as\nneural network hyperparameters.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1806.01619v2",
        "date": "2018-06-05 11:57:03+00:00"
    },
    {
        "title": "Optimising for Interpretability: Convolutional Dynamic Alignment Networks",
        "authors": [
            "Moritz B\u00f6hle",
            "Mario Fritz",
            "Bernt Schiele"
        ],
        "abstract": "We introduce a new family of neural network models called Convolutional\nDynamic Alignment Networks (CoDA Nets), which are performant classifiers with a\nhigh degree of inherent interpretability. Their core building blocks are\nDynamic Alignment Units (DAUs), which are optimised to transform their inputs\nwith dynamically computed weight vectors that align with task-relevant\npatterns. As a result, CoDA Nets model the classification prediction through a\nseries of input-dependent linear transformations, allowing for linear\ndecomposition of the output into individual input contributions. Given the\nalignment of the DAUs, the resulting contribution maps align with\ndiscriminative input patterns. These model-inherent decompositions are of high\nvisual quality and outperform existing attribution methods under quantitative\nmetrics. Further, CoDA Nets constitute performant classifiers, achieving on par\nresults to ResNet and VGG models on e.g. CIFAR-10 and TinyImagenet. Lastly,\nCoDA Nets can be combined with conventional neural network models to yield\npowerful classifiers that more easily scale to complex datasets such as\nImagenet whilst exhibiting an increased interpretable depth, i.e., the output\ncan be explained well in terms of contributions from intermediate layers within\nthe network.",
        "categories": [
            "stat.ML",
            "cs.CV",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2109.13004v1",
        "date": "2021-09-27 12:39:46+00:00"
    },
    {
        "title": "Learning to Plan Chemical Syntheses",
        "authors": [
            "Marwin H. S. Segler",
            "Mike Preuss",
            "Mark P. Waller"
        ],
        "abstract": "From medicines to materials, small organic molecules are indispensable for\nhuman well-being. To plan their syntheses, chemists employ a problem solving\ntechnique called retrosynthesis. In retrosynthesis, target molecules are\nrecursively transformed into increasingly simpler precursor compounds until a\nset of readily available starting materials is obtained. Computer-aided\nretrosynthesis would be a highly valuable tool, however, past approaches were\nslow and provided results of unsatisfactory quality. Here, we employ Monte\nCarlo Tree Search (MCTS) to efficiently discover retrosynthetic routes. MCTS\nwas combined with an expansion policy network that guides the search, and an\n\"in-scope\" filter network to pre-select the most promising retrosynthetic\nsteps. These deep neural networks were trained on 12 million reactions, which\nrepresents essentially all reactions ever published in organic chemistry. Our\nsystem solves almost twice as many molecules and is 30 times faster in\ncomparison to the traditional search method based on extracted rules and\nhand-coded heuristics. Finally after a 60 year history of computer-aided\nsynthesis planning, chemists can no longer distinguish between routes generated\nby a computer system and real routes taken from the scientific literature. We\nanticipate that our method will accelerate drug and materials discovery by\nassisting chemists to plan better syntheses faster, and by enabling fully\nautomated robot synthesis.",
        "categories": [
            "cs.AI",
            "cs.LG",
            "physics.chem-ph"
        ],
        "link": "http://arxiv.org/pdf/1708.04202v1",
        "date": "2017-08-14 16:46:08+00:00"
    },
    {
        "title": "Re-Weighted Learning for Sparsifying Deep Neural Networks",
        "authors": [
            "Igor Fedorov",
            "Bhaskar D. Rao"
        ],
        "abstract": "This paper addresses the topic of sparsifying deep neural networks (DNN's).\nWhile DNN's are powerful models that achieve state-of-the-art performance on a\nlarge number of tasks, the large number of model parameters poses serious\nstorage and computational challenges. To combat these difficulties, a growing\nline of work focuses on pruning network weights without sacrificing\nperformance. We propose a general affine scaling transformation (AST) algorithm\nto sparsify DNN's. Our approach follows in the footsteps of popular sparse\nrecovery techniques, which have yet to be explored in the context of DNN's. We\ndescribe a principled framework for transforming densely connected DNN's into\nsparsely connected ones without sacrificing network performance. Unlike\nexisting methods, our approach is able to learn sparse connections at each\nlayer simultaneously, and achieves comparable pruning results on the\narchitecture tested.",
        "categories": [
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1802.01616v1",
        "date": "2018-02-05 19:31:49+00:00"
    },
    {
        "title": "A Framework for Semi-Automatic Precision and Accuracy Analysis for Fast and Rigorous Deep Learning",
        "authors": [
            "Christoph Lauter",
            "Anastasia Volkova"
        ],
        "abstract": "Deep Neural Networks (DNN) represent a performance-hungry application.\nFloating-Point (FP) and custom floating-point-like arithmetic satisfies this\nhunger. While there is need for speed, inference in DNNs does not seem to have\nany need for precision. Many papers experimentally observe that DNNs can\nsuccessfully run at almost ridiculously low precision.\n  The aim of this paper is two-fold: first, to shed some theoretical light upon\nwhy a DNN's FP accuracy stays high for low FP precision. We observe that the\nloss of relative accuracy in the convolutional steps is recovered by the\nactivation layers, which are extremely well-conditioned. We give an\ninterpretation for the link between precision and accuracy in DNNs.\n  Second, the paper presents a software framework for semi-automatic FP error\nanalysis for the inference phase of deep-learning. Compatible with common\nTensorflow/Keras models, it leverages the frugally-deep Python/C++ library to\ntransform a neural network into C++ code in order to analyze the network's need\nfor precision. This rigorous analysis is based on Interval and Affine\narithmetics to compute absolute and relative error bounds for a DNN. We\ndemonstrate our tool with several examples.",
        "categories": [
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2002.03869v1",
        "date": "2020-02-10 15:33:19+00:00"
    },
    {
        "title": "Do Deep Generative Models Know What They Don't Know?",
        "authors": [
            "Eric Nalisnick",
            "Akihiro Matsukawa",
            "Yee Whye Teh",
            "Dilan Gorur",
            "Balaji Lakshminarayanan"
        ],
        "abstract": "A neural network deployed in the wild may be asked to make predictions for\ninputs that were drawn from a different distribution than that of the training\ndata. A plethora of work has demonstrated that it is easy to find or synthesize\ninputs for which a neural network is highly confident yet wrong. Generative\nmodels are widely viewed to be robust to such mistaken confidence as modeling\nthe density of the input features can be used to detect novel,\nout-of-distribution inputs. In this paper we challenge this assumption. We find\nthat the density learned by flow-based models, VAEs, and PixelCNNs cannot\ndistinguish images of common objects such as dogs, trucks, and horses (i.e.\nCIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher\nlikelihood to the latter when the model is trained on the former. Moreover, we\nfind evidence of this phenomenon when pairing several popular image data sets:\nFashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN.\nTo investigate this curious behavior, we focus analysis on flow-based\ngenerative models in particular since they are trained and evaluated via the\nexact marginal likelihood. We find such behavior persists even when we restrict\nthe flows to constant-volume transformations. These transformations admit some\ntheoretical analysis, and we show that the difference in likelihoods can be\nexplained by the location and variances of the data and the model curvature.\nOur results caution against using the density estimates from deep generative\nmodels to identify inputs similar to the training distribution until their\nbehavior for out-of-distribution inputs is better understood.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1810.09136v3",
        "date": "2018-10-22 08:32:02+00:00"
    },
    {
        "title": "Learning Latent Representations in Neural Networks for Clustering through Pseudo Supervision and Graph-based Activity Regularization",
        "authors": [
            "Ozsel Kilinc",
            "Ismail Uysal"
        ],
        "abstract": "In this paper, we propose a novel unsupervised clustering approach exploiting\nthe hidden information that is indirectly introduced through a pseudo\nclassification objective. Specifically, we randomly assign a pseudo\nparent-class label to each observation which is then modified by applying the\ndomain specific transformation associated with the assigned label. Generated\npseudo observation-label pairs are subsequently used to train a neural network\nwith Auto-clustering Output Layer (ACOL) that introduces multiple softmax nodes\nfor each pseudo parent-class. Due to the unsupervised objective based on\nGraph-based Activity Regularization (GAR) terms, softmax duplicates of each\nparent-class are specialized as the hidden information captured through the\nhelp of domain specific transformations is propagated during training.\nUltimately we obtain a k-means friendly latent representation. Furthermore, we\ndemonstrate how the chosen transformation type impacts performance and helps\npropagate the latent information that is useful in revealing unknown clusters.\nOur results show state-of-the-art performance for unsupervised clustering tasks\non MNIST, SVHN and USPS datasets, with the highest accuracies reported to date\nin the literature.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1802.03063v1",
        "date": "2018-02-08 22:13:59+00:00"
    },
    {
        "title": "The Hidden Convex Optimization Landscape of Two-Layer ReLU Neural Networks: an Exact Characterization of the Optimal Solutions",
        "authors": [
            "Yifei Wang",
            "Jonathan Lacotte",
            "Mert Pilanci"
        ],
        "abstract": "We prove that finding all globally optimal two-layer ReLU neural networks can\nbe performed by solving a convex optimization program with cone constraints.\nOur analysis is novel, characterizes all optimal solutions, and does not\nleverage duality-based analysis which was recently used to lift neural network\ntraining into convex spaces. Given the set of solutions of our convex\noptimization program, we show how to construct exactly the entire set of\noptimal neural networks. We provide a detailed characterization of this optimal\nset and its invariant transformations. As additional consequences of our convex\nperspective, (i) we establish that Clarke stationary points found by stochastic\ngradient descent correspond to the global optimum of a subsampled convex\nproblem (ii) we provide a polynomial-time algorithm for checking if a neural\nnetwork is a global minimum of the training loss (iii) we provide an explicit\nconstruction of a continuous path between any neural network and the global\nminimum of its sublevel set and (iv) characterize the minimal size of the\nhidden layer so that the neural network optimization landscape has no spurious\nvalleys. Overall, we provide a rich framework for studying the landscape of\nneural network training loss through convexity.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2006.05900v4",
        "date": "2020-06-10 15:38:30+00:00"
    },
    {
        "title": "Training Augmentation with Adversarial Examples for Robust Speech Recognition",
        "authors": [
            "Sining Sun",
            "Ching-Feng Yeh",
            "Mari Ostendorf",
            "Mei-Yuh Hwang",
            "Lei Xie"
        ],
        "abstract": "This paper explores the use of adversarial examples in training speech\nrecognition systems to increase robustness of deep neural network acoustic\nmodels. During training, the fast gradient sign method is used to generate\nadversarial examples augmenting the original training data. Different from\nconventional data augmentation based on data transformations, the examples are\ndynamically generated based on current acoustic model parameters. We assess the\nimpact of adversarial data augmentation in experiments on the Aurora-4 and\nCHiME-4 single-channel tasks, showing improved robustness against noise and\nchannel variation. Further improvement is obtained when combining adversarial\nexamples with teacher/student training, leading to a 23% relative word error\nrate reduction on Aurora-4.",
        "categories": [
            "cs.CL",
            "cs.LG",
            "eess.AS",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1806.02782v2",
        "date": "2018-06-07 16:53:12+00:00"
    },
    {
        "title": "Daydream: Accurately Estimating the Efficacy of Optimizations for DNN Training",
        "authors": [
            "Hongyu Zhu",
            "Amar Phanishayee",
            "Gennady Pekhimenko"
        ],
        "abstract": "Modern deep neural network (DNN) training jobs use complex and heterogeneous\nsoftware/hardware stacks. The efficacy of software-level optimizations can vary\nsignificantly when used in different deployment configurations. It is onerous\nand error-prone for ML practitioners and system developers to implement each\noptimization separately, and determine which ones will improve performance in\ntheir own configurations. Unfortunately, existing profiling tools do not aim to\nanswer predictive questions such as \"How will optimization X affect the\nperformance of my model?\". We address this critical limitation, and proposes a\nnew profiling tool, Daydream, to help programmers efficiently explore the\nefficacy of DNN optimizations. Daydream models DNN execution with a\nfine-grained dependency graph based on low-level traces collected by CUPTI, and\npredicts runtime by simulating execution based on the dependency graph.\nDaydream maps the low-level traces using DNN domain-specific knowledge, and\nintroduces a set of graph-transformation primitives that can easily model a\nwide variety of optimizations. We show that Daydream is able to model most\nmainstream DNN optimization techniques, and accurately predict the efficacy of\noptimizations that will result in significant performance improvements.",
        "categories": [
            "cs.DC",
            "cs.LG",
            "cs.PF",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2006.03318v1",
        "date": "2020-06-05 09:08:16+00:00"
    },
    {
        "title": "Automatic Cross-Replica Sharding of Weight Update in Data-Parallel Training",
        "authors": [
            "Yuanzhong Xu",
            "HyoukJoong Lee",
            "Dehao Chen",
            "Hongjun Choi",
            "Blake Hechtman",
            "Shibo Wang"
        ],
        "abstract": "In data-parallel synchronous training of deep neural networks, different\ndevices (replicas) run the same program with different partitions of the\ntraining batch, but weight update computation is repeated on all replicas,\nbecause the weights do not have a batch dimension to partition. This can be a\nbottleneck for performance and scalability in typical language models with\nlarge weights, and models with small per-replica batch size which is typical in\nlarge-scale training. This paper presents an approach to automatically shard\nthe weight update computation across replicas with efficient communication\nprimitives and data formatting, using static analysis and transformations on\nthe training computation graph. We show this technique achieves substantial\nspeedups on typical image and language models on Cloud TPUs, requiring no\nchange to model code. This technique helps close the gap between traditionally\nexpensive (ADAM) and cheap (SGD) optimizers, as they will only take a small\npart of training step time and have similar peak memory usage. It helped us to\nachieve state-of-the-art training performance in Google's MLPerf 0.6\nsubmission.",
        "categories": [
            "cs.DC",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2004.13336v1",
        "date": "2020-04-28 07:13:50+00:00"
    },
    {
        "title": "Stochastic normalizing flows as non-equilibrium transformations",
        "authors": [
            "Michele Caselle",
            "Elia Cellini",
            "Alessandro Nada",
            "Marco Panero"
        ],
        "abstract": "Normalizing flows are a class of deep generative models that provide a\npromising route to sample lattice field theories more efficiently than\nconventional Monte Carlo simulations. In this work we show that the theoretical\nframework of stochastic normalizing flows, in which neural-network layers are\ncombined with Monte Carlo updates, is the same that underlies\nout-of-equilibrium simulations based on Jarzynski's equality, which have been\nrecently deployed to compute free-energy differences in lattice gauge theories.\nWe lay out a strategy to optimize the efficiency of this extended class of\ngenerative models and present examples of applications.",
        "categories": [
            "hep-lat",
            "cond-mat.stat-mech",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2201.08862v3",
        "date": "2022-01-21 19:00:18+00:00"
    },
    {
        "title": "Efficient Feature Transformations for Discriminative and Generative Continual Learning",
        "authors": [
            "Vinay Kumar Verma",
            "Kevin J Liang",
            "Nikhil Mehta",
            "Piyush Rai",
            "Lawrence Carin"
        ],
        "abstract": "As neural networks are increasingly being applied to real-world applications,\nmechanisms to address distributional shift and sequential task learning without\nforgetting are critical. Methods incorporating network expansion have shown\npromise by naturally adding model capacity for learning new tasks while\nsimultaneously avoiding catastrophic forgetting. However, the growth in the\nnumber of additional parameters of many of these types of methods can be\ncomputationally expensive at larger scales, at times prohibitively so. Instead,\nwe propose a simple task-specific feature map transformation strategy for\ncontinual learning, which we call Efficient Feature Transformations (EFTs).\nThese EFTs provide powerful flexibility for learning new tasks, achieved with\nminimal parameters added to the base architecture. We further propose a feature\ndistance maximization strategy, which significantly improves task prediction in\nclass incremental settings, without needing expensive generative models. We\ndemonstrate the efficacy and efficiency of our method with an extensive set of\nexperiments in discriminative (CIFAR-100 and ImageNet-1K) and generative (LSUN,\nCUB-200, Cats) sequences of tasks. Even with low single-digit parameter growth\nrates, EFTs can outperform many other continual learning methods in a wide\nrange of settings.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "link": "http://arxiv.org/pdf/2103.13558v1",
        "date": "2021-03-25 01:48:14+00:00"
    },
    {
        "title": "NICE: Non-linear Independent Components Estimation",
        "authors": [
            "Laurent Dinh",
            "David Krueger",
            "Yoshua Bengio"
        ],
        "abstract": "We propose a deep learning framework for modeling complex high-dimensional\ndensities called Non-linear Independent Component Estimation (NICE). It is\nbased on the idea that a good representation is one in which the data has a\ndistribution that is easy to model. For this purpose, a non-linear\ndeterministic transformation of the data is learned that maps it to a latent\nspace so as to make the transformed data conform to a factorized distribution,\ni.e., resulting in independent latent variables. We parametrize this\ntransformation so that computing the Jacobian determinant and inverse transform\nis trivial, yet we maintain the ability to learn complex non-linear\ntransformations, via a composition of simple building blocks, each based on a\ndeep neural network. The training criterion is simply the exact log-likelihood,\nwhich is tractable. Unbiased ancestral sampling is also easy. We show that this\napproach yields good generative models on four image datasets and can be used\nfor inpainting.",
        "categories": [
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1410.8516v6",
        "date": "2014-10-30 19:44:20+00:00"
    },
    {
        "title": "Scale-invariant Bayesian Neural Networks with Connectivity Tangent Kernel",
        "authors": [
            "SungYub Kim",
            "Sihwan Park",
            "Kyungsu Kim",
            "Eunho Yang"
        ],
        "abstract": "Explaining generalizations and preventing over-confident predictions are\ncentral goals of studies on the loss landscape of neural networks. Flatness,\ndefined as loss invariability on perturbations of a pre-trained solution, is\nwidely accepted as a predictor of generalization in this context. However, the\nproblem that flatness and generalization bounds can be changed arbitrarily\naccording to the scale of a parameter was pointed out, and previous studies\npartially solved the problem with restrictions: Counter-intuitively, their\ngeneralization bounds were still variant for the function-preserving parameter\nscaling transformation or limited only to an impractical network structure. As\na more fundamental solution, we propose new prior and posterior distributions\ninvariant to scaling transformations by \\textit{decomposing} the scale and\nconnectivity of parameters, thereby allowing the resulting generalization bound\nto describe the generalizability of a broad class of networks with the more\npractical class of transformations such as weight decay with batch\nnormalization. We also show that the above issue adversely affects the\nuncertainty calibration of Laplace approximation and propose a solution using\nour invariant posterior. We empirically demonstrate our posterior provides\neffective flatness and calibration measures with low complexity in such a\npractical parameter transformation case, supporting its practical effectiveness\nin line with our rationale.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2209.15208v1",
        "date": "2022-09-30 03:31:13+00:00"
    },
    {
        "title": "Clebsch-Gordan Nets: a Fully Fourier Space Spherical Convolutional Neural Network",
        "authors": [
            "Risi Kondor",
            "Zhen Lin",
            "Shubhendu Trivedi"
        ],
        "abstract": "Recent work by Cohen \\emph{et al.} has achieved state-of-the-art results for\nlearning spherical images in a rotation invariant way by using ideas from group\nrepresentation theory and noncommutative harmonic analysis. In this paper we\npropose a generalization of this work that generally exhibits improved\nperformace, but from an implementation point of view is actually simpler. An\nunusual feature of the proposed architecture is that it uses the\nClebsch--Gordan transform as its only source of nonlinearity, thus avoiding\nrepeated forward and backward Fourier transforms. The underlying ideas of the\npaper generalize to constructing neural networks that are invariant to the\naction of other compact groups.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1806.09231v2",
        "date": "2018-06-24 23:17:05+00:00"
    },
    {
        "title": "Self Supervised Low Dose Computed Tomography Image Denoising Using Invertible Network Exploiting Inter Slice Congruence",
        "authors": [
            "Sutanu Bera",
            "Prabir Kumar Biswas"
        ],
        "abstract": "The resurgence of deep neural networks has created an alternative pathway for\nlow-dose computed tomography denoising by learning a nonlinear transformation\nfunction between low-dose CT (LDCT) and normal-dose CT (NDCT) image pairs.\nHowever, those paired LDCT and NDCT images are rarely available in the clinical\nenvironment, making deep neural network deployment infeasible. This study\nproposes a novel method for self-supervised low-dose CT denoising to alleviate\nthe requirement of paired LDCT and NDCT images. Specifically, we have trained\nan invertible neural network to minimize the pixel-based mean square distance\nbetween a noisy slice and the average of its two immediate adjacent noisy\nslices. We have shown the aforementioned is similar to training a neural\nnetwork to minimize the distance between clean NDCT and noisy LDCT image pairs.\nAgain, during the reverse mapping of the invertible network, the output image\nis mapped to the original input image, similar to cycle consistency loss.\nFinally, the trained invertible network's forward mapping is used for denoising\nLDCT images. Extensive experiments on two publicly available datasets showed\nthat our method performs favourably against other existing unsupervised\nmethods.",
        "categories": [
            "eess.IV",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2211.01618v1",
        "date": "2022-11-03 07:16:18+00:00"
    },
    {
        "title": "Greedy Shallow Networks: An Approach for Constructing and Training Neural Networks",
        "authors": [
            "Anton Dereventsov",
            "Armenak Petrosyan",
            "Clayton Webster"
        ],
        "abstract": "We present a greedy-based approach to construct an efficient single hidden\nlayer neural network with the ReLU activation that approximates a target\nfunction. In our approach we obtain a shallow network by utilizing a greedy\nalgorithm with the prescribed dictionary provided by the available training\ndata and a set of possible inner weights. To facilitate the greedy selection\nprocess we employ an integral representation of the network, based on the\nridgelet transform, that significantly reduces the cardinality of the\ndictionary and hence promotes feasibility of the greedy selection. Our approach\nallows for the construction of efficient architectures which can be treated\neither as improved initializations to be used in place of random-based\nalternatives, or as fully-trained networks in certain cases, thus potentially\nnullifying the need for backpropagation training. Numerical experiments\ndemonstrate the tenability of the proposed concept and its advantages compared\nto the conventional techniques for selecting architectures and initializations\nfor neural networks.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1905.10409v3",
        "date": "2019-05-24 19:04:10+00:00"
    },
    {
        "title": "Semantic Adversarial Examples",
        "authors": [
            "Hossein Hosseini",
            "Radha Poovendran"
        ],
        "abstract": "Deep neural networks are known to be vulnerable to adversarial examples,\ni.e., images that are maliciously perturbed to fool the model. Generating\nadversarial examples has been mostly limited to finding small perturbations\nthat maximize the model prediction error. Such images, however, contain\nartificial perturbations that make them somewhat distinguishable from natural\nimages. This property is used by several defense methods to counter adversarial\nexamples by applying denoising filters or training the model to be robust to\nsmall perturbations.\n  In this paper, we introduce a new class of adversarial examples, namely\n\"Semantic Adversarial Examples,\" as images that are arbitrarily perturbed to\nfool the model, but in such a way that the modified image semantically\nrepresents the same object as the original image. We formulate the problem of\ngenerating such images as a constrained optimization problem and develop an\nadversarial transformation based on the shape bias property of human cognitive\nsystem. In our method, we generate adversarial images by first converting the\nRGB image into the HSV (Hue, Saturation and Value) color space and then\nrandomly shifting the Hue and Saturation components, while keeping the Value\ncomponent the same. Our experimental results on CIFAR10 dataset show that the\naccuracy of VGG16 network on adversarial color-shifted images is 5.7%.",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1804.00499v1",
        "date": "2018-03-16 18:02:14+00:00"
    },
    {
        "title": "SI-Score: An image dataset for fine-grained analysis of robustness to object location, rotation and size",
        "authors": [
            "Jessica Yung",
            "Rob Romijnders",
            "Alexander Kolesnikov",
            "Lucas Beyer",
            "Josip Djolonga",
            "Neil Houlsby",
            "Sylvain Gelly",
            "Mario Lucic",
            "Xiaohua Zhai"
        ],
        "abstract": "Before deploying machine learning models it is critical to assess their\nrobustness. In the context of deep neural networks for image understanding,\nchanging the object location, rotation and size may affect the predictions in\nnon-trivial ways. In this work we perform a fine-grained analysis of robustness\nwith respect to these factors of variation using SI-Score, a synthetic dataset.\nIn particular, we investigate ResNets, Vision Transformers and CLIP, and\nidentify interesting qualitative differences between these.",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2104.04191v1",
        "date": "2021-04-09 05:00:49+00:00"
    },
    {
        "title": "Auto-labelling of Bug Report using Natural Language Processing",
        "authors": [
            "Avinash Patil",
            "Aryan Jadon"
        ],
        "abstract": "The exercise of detecting similar bug reports in bug tracking systems is\nknown as duplicate bug report detection. Having prior knowledge of a bug\nreport's existence reduces efforts put into debugging problems and identifying\nthe root cause. Rule and Query-based solutions recommend a long list of\npotential similar bug reports with no clear ranking. In addition, triage\nengineers are less motivated to spend time going through an extensive list.\nConsequently, this deters the use of duplicate bug report retrieval solutions.\nIn this paper, we have proposed a solution using a combination of NLP\ntechniques. Our approach considers unstructured and structured attributes of a\nbug report like summary, description and severity, impacted products,\nplatforms, categories, etc. It uses a custom data transformer, a deep neural\nnetwork, and a non-generalizing machine learning method to retrieve existing\nidentical bug reports. We have performed numerous experiments with significant\ndata sources containing thousands of bug reports and showcased that the\nproposed solution achieves a high retrieval accuracy of 70% for recall@5.",
        "categories": [
            "cs.SE",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2212.06334v1",
        "date": "2022-12-13 02:32:42+00:00"
    },
    {
        "title": "ASIC: Aligning Sparse in-the-wild Image Collections",
        "authors": [
            "Kamal Gupta",
            "Varun Jampani",
            "Carlos Esteves",
            "Abhinav Shrivastava",
            "Ameesh Makadia",
            "Noah Snavely",
            "Abhishek Kar"
        ],
        "abstract": "We present a method for joint alignment of sparse in-the-wild image\ncollections of an object category. Most prior works assume either ground-truth\nkeypoint annotations or a large dataset of images of a single object category.\nHowever, neither of the above assumptions hold true for the long-tail of the\nobjects present in the world. We present a self-supervised technique that\ndirectly optimizes on a sparse collection of images of a particular\nobject/object category to obtain consistent dense correspondences across the\ncollection. We use pairwise nearest neighbors obtained from deep features of a\npre-trained vision transformer (ViT) model as noisy and sparse keypoint matches\nand make them dense and accurate matches by optimizing a neural network that\njointly maps the image collection into a learned canonical grid. Experiments on\nCUB and SPair-71k benchmarks demonstrate that our method can produce globally\nconsistent and higher quality correspondences across the image collection when\ncompared to existing self-supervised methods. Code and other material will be\nmade available at \\url{https://kampta.github.io/asic}.",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2303.16201v1",
        "date": "2023-03-28 17:59:28+00:00"
    },
    {
        "title": "MC-ViViT: Multi-branch Classifier-ViViT to Detect Mild Cognitive Impairment in Older Adults using Facial Videos",
        "authors": [
            "Jian Sun",
            "Hiroko H. Dodge",
            "Mohammad H. Mahoor"
        ],
        "abstract": "Deep machine learning models including Convolutional Neural Networks (CNN)\nhave been successful in the detection of Mild Cognitive Impairment (MCI) using\nmedical images, questionnaires, and videos. This paper proposes a novel\nMulti-branch Classifier-Video Vision Transformer (MC-ViViT) model to\ndistinguish MCI from those with normal cognition by analyzing facial features.\nThe data comes from the I-CONECT, a behavioral intervention trial aimed at\nimproving cognitive function by providing frequent video chats. MC-ViViT\nextracts spatiotemporal features of videos in one branch and augments\nrepresentations by the MC module. The I-CONECT dataset is challenging as the\ndataset is imbalanced containing Hard-Easy and Positive-Negative samples, which\nimpedes the performance of MC-ViViT. We propose a loss function for Hard-Easy\nand Positive-Negative Samples (HP Loss) by combining Focal loss and AD-CORRE\nloss to address the imbalanced problem. Our experimental results on the\nI-CONECT dataset show the great potential of MC-ViViT in predicting MCI with a\nhigh accuracy of 90.63\\% accuracy on some of the interview videos.",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2304.05292v1",
        "date": "2023-04-11 15:42:20+00:00"
    },
    {
        "title": "Addressing Some Limitations of Transformers with Feedback Memory",
        "authors": [
            "Angela Fan",
            "Thibaut Lavril",
            "Edouard Grave",
            "Armand Joulin",
            "Sainbayar Sukhbaatar"
        ],
        "abstract": "Transformers have been successfully applied to sequential, auto-regressive\ntasks despite being feedforward networks. Unlike recurrent neural networks,\nTransformers use attention to capture temporal relations while processing input\ntokens in parallel. While this parallelization makes them computationally\nefficient, it restricts the model from fully exploiting the sequential nature\nof the input. The representation at a given layer can only access\nrepresentations from lower layers, rather than the higher level representations\nalready available. In this work, we propose the Feedback Transformer\narchitecture that exposes all previous representations to all future\nrepresentations, meaning the lowest representation of the current timestep is\nformed from the highest-level abstract representation of the past. We\ndemonstrate on a variety of benchmarks in language modeling, machine\ntranslation, and reinforcement learning that the increased representation\ncapacity can create small, shallow models with much stronger performance than\ncomparable Transformers.",
        "categories": [
            "cs.LG",
            "cs.CL",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2002.09402v3",
        "date": "2020-02-21 16:37:57+00:00"
    },
    {
        "title": "Convergence Guarantees of Overparametrized Wide Deep Inverse Prior",
        "authors": [
            "Nathan Buskulic",
            "Yvain Qu\u00e9au",
            "Jalal Fadili"
        ],
        "abstract": "Neural networks have become a prominent approach to solve inverse problems in\nrecent years. Amongst the different existing methods, the Deep Image/Inverse\nPriors (DIPs) technique is an unsupervised approach that optimizes a highly\noverparametrized neural network to transform a random input into an object\nwhose image under the forward model matches the observation. However, the level\nof overparametrization necessary for such methods remains an open problem. In\nthis work, we aim to investigate this question for a two-layers neural network\nwith a smooth activation function. We provide overparametrization bounds under\nwhich such network trained via continuous-time gradient descent will converge\nexponentially fast with high probability which allows to derive recovery\nprediction bounds. This work is thus a first step towards a theoretical\nunderstanding of overparametrized DIP networks, and more broadly it\nparticipates to the theoretical understanding of neural networks in inverse\nproblem settings.",
        "categories": [
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2303.11265v1",
        "date": "2023-03-20 16:49:40+00:00"
    },
    {
        "title": "Representational R\u00e9nyi heterogeneity",
        "authors": [
            "Abraham Nunes",
            "Martin Alda",
            "Timothy Bardouille",
            "Thomas Trappenberg"
        ],
        "abstract": "A discrete system's heterogeneity is measured by the R\\'enyi heterogeneity\nfamily of indices (also known as Hill numbers or Hannah--Kay indices), whose\nunits are {the numbers equivalent}. Unfortunately, numbers equivalent\nheterogeneity measures for non-categorical data require {a priori} (A)\ncategorical partitioning and (B) pairwise distance measurement on the\nobservable data space, thereby precluding application to problems with\nill-defined categories or where semantically relevant features must be learned\nas abstractions from some data. We thus introduce representational R\\'enyi\nheterogeneity (RRH), which transforms an observable domain onto a latent space\nupon which the R\\'enyi heterogeneity is both tractable and semantically\nrelevant. This method requires neither {a priori} binning nor definition of a\ndistance function on the observable space. We show that RRH can generalize\nexisting biodiversity and economic equality indices. Compared with existing\nindices on a beta-mixture distribution, we show that RRH responds more\nappropriately to changes in mixture component separation and weighting.\nFinally, we demonstrate the measurement of RRH in a set of natural images, with\nrespect to abstract representations learned by a deep neural network. The RRH\napproach will further enable heterogeneity measurement in disciplines whose\ndata do not easily conform to the assumptions of existing indices.",
        "categories": [
            "stat.ML",
            "cs.LG",
            "q-bio.QM"
        ],
        "link": "http://arxiv.org/pdf/1912.05031v3",
        "date": "2019-12-10 22:22:54+00:00"
    },
    {
        "title": "Neural Architecture Search for Class-incremental Learning",
        "authors": [
            "Shenyang Huang",
            "Vincent Fran\u00e7ois-Lavet",
            "Guillaume Rabusseau"
        ],
        "abstract": "In class-incremental learning, a model learns continuously from a sequential\ndata stream in which new classes occur. Existing methods often rely on static\narchitectures that are manually crafted. These methods can be prone to capacity\nsaturation because a neural network's ability to generalize to new concepts is\nlimited by its fixed capacity. To understand how to expand a continual learner,\nwe focus on the neural architecture design problem in the context of\nclass-incremental learning: at each time step, the learner must optimize its\nperformance on all classes observed so far by selecting the most competitive\nneural architecture. To tackle this problem, we propose Continual Neural\nArchitecture Search (CNAS): an autoML approach that takes advantage of the\nsequential nature of class-incremental learning to efficiently and adaptively\nidentify strong architectures in a continual learning setting. We employ a task\nnetwork to perform the classification task and a reinforcement learning agent\nas the meta-controller for architecture search. In addition, we apply network\ntransformations to transfer weights from previous learning step and to reduce\nthe size of the architecture search space, thus saving a large amount of\ncomputational resources. We evaluate CNAS on the CIFAR-100 dataset under varied\nincremental learning scenarios with limited computational power (1 GPU).\nExperimental results demonstrate that CNAS outperforms architectures that are\noptimized for the entire dataset. In addition, CNAS is at least an order of\nmagnitude more efficient than naively using existing autoML methods.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1909.06686v1",
        "date": "2019-09-14 22:16:02+00:00"
    },
    {
        "title": "Neural Differential Equations for Learning to Program Neural Nets Through Continuous Learning Rules",
        "authors": [
            "Kazuki Irie",
            "Francesco Faccio",
            "J\u00fcrgen Schmidhuber"
        ],
        "abstract": "Neural ordinary differential equations (ODEs) have attracted much attention\nas continuous-time counterparts of deep residual neural networks (NNs), and\nnumerous extensions for recurrent NNs have been proposed. Since the 1980s, ODEs\nhave also been used to derive theoretical results for NN learning rules, e.g.,\nthe famous connection between Oja's rule and principal component analysis. Such\nrules are typically expressed as additive iterative update processes which have\nstraightforward ODE counterparts. Here we introduce a novel combination of\nlearning rules and Neural ODEs to build continuous-time sequence processing\nnets that learn to manipulate short-term memory in rapidly changing synaptic\nconnections of other nets. This yields continuous-time counterparts of Fast\nWeight Programmers and linear Transformers. Our novel models outperform the\nbest existing Neural Controlled Differential Equation based models on various\ntime series classification tasks, while also addressing their fundamental\nscalability limitations. Our code is public.",
        "categories": [
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2206.01649v2",
        "date": "2022-06-03 15:48:53+00:00"
    },
    {
        "title": "Understanding Information Processing in Human Brain by Interpreting Machine Learning Models",
        "authors": [
            "Ilya Kuzovkin"
        ],
        "abstract": "The thesis explores the role machine learning methods play in creating\nintuitive computational models of neural processing. Combined with\ninterpretability techniques, machine learning could replace human modeler and\nshift the focus of human effort to extracting the knowledge from the ready-made\nmodels and articulating that knowledge into intuitive descroptions of reality.\nThis perspective makes the case in favor of the larger role that exploratory\nand data-driven approach to computational neuroscience could play while\ncoexisting alongside the traditional hypothesis-driven approach.\n  We exemplify the proposed approach in the context of the knowledge\nrepresentation taxonomy with three research projects that employ\ninterpretability techniques on top of machine learning methods at three\ndifferent levels of neural organization. The first study (Chapter 3) explores\nfeature importance analysis of a random forest decoder trained on intracerebral\nrecordings from 100 human subjects to identify spectrotemporal signatures that\ncharacterize local neural activity during the task of visual categorization.\nThe second study (Chapter 4) employs representation similarity analysis to\ncompare the neural responses of the areas along the ventral stream with the\nactivations of the layers of a deep convolutional neural network. The third\nstudy (Chapter 5) proposes a method that allows test subjects to visually\nexplore the state representation of their neural signal in real time. This is\nachieved by using a topology-preserving dimensionality reduction technique that\nallows to transform the neural data from the multidimensional representation\nused by the computer into a two-dimensional representation a human can grasp.\n  The approach, the taxonomy, and the examples, present a strong case for the\napplicability of machine learning methods to automatic knowledge discovery in\nneuroscience.",
        "categories": [
            "q-bio.NC",
            "cs.AI",
            "cs.LG",
            "68T07, 68T30, 92-08, 92-10, 92C20, 92B20",
            "I.2.6; I.5.4; J.3"
        ],
        "link": "http://arxiv.org/pdf/2010.08715v1",
        "date": "2020-10-17 04:37:26+00:00"
    },
    {
        "title": "Softmax Is Not an Artificial Trick: An Information-Theoretic View of Softmax in Neural Networks",
        "authors": [
            "Zhenyue Qin",
            "Dongwoo Kim"
        ],
        "abstract": "Despite great popularity of applying softmax to map the non-normalised\noutputs of a neural network to a probability distribution over predicting\nclasses, this normalised exponential transformation still seems to be\nartificial. A theoretic framework that incorporates softmax as an intrinsic\ncomponent is still lacking. In this paper, we view neural networks embedding\nsoftmax from an information-theoretic perspective. Under this view, we can\nnaturally and mathematically derive log-softmax as an inherent component in a\nneural network for evaluating the conditional mutual information between\nnetwork output vectors and labels given an input datum. We show that training\ndeterministic neural networks through maximising log-softmax is equivalent to\nenlarging the conditional mutual information, i.e., feeding label information\ninto network outputs. We also generalise our informative-theoretic perspective\nto neural networks with stochasticity and derive information upper and lower\nbounds of log-softmax. In theory, such an information-theoretic view offers\nrationality support for embedding softmax in neural networks; in practice, we\neventually demonstrate a computer vision application example of how to employ\nour information-theoretic view to filter out targeted objects on images.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1910.02629v3",
        "date": "2019-10-07 06:46:06+00:00"
    },
    {
        "title": "Different kinds of cognitive plausibility: why are transformers better than RNNs at predicting N400 amplitude?",
        "authors": [
            "James A. Michaelov",
            "Megan D. Bardolph",
            "Seana Coulson",
            "Benjamin K. Bergen"
        ],
        "abstract": "Despite being designed for performance rather than cognitive plausibility,\ntransformer language models have been found to be better at predicting metrics\nused to assess human language comprehension than language models with other\narchitectures, such as recurrent neural networks. Based on how well they\npredict the N400, a neural signal associated with processing difficulty, we\npropose and provide evidence for one possible explanation - their predictions\nare affected by the preceding context in a way analogous to the effect of\nsemantic facilitation in humans.",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2107.09648v1",
        "date": "2021-07-20 17:33:13+00:00"
    },
    {
        "title": "Properties of the After Kernel",
        "authors": [
            "Philip M. Long"
        ],
        "abstract": "The Neural Tangent Kernel (NTK) is the wide-network limit of a kernel defined\nusing neural networks at initialization, whose embedding is the gradient of the\noutput of the network with respect to its parameters. We study the \"after\nkernel\", which is defined using the same embedding, except after training, for\nneural networks with standard architectures, on binary classification problems\nextracted from MNIST and CIFAR-10, trained using SGD in a standard way. For\nsome dataset-architecture pairs, after a few epochs of neural network training,\na hard-margin SVM using the network's after kernel is much more accurate than\nwhen the network's initial kernel is used. For networks with an architecture\nsimilar to VGG, the after kernel is more \"global\", in the sense that it is less\ninvariant to transformations of input images that disrupt the global structure\nof the image while leaving the local statistics largely intact. For fully\nconnected networks, the after kernel is less global in this sense. The after\nkernel tends to be more invariant to small shifts, rotations and zooms; data\naugmentation does not improve these invariances. The (finite approximation to\nthe) conjugate kernel, obtained using the last layer of hidden nodes,\nsometimes, but not always, provides a good approximation to the NTK and the\nafter kernel.\n  Training a network with a larger learning rate (while holding the training\nerror constant) produces a better kernel, as measured by the test error of a\nhard-margin SVM. The after kernels of networks trained with larger learning\nrates tend to be more global, and more invariant to small shifts, rotations and\nzooms.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.NE"
        ],
        "link": "http://arxiv.org/pdf/2105.10585v3",
        "date": "2021-05-21 21:50:18+00:00"
    },
    {
        "title": "Detecting Reddit Users with Depression Using a Hybrid Neural Network",
        "authors": [
            "Ziyi Chen",
            "Ren Yang",
            "Sunyang Fu",
            "Nansu Zong",
            "Hongfang Liu",
            "Ming Huang"
        ],
        "abstract": "Depression is a widespread mental health issue, affecting an estimated 3.8%\nof the global population. It is also one of the main contributors to disability\nworldwide. Recently it is becoming popular for individuals to use social media\nplatforms (e.g., Reddit) to express their difficulties and health issues (e.g.,\ndepression) and seek support from other users in online communities. It opens\ngreat opportunities to automatically identify social media users with\ndepression by parsing millions of posts for potential interventions. Deep\nlearning methods have begun to dominate in the field of machine learning and\nnatural language processing (NLP) because of their ease of use, efficient\nprocessing, and state-of-the-art results on many NLP tasks. In this work, we\npropose a hybrid deep learning model which combines a pretrained sentence BERT\n(SBERT) and convolutional neural network (CNN) to detect individuals with\ndepression with their Reddit posts. The sentence BERT is used to learn the\nmeaningful representation of semantic information in each post. CNN enables the\nfurther transformation of those embeddings and the temporal identification of\nbehavioral patterns of users. We trained and evaluated the model performance to\nidentify Reddit users with depression by utilizing the Self-reported Mental\nHealth Diagnoses (SMHD) data. The hybrid deep learning model achieved an\naccuracy of 0.86 and an F1 score of 0.86 and outperformed the state-of-the-art\ndocumented result (F1 score of 0.79) by other machine learning models in the\nliterature. The results show the feasibility of the hybrid model to identify\nindividuals with depression. Although the hybrid model is validated to detect\ndepression with Reddit posts, it can be easily tuned and applied to other text\nclassification tasks and different clinical applications.",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2302.02759v1",
        "date": "2023-02-03 06:22:18+00:00"
    },
    {
        "title": "PatchGT: Transformer over Non-trainable Clusters for Learning Graph Representations",
        "authors": [
            "Han Gao",
            "Xu Han",
            "Jiaoyang Huang",
            "Jian-Xun Wang",
            "Li-Ping Liu"
        ],
        "abstract": "Recently the Transformer structure has shown good performances in graph\nlearning tasks. However, these Transformer models directly work on graph nodes\nand may have difficulties learning high-level information. Inspired by the\nvision transformer, which applies to image patches, we propose a new\nTransformer-based graph neural network: Patch Graph Transformer (PatchGT).\nUnlike previous transformer-based models for learning graph representations,\nPatchGT learns from non-trainable graph patches, not from nodes directly. It\ncan help save computation and improve the model performance. The key idea is to\nsegment a graph into patches based on spectral clustering without any trainable\nparameters, with which the model can first use GNN layers to learn patch-level\nrepresentations and then use Transformer to obtain graph-level representations.\nThe architecture leverages the spectral information of graphs and combines the\nstrengths of GNNs and Transformers. Further, we show the limitations of\nprevious hierarchical trainable clusters theoretically and empirically. We also\nprove the proposed non-trainable spectral clustering method is permutation\ninvariant and can help address the information bottlenecks in the graph.\nPatchGT achieves higher expressiveness than 1-WL-type GNNs, and the empirical\nstudy shows that PatchGT achieves competitive performances on benchmark\ndatasets and provides interpretability to its predictions. The implementation\nof our algorithm is released at our Github repo:\nhttps://github.com/tufts-ml/PatchGT.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "math.GT"
        ],
        "link": "http://arxiv.org/pdf/2211.14425v2",
        "date": "2022-11-26 01:17:23+00:00"
    },
    {
        "title": "SparDA: Accelerating Dynamic Sparse Deep Neural Networks via Sparse-Dense Transformation",
        "authors": [
            "Ningxin Zheng",
            "Huiqiang Jiang",
            "Quanlu Zhang",
            "Zhenhua Han",
            "Yuqing Yang",
            "Lingxiao Ma",
            "Fan Yang",
            "Lili Qiu",
            "Mao Yang",
            "Lidong Zhou"
        ],
        "abstract": "Due to its high cost-effectiveness, sparsity has become the most important\napproach for building efficient deep-learning models. However, commodity\naccelerators are built mainly for efficient dense computation, creating a huge\ngap for general sparse computation to leverage. Existing solutions have to use\ntime-consuming compiling to improve the efficiency of sparse kernels in an\nahead-of-time manner and thus are limited to static sparsity. A wide range of\ndynamic sparsity opportunities is missed because their sparsity patterns are\nonly known at runtime. This limits the future of building more biological\nbrain-like neural networks that should be dynamically and sparsely activated.\n  In this paper, we bridge the gap between sparse computation and commodity\naccelerators by proposing a system, called Spider, for efficiently executing\ndeep learning models with dynamic sparsity. We identify an important property\ncalled permutation invariant that applies to most deep-learning computations.\nThe property enables Spider (1) to extract dynamic sparsity patterns of tensors\nthat are only known at runtime with little overhead; and (2) to transform the\ndynamic sparse computation into an equivalent dense computation which has been\nextremely optimized on commodity accelerators. Extensive evaluation on diverse\nmodels shows Spider can extract and transform dynamic sparsity with negligible\noverhead but brings up to 9.4x speedup over state-of-art solutions.",
        "categories": [
            "cs.LG",
            "cs.NE"
        ],
        "link": "http://arxiv.org/pdf/2301.10936v1",
        "date": "2023-01-26 04:50:14+00:00"
    },
    {
        "title": "The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers",
        "authors": [
            "R\u00f3bert Csord\u00e1s",
            "Kazuki Irie",
            "J\u00fcrgen Schmidhuber"
        ],
        "abstract": "Recently, many datasets have been proposed to test the systematic\ngeneralization ability of neural networks. The companion baseline Transformers,\ntypically trained with default hyper-parameters from standard tasks, are shown\nto fail dramatically. Here we demonstrate that by revisiting model\nconfigurations as basic as scaling of embeddings, early stopping, relative\npositional embedding, and Universal Transformer variants, we can drastically\nimprove the performance of Transformers on systematic generalization. We report\nimprovements on five popular datasets: SCAN, CFQ, PCFG, COGS, and Mathematics\ndataset. Our models improve accuracy from 50% to 85% on the PCFG productivity\nsplit, and from 35% to 81% on COGS. On SCAN, relative positional embedding\nlargely mitigates the EOS decision problem (Newman et al., 2020), yielding 100%\naccuracy on the length split with a cutoff at 26. Importantly, performance\ndifferences between these models are typically invisible on the IID data split.\nThis calls for proper generalization validation sets for developing neural\nnetworks that generalize systematically. We publicly release the code to\nreproduce our results.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.NE"
        ],
        "link": "http://arxiv.org/pdf/2108.12284v4",
        "date": "2021-08-26 17:26:56+00:00"
    },
    {
        "title": "Deep learning scheme for recovery of broadband microwave photonic receiving systems in transceivers without expert knowledge and system priors",
        "authors": [
            "Shaofu Xu",
            "Rui Wang",
            "Jianping Chen",
            "Lei Yu",
            "Weiwen Zou"
        ],
        "abstract": "In regular microwave photonic (MWP) receiving systems, broadband signals are\nprocessed in the analog domain before they are transformed to the digital\ndomain for further processing and storage. However, the quality of the signals\nmay be degraded by defective photonic analog links, especially in a complicated\nMWP system. Here, we show a unified deep learning scheme that recovers the\ndistorted broadband signals as they are transformed to the digital domain. The\nneural network could automatically learn the end-to-end inverse responses of\nthe distortion effects of actual photonic analog links from data without expert\nknowledge and system priors. Hence, by shifting or augmenting the datasets, the\nneural network is potential to be generalized to various MWP receiving systems.\nWe conduct experiments by nontrivial MWP systems with complicated waveforms.\nResults validate the effectiveness, general applicability and the\nnoise-robustness of the proposed scheme, showing its superior performance in\npractical MWP systems. Therefore, the proposed deep learning scheme facilitates\nthe low-cost performance improvement of MWP receiving systems, as well as the\nnext-generation broadband transceivers, including radars, communications, and\nmicrowave imaging.",
        "categories": [
            "eess.SP",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1907.07312v2",
        "date": "2019-07-17 03:05:42+00:00"
    },
    {
        "title": "PowerNet: Efficient Representations of Polynomials and Smooth Functions by Deep Neural Networks with Rectified Power Units",
        "authors": [
            "Bo Li",
            "Shanshan Tang",
            "Haijun Yu"
        ],
        "abstract": "Deep neural network with rectified linear units (ReLU) is getting more and\nmore popular recently. However, the derivatives of the function represented by\na ReLU network are not continuous, which limit the usage of ReLU network to\nsituations only when smoothness is not required. In this paper, we construct\ndeep neural networks with rectified power units (RePU), which can give better\napproximations for smooth functions. Optimal algorithms are proposed to\nexplicitly build neural networks with sparsely connected RePUs, which we call\nPowerNets, to represent polynomials with no approximation error. For general\nsmooth functions, we first project the function to their polynomial\napproximations, then use the proposed algorithms to construct corresponding\nPowerNets. Thus, the error of best polynomial approximation provides an upper\nbound of the best RePU network approximation error. For smooth functions in\nhigher dimensional Sobolev spaces, we use fast spectral transforms for\ntensor-product grid and sparse grid discretization to get polynomial\napproximations. Our constructive algorithms show clearly a close connection\nbetween spectral methods and deep neural networks: a PowerNet with $n$ layers\ncan exactly represent polynomials up to degree $s^n$, where $s$ is the power of\nRePUs. The proposed PowerNets have potential applications in the situations\nwhere high-accuracy is desired or smoothness is required.",
        "categories": [
            "cs.LG",
            "cs.NA",
            "math.NA",
            "65M12, 65M15, 65P40"
        ],
        "link": "http://arxiv.org/pdf/1909.05136v1",
        "date": "2019-09-09 05:21:49+00:00"
    },
    {
        "title": "Complex Unitary Recurrent Neural Networks using Scaled Cayley Transform",
        "authors": [
            "Kehelwala D. G. Maduranga",
            "Kyle E. Helfrich",
            "Qiang Ye"
        ],
        "abstract": "Recurrent neural networks (RNNs) have been successfully used on a wide range\nof sequential data problems. A well known difficulty in using RNNs is the\n\\textit{vanishing or exploding gradient} problem. Recently, there have been\nseveral different RNN architectures that try to mitigate this issue by\nmaintaining an orthogonal or unitary recurrent weight matrix. One such\narchitecture is the scaled Cayley orthogonal recurrent neural network (scoRNN)\nwhich parameterizes the orthogonal recurrent weight matrix through a scaled\nCayley transform. This parametrization contains a diagonal scaling matrix\nconsisting of positive or negative one entries that can not be optimized by\ngradient descent. Thus the scaling matrix is fixed before training and a\nhyperparameter is introduced to tune the matrix for each particular task. In\nthis paper, we develop a unitary RNN architecture based on a complex scaled\nCayley transform. Unlike the real orthogonal case, the transformation uses a\ndiagonal scaling matrix consisting of entries on the complex unit circle which\ncan be optimized using gradient descent and no longer requires the tuning of a\nhyperparameter. We also provide an analysis of a potential issue of the modReLU\nactiviation function which is used in our work and several other unitary RNNs.\nIn the experiments conducted, the scaled Cayley unitary recurrent neural\nnetwork (scuRNN) achieves comparable or better results than scoRNN and other\nunitary RNNs without fixing the scaling matrix.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1811.04142v2",
        "date": "2018-11-09 21:37:36+00:00"
    },
    {
        "title": "The Dilemma Between Data Transformations and Adversarial Robustness for Time Series Application Systems",
        "authors": [
            "Sheila Alemany",
            "Niki Pissinou"
        ],
        "abstract": "Adversarial examples, or nearly indistinguishable inputs created by an\nattacker, significantly reduce machine learning accuracy. Theoretical evidence\nhas shown that the high intrinsic dimensionality of datasets facilitates an\nadversary's ability to develop effective adversarial examples in classification\nmodels. Adjacently, the presentation of data to a learning model impacts its\nperformance. For example, we have seen this through dimensionality reduction\ntechniques used to aid with the generalization of features in machine learning\napplications. Thus, data transformation techniques go hand-in-hand with\nstate-of-the-art learning models in decision-making applications such as\nintelligent medical or military systems. With this work, we explore how data\ntransformations techniques such as feature selection, dimensionality reduction,\nor trend extraction techniques may impact an adversary's ability to create\neffective adversarial samples on a recurrent neural network. Specifically, we\nanalyze it from the perspective of the data manifold and the presentation of\nits intrinsic features. Our evaluation empirically shows that feature selection\nand trend extraction techniques may increase the RNN's vulnerability. A data\ntransformation technique reduces the vulnerability to adversarial examples only\nif it approximates the dataset's intrinsic dimension, minimizes codimension,\nand maintains higher manifold coverage.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2006.10885v2",
        "date": "2020-06-18 22:43:37+00:00"
    },
    {
        "title": "Global Voxel Transformer Networks for Augmented Microscopy",
        "authors": [
            "Zhengyang Wang",
            "Yaochen Xie",
            "Shuiwang Ji"
        ],
        "abstract": "Advances in deep learning have led to remarkable success in augmented\nmicroscopy, enabling us to obtain high-quality microscope images without using\nexpensive microscopy hardware and sample preparation techniques. However,\ncurrent deep learning models for augmented microscopy are mostly U-Net based\nneural networks, thus sharing certain drawbacks that limit the performance. In\nthis work, we introduce global voxel transformer networks (GVTNets), an\nadvanced deep learning tool for augmented microscopy that overcomes intrinsic\nlimitations of the current U-Net based models and achieves improved\nperformance. GVTNets are built on global voxel transformer operators (GVTOs),\nwhich are able to aggregate global information, as opposed to local operators\nlike convolutions. We apply the proposed methods on existing datasets for three\ndifferent augmented microscopy tasks under various settings. The performance is\nsignificantly and consistently better than previous U-Net based approaches.",
        "categories": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2008.02340v2",
        "date": "2020-08-05 20:11:15+00:00"
    },
    {
        "title": "Deep Augmentation: Enhancing Self-Supervised Learning through Transformations in Higher Activation Space",
        "authors": [
            "Rickard Br\u00fcel-Gabrielsson",
            "Tongzhou Wang",
            "Manel Baradad",
            "Justin Solomon"
        ],
        "abstract": "We introduce Deep Augmentation, an approach to data augmentation using\ndropout to dynamically transform a targeted layer within a neural network, with\nthe option to use the stop-gradient operation, offering significant\nimprovements in model performance and generalization. We demonstrate the\nefficacy of Deep Augmentation through extensive experiments on contrastive\nlearning tasks in computer vision and NLP domains, where we observe substantial\nperformance gains with ResNets and Transformers as the underlying models. Our\nexperimentation reveals that targeting deeper layers with Deep Augmentation\noutperforms augmenting the input data, and the simple network- and\ndata-agnostic nature of this approach enables its seamless integration into\ncomputer vision and NLP pipelines.",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.CV"
        ],
        "link": "http://arxiv.org/pdf/2303.14537v1",
        "date": "2023-03-25 19:03:57+00:00"
    },
    {
        "title": "Numeric Encoding Options with Automunge",
        "authors": [
            "Nicholas J. Teague"
        ],
        "abstract": "Mainstream practice in machine learning with tabular data may take for\ngranted that any feature engineering beyond scaling for numeric sets is\nsuperfluous in context of deep neural networks. This paper will offer arguments\nfor potential benefits of extended encodings of numeric streams in deep\nlearning by way of a survey of options for numeric transformations as available\nin the Automunge open source python library platform for tabular data\npipelines, where transformations may be applied to distinct columns in \"family\ntree\" sets with generations and branches of derivations. Automunge\ntransformation options include normalization, binning, noise injection,\nderivatives, and more. The aggregation of these methods into family tree sets\nof transformations are demonstrated for use to present numeric features to\nmachine learning in multiple configurations of varying information content, as\nmay be applied to encode numeric sets of unknown interpretation. Experiments\ndemonstrate the realization of a novel generalized solution to data\naugmentation by noise injection for tabular learning, as may materially benefit\nmodel performance in applications with underserved training data.",
        "categories": [
            "cs.LG",
            "I.2.6"
        ],
        "link": "http://arxiv.org/pdf/2202.09496v2",
        "date": "2022-02-19 02:21:03+00:00"
    },
    {
        "title": "RF Signal Transformation and Classification using Deep Neural Networks",
        "authors": [
            "Umar Khalid",
            "Nazmul Karim",
            "Nazanin Rahnavard"
        ],
        "abstract": "Deep neural networks (DNNs) designed for computer vision and natural language\nprocessing tasks cannot be directly applied to the radio frequency (RF)\ndatasets. To address this challenge, we propose to convert the raw RF data to\ndata types that are suitable for off-the-shelf DNNs by introducing a\nconvolutional transform technique. In addition, we propose a simple 5-layer\nconvolutional neural network architecture (CONV-5) that can operate with raw RF\nI/Q data without any transformation. Further, we put forward an RF dataset,\nreferred to as RF1024, to facilitate future RF research. RF1024 consists of 8\ndifferent RF modulation classes with each class having 1000/200 training/test\nsamples. Each sample of the RF1024 dataset contains 1024 complex I/Q values.\nLastly, the experiments are performed on the RadioML2016 and RF1024 datasets to\ndemonstrate the improved classification performance.",
        "categories": [
            "eess.SP",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2204.03564v1",
        "date": "2022-04-06 05:01:59+00:00"
    },
    {
        "title": "SATformer: Transformers for SAT Solving",
        "authors": [
            "Zhengyuan Shi",
            "Min Li",
            "Sadaf Khan",
            "Hui-Ling Zhen",
            "Mingxuan Yuan",
            "Qiang Xu"
        ],
        "abstract": "In this paper, we propose SATformer, a novel Transformer-based solution for\nBoolean satisfiability (SAT) solving. Different from existing learning-based\nSAT solvers that learn at the problem instance level, SATformer learns the\nminimum unsatisfiable cores (MUC) of unsatisfiable problem instances, which\nprovide rich information for the causality of such problems. Specifically, we\napply a graph neural network (GNN) to obtain the embeddings of the clauses in\nthe conjunctive normal format (CNF). A hierarchical Transformer architecture is\napplied on the clause embeddings to capture the relationships among clauses,\nand the self-attention weight is learned to be high when those clauses forming\nUNSAT cores are attended together, and set to be low otherwise. By doing so,\nSATformer effectively learns the correlations among clauses for SAT prediction.\nExperimental results show that SATformer is more powerful than existing\nend-to-end learning-based SAT solvers.",
        "categories": [
            "cs.AI",
            "cs.LG",
            "cs.LO"
        ],
        "link": "http://arxiv.org/pdf/2209.00953v1",
        "date": "2022-09-02 11:17:39+00:00"
    },
    {
        "title": "Deep Embedded K-Means Clustering",
        "authors": [
            "Wengang Guo",
            "Kaiyan Lin",
            "Wei Ye"
        ],
        "abstract": "Recently, deep clustering methods have gained momentum because of the high\nrepresentational power of deep neural networks (DNNs) such as autoencoder. The\nkey idea is that representation learning and clustering can reinforce each\nother: Good representations lead to good clustering while good clustering\nprovides good supervisory signals to representation learning. Critical\nquestions include: 1) How to optimize representation learning and clustering?\n2) Should the reconstruction loss of autoencoder be considered always? In this\npaper, we propose DEKM (for Deep Embedded K-Means) to answer these two\nquestions. Since the embedding space generated by autoencoder may have no\nobvious cluster structures, we propose to further transform the embedding space\nto a new space that reveals the cluster-structure information. This is achieved\nby an orthonormal transformation matrix, which contains the eigenvectors of the\nwithin-class scatter matrix of K-means. The eigenvalues indicate the importance\nof the eigenvectors' contributions to the cluster-structure information in the\nnew space. Our goal is to increase the cluster-structure information. To this\nend, we discard the decoder and propose a greedy method to optimize the\nrepresentation. Representation learning and clustering are alternately\noptimized by DEKM. Experimental results on the real-world datasets demonstrate\nthat DEKM achieves state-of-the-art performance.",
        "categories": [
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2109.15149v1",
        "date": "2021-09-30 14:12:59+00:00"
    },
    {
        "title": "Graph Transformer Networks: Learning Meta-path Graphs to Improve GNNs",
        "authors": [
            "Seongjun Yun",
            "Minbyul Jeong",
            "Sungdong Yoo",
            "Seunghun Lee",
            "Sean S. Yi",
            "Raehyun Kim",
            "Jaewoo Kang",
            "Hyunwoo J. Kim"
        ],
        "abstract": "Graph Neural Networks (GNNs) have been widely applied to various fields due\nto their powerful representations of graph-structured data. Despite the success\nof GNNs, most existing GNNs are designed to learn node representations on the\nfixed and homogeneous graphs. The limitations especially become problematic\nwhen learning representations on a misspecified graph or a heterogeneous graph\nthat consists of various types of nodes and edges. To address this limitations,\nwe propose Graph Transformer Networks (GTNs) that are capable of generating new\ngraph structures, which preclude noisy connections and include useful\nconnections (e.g., meta-paths) for tasks, while learning effective node\nrepresentations on the new graphs in an end-to-end fashion. We further propose\nenhanced version of GTNs, Fast Graph Transformer Networks (FastGTNs), that\nimprove scalability of graph transformations. Compared to GTNs, FastGTNs are\n230x faster and use 100x less memory while allowing the identical graph\ntransformations as GTNs. In addition, we extend graph transformations to the\nsemantic proximity of nodes allowing non-local operations beyond meta-paths.\nExtensive experiments on both homogeneous graphs and heterogeneous graphs show\nthat GTNs and FastGTNs with non-local operations achieve the state-of-the-art\nperformance for node classification tasks. The code is available:\nhttps://github.com/seongjunyun/Graph_Transformer_Networks",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.SI"
        ],
        "link": "http://arxiv.org/pdf/2106.06218v1",
        "date": "2021-06-11 07:56:55+00:00"
    },
    {
        "title": "Evaluating Vision Transformer Methods for Deep Reinforcement Learning from Pixels",
        "authors": [
            "Tianxin Tao",
            "Daniele Reda",
            "Michiel van de Panne"
        ],
        "abstract": "Vision Transformers (ViT) have recently demonstrated the significant\npotential of transformer architectures for computer vision. To what extent can\nimage-based deep reinforcement learning also benefit from ViT architectures, as\ncompared to standard convolutional neural network (CNN) architectures? To\nanswer this question, we evaluate ViT training methods for image-based\nreinforcement learning (RL) control tasks and compare these results to a\nleading convolutional-network architecture method, RAD. For training the ViT\nencoder, we consider several recently-proposed self-supervised losses that are\ntreated as auxiliary tasks, as well as a baseline with no additional loss\nterms. We find that the CNN architectures trained using RAD still generally\nprovide superior performance. For the ViT methods, all three types of auxiliary\ntasks that we consider provide a benefit over plain ViT training. Furthermore,\nViT reconstruction-based tasks are found to significantly outperform ViT\ncontrastive-learning.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "cs.RO"
        ],
        "link": "http://arxiv.org/pdf/2204.04905v2",
        "date": "2022-04-11 07:10:58+00:00"
    },
    {
        "title": "Equiformer: Equivariant Graph Attention Transformer for 3D Atomistic Graphs",
        "authors": [
            "Yi-Lun Liao",
            "Tess Smidt"
        ],
        "abstract": "Despite their widespread success in various domains, Transformer networks\nhave yet to perform well across datasets in the domain of 3D atomistic graphs\nsuch as molecules even when 3D-related inductive biases like translational\ninvariance and rotational equivariance are considered. In this paper, we\ndemonstrate that Transformers can generalize well to 3D atomistic graphs and\npresent Equiformer, a graph neural network leveraging the strength of\nTransformer architectures and incorporating SE(3)/E(3)-equivariant features\nbased on irreducible representations (irreps). First, we propose a simple and\neffective architecture by only replacing original operations in Transformers\nwith their equivariant counterparts and including tensor products. Using\nequivariant operations enables encoding equivariant information in channels of\nirreps features without complicating graph structures. With minimal\nmodifications to Transformers, this architecture has already achieved strong\nempirical results. Second, we propose a novel attention mechanism called\nequivariant graph attention, which improves upon typical attention in\nTransformers through replacing dot product attention with multi-layer\nperceptron attention and including non-linear message passing. With these two\ninnovations, Equiformer achieves competitive results to previous models on QM9,\nMD17 and OC20 datasets.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "physics.comp-ph"
        ],
        "link": "http://arxiv.org/pdf/2206.11990v2",
        "date": "2022-06-23 21:40:37+00:00"
    },
    {
        "title": "Long-Range Transformers for Dynamic Spatiotemporal Forecasting",
        "authors": [
            "Jake Grigsby",
            "Zhe Wang",
            "Nam Nguyen",
            "Yanjun Qi"
        ],
        "abstract": "Multivariate time series forecasting focuses on predicting future values\nbased on historical context. State-of-the-art sequence-to-sequence models rely\non neural attention between timesteps, which allows for temporal learning but\nfails to consider distinct spatial relationships between variables. In\ncontrast, methods based on graph neural networks explicitly model variable\nrelationships. However, these methods often rely on predefined graphs that\ncannot change over time and perform separate spatial and temporal updates\nwithout establishing direct connections between each variable at every\ntimestep. Our work addresses these problems by translating multivariate\nforecasting into a \"spatiotemporal sequence\" formulation where each Transformer\ninput token represents the value of a single variable at a given time.\nLong-Range Transformers can then learn interactions between space, time, and\nvalue information jointly along this extended sequence. Our method, which we\ncall Spacetimeformer, achieves competitive results on benchmarks from traffic\nforecasting to electricity demand and weather prediction while learning\nspatiotemporal relationships purely from data.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2109.12218v3",
        "date": "2021-09-24 22:11:46+00:00"
    },
    {
        "title": "A Neural Network Based on First Principles",
        "authors": [
            "Paul M Baggenstoss"
        ],
        "abstract": "In this paper, a Neural network is derived from first principles, assuming\nonly that each layer begins with a linear dimension-reducing transformation.\nThe approach appeals to the principle of Maximum Entropy (MaxEnt) to find the\nposterior distribution of the input data of each layer, conditioned on the\nlayer output variables. This posterior has a well-defined mean, the conditional\nmean estimator, that is calculated using a type of neural network with\ntheoretically-derived activation functions similar to sigmoid, softplus, and\nrelu. This implicitly provides a theoretical justification for their use. A\ntheorem that finds the conditional distribution and conditional mean estimator\nunder the MaxEnt prior is proposed, unifying results for special cases.\nCombining layers results in an auto-encoder with conventional feed-forward\nanalysis network and a type of linear Bayesian belief network in the\nreconstruction path.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2002.07469v1",
        "date": "2020-02-18 10:16:59+00:00"
    },
    {
        "title": "Can Push-forward Generative Models Fit Multimodal Distributions?",
        "authors": [
            "Antoine Salmona",
            "Valentin de Bortoli",
            "Julie Delon",
            "Agn\u00e8s Desolneux"
        ],
        "abstract": "Many generative models synthesize data by transforming a standard Gaussian\nrandom variable using a deterministic neural network. Among these models are\nthe Variational Autoencoders and the Generative Adversarial Networks. In this\nwork, we call them \"push-forward\" models and study their expressivity. We show\nthat the Lipschitz constant of these generative networks has to be large in\norder to fit multimodal distributions. More precisely, we show that the total\nvariation distance and the Kullback-Leibler divergence between the generated\nand the data distribution are bounded from below by a constant depending on the\nmode separation and the Lipschitz constant. Since constraining the Lipschitz\nconstants of neural networks is a common way to stabilize generative models,\nthere is a provable trade-off between the ability of push-forward models to\napproximate multimodal distributions and the stability of their training. We\nvalidate our findings on one-dimensional and image datasets and empirically\nshow that generative models consisting of stacked networks with stochastic\ninput at each step, such as diffusion models do not suffer of such limitations.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2206.14476v2",
        "date": "2022-06-29 09:03:30+00:00"
    },
    {
        "title": "Sleep Arousal Detection from Polysomnography using the Scattering Transform and Recurrent Neural Networks",
        "authors": [
            "Philip Warrick",
            "Masun Nabhan Homsi"
        ],
        "abstract": "Sleep disorders are implicated in a growing number of health problems. In\nthis paper, we present a signal-processing/machine learning approach to\ndetecting arousals in the multi-channel polysomnographic recordings of the\nPhysionet/CinC Challenge2018 dataset.\n  Methods: Our network architecture consists of two components. Inputs were\npresented to a Scattering Transform (ST) representation layer which fed a\nrecurrent neural network for sequence learning using three layers of Long\nShort-Term Memory (LSTM). The STs were calculated for each signal with\ndownsampling parameters chosen to give approximately 1 s time resolution,\nresulting in an eighteen-fold data reduction. The LSTM layers then operated at\nthis downsampled rate.\n  Results: The proposed approach detected arousal regions on the 10% random\nsample of the hidden test set with an AUROC of 88.0% and an AUPRC of 42.1%.",
        "categories": [
            "cs.LG",
            "eess.SP",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1810.08875v1",
        "date": "2018-10-21 00:42:58+00:00"
    },
    {
        "title": "Heterogeneous Graph Neural Network with Multi-view Representation Learning",
        "authors": [
            "Zezhi Shao",
            "Yongjun Xu",
            "Wei Wei",
            "Fei Wang",
            "Zhao Zhang",
            "Feida Zhu"
        ],
        "abstract": "Graph neural networks for heterogeneous graph embedding is to project nodes\ninto a low-dimensional space by exploring the heterogeneity and semantics of\nthe heterogeneous graph. However, on the one hand, most of existing\nheterogeneous graph embedding methods either insufficiently model the local\nstructure under specific semantic, or neglect the heterogeneity when\naggregating information from it. On the other hand, representations from\nmultiple semantics are not comprehensively integrated to obtain versatile node\nembeddings. To address the problem, we propose a Heterogeneous Graph Neural\nNetwork with Multi-View Representation Learning (named MV-HetGNN) for\nheterogeneous graph embedding by introducing the idea of multi-view\nrepresentation learning. The proposed model consists of node feature\ntransformation, view-specific ego graph encoding and auto multi-view fusion to\nthoroughly learn complex structural and semantic information for generating\ncomprehensive node representations. Extensive experiments on three real-world\nheterogeneous graph datasets show that the proposed MV-HetGNN model\nconsistently outperforms all the state-of-the-art GNN baselines in various\ndownstream tasks, e.g., node classification, node clustering, and link\nprediction.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2108.13650v3",
        "date": "2021-08-31 07:18:48+00:00"
    },
    {
        "title": "Hierarchical internal representation of spectral features in deep convolutional networks trained for EEG decoding",
        "authors": [
            "Kay Gregor Hartmann",
            "Robin Tibor Schirrmeister",
            "Tonio Ball"
        ],
        "abstract": "Recently, there is increasing interest and research on the interpretability\nof machine learning models, for example how they transform and internally\nrepresent EEG signals in Brain-Computer Interface (BCI) applications. This can\nhelp to understand the limits of the model and how it may be improved, in\naddition to possibly provide insight about the data itself. Schirrmeister et\nal. (2017) have recently reported promising results for EEG decoding with deep\nconvolutional neural networks (ConvNets) trained in an end-to-end manner and,\nwith a causal visualization approach, showed that they learn to use spectral\namplitude changes in the input. In this study, we investigate how ConvNets\nrepresent spectral features through the sequence of intermediate stages of the\nnetwork. We show higher sensitivity to EEG phase features at earlier stages and\nhigher sensitivity to EEG amplitude features at later stages. Intriguingly, we\nobserved a specialization of individual stages of the network to the classical\nEEG frequency bands alpha, beta, and high gamma. Furthermore, we find first\nevidence that particularly in the last convolutional layer, the network learns\nto detect more complex oscillatory patterns beyond spectral phase and\namplitude, reminiscent of the representation of complex visual features in\nlater layers of ConvNets in computer vision tasks. Our findings thus provide\ninsights into how ConvNets hierarchically represent spectral EEG features in\ntheir intermediate layers and suggest that ConvNets can exploit and might help\nto better understand the compositional structure of EEG time series.",
        "categories": [
            "cs.LG",
            "q-bio.NC",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1711.07792v3",
        "date": "2017-11-21 14:05:25+00:00"
    },
    {
        "title": "Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks",
        "authors": [
            "Peter L. Bartlett",
            "David P. Helmbold",
            "Philip M. Long"
        ],
        "abstract": "We analyze algorithms for approximating a function $f(x) = \\Phi x$ mapping\n$\\Re^d$ to $\\Re^d$ using deep linear neural networks, i.e. that learn a\nfunction $h$ parameterized by matrices $\\Theta_1,...,\\Theta_L$ and defined by\n$h(x) = \\Theta_L \\Theta_{L-1} ... \\Theta_1 x$. We focus on algorithms that\nlearn through gradient descent on the population quadratic loss in the case\nthat the distribution over the inputs is isotropic.\n  We provide polynomial bounds on the number of iterations for gradient descent\nto approximate the least squares matrix $\\Phi$, in the case where the initial\nhypothesis $\\Theta_1 = ... = \\Theta_L = I$ has excess loss bounded by a small\nenough constant. On the other hand, we show that gradient descent fails to\nconverge for $\\Phi$ whose distance from the identity is a larger constant, and\nwe show that some forms of regularization toward the identity in each layer do\nnot help.\n  If $\\Phi$ is symmetric positive definite, we show that an algorithm that\ninitializes $\\Theta_i = I$ learns an $\\epsilon$-approximation of $f$ using a\nnumber of updates polynomial in $L$, the condition number of $\\Phi$, and\n$\\log(d/\\epsilon)$. In contrast, we show that if the least squares matrix\n$\\Phi$ is symmetric and has a negative eigenvalue, then all members of a class\nof algorithms that perform gradient descent with identity initialization, and\noptionally regularize toward the identity in each layer, fail to converge.\n  We analyze an algorithm for the case that $\\Phi$ satisfies $u^{\\top} \\Phi u >\n0$ for all $u$, but may not be symmetric. This algorithm uses two regularizers:\none that maintains the invariant $u^{\\top} \\Theta_L \\Theta_{L-1} ... \\Theta_1 u\n> 0$ for all $u$, and another that \"balances\" $\\Theta_1, ..., \\Theta_L$ so that\nthey have the same singular values.",
        "categories": [
            "cs.LG",
            "cs.NE",
            "math.OC",
            "math.ST",
            "stat.ML",
            "stat.TH"
        ],
        "link": "http://arxiv.org/pdf/1802.06093v4",
        "date": "2018-02-16 19:24:29+00:00"
    },
    {
        "title": "Discovering Invariances in Healthcare Neural Networks",
        "authors": [
            "Mohammad Taha Bahadori",
            "Layne C. Price"
        ],
        "abstract": "We study the invariance characteristics of pre-trained predictive models by\nempirically learning transformations on the input that leave the prediction\nfunction approximately unchanged. To learn invariant transformations, we\nminimize the Wasserstein distance between the predictive distribution\nconditioned on the data instances and the predictive distribution conditioned\non the transformed data instances. To avoid finding degenerate or perturbative\ntransformations, we add a similarity regularization to discourage similarity\nbetween the data and its transformed values. We theoretically analyze the\ncorrectness of the algorithm and the structure of the solutions. Applying the\nproposed technique to clinical time series data, we discover variables that\ncommonly-used LSTM models do not rely on for their prediction, especially when\nthe LSTM is trained to be adversarially robust. We also analyze the invariances\nof BioBERT on clinical notes and discover words that it is invariant to.",
        "categories": [
            "cs.LG",
            "q-bio.QM",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1911.03295v3",
        "date": "2019-11-08 14:48:05+00:00"
    },
    {
        "title": "Stiffness: A New Perspective on Generalization in Neural Networks",
        "authors": [
            "Stanislav Fort",
            "Pawe\u0142 Krzysztof Nowak",
            "Stanislaw Jastrzebski",
            "Srini Narayanan"
        ],
        "abstract": "In this paper we develop a new perspective on generalization of neural\nnetworks by proposing and investigating the concept of a neural network\nstiffness. We measure how stiff a network is by looking at how a small gradient\nstep in the network's parameters on one example affects the loss on another\nexample. Higher stiffness suggests that a network is learning features that\ngeneralize. In particular, we study how stiffness depends on 1) class\nmembership, 2) distance between data points in the input space, 3) training\niteration, and 4) learning rate. We present experiments on MNIST, FASHION\nMNIST, and CIFAR-10/100 using fully-connected and convolutional neural\nnetworks, as well as on a transformer-based NLP model. We demonstrate the\nconnection between stiffness and generalization, and observe its dependence on\nlearning rate. When training on CIFAR-100, the stiffness matrix exhibits a\ncoarse-grained behavior indicative of the model's awareness of super-class\nmembership. In addition, we measure how stiffness between two data points\ndepends on their mutual input-space distance, and establish the concept of a\ndynamical critical length -- a distance below which a parameter update based on\na data point influences its neighbors.",
        "categories": [
            "cs.LG",
            "cs.NE",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1901.09491v3",
        "date": "2019-01-28 02:49:46+00:00"
    },
    {
        "title": "A Survey on Dynamic Neural Networks for Natural Language Processing",
        "authors": [
            "Canwen Xu",
            "Julian McAuley"
        ],
        "abstract": "Effectively scaling large Transformer models is a main driver of recent\nadvances in natural language processing. Dynamic neural networks, as an\nemerging research direction, are capable of scaling up neural networks with\nsub-linear increases in computation and time by dynamically adjusting their\ncomputational path based on the input. Dynamic neural networks could be a\npromising solution to the growing parameter numbers of pretrained language\nmodels, allowing both model pretraining with trillions of parameters and faster\ninference on mobile devices. In this survey, we summarize progress of three\ntypes of dynamic neural networks in NLP: skimming, mixture of experts, and\nearly exit. We also highlight current challenges in dynamic neural networks and\ndirections for future research.",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2202.07101v2",
        "date": "2022-02-15 00:13:05+00:00"
    },
    {
        "title": "An Intelligent End-to-End Neural Architecture Search Framework for Electricity Forecasting Model Development",
        "authors": [
            "Jin Yang",
            "Yingying Huang",
            "Guangxin Jiang",
            "Ying Chen"
        ],
        "abstract": "Recent years have witnessed an exponential growth in developing deep learning\n(DL) models for the time-series electricity forecasting in power systems.\nHowever, most of the proposed models are designed based on the designers'\ninherent knowledge and experience without elaborating on the suitability of the\nproposed neural architectures. Moreover, these models cannot be self-adjusted\nto the dynamically changing data patterns due to an inflexible design of their\nstructures. Even though several latest studies have considered application of\nthe neural architecture search (NAS) technique for obtaining a network with an\noptimized structure in the electricity forecasting sector, their training\nprocess is quite time-consuming, computationally expensive and not intelligent,\nindicating that the NAS application in electricity forecasting area is still at\nan infancy phase. In this research study, we propose an intelligent automated\narchitecture search (IAAS) framework for the development of time-series\nelectricity forecasting models. The proposed framework contains two primary\ncomponents, i.e., network function-preserving transformation operation and\nreinforcement learning (RL)-based network transformation control. In the first\ncomponent, we introduce a theoretical function-preserving transformation of\nrecurrent neural networks (RNN) to the literature for capturing the hidden\ntemporal patterns within the time-series data. In the second component, we\ndevelop three RL-based transformation actors and a net pool to intelligently\nand effectively search a high-quality neural architecture. After conducting\ncomprehensive experiments on two publicly-available electricity load datasets\nand two wind power datasets, we demonstrate that the proposed IAAS framework\nsignificantly outperforms the ten existing models or methods in terms of\nforecasting accuracy and stability.",
        "categories": [
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2203.13563v1",
        "date": "2022-03-25 10:36:27+00:00"
    },
    {
        "title": "Filtered Batch Normalization",
        "authors": [
            "Andras Horvath",
            "Jalal Al-afandi"
        ],
        "abstract": "It is a common assumption that the activation of different layers in neural\nnetworks follow Gaussian distribution. This distribution can be transformed\nusing normalization techniques, such as batch-normalization, increasing\nconvergence speed and improving accuracy. In this paper we would like to\ndemonstrate, that activations do not necessarily follow Gaussian distribution\nin all layers. Neurons in deeper layers are more selective and specific which\ncan result extremely large, out-of-distribution activations.\n  We will demonstrate that one can create more consistent mean and variance\nvalues for batch normalization during training by filtering out these\nactivations which can further improve convergence speed and yield higher\nvalidation accuracy.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.NE"
        ],
        "link": "http://arxiv.org/pdf/2010.08251v1",
        "date": "2020-10-16 08:56:57+00:00"
    },
    {
        "title": "On the Turing Completeness of Modern Neural Network Architectures",
        "authors": [
            "Jorge P\u00e9rez",
            "Javier Marinkovi\u0107",
            "Pablo Barcel\u00f3"
        ],
        "abstract": "Alternatives to recurrent neural networks, in particular, architectures based\non attention or convolutions, have been gaining momentum for processing input\nsequences. In spite of their relevance, the computational properties of these\nalternatives have not yet been fully explored. We study the computational power\nof two of the most paradigmatic architectures exemplifying these mechanisms:\nthe Transformer (Vaswani et al., 2017) and the Neural GPU (Kaiser & Sutskever,\n2016). We show both models to be Turing complete exclusively based on their\ncapacity to compute and access internal dense representations of the data. In\nparticular, neither the Transformer nor the Neural GPU requires access to an\nexternal memory to become Turing complete. Our study also reveals some minimal\nsets of elements needed to obtain these completeness results.",
        "categories": [
            "cs.LG",
            "cs.FL",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1901.03429v1",
        "date": "2019-01-10 23:21:35+00:00"
    },
    {
        "title": "Towards physically consistent data-driven weather forecasting: Integrating data assimilation with equivariance-preserving deep spatial transformers",
        "authors": [
            "Ashesh Chattopadhyay",
            "Mustafa Mustafa",
            "Pedram Hassanzadeh",
            "Eviatar Bach",
            "Karthik Kashinath"
        ],
        "abstract": "There is growing interest in data-driven weather prediction (DDWP), for\nexample using convolutional neural networks such as U-NETs that are trained on\ndata from models or reanalysis. Here, we propose 3 components to integrate with\ncommonly used DDWP models in order to improve their physical consistency and\nforecast accuracy. These components are 1) a deep spatial transformer added to\nthe latent space of the U-NETs to preserve a property called equivariance,\nwhich is related to correctly capturing rotations and scalings of features in\nspatio-temporal data, 2) a data-assimilation (DA) algorithm to ingest noisy\nobservations and improve the initial conditions for next forecasts, and 3) a\nmulti-time-step algorithm, which combines forecasts from DDWP models with\ndifferent time steps through DA, improving the accuracy of forecasts at short\nintervals. To show the benefit/feasibility of each component, we use\ngeopotential height at 500~hPa (Z500) from ERA5 reanalysis and examine the\nshort-term forecast accuracy of specific setups of the DDWP framework. Results\nshow that the equivariance-preserving networks (U-STNs) clearly outperform the\nU-NETs, for example improving the forecast skill by $45\\%$. Using a sigma-point\nensemble Kalman (SPEnKF) algorithm for DA and U-STN as the forward model, we\nshow that stable, accurate DA cycles are achieved even with high observation\nnoise. The DDWP+DA framework substantially benefits from large ($O(1000)$)\nensembles that are inexpensively generated with the data-driven forward model\nin each DA cycle. The multi-time-step DDWP+DA framework also shows promises,\ne.g., it reduces the average error by factors of 2-3.",
        "categories": [
            "physics.ao-ph",
            "cs.AI",
            "cs.LG",
            "physics.comp-ph"
        ],
        "link": "http://arxiv.org/pdf/2103.09360v1",
        "date": "2021-03-16 23:15:00+00:00"
    },
    {
        "title": "Implementing a Bayes Filter in a Neural Circuit: The Case of Unknown Stimulus Dynamics",
        "authors": [
            "Sacha Sokoloski"
        ],
        "abstract": "In order to interact intelligently with objects in the world, animals must\nfirst transform neural population responses into estimates of the dynamic,\nunknown stimuli which caused them. The Bayesian solution to this problem is\nknown as a Bayes filter, which applies Bayes' rule to combine population\nresponses with the predictions of an internal model. In this paper we present a\nmethod for learning to approximate a Bayes filter when the stimulus dynamics\nare unknown. To do this we use the inferential properties of probabilistic\npopulation codes to compute Bayes' rule, and train a neural network to compute\napproximate predictions by the method of maximum likelihood. In particular, we\nperform stochastic gradient descent on the negative log-likelihood with a novel\napproximation of the gradient. We demonstrate our methods on a finite-state, a\nlinear, and a nonlinear filtering problem, and show how the hidden layer of the\nneural network develops tuning curves which are consistent with findings in\nexperimental neuroscience.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1512.07839v4",
        "date": "2015-12-22 14:52:14+00:00"
    },
    {
        "title": "A Neural ODE Interpretation of Transformer Layers",
        "authors": [
            "Yaofeng Desmond Zhong",
            "Tongtao Zhang",
            "Amit Chakraborty",
            "Biswadip Dey"
        ],
        "abstract": "Transformer layers, which use an alternating pattern of multi-head attention\nand multi-layer perceptron (MLP) layers, provide an effective tool for a\nvariety of machine learning problems. As the transformer layers use residual\nconnections to avoid the problem of vanishing gradients, they can be viewed as\nthe numerical integration of a differential equation. In this extended\nabstract, we build upon this connection and propose a modification of the\ninternal architecture of a transformer layer. The proposed model places the\nmulti-head attention sublayer and the MLP sublayer parallel to each other. Our\nexperiments show that this simple modification improves the performance of\ntransformer networks in multiple tasks. Moreover, for the image classification\ntask, we show that using neural ODE solvers with a sophisticated integration\nscheme further improves performance.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2212.06011v1",
        "date": "2022-12-12 16:18:58+00:00"
    },
    {
        "title": "Theory inspired deep network for instantaneous-frequency extraction and signal components recovery from discrete blind-source data",
        "authors": [
            "Charles K. Chui",
            "Ningning Han",
            "Hrushikesh N. Mhaskar"
        ],
        "abstract": "This paper is concerned with the inverse problem of recovering the unknown\nsignal components, along with extraction of their instantaneous frequencies\n(IFs), governed by the adaptive harmonic model (AHM), from discrete (and\npossibly non-uniform) samples of the blind-source composite signal.\n  None of the existing decomposition methods and algorithms, including the most\npopular empirical mode decomposition (EMD) computational scheme and its current\nmodifications, is capable of solving this inverse problem.\n  In order to meet the AHM formulation and to extract the IFs of the decomposed\ncomponents, called intrinsic mode functions (IMFs), each IMF of EMD is extended\nto an analytic function in the upper half of the complex plane via the Hilbert\ntransform, followed by taking the real part of the polar form of the analytic\nextension.\n  Unfortunately, this approach most often fails to resolve the inverse problem\nsatisfactorily.\n  More recently, to resolve the inverse problem, the notion of synchrosqueezed\nwavelet transform (SST) was proposed by Daubechies and Maes, and further\ndeveloped in many other papers, while a more direct method, called signal\nseparation operation (SSO), was proposed and developed in our previous work\npublished in the journal, Applied and Computational Harmonic Analysis, vol.\n30(2):243-261, 2016.\n  In the present paper, we propose a synthesis of SSO using a deep neural\nnetwork, based directly on a discrete sample set, that may be non-uniformly\nsampled, of the blind-source signal.\n  Our method is localized, as illustrated by a number of numerical examples,\nincluding components with different signal arrival and departure times.\n  It also yields short-term prediction of the signal components, along with\ntheir IFs.\n  Our neural networks are inspired by theory, designed so that they do not\nrequire any training in the traditional sense.",
        "categories": [
            "cs.LG",
            "eess.SP",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2001.12006v1",
        "date": "2020-01-31 18:54:00+00:00"
    },
    {
        "title": "PIM-DRAM: Accelerating Machine Learning Workloads using Processing in Commodity DRAM",
        "authors": [
            "Sourjya Roy",
            "Mustafa Ali",
            "Anand Raghunathan"
        ],
        "abstract": "Deep Neural Networks (DNNs) have transformed the field of machine learning\nand are widely deployed in many applications involving image, video, speech and\nnatural language processing. The increasing compute demands of DNNs have been\nwidely addressed through Graphics Processing Units (GPUs) and specialized\naccelerators. However, as model sizes grow, these von Neumann architectures\nrequire very high memory bandwidth to keep the processing elements utilized as\na majority of the data resides in the main memory. Processing in memory has\nbeen proposed as a promising solution for the memory wall bottleneck for ML\nworkloads. In this work, we propose a new DRAM-based processing-in-memory (PIM)\nmultiplication primitive coupled with intra-bank accumulation to accelerate\nmatrix vector operations in ML workloads. The proposed multiplication primitive\nadds < 1% area overhead and does not require any change in the DRAM\nperipherals. Therefore, the proposed multiplication can be easily adopted in\ncommodity DRAM chips. Subsequently, we design a DRAM-based PIM architecture,\ndata mapping scheme and dataflow for executing DNNs within DRAM. System\nevaluations performed on networks like AlexNet, VGG16 and ResNet18 show that\nthe proposed architecture, mapping, and data flow can provide up to 19.5x\nspeedup over an NVIDIA Titan Xp GPU highlighting the need to overcome the\nmemory bottleneck in future generations of DNN hardware.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.AR"
        ],
        "link": "http://arxiv.org/pdf/2105.03736v3",
        "date": "2021-05-08 16:39:24+00:00"
    },
    {
        "title": "COVID-Net US-X: Enhanced Deep Neural Network for Detection of COVID-19 Patient Cases from Convex Ultrasound Imaging Through Extended Linear-Convex Ultrasound Augmentation Learning",
        "authors": [
            "E. Zhixuan Zeng",
            "Adrian Florea",
            "Alexander Wong"
        ],
        "abstract": "As the global population continues to face significant negative impact by the\non-going COVID-19 pandemic, there has been an increasing usage of point-of-care\nultrasound (POCUS) imaging as a low-cost and effective imaging modality of\nchoice in the COVID-19 clinical workflow. A major barrier with widespread\nadoption of POCUS in the COVID-19 clinical workflow is the scarcity of expert\nclinicians that can interpret POCUS examinations, leading to considerable\ninterest in deep learning-driven clinical decision support systems to tackle\nthis challenge. A major challenge to building deep neural networks for COVID-19\nscreening using POCUS is the heterogeneity in the types of probes used to\ncapture ultrasound images (e.g., convex vs. linear probes), which can lead to\nvery different visual appearances. In this study, we explore the impact of\nleveraging extended linear-convex ultrasound augmentation learning on producing\nenhanced deep neural networks for COVID-19 assessment, where we conduct data\naugmentation on convex probe data alongside linear probe data that have been\ntransformed to better resemble convex probe data. Experimental results using an\nefficient deep columnar anti-aliased convolutional neural network designed via\na machined-driven design exploration strategy (which we name COVID-Net US-X)\nshow that the proposed extended linear-convex ultrasound augmentation learning\nsignificantly increases performance, with a gain of 5.1% in test accuracy and\n13.6% in AUC.",
        "categories": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2204.13851v1",
        "date": "2022-04-29 02:13:39+00:00"
    },
    {
        "title": "Connecting metrics for shape-texture knowledge in computer vision",
        "authors": [
            "Tiago Oliveira",
            "Tiago Marques",
            "Arlindo L. Oliveira"
        ],
        "abstract": "Modern artificial neural networks, including convolutional neural networks\nand vision transformers, have mastered several computer vision tasks, including\nobject recognition. However, there are many significant differences between the\nbehavior and robustness of these systems and of the human visual system. Deep\nneural networks remain brittle and susceptible to many changes in the image\nthat do not cause humans to misclassify images. Part of this different behavior\nmay be explained by the type of features humans and deep neural networks use in\nvision tasks. Humans tend to classify objects according to their shape while\ndeep neural networks seem to rely mostly on texture. Exploring this question is\nrelevant, since it may lead to better performing neural network architectures\nand to a better understanding of the workings of the vision system of primates.\nIn this work, we advance the state of the art in our understanding of this\nphenomenon, by extending previous analyses to a much larger set of deep neural\nnetwork architectures. We found that the performance of models in image\nclassification tasks is highly correlated with their shape bias measured at the\noutput and penultimate layer. Furthermore, our results showed that the number\nof neurons that represent shape and texture are strongly anti-correlated, thus\nproviding evidence that there is competition between these two types of\nfeatures. Finally, we observed that while in general there is a correlation\nbetween performance and shape bias, there are significant variations between\narchitecture families.",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2301.10608v1",
        "date": "2023-01-25 14:37:42+00:00"
    },
    {
        "title": "SuperSpike: Supervised learning in multi-layer spiking neural networks",
        "authors": [
            "Friedemann Zenke",
            "Surya Ganguli"
        ],
        "abstract": "A vast majority of computation in the brain is performed by spiking neural\nnetworks. Despite the ubiquity of such spiking, we currently lack an\nunderstanding of how biological spiking neural circuits learn and compute\nin-vivo, as well as how we can instantiate such capabilities in artificial\nspiking circuits in-silico. Here we revisit the problem of supervised learning\nin temporally coding multi-layer spiking neural networks. First, by using a\nsurrogate gradient approach, we derive SuperSpike, a nonlinear voltage-based\nthree factor learning rule capable of training multi-layer networks of\ndeterministic integrate-and-fire neurons to perform nonlinear computations on\nspatiotemporal spike patterns. Second, inspired by recent results on feedback\nalignment, we compare the performance of our learning rule under different\ncredit assignment strategies for propagating output errors to hidden units.\nSpecifically, we test uniform, symmetric and random feedback, finding that\nsimpler tasks can be solved with any type of feedback, while more complex tasks\nrequire symmetric feedback. In summary, our results open the door to obtaining\na better scientific understanding of learning and computation in spiking neural\nnetworks by advancing our ability to train them to solve nonlinear problems\ninvolving transformations between different spatiotemporal spike-time patterns.",
        "categories": [
            "q-bio.NC",
            "cs.LG",
            "cs.NE",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1705.11146v2",
        "date": "2017-05-31 15:31:26+00:00"
    },
    {
        "title": "Visual Context-aware Convolution Filters for Transformation-invariant Neural Network",
        "authors": [
            "Suraj Tripathi",
            "Abhay Kumar",
            "Chirag Singh"
        ],
        "abstract": "We propose a novel visual context-aware filter generation module which\nincorporates contextual information present in images into Convolutional Neural\nNetworks (CNNs). In contrast to traditional CNNs, we do not employ the same set\nof learned convolution filters for all input image instances. Our proposed\ninput-conditioned convolution filters when combined with techniques inspired by\nMulti-instance learning and max-pooling, results in a transformation-invariant\nneural network. We investigated the performance of our proposed framework on\nthree MNIST variations, which covers both rotation and scaling variance, and\nachieved 1.13% error on MNIST-rot-12k, 1.12% error on Half-rotated MNIST and\n0.68% error on Scaling MNIST, which is significantly better than the\nstate-of-the-art results. We make use of visualization to further prove the\neffectiveness of our visual context-aware convolution filters. Our proposed\nvisual context-aware convolution filter generation framework can also serve as\na plugin for any CNN based architecture and enhance its modeling capacity.",
        "categories": [
            "cs.CV",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1906.09986v1",
        "date": "2019-06-15 05:53:16+00:00"
    },
    {
        "title": "Deep Transformer Networks for Time Series Classification: The NPP Safety Case",
        "authors": [
            "Bing Zha",
            "Alessandro Vanni",
            "Yassin Hassan",
            "Tunc Aldemir",
            "Alper Yilmaz"
        ],
        "abstract": "A challenging part of dynamic probabilistic risk assessment for nuclear power\nplants is the need for large amounts of temporal simulations given various\ninitiating events and branching conditions from which representative feature\nextraction becomes complicated for subsequent applications. Artificial\nIntelligence techniques have been shown to be powerful tools in time-dependent\nsequential data processing to automatically extract and yield complex features\nfrom large data. An advanced temporal neural network referred to as the\nTransformer is used within a supervised learning fashion to model the\ntime-dependent NPP simulation data and to infer whether a given sequence of\nevents leads to core damage or not. The training and testing datasets for the\nTransformer are obtained by running 10,000 RELAP5-3D NPP blackout simulations\nwith the list of variables obtained from the RAVEN software. Each simulation is\nclassified as \"OK\" or \"CORE DAMAGE\" based on the consequence. The results show\nthat the Transformer can learn the characteristics of the sequential data and\nyield promising performance with approximately 99% classification accuracy on\nthe testing dataset.",
        "categories": [
            "cs.LG",
            "cs.NE"
        ],
        "link": "http://arxiv.org/pdf/2104.05448v2",
        "date": "2021-04-09 14:26:25+00:00"
    },
    {
        "title": "Optimizing Recurrent Neural Networks Architectures under Time Constraints",
        "authors": [
            "Junqi Jin",
            "Ziang Yan",
            "Kun Fu",
            "Nan Jiang",
            "Changshui Zhang"
        ],
        "abstract": "Recurrent neural network (RNN)'s architecture is a key factor influencing its\nperformance. We propose algorithms to optimize hidden sizes under running time\nconstraint. We convert the discrete optimization into a subset selection\nproblem. By novel transformations, the objective function becomes submodular\nand constraint becomes supermodular. A greedy algorithm with bounds is\nsuggested to solve the transformed problem. And we show how transformations\ninfluence the bounds. To speed up optimization, surrogate functions are\nproposed which balance exploration and exploitation. Experiments show that our\nalgorithms can find more accurate models or faster models than manually tuned\nstate-of-the-art and random search. We also compare popular RNN architectures\nusing our algorithms.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1608.07892v3",
        "date": "2016-08-29 02:14:48+00:00"
    },
    {
        "title": "Transformation Models for Flexible Posteriors in Variational Bayes",
        "authors": [
            "Sefan H\u00f6rtling",
            "Daniel Dold",
            "Oliver D\u00fcrr",
            "Beate Sick"
        ],
        "abstract": "The main challenge in Bayesian models is to determine the posterior for the\nmodel parameters. Already, in models with only one or few parameters, the\nanalytical posterior can only be determined in special settings. In Bayesian\nneural networks, variational inference is widely used to approximate\ndifficult-to-compute posteriors by variational distributions. Usually,\nGaussians are used as variational distributions (Gaussian-VI) which limits the\nquality of the approximation due to their limited flexibility. Transformation\nmodels on the other hand are flexible enough to fit any distribution. Here we\npresent transformation model-based variational inference (TM-VI) and\ndemonstrate that it allows to accurately approximate complex posteriors in\nmodels with one parameter and also works in a mean-field fashion for\nmulti-parameter models like neural networks.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2106.00528v1",
        "date": "2021-06-01 14:43:47+00:00"
    },
    {
        "title": "Graph Transformer Networks",
        "authors": [
            "Seongjun Yun",
            "Minbyul Jeong",
            "Raehyun Kim",
            "Jaewoo Kang",
            "Hyunwoo J. Kim"
        ],
        "abstract": "Graph neural networks (GNNs) have been widely used in representation learning\non graphs and achieved state-of-the-art performance in tasks such as node\nclassification and link prediction. However, most existing GNNs are designed to\nlearn node representations on the fixed and homogeneous graphs. The limitations\nespecially become problematic when learning representations on a misspecified\ngraph or a heterogeneous graph that consists of various types of nodes and\nedges. In this paper, we propose Graph Transformer Networks (GTNs) that are\ncapable of generating new graph structures, which involve identifying useful\nconnections between unconnected nodes on the original graph, while learning\neffective node representation on the new graphs in an end-to-end fashion. Graph\nTransformer layer, a core layer of GTNs, learns a soft selection of edge types\nand composite relations for generating useful multi-hop connections so-called\nmeta-paths. Our experiments show that GTNs learn new graph structures, based on\ndata and tasks without domain knowledge, and yield powerful node representation\nvia convolution on the new graphs. Without domain-specific graph preprocessing,\nGTNs achieved the best performance in all three benchmark node classification\ntasks against the state-of-the-art methods that require pre-defined meta-paths\nfrom domain knowledge.",
        "categories": [
            "cs.LG",
            "cs.SI",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1911.06455v2",
        "date": "2019-11-06 06:40:05+00:00"
    },
    {
        "title": "Transformer-based models and hardware acceleration analysis in autonomous driving: A survey",
        "authors": [
            "Juan Zhong",
            "Zheng Liu",
            "Xi Chen"
        ],
        "abstract": "Transformer architectures have exhibited promising performance in various\nautonomous driving applications in recent years. On the other hand, its\ndedicated hardware acceleration on portable computational platforms has become\nthe next critical step for practical deployment in real autonomous vehicles.\nThis survey paper provides a comprehensive overview, benchmark, and analysis of\nTransformer-based models specifically tailored for autonomous driving tasks\nsuch as lane detection, segmentation, tracking, planning, and decision-making.\nWe review different architectures for organizing Transformer inputs and\noutputs, such as encoder-decoder and encoder-only structures, and explore their\nrespective advantages and disadvantages. Furthermore, we discuss\nTransformer-related operators and their hardware acceleration schemes in depth,\ntaking into account key factors such as quantization and runtime. We\nspecifically illustrate the operator level comparison between layers from\nconvolutional neural network, Swin-Transformer, and Transformer with 4D\nencoder. The paper also highlights the challenges, trends, and current insights\nin Transformer-based models, addressing their hardware deployment and\nacceleration issues within the context of long-term autonomous driving\napplications.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "cs.RO",
            "cs.SY",
            "eess.SY"
        ],
        "link": "http://arxiv.org/pdf/2304.10891v1",
        "date": "2023-04-21 11:15:31+00:00"
    },
    {
        "title": "IGLOO: Slicing the Features Space to Represent Sequences",
        "authors": [
            "Vsevolod Sourkov"
        ],
        "abstract": "Historically, Recurrent neural networks (RNNs) and its variants such as LSTM\nand GRU and more recently Transformers have been the standard go-to components\nwhen processing sequential data with neural networks. One notable issue is the\nrelative difficulty to deal with long sequences (i.e. more than 20,000 steps).\nWe introduce IGLOO, a new neural network architecture which aims at being\nefficient for short sequences but also at being able to deal with long\nsequences. IGLOOs core idea is to use the relationships between non-local\npatches sliced out of the features maps of successively applied convolutions to\nbuild a representation for the sequence. We show that the model can deal with\ndependencies of more than 20,000 steps in a reasonable time frame. We stress\ntest IGLOO on the copy-memory and addition tasks, as well as permuted MNIST\n(98.4%). For a larger task we apply this new structure to the Wikitext-2\ndataset Merity et al. (2017b) and achieve a perplexity in line with baseline\nTransformers but lower than baseline AWD-LSTM. We also present how IGLOO is\nalready used today in production for bioinformatics tasks.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1807.03402v3",
        "date": "2018-07-09 21:57:20+00:00"
    },
    {
        "title": "EVNet: An Explainable Deep Network for Dimension Reduction",
        "authors": [
            "Zelin Zang",
            "Shenghui Cheng",
            "Linyan Lu",
            "Hanchen Xia",
            "Liangyu Li",
            "Yaoting Sun",
            "Yongjie Xu",
            "Lei Shang",
            "Baigui Sun",
            "Stan Z. Li"
        ],
        "abstract": "Dimension reduction (DR) is commonly utilized to capture the intrinsic\nstructure and transform high-dimensional data into low-dimensional space while\nretaining meaningful properties of the original data. It is used in various\napplications, such as image recognition, single-cell sequencing analysis, and\nbiomarker discovery. However, contemporary parametric-free and parametric DR\ntechniques suffer from several significant shortcomings, such as the inability\nto preserve global and local features and the pool generalization performance.\nOn the other hand, regarding explainability, it is crucial to comprehend the\nembedding process, especially the contribution of each part to the embedding\nprocess, while understanding how each feature affects the embedding results\nthat identify critical components and help diagnose the embedding process. To\naddress these problems, we have developed a deep neural network method called\nEVNet, which provides not only excellent performance in structural\nmaintainability but also explainability to the DR therein. EVNet starts with\ndata augmentation and a manifold-based loss function to improve embedding\nperformance. The explanation is based on saliency maps and aims to examine the\ntrained EVNet parameters and contributions of components during the embedding\nprocess. The proposed techniques are integrated with a visual interface to help\nthe user to adjust EVNet to achieve better DR performance and explainability.\nThe interactive visual interface makes it easier to illustrate the data\nfeatures, compare different DR techniques, and investigate DR. An in-depth\nexperimental comparison shows that EVNet consistently outperforms the\nstate-of-the-art methods in both performance measures and explainability.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2211.15478v1",
        "date": "2022-11-21 08:38:36+00:00"
    },
    {
        "title": "Invariant backpropagation: how to train a transformation-invariant neural network",
        "authors": [
            "Sergey Demyanov",
            "James Bailey",
            "Ramamohanarao Kotagiri",
            "Christopher Leckie"
        ],
        "abstract": "In many classification problems a classifier should be robust to small\nvariations in the input vector. This is a desired property not only for\nparticular transformations, such as translation and rotation in image\nclassification problems, but also for all others for which the change is small\nenough to retain the object perceptually indistinguishable. We propose two\nextensions of the backpropagation algorithm that train a neural network to be\nrobust to variations in the feature vector. While the first of them enforces\nrobustness of the loss function to all variations, the second method trains the\npredictions to be robust to a particular variation which changes the loss\nfunction the most. The second methods demonstrates better results, but is\nslightly slower. We analytically compare the proposed algorithm with two the\nmost similar approaches (Tangent BP and Adversarial Training), and propose\ntheir fast versions. In the experimental part we perform comparison of all\nalgorithms in terms of classification accuracy and robustness to noise on MNIST\nand CIFAR-10 datasets. Additionally we analyze how the performance of the\nproposed algorithm depends on the dataset size and data augmentation.",
        "categories": [
            "stat.ML",
            "cs.LG",
            "cs.NE"
        ],
        "link": "http://arxiv.org/pdf/1502.04434v3",
        "date": "2015-02-16 06:28:35+00:00"
    },
    {
        "title": "Pretrained Transformers for Simple Question Answering over Knowledge Graphs",
        "authors": [
            "D. Lukovnikov",
            "A. Fischer",
            "J. Lehmann"
        ],
        "abstract": "Answering simple questions over knowledge graphs is a well-studied problem in\nquestion answering. Previous approaches for this task built on recurrent and\nconvolutional neural network based architectures that use pretrained word\nembeddings. It was recently shown that finetuning pretrained transformer\nnetworks (e.g. BERT) can outperform previous approaches on various natural\nlanguage processing tasks. In this work, we investigate how well BERT performs\non SimpleQuestions and provide an evaluation of both BERT and BiLSTM-based\nmodels in datasparse scenarios.",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2001.11985v1",
        "date": "2020-01-31 18:14:17+00:00"
    },
    {
        "title": "Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation",
        "authors": [
            "Linfeng Zhang",
            "Jiebo Song",
            "Anni Gao",
            "Jingwei Chen",
            "Chenglong Bao",
            "Kaisheng Ma"
        ],
        "abstract": "Convolutional neural networks have been widely deployed in various\napplication scenarios. In order to extend the applications' boundaries to some\naccuracy-crucial domains, researchers have been investigating approaches to\nboost accuracy through either deeper or wider network structures, which brings\nwith them the exponential increment of the computational and storage cost,\ndelaying the responding time. In this paper, we propose a general training\nframework named self distillation, which notably enhances the performance\n(accuracy) of convolutional neural networks through shrinking the size of the\nnetwork rather than aggrandizing it. Different from traditional knowledge\ndistillation - a knowledge transformation methodology among networks, which\nforces student neural networks to approximate the softmax layer outputs of\npre-trained teacher neural networks, the proposed self distillation framework\ndistills knowledge within network itself. The networks are firstly divided into\nseveral sections. Then the knowledge in the deeper portion of the networks is\nsqueezed into the shallow ones. Experiments further prove the generalization of\nthe proposed self distillation framework: enhancement of accuracy at average\nlevel is 2.65%, varying from 0.61% in ResNeXt as minimum to 4.07% in VGG19 as\nmaximum. In addition, it can also provide flexibility of depth-wise scalable\ninference on resource-limited edge devices.Our codes will be released on github\nsoon.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1905.08094v1",
        "date": "2019-05-17 08:46:50+00:00"
    },
    {
        "title": "A linear approach for sparse coding by a two-layer neural network",
        "authors": [
            "Alessandro Montalto",
            "Giovanni Tessitore",
            "Roberto Prevete"
        ],
        "abstract": "Many approaches to transform classification problems from non-linear to\nlinear by feature transformation have been recently presented in the\nliterature. These notably include sparse coding methods and deep neural\nnetworks. However, many of these approaches require the repeated application of\na learning process upon the presentation of unseen data input vectors, or else\ninvolve the use of large numbers of parameters and hyper-parameters, which must\nbe chosen through cross-validation, thus increasing running time dramatically.\nIn this paper, we propose and experimentally investigate a new approach for the\npurpose of overcoming limitations of both kinds. The proposed approach makes\nuse of a linear auto-associative network (called SCNN) with just one hidden\nlayer. The combination of this architecture with a specific error function to\nbe minimized enables one to learn a linear encoder computing a sparse code\nwhich turns out to be as similar as possible to the sparse coding that one\nobtains by re-training the neural network. Importantly, the linearity of SCNN\nand the choice of the error function allow one to achieve reduced running time\nin the learning phase. The proposed architecture is evaluated on the basis of\ntwo standard machine learning tasks. Its performances are compared with those\nof recently proposed non-linear auto-associative neural networks. The overall\nresults suggest that linear encoders can be profitably used to obtain sparse\ndata representations in the context of machine learning problems, provided that\nan appropriate error function is used during the learning phase.",
        "categories": [
            "cs.LG",
            "physics.data-an"
        ],
        "link": "http://arxiv.org/pdf/1507.01892v1",
        "date": "2015-07-06 11:42:16+00:00"
    },
    {
        "title": "Be Confident! Towards Trustworthy Graph Neural Networks via Confidence Calibration",
        "authors": [
            "Xiao Wang",
            "Hongrui Liu",
            "Chuan Shi",
            "Cheng Yang"
        ],
        "abstract": "Despite Graph Neural Networks (GNNs) have achieved remarkable accuracy,\nwhether the results are trustworthy is still unexplored. Previous studies\nsuggest that many modern neural networks are over-confident on the predictions,\nhowever, surprisingly, we discover that GNNs are primarily in the opposite\ndirection, i.e., GNNs are under-confident. Therefore, the confidence\ncalibration for GNNs is highly desired. In this paper, we propose a novel\ntrustworthy GNN model by designing a topology-aware post-hoc calibration\nfunction. Specifically, we first verify that the confidence distribution in a\ngraph has homophily property, and this finding inspires us to design a\ncalibration GNN model (CaGCN) to learn the calibration function. CaGCN is able\nto obtain a unique transformation from logits of GNNs to the calibrated\nconfidence for each node, meanwhile, such transformation is able to preserve\nthe order between classes, satisfying the accuracy-preserving property.\nMoreover, we apply the calibration GNN to self-training framework, showing that\nmore trustworthy pseudo labels can be obtained with the calibrated confidence\nand further improve the performance. Extensive experiments demonstrate the\neffectiveness of our proposed model in terms of both calibration and accuracy.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2109.14285v3",
        "date": "2021-09-29 09:08:20+00:00"
    },
    {
        "title": "Deep Learning-Based Rate-Splitting Multiple Access for Reconfigurable Intelligent Surface-Aided Tera-Hertz Massive MIMO",
        "authors": [
            "Minghui Wu",
            "Zhen Gao",
            "Yang Huang",
            "Zhenyu Xiao",
            "Derrick Wing Kwan Ng",
            "Zhaoyang Zhang"
        ],
        "abstract": "Reconfigurable intelligent surface (RIS) can significantly enhance the\nservice coverage of Tera-Hertz massive multiple-input multiple-output (MIMO)\ncommunication systems. However, obtaining accurate high-dimensional channel\nstate information (CSI) with limited pilot and feedback signaling overhead is\nchallenging, severely degrading the performance of conventional spatial\ndivision multiple access. To improve the robustness against CSI imperfection,\nthis paper proposes a deep learning (DL)-based rate-splitting multiple access\n(RSMA) scheme for RIS-aided Tera-Hertz multi-user MIMO systems. Specifically,\nwe first propose a hybrid data-model driven DL-based RSMA precoding scheme,\nincluding the passive precoding at the RIS as well as the analog active\nprecoding and the RSMA digital active precoding at the base station (BS). To\nrealize the passive precoding at the RIS, we propose a Transformer-based\ndata-driven RIS reflecting network (RRN). As for the analog active precoding at\nthe BS, we propose a match-filter based analog precoding scheme considering\nthat the BS and RIS adopt the LoS-MIMO antenna array architecture. As for the\nRSMA digital active precoding at the BS, we propose a low-complexity\napproximate weighted minimum mean square error (AWMMSE) digital precoding\nscheme. Furthermore, for better precoding performance as well as lower\ncomputational complexity, a model-driven deep unfolding active precoding\nnetwork (DFAPN) is also designed by combining the proposed AWMMSE scheme with\nDL. Then, to acquire accurate CSI at the BS for the investigated RSMA precoding\nscheme to achieve higher spectral efficiency, we propose a CSI acquisition\nnetwork (CAN) with low pilot and feedback signaling overhead, where the\ndownlink pilot transmission, CSI feedback at the user equipments (UEs), and CSI\nreconstruction at the BS are modeled as an end-to-end neural network based on\nTransformer.",
        "categories": [
            "eess.SP",
            "cs.AI",
            "cs.IT",
            "cs.LG",
            "math.IT"
        ],
        "link": "http://arxiv.org/pdf/2209.08456v3",
        "date": "2022-09-18 03:07:37+00:00"
    },
    {
        "title": "Optimization-Informed Neural Networks",
        "authors": [
            "Dawen Wu",
            "Abdel Lisser"
        ],
        "abstract": "Solving constrained nonlinear optimization problems (CNLPs) is a longstanding\nproblem that arises in various fields, e.g., economics, computer science, and\nengineering. We propose optimization-informed neural networks (OINN), a deep\nlearning approach to solve CNLPs. By neurodynamic optimization methods, a CNLP\nis first reformulated as an initial value problem (IVP) involving an ordinary\ndifferential equation (ODE) system. A neural network model is then used as an\napproximate solution for this IVP, with the endpoint being the prediction to\nthe CNLP. We propose a novel training algorithm that directs the model to hold\nthe best prediction during training. In a nutshell, OINN transforms a CNLP into\na neural network training problem. By doing so, we can solve CNLPs based on\ndeep learning infrastructure only, without using standard optimization solvers\nor numerical integration solvers. The effectiveness of the proposed approach is\ndemonstrated through a collection of classical problems, e.g., variational\ninequalities, nonlinear complementary problems, and standard CNLPs.",
        "categories": [
            "math.OC",
            "cs.LG",
            "cs.NA",
            "math.NA"
        ],
        "link": "http://arxiv.org/pdf/2210.02113v2",
        "date": "2022-10-05 09:28:55+00:00"
    },
    {
        "title": "Graph Representation Learning for Road Type Classification",
        "authors": [
            "Zahra Gharaee",
            "Shreyas Kowshik",
            "Oliver Stromann",
            "Michael Felsberg"
        ],
        "abstract": "We present a novel learning-based approach to graph representations of road\nnetworks employing state-of-the-art graph convolutional neural networks. Our\napproach is applied to realistic road networks of 17 cities from Open Street\nMap. While edge features are crucial to generate descriptive graph\nrepresentations of road networks, graph convolutional networks usually rely on\nnode features only. We show that the highly representative edge features can\nstill be integrated into such networks by applying a line graph transformation.\nWe also propose a method for neighborhood sampling based on a topological\nneighborhood composed of both local and global neighbors. We compare the\nperformance of learning representations using different types of neighborhood\naggregation functions in transductive and inductive tasks and in supervised and\nunsupervised learning. Furthermore, we propose a novel aggregation approach,\nGraph Attention Isomorphism Network, GAIN. Our results show that GAIN\noutperforms state-of-the-art methods on the road type classification problem.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "link": "http://arxiv.org/pdf/2107.07791v4",
        "date": "2021-07-16 09:32:58+00:00"
    },
    {
        "title": "Audiovisual speaker conversion: jointly and simultaneously transforming facial expression and acoustic characteristics",
        "authors": [
            "Fuming Fang",
            "Xin Wang",
            "Junichi Yamagishi",
            "Isao Echizen"
        ],
        "abstract": "An audiovisual speaker conversion method is presented for simultaneously\ntransforming the facial expressions and voice of a source speaker into those of\na target speaker. Transforming the facial and acoustic features together makes\nit possible for the converted voice and facial expressions to be highly\ncorrelated and for the generated target speaker to appear and sound natural. It\nuses three neural networks: a conversion network that fuses and transforms the\nfacial and acoustic features, a waveform generation network that produces the\nwaveform from both the converted facial and acoustic features, and an image\nreconstruction network that outputs an RGB facial image also based on both the\nconverted features. The results of experiments using an emotional audiovisual\ndatabase showed that the proposed method achieved significantly higher\nnaturalness compared with one that separately transformed acoustic and facial\nfeatures.",
        "categories": [
            "eess.AS",
            "cs.CL",
            "cs.LG",
            "cs.SD",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1810.12730v2",
        "date": "2018-10-29 15:20:32+00:00"
    },
    {
        "title": "Deep Gamblers: Learning to Abstain with Portfolio Theory",
        "authors": [
            "Liu Ziyin",
            "Zhikang Wang",
            "Paul Pu Liang",
            "Ruslan Salakhutdinov",
            "Louis-Philippe Morency",
            "Masahito Ueda"
        ],
        "abstract": "We deal with the \\textit{selective classification} problem\n(supervised-learning problem with a rejection option), where we want to achieve\nthe best performance at a certain level of coverage of the data. We transform\nthe original $m$-class classification problem to $(m+1)$-class where the\n$(m+1)$-th class represents the model abstaining from making a prediction due\nto disconfidence. Inspired by portfolio theory, we propose a loss function for\nthe selective classification problem based on the doubling rate of gambling.\nMinimizing this loss function corresponds naturally to maximizing the return of\na \\textit{horse race}, where a player aims to balance between betting on an\noutcome (making a prediction) when confident and reserving one's winnings\n(abstaining) when not confident. This loss function allows us to train neural\nnetworks and characterize the disconfidence of prediction in an end-to-end\nfashion. In comparison with previous methods, our method requires almost no\nmodification to the model inference algorithm or model architecture.\nExperiments show that our method can identify uncertainty in data points, and\nachieves strong results on SVHN and CIFAR10 at various coverages of the data.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1907.00208v2",
        "date": "2019-06-29 14:04:36+00:00"
    },
    {
        "title": "Rethinking the Expressive Power of GNNs via Graph Biconnectivity",
        "authors": [
            "Bohang Zhang",
            "Shengjie Luo",
            "Liwei Wang",
            "Di He"
        ],
        "abstract": "Designing expressive Graph Neural Networks (GNNs) is a central topic in\nlearning graph-structured data. While numerous approaches have been proposed to\nimprove GNNs in terms of the Weisfeiler-Lehman (WL) test, generally there is\nstill a lack of deep understanding of what additional power they can\nsystematically and provably gain. In this paper, we take a fundamentally\ndifferent perspective to study the expressive power of GNNs beyond the WL test.\nSpecifically, we introduce a novel class of expressivity metrics via graph\nbiconnectivity and highlight their importance in both theory and practice. As\nbiconnectivity can be easily calculated using simple algorithms that have\nlinear computational costs, it is natural to expect that popular GNNs can learn\nit easily as well. However, after a thorough review of prior GNN architectures,\nwe surprisingly find that most of them are not expressive for any of these\nmetrics. The only exception is the ESAN framework (Bevilacqua et al., 2022),\nfor which we give a theoretical justification of its power. We proceed to\nintroduce a principled and more efficient approach, called the Generalized\nDistance Weisfeiler-Lehman (GD-WL), which is provably expressive for all\nbiconnectivity metrics. Practically, we show GD-WL can be implemented by a\nTransformer-like architecture that preserves expressiveness and enjoys full\nparallelizability. A set of experiments on both synthetic and real datasets\ndemonstrates that our approach can consistently outperform prior GNN\narchitectures.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2301.09505v2",
        "date": "2023-01-23 15:58:59+00:00"
    },
    {
        "title": "Deep Action Sequence Learning for Causal Shape Transformation",
        "authors": [
            "Kin Gwn Lore",
            "Daniel Stoecklein",
            "Michael Davies",
            "Baskar Ganapathysubramanian",
            "Soumik Sarkar"
        ],
        "abstract": "Deep learning became the method of choice in recent year for solving a wide\nvariety of predictive analytics tasks. For sequence prediction, recurrent\nneural networks (RNN) are often the go-to architecture for exploiting\nsequential information where the output is dependent on previous computation.\nHowever, the dependencies of the computation lie in the latent domain which may\nnot be suitable for certain applications involving the prediction of a\nstep-wise transformation sequence that is dependent on the previous computation\nonly in the visible domain. We propose that a hybrid architecture of\nconvolution neural networks (CNN) and stacked autoencoders (SAE) is sufficient\nto learn a sequence of actions that nonlinearly transforms an input shape or\ndistribution into a target shape or distribution with the same support. While\nsuch a framework can be useful in a variety of problems such as robotic path\nplanning, sequential decision-making in games, and identifying material\nprocessing pathways to achieve desired microstructures, the application of the\nframework is exemplified by the control of fluid deformations in a microfluidic\nchannel by deliberately placing a sequence of pillars. Learning of a multistep\ntopological transform has significant implications for rapid advances in\nmaterial science and biomedical applications.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "cs.NE"
        ],
        "link": "http://arxiv.org/pdf/1605.05368v3",
        "date": "2016-05-17 21:07:18+00:00"
    },
    {
        "title": "Neuronal Learning Analysis using Cycle-Consistent Adversarial Networks",
        "authors": [
            "Bryan M. Li",
            "Theoklitos Amvrosiadis",
            "Nathalie Rochefort",
            "Arno Onken"
        ],
        "abstract": "Understanding how activity in neural circuits reshapes following task\nlearning could reveal fundamental mechanisms of learning. Thanks to the recent\nadvances in neural imaging technologies, high-quality recordings can be\nobtained from hundreds of neurons over multiple days or even weeks. However,\nthe complexity and dimensionality of population responses pose significant\nchallenges for analysis. Existing methods of studying neuronal adaptation and\nlearning often impose strong assumptions on the data or model, resulting in\nbiased descriptions that do not generalize. In this work, we use a variant of\ndeep generative models called - CycleGAN, to learn the unknown mapping between\npre- and post-learning neural activities recorded $\\textit{in vivo}$. We\ndevelop an end-to-end pipeline to preprocess, train and evaluate calcium\nfluorescence signals, and a procedure to interpret the resulting deep learning\nmodels. To assess the validity of our method, we first test our framework on a\nsynthetic dataset with known ground-truth transformation. Subsequently, we\napplied our method to neural activities recorded from the primary visual cortex\nof behaving mice, where the mice transition from novice to expert-level\nperformance in a visual-based virtual reality experiment. We evaluate model\nperformance on generated calcium signals and their inferred spike trains. To\nmaximize performance, we derive a novel approach to pre-sort neurons such that\nconvolutional-based networks can take advantage of the spatial information that\nexists in neural activities. In addition, we incorporate visual explanation\nmethods to improve the interpretability of our work and gain insights into the\nlearning process as manifested in the cellular activities. Together, our\nresults demonstrate that analyzing neuronal learning processes with data-driven\ndeep unsupervised methods holds the potential to unravel changes in an unbiased\nway.",
        "categories": [
            "q-bio.NC",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2111.13073v1",
        "date": "2021-11-25 13:24:19+00:00"
    },
    {
        "title": "When Work Matters: Transforming Classical Network Structures to Graph CNN",
        "authors": [
            "Wenting Zhao",
            "Chunyan Xu",
            "Zhen Cui",
            "Tong Zhang",
            "Jiatao Jiang",
            "Zhenyu Zhang",
            "Jian Yang"
        ],
        "abstract": "Numerous pattern recognition applications can be formed as learning from\ngraph-structured data, including social network, protein-interaction network,\nthe world wide web data, knowledge graph, etc. While convolutional neural\nnetwork (CNN) facilitates great advances in gridded image/video understanding\ntasks, very limited attention has been devoted to transform these successful\nnetwork structures (including Inception net, Residual net, Dense net, etc.) to\nestablish convolutional networks on graph, due to its irregularity and\ncomplexity geometric topologies (unordered vertices, unfixed number of adjacent\nedges/vertices). In this paper, we aim to give a comprehensive analysis of when\nwork matters by transforming different classical network structures to graph\nCNN, particularly in the basic graph recognition problem. Specifically, we\nfirstly review the general graph CNN methods, especially in its spectral\nfiltering operation on the irregular graph data. We then introduce the basic\nstructures of ResNet, Inception and DenseNet into graph CNN and construct these\nnetwork structures on graph, named as G_ResNet, G_Inception, G_DenseNet. In\nparticular, it seeks to help graph CNNs by shedding light on how these\nclassical network structures work and providing guidelines for choosing\nappropriate graph network frameworks. Finally, we comprehensively evaluate the\nperformance of these different network structures on several public graph\ndatasets (including social networks and bioinformatic datasets), and\ndemonstrate how different network structures work on graph CNN in the graph\nrecognition task.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1807.02653v1",
        "date": "2018-07-07 12:12:38+00:00"
    },
    {
        "title": "Training Recipe for N:M Structured Sparsity with Decaying Pruning Mask",
        "authors": [
            "Sheng-Chun Kao",
            "Amir Yazdanbakhsh",
            "Suvinay Subramanian",
            "Shivani Agrawal",
            "Utku Evci",
            "Tushar Krishna"
        ],
        "abstract": "Sparsity has become one of the promising methods to compress and accelerate\nDeep Neural Networks (DNNs). Among different categories of sparsity, structured\nsparsity has gained more attention due to its efficient execution on modern\naccelerators. Particularly, N:M sparsity is attractive because there are\nalready hardware accelerator architectures that can leverage certain forms of\nN:M structured sparsity to yield higher compute-efficiency. In this work, we\nfocus on N:M sparsity and extensively study and evaluate various training\nrecipes for N:M sparsity in terms of the trade-off between model accuracy and\ncompute cost (FLOPs). Building upon this study, we propose two new decay-based\npruning methods, namely \"pruning mask decay\" and \"sparse structure decay\". Our\nevaluations indicate that these proposed methods consistently deliver\nstate-of-the-art (SOTA) model accuracy, comparable to unstructured sparsity, on\na Transformer-based model for a translation task. The increase in the accuracy\nof the sparse model using the new training recipes comes at the cost of\nmarginal increase in the total training compute (FLOPs).",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.AR",
            "cs.PF"
        ],
        "link": "http://arxiv.org/pdf/2209.07617v1",
        "date": "2022-09-15 21:30:55+00:00"
    },
    {
        "title": "Astronomia ex machina: a history, primer, and outlook on neural networks in astronomy",
        "authors": [
            "Michael J. Smith",
            "James E. Geach"
        ],
        "abstract": "In recent years, deep learning has infiltrated every field it has touched,\nreducing the need for specialist knowledge and automating the process of\nknowledge discovery from data. This review argues that astronomy is no\ndifferent, and that we are currently in the midst of a deep learning revolution\nthat is transforming the way we do astronomy. We trace the history of\nastronomical connectionism from the early days of multilayer perceptrons,\nthrough the second wave of convolutional and recurrent neural networks, to the\ncurrent third wave of self-supervised and unsupervised deep learning. We then\npredict that we will soon enter a fourth wave of astronomical connectionism, in\nwhich finetuned versions of an all-encompassing 'foundation' model will replace\nexpertly crafted deep learning models. We argue that such a model can only be\nbrought about through a symbiotic relationship between astronomy and\nconnectionism, whereby astronomy provides high quality multimodal data to train\nthe foundation model, and in turn the foundation model is used to advance\nastronomical research.",
        "categories": [
            "astro-ph.IM",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2211.03796v1",
        "date": "2022-11-07 19:00:00+00:00"
    },
    {
        "title": "Probabilistic Verification of ReLU Neural Networks via Characteristic Functions",
        "authors": [
            "Joshua Pilipovsky",
            "Vignesh Sivaramakrishnan",
            "Meeko M. K. Oishi",
            "Panagiotis Tsiotras"
        ],
        "abstract": "Verifying the input-output relationships of a neural network so as to achieve\nsome desired performance specification is a difficult, yet important, problem\ndue to the growing ubiquity of neural nets in many engineering applications. We\nuse ideas from probability theory in the frequency domain to provide\nprobabilistic verification guarantees for ReLU neural networks. Specifically,\nwe interpret a (deep) feedforward neural network as a discrete dynamical system\nover a finite horizon that shapes distributions of initial states, and use\ncharacteristic functions to propagate the distribution of the input data\nthrough the network. Using the inverse Fourier transform, we obtain the\ncorresponding cumulative distribution function of the output set, which can be\nused to check if the network is performing as expected given any random point\nfrom the input set. The proposed approach does not require distributions to\nhave well-defined moments or moment generating functions. We demonstrate our\nproposed approach on two examples, and compare its performance to related\napproaches.",
        "categories": [
            "cs.LG",
            "cs.SY",
            "eess.SY",
            "math.OC"
        ],
        "link": "http://arxiv.org/pdf/2212.01544v1",
        "date": "2022-12-03 05:53:57+00:00"
    },
    {
        "title": "The gap between theory and practice in function approximation with deep neural networks",
        "authors": [
            "Ben Adcock",
            "Nick Dexter"
        ],
        "abstract": "Deep learning (DL) is transforming industry as decision-making processes are\nbeing automated by deep neural networks (DNNs) trained on real-world data.\nDriven partly by rapidly-expanding literature on DNN approximation theory\nshowing they can approximate a rich variety of functions, such tools are\nincreasingly being considered for problems in scientific computing. Yet, unlike\ntraditional algorithms in this field, little is known about DNNs from the\nprinciples of numerical analysis, e.g., stability, accuracy, computational\nefficiency and sample complexity. In this paper we introduce a computational\nframework for examining DNNs in practice, and use it to study empirical\nperformance with regard to these issues. We study performance of DNNs of\ndifferent widths & depths on test functions in various dimensions, including\nsmooth and piecewise smooth functions. We also compare DL against best-in-class\nmethods for smooth function approx. based on compressed sensing (CS). Our main\nconclusion from these experiments is that there is a crucial gap between the\napproximation theory of DNNs and their practical performance, with trained DNNs\nperforming relatively poorly on functions for which there are strong\napproximation results (e.g. smooth functions), yet performing well in\ncomparison to best-in-class methods for other functions. To analyze this gap\nfurther, we provide some theoretical insights. We establish a practical\nexistence theorem, asserting existence of a DNN architecture and training\nprocedure that offers the same performance as CS. This establishes a key\ntheoretical benchmark, showing the gap can be closed, albeit via a strategy\nguaranteed to perform as well as, but no better than, current best-in-class\nschemes. Nevertheless, it demonstrates the promise of practical DNN approx., by\nhighlighting potential for better schemes through careful design of DNN\narchitectures and training strategies.",
        "categories": [
            "cs.LG",
            "cs.NA",
            "math.NA",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2001.07523v3",
        "date": "2020-01-16 20:08:56+00:00"
    },
    {
        "title": "Training Input-Output Recurrent Neural Networks through Spectral Methods",
        "authors": [
            "Hanie Sedghi",
            "Anima Anandkumar"
        ],
        "abstract": "We consider the problem of training input-output recurrent neural networks\n(RNN) for sequence labeling tasks. We propose a novel spectral approach for\nlearning the network parameters. It is based on decomposition of the\ncross-moment tensor between the output and a non-linear transformation of the\ninput, based on score functions. We guarantee consistent learning with\npolynomial sample and computational complexity under transparent conditions\nsuch as non-degeneracy of model parameters, polynomial activations for the\nneurons, and a Markovian evolution of the input sequence. We also extend our\nresults to Bidirectional RNN which uses both previous and future information to\noutput the label at each time point, and is employed in many NLP tasks such as\nPOS tagging.",
        "categories": [
            "cs.LG",
            "cs.NE",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1603.00954v5",
        "date": "2016-03-03 03:14:47+00:00"
    },
    {
        "title": "Neural Attention Forests: Transformer-Based Forest Improvement",
        "authors": [
            "Andrei V. Konstantinov",
            "Lev V. Utkin",
            "Alexey A. Lukashin",
            "Vladimir A. Muliukha"
        ],
        "abstract": "A new approach called NAF (the Neural Attention Forest) for solving\nregression and classification tasks under tabular training data is proposed.\nThe main idea behind the proposed NAF model is to introduce the attention\nmechanism into the random forest by assigning attention weights calculated by\nneural networks of a specific form to data in leaves of decision trees and to\nthe random forest itself in the framework of the Nadaraya-Watson kernel\nregression. In contrast to the available models like the attention-based random\nforest, the attention weights and the Nadaraya-Watson regression are\nrepresented in the form of neural networks whose weights can be regarded as\ntrainable parameters. The first part of neural networks with shared weights is\ntrained for all trees and computes attention weights of data in leaves. The\nsecond part aggregates outputs of the tree networks and aims to minimize the\ndifference between the random forest prediction and the truth target value from\na training set. The neural network is trained in an end-to-end manner. The\ncombination of the random forest and neural networks implementing the attention\nmechanism forms a transformer for enhancing the forest predictions. Numerical\nexperiments with real datasets illustrate the proposed method. The code\nimplementing the approach is publicly available.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2304.05980v1",
        "date": "2023-04-12 17:01:38+00:00"
    },
    {
        "title": "Combining optimal path search with task-dependent learning in a neural network",
        "authors": [
            "Tomas Kulvicius",
            "Minija Tamosiunaite",
            "Florentin W\u00f6rg\u00f6tter"
        ],
        "abstract": "Finding optimal paths in connected graphs requires determining the smallest\ntotal cost for traveling along the graph's edges. This problem can be solved by\nseveral classical algorithms where, usually, costs are predefined for all\nedges. Conventional planning methods can, thus, normally not be used when\nwanting to change costs in an adaptive way following the requirements of some\ntask. Here we show that one can define a neural network representation of path\nfinding problems by transforming cost values into synaptic weights, which\nallows for online weight adaptation using network learning mechanisms. When\nstarting with an initial activity value of one, activity propagation in this\nnetwork will lead to solutions, which are identical to those found by the\nBellman Ford algorithm. The neural network has the same algorithmic complexity\nas Bellman Ford and, in addition, we can show that network learning mechanisms\n(such as Hebbian learning) can adapt the weights in the network augmenting the\nresulting paths according to some task at hand. We demonstrate this by learning\nto navigate in an environment with obstacles as well as by learning to follow\ncertain sequences of path nodes. Hence, the here-presented novel algorithm may\nopen up a different regime of applications where path-augmentation (by\nlearning) is directly coupled with path finding in a natural way.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2201.11104v3",
        "date": "2022-01-26 18:29:00+00:00"
    },
    {
        "title": "Tighter Sparse Approximation Bounds for ReLU Neural Networks",
        "authors": [
            "Carles Domingo-Enrich",
            "Youssef Mroueh"
        ],
        "abstract": "A well-known line of work (Barron, 1993; Breiman, 1993; Klusowski & Barron,\n2018) provides bounds on the width $n$ of a ReLU two-layer neural network\nneeded to approximate a function $f$ over the ball\n$\\mathcal{B}_R(\\mathbb{R}^d)$ up to error $\\epsilon$, when the Fourier based\nquantity $C_f = \\frac{1}{(2\\pi)^{d/2}} \\int_{\\mathbb{R}^d} \\|\\xi\\|^2\n|\\hat{f}(\\xi)| \\ d\\xi$ is finite. More recently Ongie et al. (2019) used the\nRadon transform as a tool for analysis of infinite-width ReLU two-layer\nnetworks. In particular, they introduce the concept of Radon-based\n$\\mathcal{R}$-norms and show that a function defined on $\\mathbb{R}^d$ can be\nrepresented as an infinite-width two-layer neural network if and only if its\n$\\mathcal{R}$-norm is finite. In this work, we extend the framework of Ongie et\nal. (2019) and define similar Radon-based semi-norms ($\\mathcal{R},\n\\mathcal{U}$-norms) such that a function admits an infinite-width neural\nnetwork representation on a bounded open set $\\mathcal{U} \\subseteq\n\\mathbb{R}^d$ when its $\\mathcal{R}, \\mathcal{U}$-norm is finite. Building on\nthis, we derive sparse (finite-width) neural network approximation bounds that\nrefine those of Breiman (1993); Klusowski & Barron (2018). Finally, we show\nthat infinite-width neural network representations on bounded open sets are not\nunique and study their structure, providing a functional view of mode\nconnectivity.",
        "categories": [
            "stat.ML",
            "cs.LG",
            "math.ST",
            "stat.TH"
        ],
        "link": "http://arxiv.org/pdf/2110.03673v2",
        "date": "2021-10-07 17:57:20+00:00"
    },
    {
        "title": "Feature-level augmentation to improve robustness of deep neural networks to affine transformations",
        "authors": [
            "Adrian Sandru",
            "Mariana-Iuliana Georgescu",
            "Radu Tudor Ionescu"
        ],
        "abstract": "Recent studies revealed that convolutional neural networks do not generalize\nwell to small image transformations, e.g. rotations by a few degrees or\ntranslations of a few pixels. To improve the robustness to such\ntransformations, we propose to introduce data augmentation at intermediate\nlayers of the neural architecture, in addition to the common data augmentation\napplied on the input images. By introducing small perturbations to activation\nmaps (features) at various levels, we develop the capacity of the neural\nnetwork to cope with such transformations. We conduct experiments on three\nimage classification benchmarks (Tiny ImageNet, Caltech-256 and Food-101),\nconsidering two different convolutional architectures (ResNet-18 and\nDenseNet-121). When compared with two state-of-the-art stabilization methods,\nthe empirical results show that our approach consistently attains the best\ntrade-off between accuracy and mean flip rate.",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2202.05152v4",
        "date": "2022-02-10 17:14:58+00:00"
    },
    {
        "title": "Data2Vis: Automatic Generation of Data Visualizations Using Sequence to Sequence Recurrent Neural Networks",
        "authors": [
            "Victor Dibia",
            "\u00c7a\u011fatay Demiralp"
        ],
        "abstract": "Rapidly creating effective visualizations using expressive grammars is\nchallenging for users who have limited time and limited skills in statistics\nand data visualization. Even high-level, dedicated visualization tools often\nrequire users to manually select among data attributes, decide which\ntransformations to apply, and specify mappings between visual encoding\nvariables and raw or transformed attributes.\n  In this paper we introduce Data2Vis, a neural translation model for\nautomatically generating visualizations from given datasets. We formulate\nvisualization generation as a sequence to sequence translation problem where\ndata specifications are mapped to visualization specifications in a declarative\nlanguage (Vega-Lite). To this end, we train a multilayered attention-based\nrecurrent neural network (RNN) with long short-term memory (LSTM) units on a\ncorpus of visualization specifications.\n  Qualitative results show that our model learns the vocabulary and syntax for\na valid visualization specification, appropriate transformations (count, bins,\nmean) and how to use common data selection patterns that occur within data\nvisualizations. Data2Vis generates visualizations that are comparable to\nmanually-created visualizations in a fraction of the time, with potential to\nlearn more complex visualization strategies at scale.",
        "categories": [
            "cs.HC",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1804.03126v3",
        "date": "2018-04-09 17:48:23+00:00"
    },
    {
        "title": "Visual Mesh: Real-time Object Detection Using Constant Sample Density",
        "authors": [
            "Trent Houliston",
            "Stephan K. Chalup"
        ],
        "abstract": "This paper proposes an enhancement of convolutional neural networks for\nobject detection in resource-constrained robotics through a geometric input\ntransformation called Visual Mesh. It uses object geometry to create a graph in\nvision space, reducing computational complexity by normalizing the pixel and\nfeature density of objects. The experiments compare the Visual Mesh with\nseveral other fast convolutional neural networks. The results demonstrate\nexecution times sixteen times quicker than the fastest competitor tested, while\nachieving outstanding accuracy.",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CG",
            "cs.LG",
            "cs.RO",
            "68T45, 68T40",
            "I.2.10; I.2.6; I.2.9"
        ],
        "link": "http://arxiv.org/pdf/1807.08405v1",
        "date": "2018-07-23 02:21:31+00:00"
    },
    {
        "title": "PyHealth: A Python Library for Health Predictive Models",
        "authors": [
            "Yue Zhao",
            "Zhi Qiao",
            "Cao Xiao",
            "Lucas Glass",
            "Jimeng Sun"
        ],
        "abstract": "Despite the explosion of interest in healthcare AI research, the\nreproducibility and benchmarking of those research works are often limited due\nto the lack of standard benchmark datasets and diverse evaluation metrics. To\naddress this reproducibility challenge, we develop PyHealth, an open-source\nPython toolbox for developing various predictive models on healthcare data.\n  PyHealth consists of data preprocessing module, predictive modeling module,\nand evaluation module. The target users of PyHealth are both computer science\nresearchers and healthcare data scientists. With PyHealth, they can conduct\ncomplex machine learning pipelines on healthcare datasets with fewer than ten\nlines of code. The data preprocessing module enables the transformation of\ncomplex healthcare datasets such as longitudinal electronic health records,\nmedical images, continuous signals (e.g., electrocardiogram), and clinical\nnotes into machine learning friendly formats. The predictive modeling module\nprovides more than 30 machine learning models, including established ensemble\ntrees and deep neural network-based approaches, via a unified but extendable\nAPI designed for both researchers and practitioners. The evaluation module\nprovides various evaluation strategies (e.g., cross-validation and\ntrain-validation-test split) and predictive model metrics.\n  With robustness and scalability in mind, best practices such as unit testing,\ncontinuous integration, code coverage, and interactive examples are introduced\nin the library's development. PyHealth can be installed through the Python\nPackage Index (PyPI) or https://github.com/yzhao062/PyHealth .",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2101.04209v1",
        "date": "2021-01-11 22:02:08+00:00"
    },
    {
        "title": "Sequence Prediction using Spectral RNNs",
        "authors": [
            "Moritz Wolter",
            "Juergen Gall",
            "Angela Yao"
        ],
        "abstract": "Fourier methods have a long and proven track record as an excellent tool in\ndata processing. As memory and computational constraints gain importance in\nembedded and mobile applications, we propose to combine Fourier methods and\nrecurrent neural network architectures. The short-time Fourier transform allows\nus to efficiently process multiple samples at a time. Additionally, weight\nreductions trough low pass filtering is possible. We predict time series data\ndrawn from the chaotic Mackey-Glass differential equation and real-world power\nload and motion capture data.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1812.05645v3",
        "date": "2018-12-13 19:26:22+00:00"
    },
    {
        "title": "Graph Convolutional Gaussian Processes For Link Prediction",
        "authors": [
            "Felix L. Opolka",
            "Pietro Li\u00f2"
        ],
        "abstract": "Link prediction aims to reveal missing edges in a graph. We address this task\nwith a Gaussian process that is transformed using simplified graph convolutions\nto better leverage the inductive bias of the domain. To scale the Gaussian\nprocess model to large graphs, we introduce a variational inducing point method\nthat places pseudo inputs on a graph-structured domain. We evaluate our model\non eight large graphs with up to thousands of nodes and report consistent\nimprovements over existing Gaussian process models as well as competitive\nperformance when compared to state-of-the-art graph neural network approaches.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2002.04337v1",
        "date": "2020-02-11 12:12:21+00:00"
    },
    {
        "title": "Altering Backward Pass Gradients improves Convergence",
        "authors": [
            "Bishshoy Das",
            "Milton Mondal",
            "Brejesh Lall",
            "Shiv Dutt Joshi",
            "Sumantra Dutta Roy"
        ],
        "abstract": "In standard neural network training, the gradients in the backward pass are\ndetermined by the forward pass. As a result, the two stages are coupled. This\nis how most neural networks are trained currently. However, gradient\nmodification in the backward pass has seldom been studied in the literature. In\nthis paper we explore decoupled training, where we alter the gradients in the\nbackward pass. We propose a simple yet powerful method called PowerGrad\nTransform, that alters the gradients before the weight update in the backward\npass and significantly enhances the predictive performance of the neural\nnetwork. PowerGrad Transform trains the network to arrive at a better optima at\nconvergence. It is computationally extremely efficient, virtually adding no\nadditional cost to either memory or compute, but results in improved final\naccuracies on both the training and test sets. PowerGrad Transform is easy to\nintegrate into existing training routines, requiring just a few lines of code.\nPowerGrad Transform accelerates training and makes it possible for the network\nto better fit the training data. With decoupled training, PowerGrad Transform\nimproves baseline accuracies for ResNet-50 by 0.73%, for SE-ResNet-50 by 0.66%\nand by more than 1.0% for the non-normalized ResNet-18 network on the ImageNet\nclassification task.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2111.12495v3",
        "date": "2021-11-24 13:47:36+00:00"
    },
    {
        "title": "Towards Interpretable Sleep Stage Classification Using Cross-Modal Transformers",
        "authors": [
            "Jathurshan Pradeepkumar",
            "Mithunjha Anandakumar",
            "Vinith Kugathasan",
            "Dhinesh Suntharalingham",
            "Simon L. Kappel",
            "Anjula C. De Silva",
            "Chamira U. S. Edussooriya"
        ],
        "abstract": "Accurate sleep stage classification is significant for sleep health\nassessment. In recent years, several machine-learning based sleep staging\nalgorithms have been developed, and in particular, deep-learning based\nalgorithms have achieved performance on par with human annotation. Despite the\nimproved performance, a limitation of most deep-learning based algorithms is\ntheir black-box behavior, which has limited their use in clinical settings.\nHere, we propose a cross-modal transformer, which is a transformer-based method\nfor sleep stage classification. The proposed cross-modal transformer consists\nof a novel cross-modal transformer encoder architecture along with a\nmulti-scale one-dimensional convolutional neural network for automatic\nrepresentation learning. Our method outperforms the state-of-the-art methods\nand eliminates the black-box behavior of deep-learning models by utilizing the\ninterpretability aspect of the attention modules. Furthermore, our method\nprovides considerable reductions in the number of parameters and training time\ncompared to the state-of-the-art methods. Our code is available at\nhttps://github.com/Jathurshan0330/Cross-Modal-Transformer.",
        "categories": [
            "cs.LG",
            "eess.SP"
        ],
        "link": "http://arxiv.org/pdf/2208.06991v2",
        "date": "2022-08-15 03:39:08+00:00"
    },
    {
        "title": "Refactoring Neural Networks for Verification",
        "authors": [
            "David Shriver",
            "Dong Xu",
            "Sebastian Elbaum",
            "Matthew B. Dwyer"
        ],
        "abstract": "Deep neural networks (DNN) are growing in capability and applicability. Their\neffectiveness has led to their use in safety critical and autonomous systems,\nyet there is a dearth of cost-effective methods available for reasoning about\nthe behavior of a DNN. In this paper, we seek to expand the applicability and\nscalability of existing DNN verification techniques through DNN refactoring. A\nDNN refactoring defines (a) the transformation of the DNN's architecture, i.e.,\nthe number and size of its layers, and (b) the distillation of the learned\nrelationships between the input features and function outputs of the original\nto train the transformed network. Unlike with traditional code refactoring, DNN\nrefactoring does not guarantee functional equivalence of the two networks, but\nrather it aims to preserve the accuracy of the original network while producing\na simpler network that is amenable to more efficient property verification. We\npresent an automated framework for DNN refactoring, and demonstrate its\npotential effectiveness through three case studies on networks used in\nautonomous systems.",
        "categories": [
            "cs.NE",
            "cs.LG",
            "cs.SE"
        ],
        "link": "http://arxiv.org/pdf/1908.08026v1",
        "date": "2019-08-06 21:51:05+00:00"
    },
    {
        "title": "Wide Bayesian neural networks have a simple weight posterior: theory and accelerated sampling",
        "authors": [
            "Jiri Hron",
            "Roman Novak",
            "Jeffrey Pennington",
            "Jascha Sohl-Dickstein"
        ],
        "abstract": "We introduce repriorisation, a data-dependent reparameterisation which\ntransforms a Bayesian neural network (BNN) posterior to a distribution whose KL\ndivergence to the BNN prior vanishes as layer widths grow. The repriorisation\nmap acts directly on parameters, and its analytic simplicity complements the\nknown neural network Gaussian process (NNGP) behaviour of wide BNNs in function\nspace. Exploiting the repriorisation, we develop a Markov chain Monte Carlo\n(MCMC) posterior sampling algorithm which mixes faster the wider the BNN. This\ncontrasts with the typically poor performance of MCMC in high dimensions. We\nobserve up to 50x higher effective sample size relative to no reparametrisation\nfor both fully-connected and residual networks. Improvements are achieved at\nall widths, with the margin between reparametrised and standard BNNs growing\nwith layer width.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2206.07673v1",
        "date": "2022-06-15 17:11:08+00:00"
    },
    {
        "title": "In What Ways Are Deep Neural Networks Invariant and How Should We Measure This?",
        "authors": [
            "Henry Kvinge",
            "Tegan H. Emerson",
            "Grayson Jorgenson",
            "Scott Vasquez",
            "Timothy Doster",
            "Jesse D. Lew"
        ],
        "abstract": "It is often said that a deep learning model is \"invariant\" to some specific\ntype of transformation. However, what is meant by this statement strongly\ndepends on the context in which it is made. In this paper we explore the nature\nof invariance and equivariance of deep learning models with the goal of better\nunderstanding the ways in which they actually capture these concepts on a\nformal level. We introduce a family of invariance and equivariance metrics that\nallows us to quantify these properties in a way that disentangles them from\nother metrics such as loss or accuracy. We use our metrics to better understand\nthe two most popular methods used to build invariance into networks: data\naugmentation and equivariant layers. We draw a range of conclusions about\ninvariance and equivariance in deep learning models, ranging from whether\ninitializing a model with pretrained weights has an effect on a trained model's\ninvariance, to the extent to which invariance learned via training can\ngeneralize to out-of-distribution data.",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "link": "http://arxiv.org/pdf/2210.03773v1",
        "date": "2022-10-07 18:43:21+00:00"
    },
    {
        "title": "Fourier Imager Network (FIN): A deep neural network for hologram reconstruction with superior external generalization",
        "authors": [
            "Hanlong Chen",
            "Luzhe Huang",
            "Tairan Liu",
            "Aydogan Ozcan"
        ],
        "abstract": "Deep learning-based image reconstruction methods have achieved remarkable\nsuccess in phase recovery and holographic imaging. However, the generalization\nof their image reconstruction performance to new types of samples never seen by\nthe network remains a challenge. Here we introduce a deep learning framework,\ntermed Fourier Imager Network (FIN), that can perform end-to-end phase recovery\nand image reconstruction from raw holograms of new types of samples, exhibiting\nunprecedented success in external generalization. FIN architecture is based on\nspatial Fourier transform modules that process the spatial frequencies of its\ninputs using learnable filters and a global receptive field. Compared with\nexisting convolutional deep neural networks used for hologram reconstruction,\nFIN exhibits superior generalization to new types of samples, while also being\nmuch faster in its image inference speed, completing the hologram\nreconstruction task in ~0.04 s per 1 mm^2 of the sample area. We experimentally\nvalidated the performance of FIN by training it using human lung tissue samples\nand blindly testing it on human prostate, salivary gland tissue and Pap smear\nsamples, proving its superior external generalization and image reconstruction\nspeed. Beyond holographic microscopy and quantitative phase imaging, FIN and\nthe underlying neural network architecture might open up various new\nopportunities to design broadly generalizable deep learning models in\ncomputational imaging and machine vision fields.",
        "categories": [
            "cs.CV",
            "cs.LG",
            "physics.optics"
        ],
        "link": "http://arxiv.org/pdf/2204.10533v1",
        "date": "2022-04-22 06:56:24+00:00"
    },
    {
        "title": "Pure Transformers are Powerful Graph Learners",
        "authors": [
            "Jinwoo Kim",
            "Tien Dat Nguyen",
            "Seonwoo Min",
            "Sungjun Cho",
            "Moontae Lee",
            "Honglak Lee",
            "Seunghoon Hong"
        ],
        "abstract": "We show that standard Transformers without graph-specific modifications can\nlead to promising results in graph learning both in theory and practice. Given\na graph, we simply treat all nodes and edges as independent tokens, augment\nthem with token embeddings, and feed them to a Transformer. With an appropriate\nchoice of token embeddings, we prove that this approach is theoretically at\nleast as expressive as an invariant graph network (2-IGN) composed of\nequivariant linear layers, which is already more expressive than all\nmessage-passing Graph Neural Networks (GNN). When trained on a large-scale\ngraph dataset (PCQM4Mv2), our method coined Tokenized Graph Transformer\n(TokenGT) achieves significantly better results compared to GNN baselines and\ncompetitive results compared to Transformer variants with sophisticated\ngraph-specific inductive bias. Our implementation is available at\nhttps://github.com/jw9730/tokengt.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2207.02505v2",
        "date": "2022-07-06 08:13:06+00:00"
    },
    {
        "title": "A Convolutional Transformation Network for Malware Classification",
        "authors": [
            "Duc-Ly Vu",
            "Trong-Kha Nguyen",
            "Tam V. Nguyen",
            "Tu N. Nguyen",
            "Fabio Massacci",
            "Phu H. Phung"
        ],
        "abstract": "Modern malware evolves various detection avoidance techniques to bypass the\nstate-of-the-art detection methods. An emerging trend to deal with this issue\nis the combination of image transformation and machine learning techniques to\nclassify and detect malware. However, existing works in this field only perform\nsimple image transformation methods that limit the accuracy of the detection.\nIn this paper, we introduce a novel approach to classify malware by using a\ndeep network on images transformed from binary samples. In particular, we first\ndevelop a novel hybrid image transformation method to convert binaries into\ncolor images that convey the binary semantics. The images are trained by a deep\nconvolutional neural network that later classifies the test inputs into benign\nor malicious categories. Through the extensive experiments, our proposed method\nsurpasses all baselines and achieves 99.14% in terms of accuracy on the testing\nset.",
        "categories": [
            "cs.CR",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1909.07227v1",
        "date": "2019-09-16 14:17:04+00:00"
    },
    {
        "title": "Operational vs Convolutional Neural Networks for Image Denoising",
        "authors": [
            "Junaid Malik",
            "Serkan Kiranyaz",
            "Moncef Gabbouj"
        ],
        "abstract": "Convolutional Neural Networks (CNNs) have recently become a favored technique\nfor image denoising due to its adaptive learning ability, especially with a\ndeep configuration. However, their efficacy is inherently limited owing to\ntheir homogenous network formation with the unique use of linear convolution.\nIn this study, we propose a heterogeneous network model which allows greater\nflexibility for embedding additional non-linearity at the core of the data\ntransformation. To this end, we propose the idea of an operational neuron or\nOperational Neural Networks (ONN), which enables a flexible non-linear and\nheterogeneous configuration employing both inter and intra-layer neuronal\ndiversity. Furthermore, we propose a robust operator search strategy inspired\nby the Hebbian theory, called the Synaptic Plasticity Monitoring (SPM) which\ncan make data-driven choices for non-linearities in any architecture. An\nextensive set of comparative evaluations of ONNs and CNNs over two severe image\ndenoising problems yield conclusive evidence that ONNs enriched by non-linear\noperators can achieve a superior denoising performance against CNNs with both\nequivalent and well-known deep configurations.",
        "categories": [
            "eess.IV",
            "cs.CV",
            "cs.LG",
            "cs.NE"
        ],
        "link": "http://arxiv.org/pdf/2009.00612v1",
        "date": "2020-09-01 12:15:28+00:00"
    },
    {
        "title": "Inflation forecasting with attention based transformer neural networks",
        "authors": [
            "Maximilian Tschuchnig",
            "Petra Tschuchnig",
            "Cornelia Ferner",
            "Michael Gadermayr"
        ],
        "abstract": "Inflation is a major determinant for allocation decisions and its forecast is\na fundamental aim of governments and central banks. However, forecasting\ninflation is not a trivial task, as its prediction relies on low frequency,\nhighly fluctuating data with unclear explanatory variables. While classical\nmodels show some possibility of predicting inflation, reliably beating the\nrandom walk benchmark remains difficult. Recently, (deep) neural networks have\nshown impressive results in a multitude of applications, increasingly setting\nthe new state-of-the-art. This paper investigates the potential of the\ntransformer deep neural network architecture to forecast different inflation\nrates. The results are compared to a study on classical time series and machine\nlearning models. We show that our adapted transformer, on average, outperforms\nthe baseline in 6 out of 16 experiments, showing best scores in two out of four\ninvestigated inflation rates. Our results demonstrate that a transformer based\nneural network can outperform classical regression and machine learning models\nin certain inflation rates and forecasting horizons.",
        "categories": [
            "econ.EM",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2303.15364v2",
        "date": "2023-03-13 13:36:16+00:00"
    },
    {
        "title": "Analysis of Irregular Spatial Data with Machine Learning: Classification of Building Patterns with a Graph Convolutional Neural Network",
        "authors": [
            "Xiongfeng Yan",
            "Tinghua Ai"
        ],
        "abstract": "Machine learning methods such as convolutional neural networks (CNNs) are\nbecoming an integral part of scientific research in many disciplines, spatial\nvector data often fail to be analyzed using these powerful learning methods\nbecause of its irregularities. With the aid of graph Fourier transform and\nconvolution theorem, it is possible to convert the convolution as a point-wise\nproduct in Fourier domain and construct a learning architecture of CNN on graph\nfor the analysis task of irregular spatial data. In this study, we used the\nclassification task of building patterns as a case study to test this method,\nand experiments showed that this method has achieved outstanding results in\nidentifying regular and irregular patterns, and has significantly improved in\ncomparing with other methods.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1809.08196v1",
        "date": "2018-09-21 16:37:24+00:00"
    },
    {
        "title": "Bayesian Perceptron: Towards fully Bayesian Neural Networks",
        "authors": [
            "Marco F. Huber"
        ],
        "abstract": "Artificial neural networks (NNs) have become the de facto standard in machine\nlearning. They allow learning highly nonlinear transformations in a plethora of\napplications. However, NNs usually only provide point estimates without\nsystematically quantifying corresponding uncertainties. In this paper a novel\napproach towards fully Bayesian NNs is proposed, where training and predictions\nof a perceptron are performed within the Bayesian inference framework in\nclosed-form. The weights and the predictions of the perceptron are considered\nGaussian random variables. Analytical expressions for predicting the\nperceptron's output and for learning the weights are provided for commonly used\nactivation functions like sigmoid or ReLU. This approach requires no\ncomputationally expensive gradient calculations and further allows sequential\nlearning.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2009.01730v2",
        "date": "2020-09-03 15:08:49+00:00"
    },
    {
        "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention",
        "authors": [
            "Angelos Katharopoulos",
            "Apoorv Vyas",
            "Nikolaos Pappas",
            "Fran\u00e7ois Fleuret"
        ],
        "abstract": "Transformers achieve remarkable performance in several tasks but due to their\nquadratic complexity, with respect to the input's length, they are\nprohibitively slow for very long sequences. To address this limitation, we\nexpress the self-attention as a linear dot-product of kernel feature maps and\nmake use of the associativity property of matrix products to reduce the\ncomplexity from $\\mathcal{O}\\left(N^2\\right)$ to $\\mathcal{O}\\left(N\\right)$,\nwhere $N$ is the sequence length. We show that this formulation permits an\niterative implementation that dramatically accelerates autoregressive\ntransformers and reveals their relationship to recurrent neural networks. Our\nlinear transformers achieve similar performance to vanilla transformers and\nthey are up to 4000x faster on autoregressive prediction of very long\nsequences.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2006.16236v3",
        "date": "2020-06-29 17:55:38+00:00"
    },
    {
        "title": "Training Neural Networks using SAT solvers",
        "authors": [
            "Subham S. Sahoo"
        ],
        "abstract": "We propose an algorithm to explore the global optimization method, using SAT\nsolvers, for training a neural net. Deep Neural Networks have achieved great\nfeats in tasks like-image recognition, speech recognition, etc. Much of their\nsuccess can be attributed to the gradient-based optimisation methods, which\nscale well to huge datasets while still giving solutions, better than any other\nexisting methods. However, there exist learning problems like the parity\nfunction and the Fast Fourier Transform, where a neural network using\ngradient-based optimisation algorithm can not capture the underlying structure\nof the learning task properly. Thus, exploring global optimisation methods is\nof utmost interest as the gradient-based methods get stuck in local optima. In\nthe experiments, we demonstrate the effectiveness of our algorithm against the\nADAM optimiser in certain tasks like parity learning. However, in the case of\nimage classification on the MNIST Dataset, the performance of our algorithm was\nless than satisfactory. We further discuss the role of the size of the training\ndataset and the hyper-parameter settings in keeping things scalable for a SAT\nsolver.",
        "categories": [
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2206.04833v1",
        "date": "2022-06-10 01:31:12+00:00"
    },
    {
        "title": "Multi-Transformer: A New Neural Network-Based Architecture for Forecasting S&P Volatility",
        "authors": [
            "Eduardo Ramos-P\u00e9rez",
            "Pablo J. Alonso-Gonz\u00e1lez",
            "Jos\u00e9 Javier N\u00fa\u00f1ez-Vel\u00e1zquez"
        ],
        "abstract": "Events such as the Financial Crisis of 2007-2008 or the COVID-19 pandemic\ncaused significant losses to banks and insurance entities. They also\ndemonstrated the importance of using accurate equity risk models and having a\nrisk management function able to implement effective hedging strategies. Stock\nvolatility forecasts play a key role in the estimation of equity risk and,\nthus, in the management actions carried out by financial institutions.\nTherefore, this paper has the aim of proposing more accurate stock volatility\nmodels based on novel machine and deep learning techniques. This paper\nintroduces a neural network-based architecture, called Multi-Transformer.\nMulti-Transformer is a variant of Transformer models, which have already been\nsuccessfully applied in the field of natural language processing. Indeed, this\npaper also adapts traditional Transformer layers in order to be used in\nvolatility forecasting models. The empirical results obtained in this paper\nsuggest that the hybrid models based on Multi-Transformer and Transformer\nlayers are more accurate and, hence, they lead to more appropriate risk\nmeasures than other autoregressive algorithms or hybrid models based on feed\nforward layers or long short term memory cells.",
        "categories": [
            "q-fin.CP",
            "cs.LG",
            "stat.CO"
        ],
        "link": "http://arxiv.org/pdf/2109.12621v1",
        "date": "2021-09-26 14:47:04+00:00"
    },
    {
        "title": "An Attention-based Graph Neural Network for Heterogeneous Structural Learning",
        "authors": [
            "Huiting Hong",
            "Hantao Guo",
            "Yucheng Lin",
            "Xiaoqing Yang",
            "Zang Li",
            "Jieping Ye"
        ],
        "abstract": "In this paper, we focus on graph representation learning of heterogeneous\ninformation network (HIN), in which various types of vertices are connected by\nvarious types of relations. Most of the existing methods conducted on HIN\nrevise homogeneous graph embedding models via meta-paths to learn\nlow-dimensional vector space of HIN. In this paper, we propose a novel\nHeterogeneous Graph Structural Attention Neural Network (HetSANN) to directly\nencode structural information of HIN without meta-path and achieve more\ninformative representations. With this method, domain experts will not be\nneeded to design meta-path schemes and the heterogeneous information can be\nprocessed automatically by our proposed model. Specifically, we implicitly\nrepresent heterogeneous information using the following two methods: 1) we\nmodel the transformation between heterogeneous vertices through a projection in\nlow-dimensional entity spaces; 2) afterwards, we apply the graph neural network\nto aggregate multi-relational information of projected neighborhood by means of\nattention mechanism. We also present three extensions of HetSANN, i.e.,\nvoices-sharing product attention for the pairwise relationships in HIN,\ncycle-consistency loss to retain the transformation between heterogeneous\nentity spaces, and multi-task learning with full use of information. The\nexperiments conducted on three public datasets demonstrate that our proposed\nmodels achieve significant and consistent improvements compared to\nstate-of-the-art solutions.",
        "categories": [
            "cs.LG",
            "cs.SI",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1912.10832v1",
        "date": "2019-12-19 06:20:48+00:00"
    },
    {
        "title": "T$^2$LR-Net: An Unrolling Reconstruction Network Learning Transformed Tensor Low-Rank prior for Dynamic MR Imaging",
        "authors": [
            "Yinghao Zhang",
            "Yue Hu"
        ],
        "abstract": "While the methods exploiting the tensor low-rank prior are booming in\nhigh-dimensional data processing and have obtained satisfying performance,\ntheir applications in dynamic magnetic resonance (MR) image reconstruction are\nlimited. In this paper, we concentrate on the tensor singular value\ndecomposition (t-SVD), which is based on the Fast Fourier Transform (FFT) and\nonly provides the definite and limited tensor low-rank prior in the FFT domain,\nheavily reliant upon how closely the data and the FFT domain match up. By\ngeneralizing the FFT into an arbitrary unitary transformation of the\ntransformed t-SVD and proposing the transformed tensor nuclear norm (TTNN), we\nintroduce a flexible model based on TTNN with the ability to exploit the tensor\nlow-rank prior of a transformed domain in a larger transformation space and\nelaborately design an iterative optimization algorithm based on the alternating\ndirection method of multipliers (ADMM), which is further unrolled into a\nmodel-based deep unrolling reconstruction network to learn the transformed\ntensor low-rank prior (T$^2$LR-Net). The convolutional neural network (CNN) is\nincorporated within the T$^2$LR-Net to learn the best-matched transform from\nthe dynamic MR image dataset. The unrolling reconstruction network also\nprovides a new perspective on the low-rank prior utilization by exploiting the\nlow-rank prior in the CNN-extracted feature domain. Experimental results on two\ncardiac cine MR datasets demonstrate that the proposed framework can provide\nimproved recovery results compared with the state-of-the-art optimization-based\nand unrolling network-based methods.",
        "categories": [
            "eess.IV",
            "cs.CV",
            "cs.LG",
            "I.4.5; I.2.6; I.4.1"
        ],
        "link": "http://arxiv.org/pdf/2209.03832v1",
        "date": "2022-09-08 14:11:02+00:00"
    },
    {
        "title": "Streetscape augmentation using generative adversarial networks: insights related to health and wellbeing",
        "authors": [
            "Jasper S. Wijnands",
            "Kerry A. Nice",
            "Jason Thompson",
            "Haifeng Zhao",
            "Mark Stevenson"
        ],
        "abstract": "Deep learning using neural networks has provided advances in image style\ntransfer, merging the content of one image (e.g., a photo) with the style of\nanother (e.g., a painting). Our research shows this concept can be extended to\nanalyse the design of streetscapes in relation to health and wellbeing\noutcomes. An Australian population health survey (n=34,000) was used to\nidentify the spatial distribution of health and wellbeing outcomes, including\ngeneral health and social capital. For each outcome, the most and least\ndesirable locations formed two domains. Streetscape design was sampled using\naround 80,000 Google Street View images per domain. Generative adversarial\nnetworks translated these images from one domain to the other, preserving the\nmain structure of the input image, but transforming the `style' from locations\nwhere self-reported health was bad to locations where it was good. These\ntranslations indicate that areas in Melbourne with good general health are\ncharacterised by sufficient green space and compactness of the urban\nenvironment, whilst streetscape imagery related to high social capital\ncontained more and wider footpaths, fewer fences and more grass. Beyond\nidentifying relationships, the method is a first step towards\ncomputer-generated design interventions that have the potential to improve\npopulation health and wellbeing.",
        "categories": [
            "cs.CY",
            "cs.CV",
            "cs.LG",
            "stat.ML",
            "I.2.10; J.5"
        ],
        "link": "http://arxiv.org/pdf/1905.06464v1",
        "date": "2019-05-14 03:13:15+00:00"
    },
    {
        "title": "Deep Transform: Cocktail Party Source Separation via Complex Convolution in a Deep Neural Network",
        "authors": [
            "Andrew J. R. Simpson"
        ],
        "abstract": "Convolutional deep neural networks (DNN) are state of the art in many\nengineering problems but have not yet addressed the issue of how to deal with\ncomplex spectrograms. Here, we use circular statistics to provide a convenient\nprobabilistic estimate of spectrogram phase in a complex convolutional DNN. In\na typical cocktail party source separation scenario, we trained a convolutional\nDNN to re-synthesize the complex spectrograms of two source speech signals\ngiven a complex spectrogram of the monaural mixture - a discriminative deep\ntransform (DT). We then used this complex convolutional DT to obtain\nprobabilistic estimates of the magnitude and phase components of the source\nspectrograms. Our separation results are on a par with equivalent binary-mask\nbased non-complex separation approaches.",
        "categories": [
            "cs.SD",
            "cs.LG",
            "cs.NE",
            "68Txx"
        ],
        "link": "http://arxiv.org/pdf/1504.02945v1",
        "date": "2015-04-12 08:44:56+00:00"
    },
    {
        "title": "Fully Hyperbolic Neural Networks",
        "authors": [
            "Weize Chen",
            "Xu Han",
            "Yankai Lin",
            "Hexu Zhao",
            "Zhiyuan Liu",
            "Peng Li",
            "Maosong Sun",
            "Jie Zhou"
        ],
        "abstract": "Hyperbolic neural networks have shown great potential for modeling complex\ndata. However, existing hyperbolic networks are not completely hyperbolic, as\nthey encode features in a hyperbolic space yet formalize most of their\noperations in the tangent space (a Euclidean subspace) at the origin of the\nhyperbolic space. This hybrid method greatly limits the modeling ability of\nnetworks. In this paper, we propose a fully hyperbolic framework to build\nhyperbolic networks based on the Lorentz model by adapting the Lorentz\ntransformations (including boost and rotation) to formalize essential\noperations of neural networks. Moreover, we also prove that linear\ntransformation in tangent spaces used by existing hyperbolic networks is a\nrelaxation of the Lorentz rotation and does not include the boost, implicitly\nlimiting the capabilities of existing hyperbolic networks. The experimental\nresults on four NLP tasks show that our method has better performance for\nbuilding both shallow and deep networks. Our code will be released to\nfacilitate follow-up research.",
        "categories": [
            "cs.CL",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2105.14686v3",
        "date": "2021-05-31 03:36:49+00:00"
    },
    {
        "title": "On Data Sampling Strategies for Training Neural Network Speech Separation Models",
        "authors": [
            "William Ravenscroft",
            "Stefan Goetze",
            "Thomas Hain"
        ],
        "abstract": "Speech separation remains an important area of multi-speaker signal\nprocessing. Deep neural network (DNN) models have attained the best performance\non many speech separation benchmarks. Some of these models can take significant\ntime to train and have high memory requirements. Previous work has proposed\nshortening training examples to address these issues but the impact of this on\nmodel performance is not yet well understood. In this work, the impact of\napplying these training signal length (TSL) limits is analysed for two speech\nseparation models: SepFormer, a transformer model, and Conv-TasNet, a\nconvolutional model. The WJS0-2Mix, WHAMR and Libri2Mix datasets are analysed\nin terms of signal length distribution and its impact on training efficiency.\nIt is demonstrated that, for specific distributions, applying specific TSL\nlimits results in better performance. This is shown to be mainly due to\nrandomly sampling the start index of the waveforms resulting in more unique\nexamples for training. A SepFormer model trained using a TSL limit of 4.42s and\ndynamic mixing (DM) is shown to match the best-performing SepFormer model\ntrained with DM and unlimited signal lengths. Furthermore, the 4.42s TSL limit\nresults in a 44% reduction in training time with WHAMR.",
        "categories": [
            "cs.SD",
            "cs.AI",
            "cs.LG",
            "cs.NE",
            "eess.AS"
        ],
        "link": "http://arxiv.org/pdf/2304.07142v1",
        "date": "2023-04-14 14:05:52+00:00"
    },
    {
        "title": "Earthquake Nowcasting with Deep Learning",
        "authors": [
            "Geoffrey Fox",
            "John Rundle",
            "Andrea Donnellan",
            "Bo Feng"
        ],
        "abstract": "We review previous approaches to nowcasting earthquakes and introduce new\napproaches based on deep learning using three distinct models based on\nrecurrent neural networks and transformers. We discuss different choices for\nobservables and measures presenting promising initial results for a region of\nSouthern California from 1950-2020. Earthquake activity is predicted as a\nfunction of 0.1-degree spatial bins for time periods varying from two weeks to\nfour years. The overall quality is measured by the Nash Sutcliffe Efficiency\ncomparing the deviation of nowcast and observation with the variance over time\nin each spatial region. The software is available as open-source together with\nthe preprocessed data from the USGS.",
        "categories": [
            "physics.geo-ph",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2201.01869v1",
        "date": "2021-12-18 16:55:59+00:00"
    },
    {
        "title": "Distinguishing a planetary transit from false positives: a Transformer-based classification for planetary transit signals",
        "authors": [
            "Helem Salinas",
            "Karim Pichara",
            "Rafael Brahm",
            "Francisco P\u00e9rez-Galarce",
            "Domingo Mery"
        ],
        "abstract": "Current space-based missions, such as the Transiting Exoplanet Survey\nSatellite (TESS), provide a large database of light curves that must be\nanalysed efficiently and systematically. In recent years, deep learning (DL)\nmethods, particularly convolutional neural networks (CNN), have been used to\nclassify transit signals of candidate exoplanets automatically. However, CNNs\nhave some drawbacks; for example, they require many layers to capture\ndependencies on sequential data, such as light curves, making the network so\nlarge that it eventually becomes impractical. The self-attention mechanism is a\nDL technique that attempts to mimic the action of selectively focusing on some\nrelevant things while ignoring others. Models, such as the Transformer\narchitecture, were recently proposed for sequential data with successful\nresults. Based on these successful models, we present a new architecture for\nthe automatic classification of transit signals. Our proposed architecture is\ndesigned to capture the most significant features of a transit signal and\nstellar parameters through the self-attention mechanism. In addition to model\nprediction, we take advantage of attention map inspection, obtaining a more\ninterpretable DL approach. Thus, we can identify the relevance of each element\nto differentiate a transit signal from false positives, simplifying the manual\nexamination of candidates. We show that our architecture achieves competitive\nresults concerning the CNNs applied for recognizing exoplanetary transit\nsignals in data from the TESS telescope. Based on these results, we demonstrate\nthat applying this state-of-the-art DL model to light curves can be a powerful\ntechnique for transit signal detection while offering a level of\ninterpretability.",
        "categories": [
            "astro-ph.EP",
            "astro-ph.IM",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2304.14283v1",
        "date": "2023-04-27 15:43:25+00:00"
    },
    {
        "title": "Transform-Invariant Convolutional Neural Networks for Image Classification and Search",
        "authors": [
            "Xu Shen",
            "Xinmei Tian",
            "Anfeng He",
            "Shaoyan Sun",
            "Dacheng Tao"
        ],
        "abstract": "Convolutional neural networks (CNNs) have achieved state-of-the-art results\non many visual recognition tasks. However, current CNN models still exhibit a\npoor ability to be invariant to spatial transformations of images. Intuitively,\nwith sufficient layers and parameters, hierarchical combinations of convolution\n(matrix multiplication and non-linear activation) and pooling operations should\nbe able to learn a robust mapping from transformed input images to\ntransform-invariant representations. In this paper, we propose randomly\ntransforming (rotation, scale, and translation) feature maps of CNNs during the\ntraining stage. This prevents complex dependencies of specific rotation, scale,\nand translation levels of training images in CNN models. Rather, each\nconvolutional kernel learns to detect a feature that is generally helpful for\nproducing the transform-invariant answer given the combinatorially large\nvariety of transform levels of its input feature maps. In this way, we do not\nrequire any extra training supervision or modification to the optimization\nprocess and training images. We show that random transformation provides\nsignificant improvements of CNNs on many benchmark tasks, including small-scale\nimage recognition, large-scale image recognition, and image retrieval. The code\nis available at https://github.com/jasonustc/caffe-multigpu/tree/TICNN.",
        "categories": [
            "cs.CV",
            "cs.LG",
            "eess.IV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1912.01447v1",
        "date": "2019-11-28 13:09:21+00:00"
    },
    {
        "title": "Aesthetic-based Clothing Recommendation",
        "authors": [
            "Wenhui Yu",
            "Huidi Zhang",
            "Xiangnan He",
            "Xu Chen",
            "Li Xiong",
            "Zheng Qin"
        ],
        "abstract": "Recently, product images have gained increasing attention in clothing\nrecommendation since the visual appearance of clothing products has a\nsignificant impact on consumers' decision. Most existing methods rely on\nconventional features to represent an image, such as the visual features\nextracted by convolutional neural networks (CNN features) and the\nscale-invariant feature transform algorithm (SIFT features), color histograms,\nand so on. Nevertheless, one important type of features, the \\emph{aesthetic\nfeatures}, is seldom considered. It plays a vital role in clothing\nrecommendation since a users' decision depends largely on whether the clothing\nis in line with her aesthetics, however the conventional image features cannot\nportray this directly. To bridge this gap, we propose to introduce the\naesthetic information, which is highly relevant with user preference, into\nclothing recommender systems. To achieve this, we first present the aesthetic\nfeatures extracted by a pre-trained neural network, which is a brain-inspired\ndeep structure trained for the aesthetic assessment task. Considering that the\naesthetic preference varies significantly from user to user and by time, we\nthen propose a new tensor factorization model to incorporate the aesthetic\nfeatures in a personalized manner. We conduct extensive experiments on\nreal-world datasets, which demonstrate that our approach can capture the\naesthetic preference of users and significantly outperform several\nstate-of-the-art recommendation methods.",
        "categories": [
            "cs.IR",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1809.05822v1",
        "date": "2018-09-16 06:20:36+00:00"
    },
    {
        "title": "A State-of-the-art Survey of Artificial Neural Networks for Whole-slide Image Analysis:from Popular Convolutional Neural Networks to Potential Visual Transformers",
        "authors": [
            "Xintong Li",
            "Weiming Hu",
            "Chen Li",
            "Tao Jiang",
            "Hongzan Sun",
            "Xiaoyan Li",
            "Xinyu Huang",
            "Marcin Grzegorzek"
        ],
        "abstract": "To increase the objectivity and accuracy of pathologists' work, artificial\nneural network(ANN) methods have been generally needed in the segmentation,\nclassification, and detection of histopathological WSI. In this paper, WSI\nanalysis methods based on ANN are reviewed. Firstly, the development status of\nWSI and ANN methods is introduced. Secondly, we summarize the common ANN\nmethods. Next, we discuss publicly available WSI datasets and evaluation\nmetrics. These ANN architectures for WSI processing are divided into classical\nneural networks and deep neural networks(DNNs) and then analyzed. Finally, the\napplication prospect of the analytical method in this field is discussed. The\nimportant potential method is Visual Transformers.",
        "categories": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2104.06243v3",
        "date": "2021-04-13 14:39:33+00:00"
    },
    {
        "title": "Relational Attention: Generalizing Transformers for Graph-Structured Tasks",
        "authors": [
            "Cameron Diao",
            "Ricky Loynd"
        ],
        "abstract": "Transformers flexibly operate over sets of real-valued vectors representing\ntask-specific entities and their attributes, where each vector might encode one\nword-piece token and its position in a sequence, or some piece of information\nthat carries no position at all. But as set processors, transformers are at a\ndisadvantage in reasoning over more general graph-structured data where nodes\nrepresent entities and edges represent relations between entities. To address\nthis shortcoming, we generalize transformer attention to consider and update\nedge vectors in each transformer layer. We evaluate this relational transformer\non a diverse array of graph-structured tasks, including the large and\nchallenging CLRS Algorithmic Reasoning Benchmark. There, it dramatically\noutperforms state-of-the-art graph neural networks expressly designed to reason\nover graph-structured data. Our analysis demonstrates that these gains are\nattributable to relational attention's inherent ability to leverage the greater\nexpressivity of graphs over sets.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2210.05062v3",
        "date": "2022-10-11 00:25:04+00:00"
    },
    {
        "title": "Bounded Rational Decision-Making in Feedforward Neural Networks",
        "authors": [
            "Felix Leibfried",
            "Daniel Alexander Braun"
        ],
        "abstract": "Bounded rational decision-makers transform sensory input into motor output\nunder limited computational resources. Mathematically, such decision-makers can\nbe modeled as information-theoretic channels with limited transmission rate.\nHere, we apply this formalism for the first time to multilayer feedforward\nneural networks. We derive synaptic weight update rules for two scenarios,\nwhere either each neuron is considered as a bounded rational decision-maker or\nthe network as a whole. In the update rules, bounded rationality translates\ninto information-theoretically motivated types of regularization in weight\nspace. In experiments on the MNIST benchmark classification task for\nhandwritten digits, we show that such information-theoretic regularization\nsuccessfully prevents overfitting across different architectures and attains\nresults that are competitive with other recent techniques like dropout,\ndropconnect and Bayes by backprop, for both ordinary and convolutional neural\nnetworks.",
        "categories": [
            "cs.AI",
            "cs.LG",
            "cs.NE"
        ],
        "link": "http://arxiv.org/pdf/1602.08332v2",
        "date": "2016-02-26 14:15:03+00:00"
    },
    {
        "title": "Strongly Augmented Contrastive Clustering",
        "authors": [
            "Xiaozhi Deng",
            "Dong Huang",
            "Ding-Hua Chen",
            "Chang-Dong Wang",
            "Jian-Huang Lai"
        ],
        "abstract": "Deep clustering has attracted increasing attention in recent years due to its\ncapability of joint representation learning and clustering via deep neural\nnetworks. In its latest developments, the contrastive learning has emerged as\nan effective technique to substantially enhance the deep clustering\nperformance. However, the existing contrastive learning based deep clustering\nalgorithms mostly focus on some carefully-designed augmentations (often with\nlimited transformations to preserve the structure), referred to as weak\naugmentations, but cannot go beyond the weak augmentations to explore the more\nopportunities in stronger augmentations (with more aggressive transformations\nor even severe distortions). In this paper, we present an end-to-end deep\nclustering approach termed Strongly Augmented Contrastive Clustering (SACC),\nwhich extends the conventional two-augmentation-view paradigm to multiple views\nand jointly leverages strong and weak augmentations for strengthened deep\nclustering. Particularly, we utilize a backbone network with triply-shared\nweights, where a strongly augmented view and two weakly augmented views are\nincorporated. Based on the representations produced by the backbone, the\nweak-weak view pair and the strong-weak view pairs are simultaneously exploited\nfor the instance-level contrastive learning (via an instance projector) and the\ncluster-level contrastive learning (via a cluster projector), which, together\nwith the backbone, can be jointly optimized in a purely unsupervised manner.\nExperimental results on five challenging image datasets have shown the\nsuperiority of our SACC approach over the state-of-the-art. The code is\navailable at https://github.com/dengxiaozhi/SACC.",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "link": "http://arxiv.org/pdf/2206.00380v2",
        "date": "2022-06-01 10:30:59+00:00"
    },
    {
        "title": "For Manifold Learning, Deep Neural Networks can be Locality Sensitive Hash Functions",
        "authors": [
            "Nishanth Dikkala",
            "Gal Kaplun",
            "Rina Panigrahy"
        ],
        "abstract": "It is well established that training deep neural networks gives useful\nrepresentations that capture essential features of the inputs. However, these\nrepresentations are poorly understood in theory and practice. In the context of\nsupervised learning an important question is whether these representations\ncapture features informative for classification, while filtering out\nnon-informative noisy ones. We explore a formalization of this question by\nconsidering a generative process where each class is associated with a\nhigh-dimensional manifold and different classes define different manifolds.\nUnder this model, each input is produced using two latent vectors: (i) a\n\"manifold identifier\" $\\gamma$ and; (ii)~a \"transformation parameter\" $\\theta$\nthat shifts examples along the surface of a manifold. E.g., $\\gamma$ might\nrepresent a canonical image of a dog, and $\\theta$ might stand for variations\nin pose, background or lighting. We provide theoretical and empirical evidence\nthat neural representations can be viewed as LSH-like functions that map each\ninput to an embedding that is a function of solely the informative $\\gamma$ and\ninvariant to $\\theta$, effectively recovering the manifold identifier $\\gamma$.\nAn important consequence of this behavior is one-shot learning to unseen\nclasses.",
        "categories": [
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2103.06875v1",
        "date": "2021-03-11 18:57:47+00:00"
    },
    {
        "title": "Efficient Gradient-Based Inference through Transformations between Bayes Nets and Neural Nets",
        "authors": [
            "Diederik P. Kingma",
            "Max Welling"
        ],
        "abstract": "Hierarchical Bayesian networks and neural networks with stochastic hidden\nunits are commonly perceived as two separate types of models. We show that\neither of these types of models can often be transformed into an instance of\nthe other, by switching between centered and differentiable non-centered\nparameterizations of the latent variables. The choice of parameterization\ngreatly influences the efficiency of gradient-based posterior inference; we\nshow that they are often complementary to eachother, we clarify when each\nparameterization is preferred and show how inference can be made robust. In the\nnon-centered form, a simple Monte Carlo estimator of the marginal likelihood\ncan be used for learning the parameters. Theoretical results are supported by\nexperiments.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1402.0480v5",
        "date": "2014-02-03 19:39:20+00:00"
    },
    {
        "title": "Multi-Step Prediction of Occupancy Grid Maps with Recurrent Neural Networks",
        "authors": [
            "Nima Mohajerin",
            "Mohsen Rohani"
        ],
        "abstract": "We investigate the multi-step prediction of the drivable space, represented\nby Occupancy Grid Maps (OGMs), for autonomous vehicles. Our motivation is that\naccurate multi-step prediction of the drivable space can efficiently improve\npath planning and navigation resulting in safe, comfortable and optimum paths\nin autonomous driving. We train a variety of Recurrent Neural Network (RNN)\nbased architectures on the OGM sequences from the KITTI dataset. The results\ndemonstrate significant improvement of the prediction accuracy using our\nproposed difference learning method, incorporating motion related features,\nover the state of the art. We remove the egomotion from the OGM sequences by\ntransforming them into a common frame. Although in the transformed sequences\nthe KITTI dataset is heavily biased toward static objects, by learning the\ndifference between subsequent OGMs, our proposed method provides accurate\nprediction over both the static and moving objects.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "cs.RO",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1812.09395v3",
        "date": "2018-12-21 22:27:10+00:00"
    },
    {
        "title": "Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks",
        "authors": [
            "Juho Lee",
            "Yoonho Lee",
            "Jungtaek Kim",
            "Adam R. Kosiorek",
            "Seungjin Choi",
            "Yee Whye Teh"
        ],
        "abstract": "Many machine learning tasks such as multiple instance learning, 3D shape\nrecognition, and few-shot image classification are defined on sets of\ninstances. Since solutions to such problems do not depend on the order of\nelements of the set, models used to address them should be permutation\ninvariant. We present an attention-based neural network module, the Set\nTransformer, specifically designed to model interactions among elements in the\ninput set. The model consists of an encoder and a decoder, both of which rely\non attention mechanisms. In an effort to reduce computational complexity, we\nintroduce an attention scheme inspired by inducing point methods from sparse\nGaussian process literature. It reduces the computation time of self-attention\nfrom quadratic to linear in the number of elements in the set. We show that our\nmodel is theoretically attractive and we evaluate it on a range of tasks,\ndemonstrating the state-of-the-art performance compared to recent methods for\nset-structured data.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1810.00825v3",
        "date": "2018-10-01 17:10:03+00:00"
    },
    {
        "title": "Artificial Intelligence for the Metaverse: A Survey",
        "authors": [
            "Thien Huynh-The",
            "Quoc-Viet Pham",
            "Xuan-Qui Pham",
            "Thanh Thi Nguyen",
            "Zhu Han",
            "Dong-Seong Kim"
        ],
        "abstract": "Along with the massive growth of the Internet from the 1990s until now,\nvarious innovative technologies have been created to bring users breathtaking\nexperiences with more virtual interactions in cyberspace. Many virtual\nenvironments with thousands of services and applications, from social networks\nto virtual gaming worlds, have been developed with immersive experience and\ndigital transformation, but most are incoherent instead of being integrated\ninto a platform. In this context, metaverse, a term formed by combining meta\nand universe, has been introduced as a shared virtual world that is fueled by\nmany emerging technologies, such as fifth-generation networks and beyond,\nvirtual reality, and artificial intelligence (AI). Among such technologies, AI\nhas shown the great importance of processing big data to enhance immersive\nexperience and enable human-like intelligence of virtual agents. In this\nsurvey, we make a beneficial effort to explore the role of AI in the foundation\nand development of the metaverse. We first deliver a preliminary of AI,\nincluding machine learning algorithms and deep learning architectures, and its\nrole in the metaverse. We then convey a comprehensive investigation of AI-based\nmethods concerning six technical aspects that have potentials for the\nmetaverse: natural language processing, machine vision, blockchain, networking,\ndigital twin, and neural interface, and being potential for the metaverse.\nSubsequently, several AI-aided applications, such as healthcare, manufacturing,\nsmart cities, and gaming, are studied to be deployed in the virtual worlds.\nFinally, we conclude the key contribution of this survey and open some future\nresearch directions in AI for the metaverse.",
        "categories": [
            "cs.CY",
            "cs.AI",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2202.10336v1",
        "date": "2022-02-15 03:34:56+00:00"
    },
    {
        "title": "A Robust Approach for Securing Audio Classification Against Adversarial Attacks",
        "authors": [
            "Mohammad Esmaeilpour",
            "Patrick Cardinal",
            "Alessandro Lameiras Koerich"
        ],
        "abstract": "Adversarial audio attacks can be considered as a small perturbation\nunperceptive to human ears that is intentionally added to the audio signal and\ncauses a machine learning model to make mistakes. This poses a security concern\nabout the safety of machine learning models since the adversarial attacks can\nfool such models toward the wrong predictions. In this paper we first review\nsome strong adversarial attacks that may affect both audio signals and their 2D\nrepresentations and evaluate the resiliency of the most common machine learning\nmodel, namely deep learning models and support vector machines (SVM) trained on\n2D audio representations such as short time Fourier transform (STFT), discrete\nwavelet transform (DWT) and cross recurrent plot (CRP) against several\nstate-of-the-art adversarial attacks. Next, we propose a novel approach based\non pre-processed DWT representation of audio signals and SVM to secure audio\nsystems against adversarial attacks. The proposed architecture has several\npreprocessing modules for generating and enhancing spectrograms including\ndimension reduction and smoothing. We extract features from small patches of\nthe spectrograms using speeded up robust feature (SURF) algorithm which are\nfurther used to generate a codebook using the K-Means++ algorithm. Finally,\ncodewords are used to train a SVM on the codebook of the SURF-generated\nvectors. All these steps yield to a novel approach for audio classification\nthat provides a good trade-off between accuracy and resilience. Experimental\nresults on three environmental sound datasets show the competitive performance\nof proposed approach compared to the deep neural networks both in terms of\naccuracy and robustness against strong adversarial attacks.",
        "categories": [
            "cs.LG",
            "cs.CR",
            "cs.SD",
            "eess.AS",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1904.10990v2",
        "date": "2019-04-24 18:07:52+00:00"
    },
    {
        "title": "Sparse-View X-Ray CT Reconstruction Using $\\ell_1$ Prior with Learned Transform",
        "authors": [
            "Xuehang Zheng",
            "Il Yong Chun",
            "Zhipeng Li",
            "Yong Long",
            "Jeffrey A. Fessler"
        ],
        "abstract": "A major challenge in X-ray computed tomography (CT) is reducing radiation\ndose while maintaining high quality of reconstructed images. To reduce the\nradiation dose, one can reduce the number of projection views (sparse-view CT);\nhowever, it becomes difficult to achieve high-quality image reconstruction as\nthe number of projection views decreases. Researchers have applied the concept\nof learning sparse representations from (high-quality) CT image dataset to the\nsparse-view CT reconstruction. We propose a new statistical CT reconstruction\nmodel that combines penalized weighted-least squares (PWLS) and $\\ell_1$ prior\nwith learned sparsifying transform (PWLS-ST-$\\ell_1$), and a corresponding\nefficient algorithm based on Alternating Direction Method of Multipliers\n(ADMM). To moderate the difficulty of tuning ADMM parameters, we propose a new\nADMM parameter selection scheme based on approximated condition numbers. We\ninterpret the proposed model by analyzing the minimum mean square error of its\n($\\ell_2$-norm relaxed) image update estimator. Our results with the extended\ncardiac-torso (XCAT) phantom data and clinical chest data show that, for\nsparse-view 2D fan-beam CT and 3D axial cone-beam CT, PWLS-ST-$\\ell_1$ improves\nthe quality of reconstructed images compared to the CT reconstruction methods\nusing edge-preserving regularizer and $\\ell_2$ prior with learned ST. These\nresults also show that, for sparse-view 2D fan-beam CT, PWLS-ST-$\\ell_1$\nachieves comparable or better image quality and requires much shorter runtime\nthan PWLS-DL using a learned overcomplete dictionary. Our results with clinical\nchest data show that, methods using the unsupervised learned prior generalize\nbetter than a state-of-the-art deep \"denoising\" neural network that does not\nuse a physical imaging model.",
        "categories": [
            "stat.ML",
            "cs.LG",
            "physics.med-ph"
        ],
        "link": "http://arxiv.org/pdf/1711.00905v3",
        "date": "2017-11-02 19:46:45+00:00"
    },
    {
        "title": "Learning Distributions by Generative Adversarial Networks: Approximation and Generalization",
        "authors": [
            "Yunfei Yang"
        ],
        "abstract": "We study how well generative adversarial networks (GAN) learn probability\ndistributions from finite samples by analyzing the convergence rates of these\nmodels. Our analysis is based on a new oracle inequality that decomposes the\nestimation error of GAN into the discriminator and generator approximation\nerrors, generalization error and optimization error. To estimate the\ndiscriminator approximation error, we establish error bounds on approximating\nH\\\"older functions by ReLU neural networks, with explicit upper bounds on the\nLipschitz constant of the network or norm constraint on the weights. For\ngenerator approximation error, we show that neural network can approximately\ntransform a low-dimensional source distribution to a high-dimensional target\ndistribution and bound such approximation error by the width and depth of\nneural network. Combining the approximation results with generalization bounds\nof neural networks from statistical learning theory, we establish the\nconvergence rates of GANs in various settings, when the error is measured by a\ncollection of integral probability metrics defined through H\\\"older classes,\nincluding the Wasserstein distance as a special case. In particular, for\ndistributions concentrated around a low-dimensional set, we show that the\nconvergence rates of GANs do not depend on the high ambient dimension, but on\nthe lower intrinsic dimension.",
        "categories": [
            "cs.LG",
            "cs.NA",
            "math.NA",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2205.12601v1",
        "date": "2022-05-25 09:26:17+00:00"
    },
    {
        "title": "ZerO Initialization: Initializing Neural Networks with only Zeros and Ones",
        "authors": [
            "Jiawei Zhao",
            "Florian Sch\u00e4fer",
            "Anima Anandkumar"
        ],
        "abstract": "Deep neural networks are usually initialized with random weights, with\nadequately selected initial variance to ensure stable signal propagation during\ntraining. However, selecting the appropriate variance becomes challenging\nespecially as the number of layers grows. In this work, we replace random\nweight initialization with a fully deterministic initialization scheme, viz.,\nZerO, which initializes the weights of networks with only zeros and ones (up to\na normalization factor), based on identity and Hadamard transforms. Through\nboth theoretical and empirical studies, we demonstrate that ZerO is able to\ntrain networks without damaging their expressivity. Applying ZerO on ResNet\nachieves state-of-the-art performance on various datasets, including ImageNet,\nwhich suggests random weights may be unnecessary for network initialization. In\naddition, ZerO has many benefits, such as training ultra deep networks (without\nbatch-normalization), exhibiting low-rank learning trajectories that result in\nlow-rank and sparse solutions, and improving training reproducibility.",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "link": "http://arxiv.org/pdf/2110.12661v3",
        "date": "2021-10-25 06:17:33+00:00"
    },
    {
        "title": "Knowledge Matters: Importance of Prior Information for Optimization",
        "authors": [
            "\u00c7a\u011flar G\u00fcl\u00e7ehre",
            "Yoshua Bengio"
        ],
        "abstract": "We explore the effect of introducing prior information into the intermediate\nlevel of neural networks for a learning task on which all the state-of-the-art\nmachine learning algorithms tested failed to learn. We motivate our work from\nthe hypothesis that humans learn such intermediate concepts from other\nindividuals via a form of supervision or guidance using a curriculum. The\nexperiments we have conducted provide positive evidence in favor of this\nhypothesis. In our experiments, a two-tiered MLP architecture is trained on a\ndataset with 64x64 binary inputs images, each image with three sprites. The\nfinal task is to decide whether all the sprites are the same or one of them is\ndifferent. Sprites are pentomino tetris shapes and they are placed in an image\nwith different locations using scaling and rotation transformations. The first\npart of the two-tiered MLP is pre-trained with intermediate-level targets being\nthe presence of sprites at each location, while the second part takes the\noutput of the first part as input and predicts the final task's target binary\nevent. The two-tiered MLP architecture, with a few tens of thousand examples,\nwas able to learn the task perfectly, whereas all other algorithms (include\nunsupervised pre-training, but also traditional algorithms like SVMs, decision\ntrees and boosting) all perform no better than chance. We hypothesize that the\noptimization difficulty involved when the intermediate pre-training is not\nperformed is due to the {\\em composition} of two highly non-linear tasks. Our\nfindings are also consistent with hypotheses on cultural learning inspired by\nthe observations of optimization problems with deep learning, presumably\nbecause of effective local minima.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "cs.NE",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1301.4083v6",
        "date": "2013-01-17 13:06:52+00:00"
    },
    {
        "title": "Vision Transformer for Learning Driving Policies in Complex Multi-Agent Environments",
        "authors": [
            "Eshagh Kargar",
            "Ville Kyrki"
        ],
        "abstract": "Driving in a complex urban environment is a difficult task that requires a\ncomplex decision policy. In order to make informed decisions, one needs to gain\nan understanding of the long-range context and the importance of other\nvehicles. In this work, we propose to use Vision Transformer (ViT) to learn a\ndriving policy in urban settings with birds-eye-view (BEV) input images. The\nViT network learns the global context of the scene more effectively than with\nearlier proposed Convolutional Neural Networks (ConvNets). Furthermore, ViT's\nattention mechanism helps to learn an attention map for the scene which allows\nthe ego car to determine which surrounding cars are important to its next\ndecision. We demonstrate that a DQN agent with a ViT backbone outperforms\nbaseline algorithms with ConvNet backbones pre-trained in various ways. In\nparticular, the proposed method helps reinforcement learning algorithms to\nlearn faster, with increased performance and less data than baselines.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.MA",
            "cs.RO"
        ],
        "link": "http://arxiv.org/pdf/2109.06514v1",
        "date": "2021-09-14 08:18:47+00:00"
    },
    {
        "title": "Encoding Involutory Invariances in Neural Networks",
        "authors": [
            "Anwesh Bhattacharya",
            "Marios Mattheakis",
            "Pavlos Protopapas"
        ],
        "abstract": "In certain situations, neural networks are trained upon data that obey\nunderlying symmetries. However, the predictions do not respect the symmetries\nexactly unless embedded in the network structure. In this work, we introduce\narchitectures that embed a special kind of symmetry namely, invariance with\nrespect to involutory linear/affine transformations up to parity $p=\\pm 1$. We\nprovide rigorous theorems to show that the proposed network ensures such an\ninvariance and present qualitative arguments for a special universal\napproximation theorem. An adaption of our techniques to CNN tasks for datasets\nwith inherent horizontal/vertical reflection symmetry is demonstrated.\nExtensive experiments indicate that the proposed model outperforms baseline\nfeed-forward and physics-informed neural networks while identically respecting\nthe underlying symmetry.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.NE"
        ],
        "link": "http://arxiv.org/pdf/2106.12891v2",
        "date": "2021-06-07 16:07:15+00:00"
    },
    {
        "title": "Small Transformers Compute Universal Metric Embeddings",
        "authors": [
            "Anastasis Kratsios",
            "Valentin Debarnot",
            "Ivan Dokmani\u0107"
        ],
        "abstract": "We study representations of data from an arbitrary metric space $\\mathcal{X}$\nin the space of univariate Gaussian mixtures with a transport metric (Delon and\nDesolneux 2020). We derive embedding guarantees for feature maps implemented by\nsmall neural networks called \\emph{probabilistic transformers}. Our guarantees\nare of memorization type: we prove that a probabilistic transformer of depth\nabout $n\\log(n)$ and width about $n^2$ can bi-H\\\"{o}lder embed any $n$-point\ndataset from $\\mathcal{X}$ with low metric distortion, thus avoiding the curse\nof dimensionality. We further derive probabilistic bi-Lipschitz guarantees,\nwhich trade off the amount of distortion and the probability that a randomly\nchosen pair of points embeds with that distortion. If $\\mathcal{X}$'s geometry\nis sufficiently regular, we obtain stronger, bi-Lipschitz guarantees for all\npoints in the dataset. As applications, we derive neural embedding guarantees\nfor datasets from Riemannian manifolds, metric trees, and certain types of\ncombinatorial graphs. When instead embedding into multivariate Gaussian\nmixtures, we show that probabilistic transformers can compute bi-H\\\"{o}lder\nembeddings with arbitrarily small distortion.",
        "categories": [
            "cs.LG",
            "cs.NE",
            "math.CO",
            "math.MG",
            "stat.ML",
            "68T07, 30L05, 68R12, 68T30, 05C12"
        ],
        "link": "http://arxiv.org/pdf/2209.06788v2",
        "date": "2022-09-14 17:12:41+00:00"
    },
    {
        "title": "Training Deep Networks from Zero to Hero: avoiding pitfalls and going beyond",
        "authors": [
            "Moacir Antonelli Ponti",
            "Fernando Pereira dos Santos",
            "Leo Sampaio Ferraz Ribeiro",
            "Gabriel Biscaro Cavallari"
        ],
        "abstract": "Training deep neural networks may be challenging in real world data. Using\nmodels as black-boxes, even with transfer learning, can result in poor\ngeneralization or inconclusive results when it comes to small datasets or\nspecific applications. This tutorial covers the basic steps as well as more\nrecent options to improve models, in particular, but not restricted to,\nsupervised learning. It can be particularly useful in datasets that are not as\nwell-prepared as those in challenges, and also under scarce annotation and/or\nsmall data. We describe basic procedures: as data preparation, optimization and\ntransfer learning, but also recent architectural choices such as use of\ntransformer modules, alternative convolutional layers, activation functions,\nwide and deep networks, as well as training procedures including as curriculum,\ncontrastive and self-supervised learning.",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "link": "http://arxiv.org/pdf/2109.02752v2",
        "date": "2021-09-06 21:31:42+00:00"
    },
    {
        "title": "Revisiting Rubik's Cube: Self-supervised Learning with Volume-wise Transformation for 3D Medical Image Segmentation",
        "authors": [
            "Xing Tao",
            "Yuexiang Li",
            "Wenhui Zhou",
            "Kai Ma",
            "Yefeng Zheng"
        ],
        "abstract": "Deep learning highly relies on the quantity of annotated data. However, the\nannotations for 3D volumetric medical data require experienced physicians to\nspend hours or even days for investigation. Self-supervised learning is a\npotential solution to get rid of the strong requirement of training data by\ndeeply exploiting raw data information. In this paper, we propose a novel\nself-supervised learning framework for volumetric medical images. Specifically,\nwe propose a context restoration task, i.e., Rubik's cube++, to pre-train 3D\nneural networks. Different from the existing context-restoration-based\napproaches, we adopt a volume-wise transformation for context permutation,\nwhich encourages network to better exploit the inherent 3D anatomical\ninformation of organs. Compared to the strategy of training from scratch,\nfine-tuning from the Rubik's cube++ pre-trained weight can achieve better\nperformance in various tasks such as pancreas segmentation and brain tissue\nsegmentation. The experimental results show that our self-supervised learning\nmethod can significantly improve the accuracy of 3D deep learning networks on\nvolumetric medical datasets without the use of extra data.",
        "categories": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2007.08826v1",
        "date": "2020-07-17 08:53:53+00:00"
    },
    {
        "title": "Using Non-invertible Data Transformations to Build Adversarial-Robust Neural Networks",
        "authors": [
            "Qinglong Wang",
            "Wenbo Guo",
            "Alexander G. Ororbia II",
            "Xinyu Xing",
            "Lin Lin",
            "C. Lee Giles",
            "Xue Liu",
            "Peng Liu",
            "Gang Xiong"
        ],
        "abstract": "Deep neural networks have proven to be quite effective in a wide variety of\nmachine learning tasks, ranging from improved speech recognition systems to\nadvancing the development of autonomous vehicles. However, despite their\nsuperior performance in many applications, these models have been recently\nshown to be susceptible to a particular type of attack possible through the\ngeneration of particular synthetic examples referred to as adversarial samples.\nThese samples are constructed by manipulating real examples from the training\ndata distribution in order to \"fool\" the original neural model, resulting in\nmisclassification (with high confidence) of previously correctly classified\nsamples. Addressing this weakness is of utmost importance if deep neural\narchitectures are to be applied to critical applications, such as those in the\ndomain of cybersecurity. In this paper, we present an analysis of this\nfundamental flaw lurking in all neural architectures to uncover limitations of\npreviously proposed defense mechanisms. More importantly, we present a unifying\nframework for protecting deep neural models using a non-invertible data\ntransformation--developing two adversary-resilient architectures utilizing both\nlinear and nonlinear dimensionality reduction. Empirical results indicate that\nour framework provides better robustness compared to state-of-art solutions\nwhile having negligible degradation in accuracy.",
        "categories": [
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1610.01934v5",
        "date": "2016-10-06 16:20:45+00:00"
    },
    {
        "title": "Hierarchical Classification of Research Fields in the \"Web of Science\" Using Deep Learning",
        "authors": [
            "Susie Xi Rao",
            "Peter H. Egger",
            "Ce Zhang"
        ],
        "abstract": "The scholarly publication space is growing steadily not just in numbers but\nalso in complexity due to collaboration between individuals from within and\nacross fields of research. This paper presents a hierarchical classification\nsystem that automatically categorizes a scholarly publication using its\nabstract into a three-tier hierarchical label set of fields\n(discipline-field-subfield). This system enables a holistic view about the\ninterdependence of research activities in the mentioned hierarchical tiers in\nterms of knowledge production through articles and impact through citations.\n  The classification system (44 disciplines - 738 fields - 1,501 subfields)\nutilizes and is able to cope with 160 million abstract snippets in Microsoft\nAcademic Graph (Version 2018-05-17) using batch training in a modularized and\ndistributed fashion to address and assess interdisciplinarity and inter-field\nclassifications. In addition, we have explored multi-class classifications in\nboth the single-label and multi-label settings. In total, we have conducted\n3,140 experiments, in all models (Convolutional Neural Networks, Recurrent\nNeural Networks, Transformers), the classification accuracy is > 90% in 77.84%\nand 78.83% of the single-label and multi-label classifications, respectively.\nWe examine the advantages of our classification by its ability to better align\nresearch texts and output with disciplines, to adequately classify them in an\nautomated way, as well as to capture the degree of interdisciplinarity in a\npublication which enables downstream analytics such as field\ninterdisciplinarity. This system (a set of pretrained models) can serve as a\nbackbone to an interactive system of indexing scientific publications.",
        "categories": [
            "cs.DL",
            "cs.AI",
            "cs.LG",
            "68T50",
            "I.2"
        ],
        "link": "http://arxiv.org/pdf/2302.00390v1",
        "date": "2023-02-01 11:59:17+00:00"
    },
    {
        "title": "Rethinking the Reverse-engineering of Trojan Triggers",
        "authors": [
            "Zhenting Wang",
            "Kai Mei",
            "Hailun Ding",
            "Juan Zhai",
            "Shiqing Ma"
        ],
        "abstract": "Deep Neural Networks are vulnerable to Trojan (or backdoor) attacks.\nReverse-engineering methods can reconstruct the trigger and thus identify\naffected models. Existing reverse-engineering methods only consider input space\nconstraints, e.g., trigger size in the input space. Expressly, they assume the\ntriggers are static patterns in the input space and fail to detect models with\nfeature space triggers such as image style transformations. We observe that\nboth input-space and feature-space Trojans are associated with feature space\nhyperplanes. Based on this observation, we design a novel reverse-engineering\nmethod that exploits the feature space constraint to reverse-engineer Trojan\ntriggers. Results on four datasets and seven different attacks demonstrate that\nour solution effectively defends both input-space and feature-space Trojans. It\noutperforms state-of-the-art reverse-engineering methods and other types of\ndefenses in both Trojaned model detection and mitigation tasks. On average, the\ndetection accuracy of our method is 93\\%. For Trojan mitigation, our method can\nreduce the ASR (attack success rate) to only 0.26\\% with the BA (benign\naccuracy) remaining nearly unchanged. Our code can be found at\nhttps://github.com/RU-System-Software-and-Security/FeatureRE.",
        "categories": [
            "cs.CR",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2210.15127v1",
        "date": "2022-10-27 02:25:18+00:00"
    },
    {
        "title": "Text classification problems via BERT embedding method and graph convolutional neural network",
        "authors": [
            "Loc Hoang Tran",
            "Tuan Tran",
            "An Mai"
        ],
        "abstract": "This paper presents the novel way combining the BERT embedding method and the\ngraph convolutional neural network. This combination is employed to solve the\ntext classification problem. Initially, we apply the BERT embedding method to\nthe texts (in the BBC news dataset and the IMDB movie reviews dataset) in order\nto transform all the texts to numerical vector. Then, the graph convolutional\nneural network will be applied to these numerical vectors to classify these\ntexts into their ap-propriate classes/labels. Experiments show that the\nperformance of the graph convolutional neural network model is better than the\nperfor-mances of the combination of the BERT embedding method with clas-sical\nmachine learning models.",
        "categories": [
            "cs.CL",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2111.15379v3",
        "date": "2021-11-30 13:26:11+00:00"
    },
    {
        "title": "SFSegNet: Parse Freehand Sketches using Deep Fully Convolutional Networks",
        "authors": [
            "Junkun Jiang",
            "Ruomei Wang",
            "Shujin Lin",
            "Fei Wang"
        ],
        "abstract": "Parsing sketches via semantic segmentation is attractive but challenging,\nbecause (i) free-hand drawings are abstract with large variances in depicting\nobjects due to different drawing styles and skills; (ii) distorting lines drawn\non the touchpad make sketches more difficult to be recognized; (iii) the\nhigh-performance image segmentation via deep learning technologies needs\nenormous annotated sketch datasets during the training stage. In this paper, we\npropose a Sketch-target deep FCN Segmentation Network(SFSegNet) for automatic\nfree-hand sketch segmentation, labeling each sketch in a single object with\nmultiple parts. SFSegNet has an end-to-end network process between the input\nsketches and the segmentation results, composed of 2 parts: (i) a modified deep\nFully Convolutional Network(FCN) using a reweighting strategy to ignore\nbackground pixels and classify which part each pixel belongs to; (ii) affine\ntransform encoders that attempt to canonicalize the shaking strokes. We train\nour network with the dataset that consists of 10,000 annotated sketches, to\nfind an extensively applicable model to segment stokes semantically in one\nground truth. Extensive experiments are carried out and segmentation results\nshow that our method outperforms other state-of-the-art networks.",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1908.05389v1",
        "date": "2019-08-15 01:35:36+00:00"
    },
    {
        "title": "An amplitudes-perturbation data augmentation method in convolutional neural networks for EEG decoding",
        "authors": [
            "Xian-Rui Zhang",
            "Meng-Ying Lei",
            "Yang Li"
        ],
        "abstract": "Brain-Computer Interface (BCI) system provides a pathway between humans and\nthe outside world by analyzing brain signals which contain potential neural\ninformation. Electroencephalography (EEG) is one of most commonly used brain\nsignals and EEG recognition is an important part of BCI system. Recently,\nconvolutional neural networks (ConvNet) in deep learning are becoming the new\ncutting edge tools to tackle the problem of EEG recognition. However, training\nan effective deep learning model requires a big number of data, which limits\nthe application of EEG datasets with a small number of samples. In order to\nsolve the issue of data insufficiency in deep learning for EEG decoding, we\npropose a novel data augmentation method that add perturbations to amplitudes\nof EEG signals after transform them to frequency domain. In experiments, we\nexplore the performance of signal recognition with the state-of-the-art models\nbefore and after data augmentation on BCI Competition IV dataset 2a and our\nlocal dataset. The results show that our data augmentation technique can\nimprove the accuracy of EEG recognition effectively.",
        "categories": [
            "eess.SP",
            "cs.HC",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1811.02353v1",
        "date": "2018-11-06 14:00:05+00:00"
    },
    {
        "title": "Orthogonal Weight Normalization: Solution to Optimization over Multiple Dependent Stiefel Manifolds in Deep Neural Networks",
        "authors": [
            "Lei Huang",
            "Xianglong Liu",
            "Bo Lang",
            "Adams Wei Yu",
            "Yongliang Wang",
            "Bo Li"
        ],
        "abstract": "Orthogonal matrix has shown advantages in training Recurrent Neural Networks\n(RNNs), but such matrix is limited to be square for the hidden-to-hidden\ntransformation in RNNs. In this paper, we generalize such square orthogonal\nmatrix to orthogonal rectangular matrix and formulating this problem in\nfeed-forward Neural Networks (FNNs) as Optimization over Multiple Dependent\nStiefel Manifolds (OMDSM). We show that the rectangular orthogonal matrix can\nstabilize the distribution of network activations and regularize FNNs. We also\npropose a novel orthogonal weight normalization method to solve OMDSM.\nParticularly, it constructs orthogonal transformation over proxy parameters to\nensure the weight matrix is orthogonal and back-propagates gradient information\nthrough the transformation during training. To guarantee stability, we minimize\nthe distortions between proxy parameters and canonical weights over all\ntractable orthogonal transformations. In addition, we design an orthogonal\nlinear module (OLM) to learn orthogonal filter banks in practice, which can be\nused as an alternative to standard linear module. Extensive experiments\ndemonstrate that by simply substituting OLM for standard linear module without\nrevising any experimental protocols, our method largely improves the\nperformance of the state-of-the-art networks, including Inception and residual\nnetworks on CIFAR and ImageNet datasets. In particular, we have reduced the\ntest error of wide residual network on CIFAR-100 from 20.04% to 18.61% with\nsuch simple substitution. Our code is available online for result reproduction.",
        "categories": [
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1709.06079v2",
        "date": "2017-09-16 09:56:55+00:00"
    },
    {
        "title": "Adversarial Risk Bounds via Function Transformation",
        "authors": [
            "Justin Khim",
            "Po-Ling Loh"
        ],
        "abstract": "We derive bounds for a notion of adversarial risk, designed to characterize\nthe robustness of linear and neural network classifiers to adversarial\nperturbations. Specifically, we introduce a new class of function\ntransformations with the property that the risk of the transformed functions\nupper-bounds the adversarial risk of the original functions. This reduces the\nproblem of deriving bounds on the adversarial risk to the problem of deriving\nrisk bounds using standard learning-theoretic techniques. We then derive bounds\non the Rademacher complexities of the transformed function classes, obtaining\nerror rates on the same order as the generalization error of the original\nfunction classes. We also discuss extensions of our theory to multiclass\nclassification and regression. Finally, we provide two algorithms for\noptimizing the adversarial risk bounds in the linear case, and discuss\nconnections to regularization and distributional robustness.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1810.09519v2",
        "date": "2018-10-22 19:51:20+00:00"
    },
    {
        "title": "Neural Canonical Transformation with Symplectic Flows",
        "authors": [
            "Shuo-Hui Li",
            "Chen-Xiao Dong",
            "Linfeng Zhang",
            "Lei Wang"
        ],
        "abstract": "Canonical transformation plays a fundamental role in simplifying and solving\nclassical Hamiltonian systems. We construct flexible and powerful canonical\ntransformations as generative models using symplectic neural networks. The\nmodel transforms physical variables towards a latent representation with an\nindependent harmonic oscillator Hamiltonian. Correspondingly, the phase space\ndensity of the physical system flows towards a factorized Gaussian distribution\nin the latent space. Since the canonical transformation preserves the\nHamiltonian evolution, the model captures nonlinear collective modes in the\nlearned latent representation. We present an efficient implementation of\nsymplectic neural coordinate transformations and two ways to train the model.\nThe variational free energy calculation is based on the analytical form of\nphysical Hamiltonian. While the phase space density estimation only requires\nsamples in the coordinate space for separable Hamiltonians. We demonstrate\nappealing features of neural canonical transformation using toy problems\nincluding two-dimensional ring potential and harmonic chain. Finally, we apply\nthe approach to real-world problems such as identifying slow collective modes\nin alanine dipeptide and conceptual compression of the MNIST dataset.",
        "categories": [
            "cond-mat.stat-mech",
            "cs.LG",
            "physics.comp-ph",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1910.00024v3",
        "date": "2019-09-30 18:00:05+00:00"
    },
    {
        "title": "Image Transformation Network for Privacy-Preserving Deep Neural Networks and Its Security Evaluation",
        "authors": [
            "Hiroki Ito",
            "Yuma Kinoshita",
            "Hitoshi Kiya"
        ],
        "abstract": "We propose a transformation network for generating visually-protected images\nfor privacy-preserving DNNs. The proposed transformation network is trained by\nusing a plain image dataset so that plain images are transformed into visually\nprotected ones. Conventional perceptual encryption methods have a weak\nvisual-protection performance and some accuracy degradation in image\nclassification. In contrast, the proposed network enables us not only to\nstrongly protect visual information but also to maintain the image\nclassification accuracy that using plain images achieves. In an image\nclassification experiment, the proposed network is demonstrated to strongly\nprotect visual information on plain images without any performance degradation\nunder the use of CIFAR datasets. In addition, it is shown that the visually\nprotected images are robust against a DNN-based attack, called inverse\ntransformation network attack (ITN-Attack) in an experiment.",
        "categories": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2008.03143v1",
        "date": "2020-08-07 12:58:45+00:00"
    },
    {
        "title": "Solving Visual Analogies Using Neural Algorithmic Reasoning",
        "authors": [
            "Atharv Sonwane",
            "Gautam Shroff",
            "Lovekesh Vig",
            "Ashwin Srinivasan",
            "Tirtharaj Dash"
        ],
        "abstract": "We consider a class of visual analogical reasoning problems that involve\ndiscovering the sequence of transformations by which pairs of input/output\nimages are related, so as to analogously transform future inputs. This program\nsynthesis task can be easily solved via symbolic search. Using a variation of\nthe `neural analogical reasoning' approach of (Velickovic and Blundell 2021),\nwe instead search for a sequence of elementary neural network transformations\nthat manipulate distributed representations derived from a symbolic space, to\nwhich input images are directly encoded. We evaluate the extent to which our\n`neural reasoning' approach generalizes for images with unseen shapes and\npositions.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2111.10361v1",
        "date": "2021-11-19 18:48:16+00:00"
    },
    {
        "title": "Consistent Feature Construction with Constrained Genetic Programming for Experimental Physics",
        "authors": [
            "No\u00eblie Cherrier",
            "Jean-Philippe Poli",
            "Maxime Defurne",
            "Franck Sabati\u00e9"
        ],
        "abstract": "A good feature representation is a determinant factor to achieve high\nperformance for many machine learning algorithms in terms of classification.\nThis is especially true for techniques that do not build complex internal\nrepresentations of data (e.g. decision trees, in contrast to deep neural\nnetworks). To transform the feature space, feature construction techniques\nbuild new high-level features from the original ones. Among these techniques,\nGenetic Programming is a good candidate to provide interpretable features\nrequired for data analysis in high energy physics. Classically, original\nfeatures or higher-level features based on physics first principles are used as\ninputs for training. However, physicists would benefit from an automatic and\ninterpretable feature construction for the classification of particle collision\nevents.\n  Our main contribution consists in combining different aspects of Genetic\nProgramming and applying them to feature construction for experimental physics.\nIn particular, to be applicable to physics, dimensional consistency is enforced\nusing grammars.\n  Results of experiments on three physics datasets show that the constructed\nfeatures can bring a significant gain to the classification accuracy. To the\nbest of our knowledge, it is the first time a method is proposed for\ninterpretable feature construction with units of measurement, and that experts\nin high-energy physics validate the overall approach as well as the\ninterpretability of the built features.",
        "categories": [
            "cs.NE",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1908.08005v1",
        "date": "2019-08-17 10:55:15+00:00"
    },
    {
        "title": "Streamable Neural Audio Synthesis With Non-Causal Convolutions",
        "authors": [
            "Antoine Caillon",
            "Philippe Esling"
        ],
        "abstract": "Deep learning models are mostly used in an offline inference fashion.\nHowever, this strongly limits the use of these models inside audio generation\nsetups, as most creative workflows are based on real-time digital signal\nprocessing. Although approaches based on recurrent networks can be naturally\nadapted to this buffer-based computation, the use of convolutions still poses\nsome serious challenges. To tackle this issue, the use of causal streaming\nconvolutions have been proposed. However, this requires specific complexified\ntraining and can impact the resulting audio quality.\n  In this paper, we introduce a new method allowing to produce non-causal\nstreaming models. This allows to make any convolutional model compatible with\nreal-time buffer-based processing. As our method is based on a post-training\nreconfiguration of the model, we show that it is able to transform models\ntrained without causal constraints into a streaming model. We show how our\nmethod can be adapted to fit complex architectures with parallel branches. To\nevaluate our method, we apply it on the recent RAVE model, which provides\nhigh-quality real-time audio synthesis. We test our approach on multiple music\nand speech datasets and show that it is faster than overlap-add methods, while\nhaving no impact on the generation quality. Finally, we introduce two\nopen-source implementation of our work as Max/MSP and PureData externals, and\nas a VST audio plugin. This allows to endow traditional digital audio\nworkstation with real-time neural audio synthesis on a laptop CPU.",
        "categories": [
            "cs.SD",
            "cs.LG",
            "eess.AS",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2204.07064v1",
        "date": "2022-04-14 16:00:32+00:00"
    },
    {
        "title": "Prediction of Geometric Transformation on Cardiac MRI via Convolutional Neural Network",
        "authors": [
            "Xin Gao"
        ],
        "abstract": "In the field of medical image, deep convolutional neural networks(ConvNets)\nhave achieved great success in the classification, segmentation, and\nregistration tasks thanks to their unparalleled capacity to learn image\nfeatures. However, these tasks often require large amounts of manually\nannotated data and are labor-intensive. Therefore, it is of significant\nimportance for us to study unsupervised semantic feature learning tasks. In our\nwork, we propose to learn features in medical images by training ConvNets to\nrecognize the geometric transformation applied to images and present a simple\nself-supervised task that can easily predict the geometric transformation. We\nprecisely define a set of geometric transformations in mathematical terms and\ngeneralize this model to 3D, taking into account the distinction between\nspatial and time dimensions. We evaluated our self-supervised method on CMR\nimages of different modalities (bSSFP, T2, LGE) and achieved accuracies of\n96.4%, 97.5%, and 96.4%, respectively. The code and models of our paper will be\npublished on: https://github.com/gaoxin492/Geometric_Transformation_CMR",
        "categories": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2211.06641v1",
        "date": "2022-11-12 11:29:14+00:00"
    },
    {
        "title": "Semi-supervised Learning of Galaxy Morphology using Equivariant Transformer Variational Autoencoders",
        "authors": [
            "Mizu Nishikawa-Toomey",
            "Lewis Smith",
            "Yarin Gal"
        ],
        "abstract": "The growth in the number of galaxy images is much faster than the speed at\nwhich these galaxies can be labelled by humans. However, by leveraging the\ninformation present in the ever growing set of unlabelled images,\nsemi-supervised learning could be an effective way of reducing the required\nlabelling and increasing classification accuracy. We develop a Variational\nAutoencoder (VAE) with Equivariant Transformer layers with a classifier network\nfrom the latent space. We show that this novel architecture leads to\nimprovements in accuracy when used for the galaxy morphology classification\ntask on the Galaxy Zoo data set. In addition we show that pre-training the\nclassifier network as part of the VAE using the unlabelled data leads to higher\naccuracy with fewer labels compared to exiting approaches. This novel VAE has\nthe potential to automate galaxy morphology classification with reduced human\nlabelling efforts.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2011.08714v1",
        "date": "2020-11-17 15:41:18+00:00"
    },
    {
        "title": "Equivariant Transformer Networks",
        "authors": [
            "Kai Sheng Tai",
            "Peter Bailis",
            "Gregory Valiant"
        ],
        "abstract": "How can prior knowledge on the transformation invariances of a domain be\nincorporated into the architecture of a neural network? We propose Equivariant\nTransformers (ETs), a family of differentiable image-to-image mappings that\nimprove the robustness of models towards pre-defined continuous transformation\ngroups. Through the use of specially-derived canonical coordinate systems, ETs\nincorporate functions that are equivariant by construction with respect to\nthese transformations. We show empirically that ETs can be flexibly composed to\nimprove model robustness towards more complicated transformation groups in\nseveral parameters. On a real-world image classification task, ETs improve the\nsample efficiency of ResNet classifiers, achieving relative improvements in\nerror rate of up to 15% in the limited data regime while increasing model\nparameter count by less than 1%.",
        "categories": [
            "cs.CV",
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1901.11399v2",
        "date": "2019-01-25 22:29:48+00:00"
    },
    {
        "title": "Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark",
        "authors": [
            "Shiv Ram Dubey",
            "Satish Kumar Singh",
            "Bidyut Baran Chaudhuri"
        ],
        "abstract": "Neural networks have shown tremendous growth in recent years to solve\nnumerous problems. Various types of neural networks have been introduced to\ndeal with different types of problems. However, the main goal of any neural\nnetwork is to transform the non-linearly separable input data into more\nlinearly separable abstract features using a hierarchy of layers. These layers\nare combinations of linear and nonlinear functions. The most popular and common\nnon-linearity layers are activation functions (AFs), such as Logistic Sigmoid,\nTanh, ReLU, ELU, Swish and Mish. In this paper, a comprehensive overview and\nsurvey is presented for AFs in neural networks for deep learning. Different\nclasses of AFs such as Logistic Sigmoid and Tanh based, ReLU based, ELU based,\nand Learning based are covered. Several characteristics of AFs such as output\nrange, monotonicity, and smoothness are also pointed out. A performance\ncomparison is also performed among 18 state-of-the-art AFs with different\nnetworks on different types of data. The insights of AFs are presented to\nbenefit the researchers for doing further research and practitioners to select\namong different choices. The code used for experimental comparison is released\nat: \\url{https://github.com/shivram1987/ActivationFunctions}.",
        "categories": [
            "cs.LG",
            "cs.NE"
        ],
        "link": "http://arxiv.org/pdf/2109.14545v3",
        "date": "2021-09-29 16:41:19+00:00"
    },
    {
        "title": "Block Neural Autoregressive Flow",
        "authors": [
            "Nicola De Cao",
            "Ivan Titov",
            "Wilker Aziz"
        ],
        "abstract": "Normalising flows (NFS) map two density functions via a differentiable\nbijection whose Jacobian determinant can be computed efficiently. Recently, as\nan alternative to hand-crafted bijections, Huang et al. (2018) proposed neural\nautoregressive flow (NAF) which is a universal approximator for density\nfunctions. Their flow is a neural network (NN) whose parameters are predicted\nby another NN. The latter grows quadratically with the size of the former and\nthus an efficient technique for parametrization is needed. We propose block\nneural autoregressive flow (B-NAF), a much more compact universal approximator\nof density functions, where we model a bijection directly using a single\nfeed-forward network. Invertibility is ensured by carefully designing each\naffine transformation with block matrices that make the flow autoregressive and\n(strictly) monotone. We compare B-NAF to NAF and other established flows on\ndensity estimation and approximate inference for latent variable models. Our\nproposed flow is competitive across datasets while using orders of magnitude\nfewer parameters.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1904.04676v1",
        "date": "2019-04-09 13:54:55+00:00"
    },
    {
        "title": "How Powerful are Graph Neural Networks?",
        "authors": [
            "Keyulu Xu",
            "Weihua Hu",
            "Jure Leskovec",
            "Stefanie Jegelka"
        ],
        "abstract": "Graph Neural Networks (GNNs) are an effective framework for representation\nlearning of graphs. GNNs follow a neighborhood aggregation scheme, where the\nrepresentation vector of a node is computed by recursively aggregating and\ntransforming representation vectors of its neighboring nodes. Many GNN variants\nhave been proposed and have achieved state-of-the-art results on both node and\ngraph classification tasks. However, despite GNNs revolutionizing graph\nrepresentation learning, there is limited understanding of their\nrepresentational properties and limitations. Here, we present a theoretical\nframework for analyzing the expressive power of GNNs to capture different graph\nstructures. Our results characterize the discriminative power of popular GNN\nvariants, such as Graph Convolutional Networks and GraphSAGE, and show that\nthey cannot learn to distinguish certain simple graph structures. We then\ndevelop a simple architecture that is provably the most expressive among the\nclass of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism\ntest. We empirically validate our theoretical findings on a number of graph\nclassification benchmarks, and demonstrate that our model achieves\nstate-of-the-art performance.",
        "categories": [
            "cs.LG",
            "cs.CV",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1810.00826v3",
        "date": "2018-10-01 17:11:31+00:00"
    },
    {
        "title": "Multimodal Representation Learning using Deep Multiset Canonical Correlation",
        "authors": [
            "Krishna Somandepalli",
            "Naveen Kumar",
            "Ruchir Travadi",
            "Shrikanth Narayanan"
        ],
        "abstract": "We propose Deep Multiset Canonical Correlation Analysis (dMCCA) as an\nextension to representation learning using CCA when the underlying signal is\nobserved across multiple (more than two) modalities. We use deep learning\nframework to learn non-linear transformations from different modalities to a\nshared subspace such that the representations maximize the ratio of between-\nand within-modality covariance of the observations. Unlike linear discriminant\nanalysis, we do not need class information to learn these representations, and\nwe show that this model can be trained for complex data using mini-batches.\nUsing synthetic data experiments, we show that dMCCA can effectively recover\nthe common signal across the different modalities corrupted by multiplicative\nand additive noise. We also analyze the sensitivity of our model to recover the\ncorrelated components with respect to mini-batch size and dimension of the\nembeddings. Performance evaluation on noisy handwritten datasets shows that our\nmodel outperforms other CCA-based approaches and is comparable to deep neural\nnetwork models trained end-to-end on this dataset.",
        "categories": [
            "cs.LG",
            "eess.SP"
        ],
        "link": "http://arxiv.org/pdf/1904.01775v1",
        "date": "2019-04-03 05:30:30+00:00"
    },
    {
        "title": "On the Universality of Invariant Networks",
        "authors": [
            "Haggai Maron",
            "Ethan Fetaya",
            "Nimrod Segol",
            "Yaron Lipman"
        ],
        "abstract": "Constraining linear layers in neural networks to respect symmetry\ntransformations from a group $G$ is a common design principle for invariant\nnetworks that has found many applications in machine learning.\n  In this paper, we consider a fundamental question that has received little\nattention to date: Can these networks approximate any (continuous) invariant\nfunction?\n  We tackle the rather general case where $G\\leq S_n$ (an arbitrary subgroup of\nthe symmetric group) that acts on $\\mathbb{R}^n$ by permuting coordinates. This\nsetting includes several recent popular invariant networks. We present two main\nresults: First, $G$-invariant networks are universal if high-order tensors are\nallowed. Second, there are groups $G$ for which higher-order tensors are\nunavoidable for obtaining universality.\n  $G$-invariant networks consisting of only first-order tensors are of special\ninterest due to their practical value. We conclude the paper by proving a\nnecessary condition for the universality of $G$-invariant networks that\nincorporate only first-order tensors.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1901.09342v4",
        "date": "2019-01-27 09:29:59+00:00"
    },
    {
        "title": "Following High-level Navigation Instructions on a Simulated Quadcopter with Imitation Learning",
        "authors": [
            "Valts Blukis",
            "Nataly Brukhim",
            "Andrew Bennett",
            "Ross A. Knepper",
            "Yoav Artzi"
        ],
        "abstract": "We introduce a method for following high-level navigation instructions by\nmapping directly from images, instructions and pose estimates to continuous\nlow-level velocity commands for real-time control. The Grounded Semantic\nMapping Network (GSMN) is a fully-differentiable neural network architecture\nthat builds an explicit semantic map in the world reference frame by\nincorporating a pinhole camera projection model within the network. The\ninformation stored in the map is learned from experience, while the\nlocal-to-world transformation is computed explicitly. We train the model using\nDAggerFM, a modified variant of DAgger that trades tabular convergence\nguarantees for improved training speed and memory use. We test GSMN in virtual\nenvironments on a realistic quadcopter simulator and show that incorporating an\nexplicit mapping and grounding modules allows GSMN to outperform strong neural\nbaselines and almost reach an expert policy performance. Finally, we analyze\nthe learned map representations and show that using an explicit map leads to an\ninterpretable instruction-following model.",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.LG",
            "cs.RO"
        ],
        "link": "http://arxiv.org/pdf/1806.00047v1",
        "date": "2018-05-31 18:42:26+00:00"
    },
    {
        "title": "TASTE: Temporal and Static Tensor Factorization for Phenotyping Electronic Health Records",
        "authors": [
            "Ardavan Afshar",
            "Ioakeim Perros",
            "Haesun Park",
            "Christopher deFilippi",
            "Xiaowei Yan",
            "Walter Stewart",
            "Joyce Ho",
            "Jimeng Sun"
        ],
        "abstract": "Phenotyping electronic health records (EHR) focuses on defining meaningful\npatient groups (e.g., heart failure group and diabetes group) and identifying\nthe temporal evolution of patients in those groups. Tensor factorization has\nbeen an effective tool for phenotyping. Most of the existing works assume\neither a static patient representation with aggregate data or only model\ntemporal data. However, real EHR data contain both temporal (e.g., longitudinal\nclinical visits) and static information (e.g., patient demographics), which are\ndifficult to model simultaneously. In this paper, we propose Temporal And\nStatic TEnsor factorization (TASTE) that jointly models both static and\ntemporal information to extract phenotypes. TASTE combines the PARAFAC2 model\nwith non-negative matrix factorization to model a temporal and a static tensor.\nTo fit the proposed model, we transform the original problem into simpler ones\nwhich are optimally solved in an alternating fashion. For each of the\nsub-problems, our proposed mathematical reformulations lead to efficient\nsub-problem solvers. Comprehensive experiments on large EHR data from a heart\nfailure (HF) study confirmed that TASTE is up to 14x faster than several\nbaselines and the resulting phenotypes were confirmed to be clinically\nmeaningful by a cardiologist. Using 80 phenotypes extracted by TASTE, a simple\nlogistic regression can achieve the same level of area under the curve (AUC)\nfor HF prediction compared to a deep learning model using recurrent neural\nnetworks (RNN) with 345 features.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1911.05843v1",
        "date": "2019-11-13 22:28:57+00:00"
    },
    {
        "title": "Domain Transformer: Predicting Samples of Unseen, Future Domains",
        "authors": [
            "Johannes Schneider"
        ],
        "abstract": "The data distribution commonly evolves over time leading to problems such as\nconcept drift that often decrease classifier performance. Current techniques\nare not adequate for this problem because they either require detailed\nknowledge of the transformation or are not suited for anticipating unseen\ndomains but can only adapt to domains, where data samples are available. We\nseek to predict unseen data (and their labels) allowing us to tackle challenges\ns a non-constant data distribution in a proactive manner rather than detecting\nand reacting to already existing changes that might already have led to errors.\nTo this end, we learn a domain transformer in an unsupervised manner that\nallows generating data of unseen domains. Our approach first matches\nindependently learned latent representations of two given domains obtained from\nan auto-encoder using a Cycle-GAN. In turn, a transformation of the original\nsamples can be learned that can be applied iteratively to extrapolate to unseen\ndomains. Our evaluation of CNNs on image data confirms the usefulness of the\napproach. It also achieves very good results on the well-known problem of\nunsupervised domain adaption, where only labels but no samples have to be\npredicted. Code is available at https://github.com/JohnTailor/DoTra.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2106.06057v2",
        "date": "2021-06-10 21:20:00+00:00"
    },
    {
        "title": "The Geometry of Neural Nets' Parameter Spaces Under Reparametrization",
        "authors": [
            "Agustinus Kristiadi",
            "Felix Dangel",
            "Philipp Hennig"
        ],
        "abstract": "Model reparametrization -- transforming the parameter space via a bijective\ndifferentiable map -- is a popular way to improve the training of neural\nnetworks. But reparametrizations have also been problematic since they induce\ninconsistencies in, e.g., Hessian-based flatness measures, optimization\ntrajectories, and modes of probability density functions. This complicates\ndownstream analyses, e.g. one cannot make a definitive statement about the\nconnection between flatness and generalization. In this work, we study the\ninvariance quantities of neural nets under reparametrization from the\nperspective of Riemannian geometry. We show that this notion of invariance is\nan inherent property of any neural net, as long as one acknowledges the\nassumptions about the metric that is always present, albeit often implicitly,\nand uses the correct transformation rules under reparametrization. We present\ndiscussions on measuring the flatness of minima, in optimization, and in\nprobability-density maximization, along with applications in studying the\nbiases of optimizers and in Bayesian inference.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/2302.07384v1",
        "date": "2023-02-14 22:48:24+00:00"
    },
    {
        "title": "Classifying Process Instances Using Recurrent Neural Networks",
        "authors": [
            "Markku Hinkka",
            "Teemu Lehto",
            "Keijo Heljanko",
            "Alexander Jung"
        ],
        "abstract": "Process Mining consists of techniques where logs created by operative systems\nare transformed into process models. In process mining tools it is often\ndesired to be able to classify ongoing process instances, e.g., to predict how\nlong the process will still require to complete, or to classify process\ninstances to different classes based only on the activities that have occurred\nin the process instance thus far. Recurrent neural networks and its subclasses,\nsuch as Gated Recurrent Unit (GRU) and Long Short-Term Memory (LSTM), have been\ndemonstrated to be able to learn relevant temporal features for subsequent\nclassification tasks. In this paper we apply recurrent neural networks to\nclassifying process instances. The proposed model is trained in a supervised\nfashion using labeled process instances extracted from event log traces. This\nis the first time we know of GRU having been used in classifying business\nprocess instances. Our main experimental results shows that GRU outperforms\nLSTM remarkably in training time while giving almost identical accuracies to\nLSTM models. Additional contributions of our paper are improving the\nclassification model training time by filtering infrequent activities, which is\na technique commonly used, e.g., in Natural Language Processing (NLP).",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1809.05896v1",
        "date": "2018-09-16 15:33:24+00:00"
    },
    {
        "title": "Multivariate Time Series Classification using Dilated Convolutional Neural Network",
        "authors": [
            "Omolbanin Yazdanbakhsh",
            "Scott Dick"
        ],
        "abstract": "Multivariate time series classification is a high value and well-known\nproblem in machine learning community. Feature extraction is a main step in\nclassification tasks. Traditional approaches employ hand-crafted features for\nclassification while convolutional neural networks (CNN) are able to extract\nfeatures automatically. In this paper, we use dilated convolutional neural\nnetwork for multivariate time series classification. To deploy dilated CNN, a\nmultivariate time series is transformed into an image-like style and stacks of\ndilated and strided convolutions are applied to extract in and between features\nof variates in time series simultaneously. We evaluate our model on two human\nactivity recognition time series, finding that the automatic features extracted\nfor the time series can be as effective as hand-crafted features.",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1905.01697v1",
        "date": "2019-05-05 14:59:22+00:00"
    },
    {
        "title": "LISA: Explaining Recurrent Neural Network Judgments via Layer-wIse Semantic Accumulation and Example to Pattern Transformation",
        "authors": [
            "Pankaj Gupta",
            "Hinrich Sch\u00fctze"
        ],
        "abstract": "Recurrent neural networks (RNNs) are temporal networks and cumulative in\nnature that have shown promising results in various natural language processing\ntasks. Despite their success, it still remains a challenge to understand their\nhidden behavior. In this work, we analyze and interpret the cumulative nature\nof RNN via a proposed technique named as Layer-wIse-Semantic-Accumulation\n(LISA) for explaining decisions and detecting the most likely (i.e., saliency)\npatterns that the network relies on while decision making. We demonstrate (1)\nLISA: \"How an RNN accumulates or builds semantics during its sequential\nprocessing for a given text example and expected response\" (2) Example2pattern:\n\"How the saliency patterns look like for each category in the data according to\nthe network in decision making\". We analyse the sensitiveness of RNNs about\ndifferent inputs to check the increase or decrease in prediction scores and\nfurther extract the saliency patterns learned by the network. We employ two\nrelation classification datasets: SemEval 10 Task 8 and TAC KBP Slot Filling to\nexplain RNN predictions via the LISA and example2pattern.",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.IR",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1808.01591v1",
        "date": "2018-08-05 09:50:47+00:00"
    },
    {
        "title": "Heterogeneous Multi-task Metric Learning across Multiple Domains",
        "authors": [
            "Yong Luo",
            "Yonggang Wen",
            "Dacheng Tao"
        ],
        "abstract": "Distance metric learning (DML) plays a crucial role in diverse machine\nlearning algorithms and applications. When the labeled information in target\ndomain is limited, transfer metric learning (TML) helps to learn the metric by\nleveraging the sufficient information from other related domains. Multi-task\nmetric learning (MTML), which can be regarded as a special case of TML,\nperforms transfer across all related domains. Current TML tools usually assume\nthat the same feature representation is exploited for different domains.\nHowever, in real-world applications, data may be drawn from heterogeneous\ndomains. Heterogeneous transfer learning approaches can be adopted to remedy\nthis drawback by deriving a metric from the learned transformation across\ndifferent domains. But they are often limited in that only two domains can be\nhandled. To appropriately handle multiple domains, we develop a novel\nheterogeneous multi-task metric learning (HMTML) framework. In HMTML, the\nmetrics of all different domains are learned together. The transformations\nderived from the metrics are utilized to induce a common subspace, and the\nhigh-order covariance among the predictive structures of these domains is\nmaximized in this subspace. There do exist a few heterogeneous transfer\nlearning approaches that deal with multiple domains, but the high-order\nstatistics (correlation information), which can only be exploited by\nsimultaneously examining all domains, is ignored in these approaches. Compared\nwith them, the proposed HMTML can effectively explore such high-order\ninformation, thus obtaining more reliable feature transformations and metrics.\nEffectiveness of our method is validated by the extensive and intensive\nexperiments on text categorization, scene classification, and social image\nannotation.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1904.04081v1",
        "date": "2019-04-08 13:59:36+00:00"
    },
    {
        "title": "Improving Transformation-based Defenses against Adversarial Examples with First-order Perturbations",
        "authors": [
            "Haimin Zhang",
            "Min Xu"
        ],
        "abstract": "Deep neural networks have been successfully applied in various machine\nlearning tasks. However, studies show that neural networks are susceptible to\nadversarial attacks. This exposes a potential threat to neural network-based\nintelligent systems. We observe that the probability of the correct result\noutputted by the neural network increases by applying small first-order\nperturbations generated for non-predicted class labels to adversarial examples.\nBased on this observation, we propose a method for counteracting adversarial\nperturbations to improve adversarial robustness. In the proposed method, we\nrandomly select a number of class labels and generate small first-order\nperturbations for these selected labels. The generated perturbations are added\ntogether and then clamped onto a specified space. The obtained perturbation is\nfinally added to the adversarial example to counteract the adversarial\nperturbation contained in the example. The proposed method is applied at\ninference time and does not require retraining or finetuning the model. We\nexperimentally validate the proposed method on CIFAR-10 and CIFAR-100. The\nresults demonstrate that our method effectively improves the defense\nperformance of several transformation-based defense methods, especially against\nstrong adversarial examples generated using more iterations.",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2103.04565v2",
        "date": "2021-03-08 06:27:24+00:00"
    },
    {
        "title": "Unsupervised Domain Adaptation with Random Walks on Target Labelings",
        "authors": [
            "Twan van Laarhoven",
            "Elena Marchiori"
        ],
        "abstract": "Unsupervised Domain Adaptation (DA) is used to automatize the task of\nlabeling data: an unlabeled dataset (target) is annotated using a labeled\ndataset (source) from a related domain. We cast domain adaptation as the\nproblem of finding stable labels for target examples. A new definition of label\nstability is proposed, motivated by a generalization error bound for large\nmargin linear classifiers: a target labeling is stable when, with high\nprobability, a classifier trained on a random subsample of the target with that\nlabeling yields the same labeling. We find stable labelings using a random walk\non a directed graph with transition probabilities based on labeling stability.\nThe majority vote of those labelings visited by the walk yields a stable label\nfor each target example. The resulting domain adaptation algorithm is\nstrikingly easy to implement and apply: It does not rely on data\ntransformations, which are in general computational prohibitive in the presence\nof many input features, and does not need to access the source data, which is\nadvantageous when data sharing is restricted. By acting on the original feature\nspace, our method is able to take full advantage of deep features from external\npre-trained neural networks, as demonstrated by the results of our experiments.",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/1706.05335v2",
        "date": "2017-06-16 16:21:10+00:00"
    },
    {
        "title": "Shifted and Squeezed 8-bit Floating Point format for Low-Precision Training of Deep Neural Networks",
        "authors": [
            "L\u00e9opold Cambier",
            "Anahita Bhiwandiwalla",
            "Ting Gong",
            "Mehran Nekuii",
            "Oguz H Elibol",
            "Hanlin Tang"
        ],
        "abstract": "Training with larger number of parameters while keeping fast iterations is an\nincreasingly adopted strategy and trend for developing better performing Deep\nNeural Network (DNN) models. This necessitates increased memory footprint and\ncomputational requirements for training. Here we introduce a novel methodology\nfor training deep neural networks using 8-bit floating point (FP8) numbers.\nReduced bit precision allows for a larger effective memory and increased\ncomputational speed. We name this method Shifted and Squeezed FP8 (S2FP8). We\nshow that, unlike previous 8-bit precision training methods, the proposed\nmethod works out-of-the-box for representative models: ResNet-50, Transformer\nand NCF. The method can maintain model accuracy without requiring fine-tuning\nloss scaling parameters or keeping certain layers in single precision. We\nintroduce two learnable statistics of the DNN tensors - shifted and squeezed\nfactors that are used to optimally adjust the range of the tensors in 8-bits,\nthus minimizing the loss in information due to quantization.",
        "categories": [
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2001.05674v1",
        "date": "2020-01-16 06:38:27+00:00"
    },
    {
        "title": "In-context Reinforcement Learning with Algorithm Distillation",
        "authors": [
            "Michael Laskin",
            "Luyu Wang",
            "Junhyuk Oh",
            "Emilio Parisotto",
            "Stephen Spencer",
            "Richie Steigerwald",
            "DJ Strouse",
            "Steven Hansen",
            "Angelos Filos",
            "Ethan Brooks",
            "Maxime Gazeau",
            "Himanshu Sahni",
            "Satinder Singh",
            "Volodymyr Mnih"
        ],
        "abstract": "We propose Algorithm Distillation (AD), a method for distilling reinforcement\nlearning (RL) algorithms into neural networks by modeling their training\nhistories with a causal sequence model. Algorithm Distillation treats learning\nto reinforcement learn as an across-episode sequential prediction problem. A\ndataset of learning histories is generated by a source RL algorithm, and then a\ncausal transformer is trained by autoregressively predicting actions given\ntheir preceding learning histories as context. Unlike sequential policy\nprediction architectures that distill post-learning or expert sequences, AD is\nable to improve its policy entirely in-context without updating its network\nparameters. We demonstrate that AD can reinforcement learn in-context in a\nvariety of environments with sparse rewards, combinatorial task structure, and\npixel-based observations, and find that AD learns a more data-efficient RL\nalgorithm than the one that generated the source data.",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "link": "http://arxiv.org/pdf/2210.14215v1",
        "date": "2022-10-25 17:57:49+00:00"
    },
    {
        "title": "Implicit Neural Convolutional Kernels for Steerable CNNs",
        "authors": [
            "Maksim Zhdanov",
            "Nico Hoffmann",
            "Gabriele Cesa"
        ],
        "abstract": "Steerable convolutional neural networks (CNNs) provide a general framework\nfor building neural networks equivariant to translations and other\ntransformations belonging to an origin-preserving group $G$, such as\nreflections and rotations. They rely on standard convolutions with\n$G$-steerable kernels obtained by analytically solving the group-specific\nequivariance constraint imposed onto the kernel space. As the solution is\ntailored to a particular group $G$, the implementation of a kernel basis does\nnot generalize to other symmetry transformations, which complicates the\ndevelopment of group equivariant models. We propose using implicit neural\nrepresentation via multi-layer perceptrons (MLPs) to parameterize $G$-steerable\nkernels. The resulting framework offers a simple and flexible way to implement\nSteerable CNNs and generalizes to any group $G$ for which a $G$-equivariant MLP\ncan be built. We apply our method to point cloud (ModelNet-40) and molecular\ndata (QM9) and demonstrate a significant improvement in performance compared to\nstandard Steerable CNNs.",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "link": "http://arxiv.org/pdf/2212.06096v1",
        "date": "2022-12-12 18:10:33+00:00"
    },
    {
        "title": "Understanding neural networks with reproducing kernel Banach spaces",
        "authors": [
            "Francesca Bartolucci",
            "Ernesto De Vito",
            "Lorenzo Rosasco",
            "Stefano Vigogna"
        ],
        "abstract": "Characterizing the function spaces corresponding to neural networks can\nprovide a way to understand their properties. In this paper we discuss how the\ntheory of reproducing kernel Banach spaces can be used to tackle this\nchallenge. In particular, we prove a representer theorem for a wide class of\nreproducing kernel Banach spaces that admit a suitable integral representation\nand include one hidden layer neural networks of possibly infinite width.\nFurther, we show that, for a suitable class of ReLU activation functions, the\nnorm in the corresponding reproducing kernel Banach space can be characterized\nin terms of the inverse Radon transform of a bounded real measure, with norm\ngiven by the total variation norm of the measure. Our analysis simplifies and\nextends recent results in [34,29,30].",
        "categories": [
            "stat.ML",
            "cs.LG",
            "math.FA"
        ],
        "link": "http://arxiv.org/pdf/2109.09710v2",
        "date": "2021-09-20 17:32:30+00:00"
    },
    {
        "title": "Physics-Informed Deep Neural Operator Networks",
        "authors": [
            "Somdatta Goswami",
            "Aniruddha Bora",
            "Yue Yu",
            "George Em Karniadakis"
        ],
        "abstract": "Standard neural networks can approximate general nonlinear operators,\nrepresented either explicitly by a combination of mathematical operators, e.g.,\nin an advection-diffusion-reaction partial differential equation, or simply as\na black box, e.g., a system-of-systems. The first neural operator was the Deep\nOperator Network (DeepONet), proposed in 2019 based on rigorous approximation\ntheory. Since then, a few other less general operators have been published,\ne.g., based on graph neural networks or Fourier transforms. For black box\nsystems, training of neural operators is data-driven only but if the governing\nequations are known they can be incorporated into the loss function during\ntraining to develop physics-informed neural operators. Neural operators can be\nused as surrogates in design problems, uncertainty quantification, autonomous\nsystems, and almost in any application requiring real-time inference. Moreover,\nindependently pre-trained DeepONets can be used as components of a complex\nmulti-physics system by coupling them together with relatively light training.\nHere, we present a review of DeepONet, the Fourier neural operator, and the\ngraph neural operator, as well as appropriate extensions with feature\nexpansions, and highlight their usefulness in diverse applications in\ncomputational mechanics, including porous media, fluid mechanics, and solid\nmechanics.",
        "categories": [
            "cs.LG",
            "cs.NA",
            "math.NA"
        ],
        "link": "http://arxiv.org/pdf/2207.05748v2",
        "date": "2022-07-08 12:29:09+00:00"
    },
    {
        "title": "Similarity of Neural Network Representations Revisited",
        "authors": [
            "Simon Kornblith",
            "Mohammad Norouzi",
            "Honglak Lee",
            "Geoffrey Hinton"
        ],
        "abstract": "Recent work has sought to understand the behavior of neural networks by\ncomparing representations between layers and between different trained models.\nWe examine methods for comparing neural network representations based on\ncanonical correlation analysis (CCA). We show that CCA belongs to a family of\nstatistics for measuring multivariate similarity, but that neither CCA nor any\nother statistic that is invariant to invertible linear transformation can\nmeasure meaningful similarities between representations of higher dimension\nthan the number of data points. We introduce a similarity index that measures\nthe relationship between representational similarity matrices and does not\nsuffer from this limitation. This similarity index is equivalent to centered\nkernel alignment (CKA) and is also closely connected to CCA. Unlike CCA, CKA\ncan reliably identify correspondences between representations in networks\ntrained from different initializations.",
        "categories": [
            "cs.LG",
            "q-bio.NC",
            "stat.ML"
        ],
        "link": "http://arxiv.org/pdf/1905.00414v4",
        "date": "2019-05-01 17:57:26+00:00"
    },
    {
        "title": "Deep Auto-encoder with Neural Response",
        "authors": [
            "Xuming Ran",
            "Jie Zhang",
            "Ziyuan Ye",
            "Haiyan Wu",
            "Qi Xu",
            "Huihui Zhou",
            "Quanying Liu"
        ],
        "abstract": "Artificial neural network (ANN) is a versatile tool to study the neural\nrepresentation in the ventral visual stream, and the knowledge in neuroscience\nin return inspires ANN models to improve performance in the task. However, it\nis still unclear how to merge these two directions into a unified framework. In\nthis study, we propose an integrated framework called Deep Autoencoder with\nNeural Response (DAE-NR), which incorporates information from ANN and the\nvisual cortex to achieve better image reconstruction performance and higher\nneural representation similarity between biological and artificial neurons. The\nsame visual stimuli (i.e., natural images) are input to both the mice brain and\nDAE-NR. The encoder of DAE-NR jointly learns the dependencies from neural spike\nencoding and image reconstruction. For the neural spike encoding task, the\nfeatures derived from a specific hidden layer of the encoder are transformed by\na mapping function to predict the ground-truth neural response under the\nconstraint of image reconstruction. Simultaneously, for the image\nreconstruction task, the latent representation obtained by the encoder is\nassigned to a decoder to restore the original image under the guidance of\nneural information. In DAE-NR, the learning process of encoder, mapping\nfunction and decoder are all implicitly constrained by these two tasks. Our\nexperiments demonstrate that if and only if with the joint learning, DAE-NRs\ncan improve the performance of visual image reconstruction and increase the\nrepresentation similarity between biological neurons and artificial neurons.\nThe DAE-NR offers a new perspective on the integration of computer vision and\nneuroscience.",
        "categories": [
            "cs.LG"
        ],
        "link": "http://arxiv.org/pdf/2111.15309v2",
        "date": "2021-11-30 11:44:17+00:00"
    }
]